
#use("@/examples/os/kernel/pages.fr");
#use("@/lib/sys/sync/mutex.fr");

kernel :: @static(Kernel);
Kernel :: @struct {
    physical: Physical;
    // for now i always increase the virtual addresses being handed out 
    // so you can tell which address space a pointer is in for debugging. 
    next_vaddr: i64;
    tasks: Array(Task, 16);
    next_task_to_run: OnePerCore(i64);
    next_task_to_spawn: i64;
    env: rawptr;
    interrupt_stack: OnePerCore([]u8);
    idle_task: OnePerCore(Task);
    page_size: i64;
    page_index_bits: []i64;
    kernel_image: []u8;
    user_image: []u8;
    ram: []u8;
    uart: ?Pl011.Self;
    mmu_enabled: bool;
    devices: FixedList(Virt.Device, 8);
    handles: FixedList(Virt.DeviceHandle, 16);
    syscall_lock: SpinMutex;
    gic: Gic.Self;
    spin_lock_ticks: i64;
};

fn FixedList($T: Type, $capacity: i64) Type = {
    Self :: @struct(maybe_uninit: Array(T, capacity), len: i64);
    
    fn items(self: *Self) []T = 
        self.maybe_uninit&.items().slice(0, self.len);
        
    fn push(self: *Self, value: T) void = {
        self.maybe_uninit&[self.len] = value;
        self.len += 1;
    }
    
    fn len(self: *Self) i64 = 
        self.len;
    
    fn pop(self: *Self) ?T = {
        @if(self.len <= 0) return(.None);
        self.len -= 1;
        (Some = self.maybe_uninit&[self.len])
    }
    
    fn cap(self: *Self) i64 = 
        capacity;
    
    ::DeriveAsSliceIndexable(*Self, T); 
    
    Self
}

DT :: import("@/examples/os/drivers/devicetree.fr");

// this is called by emit_entry's _start
main :: fn(device_tree: i64, _sp: i64, kernel_image_base: i64) void = {
    ::enum(Task.State);
    kernel.uart = .None;
    FullHeader :: import("@/examples/os/build.fr").FullHeader;
    if device_tree == 0 || kernel_image_base == -4 {
        kpanic("qemu running as elf instead of as linux image");
    };
    h := FullHeader.ptr_from_int(kernel_image_base)[].build;
    kernel_image_size := h.header + h.user + h.kernel;
    root_task_elf_bytes := u8.ptr_from_int(kernel_image_base + h.header).slice(h.user);
    // TODO: could allow data relocations in the kernel. 
    //       just read elf sections here and do it based on kernel_image_base-link_image_base
    //       but be careful because the linux header stomps part of the elf header. 
    
    dt := DT.Header.ptr_from_int(device_tree)[];
    DT.Header.byte_swap_fields(dt&);
    if dt.magic != DT.MAGIC {
        kpanic("bad dt magic");
    };
    device_tree_bytes := u8.ptr_from_int(device_tree).slice(dt.totalsize.zext());
    
    check_system_registers();
    choose_page_size();
    ram := find_physical_memory(DT'iter(device_tree_bytes));
    kernel.ram = ram;

    end_of_kernel := (kernel_image_base + kernel_image_size).ualign_to(kernel.page_size);
    before_end_of_kernel := end_of_kernel - u8.int_from_ptr(ram.ptr);
    kernel.kernel_image = u8.ptr_from_int(kernel_image_base).slice(end_of_kernel - kernel_image_base);
    kernel.physical.remaining = (
         ptr = u8.ptr_from_int(end_of_kernel), 
         len = ram.len - before_end_of_kernel,
    );
    // offset chosen by fair die roll, just has to not overlap ram
    kernel.next_vaddr = u8.int_from_ptr(ram.end_pointer()) + 0x40000000;  
    kernel.env = get_dynamic_context();
    
    // if device tree is too close to start of ram, 
    // we'll stomp it before being able to copy to user space, 
    // so skip over it before doing anything interesting. 
    // HACK: wasteful because i don't represent non-contiguous free space well 
    space_until_dt := ptr_diff(ram.ptr, device_tree_bytes.ptr);
    if space_until_dt > 0 && space_until_dt < 1.shift_left(26) {
        waste := (space_until_dt + device_tree_bytes.len).ualign_to(kernel.page_size);
        kernel.physical.remaining = kernel.physical.remaining.rest(waste);
    };
    
    kernel.uart = Pl011'find(DT'iter(device_tree_bytes));
    pci := Pci'read_device_tree(DT'iter(device_tree_bytes));
    if pci& { pci |
        Virt'discover_all(pci);
        
        // TODO: make this work after turning on the mmu
        each kernel.handles& { it |
            if init(it) { device |
                kernel.devices&.push(device);
            };
        };
    };
    gic := Gic'find(DT'iter(device_tree_bytes));
    kernel.gic = gic;
    init_interrupt_vector();
    init(gic);
    init_timer(gic);
    if kernel.uart& { uart |
        init(uart[], gic);
    };
    tt := setup_virtual_memory(ram, pci);
    
    first_console := true;
    each kernel.devices& { it |
        if it.intid != -1 {
            gic.set_group1(it.intid);
            gic.enable(it.intid);
            if(kernel.uart, fn(jt) => if(jt.irq == it.intid, => kpanic("intid conflict")));
            @if(timer_irq == it.intid) kpanic("intid conflict");
        };
        
        // TODO: use boot args to choose which to use for stdio? 
        if it.kind.id == 3 {
            if first_console {
                // TODO: i should use the interrupt instead of spinning for printing. 
                //       but clearly my interrupt handling is broken anyway, 
                //       because if i allow it, random syscall results disappear
                //       :FUCKED
                // q[1] is TRANSMIT for printing
                it.queues[1].driver.head.flags = 1;  // don't notify when the device consumes a buffer
                //it.queues[1].device.head.flags = 1;  // don't notify when the device returns a buffer. i think it does nothing
                
                first_console = false;
                label := @if(kernel.uart&.is_none(), "stdio.console", "other.console");
                set_cstr(it.label&, label);
            };
        };
        
        if it.kind.id == 26 {
            config := it.device_config.unwrap();
            config := config&.pop_type(Virt.FsConfig);
            tag := config.tag&.get_cstr();
            append_cstr(it.label&, tag);
            append_cstr(it.label&, ".fs");
        };
        
        if it.kind.id == 16 {
            set_cstr(it.label&, "Screen.gpu");
        };
    };

    if pci& { pci |
        Xhci :: import("@/examples/os/drivers/usb/xhci.fr");
        Xhci'discover_all(pci, gic);
    };
    
    each kernel.interrupt_stack.items& { it |
        it[] = kernel.physical&.map_contiguous(8.shift_left(16), 1);
    };

    ::[]Task;
    each kernel.idle_task.items& { task |
        task.mem.root = tt;
        task.mcontext.elr = launder_to_user_pointer(spin);
        // no stack is fine because spin is an asmfunction with no callees
    };

    each kernel.tasks& { it |
        it.state = .Free;
    };
    kernel.next_task_to_spawn = 1;
    init_root_task(tt, root_task_elf_bytes, device_tree_bytes);
    
    wake_secondary_cores(DT'iter(device_tree_bytes));
    sys_set(*Task, .TPIDR_EL1, kernel.idle_task&.current());
    return_to_scheduler();
}

#where(PointerTo(fn(T) => T.get_type_info_ref().is(.Array)))
fn set_cstr(self: ~T, src: Str) bool = {
    self[0] = 0;
    append_cstr(self, src)
}

#where(PointerTo(fn(T) => T.get_type_info_ref().is(.Array)))
fn append_cstr(self: ~T, src: Str) bool = {
    dest := self.items();
    i := 0;
    while => dest[i] != 0 {
        @if(i >= dest.len) return(false);
        i += 1;
    };
    dest = dest.rest(i);
    @if(src.len + 1 > dest.len) return(false);
    dest.slice(0, src.len).copy_from(src);
    self[i + src.len] = 0;
    true
}

#where(PointerTo(fn(T) => T.get_type_info_ref().is(.Array)))
fn get_cstr(self: ~T) Str = {
    self := self.items();
    self&.pop_cstr()
}

check_system_registers :: fn() void = {
    el := i64.sys_get(.CurrentEL).shift_right_logical(2);
    if el != 1 {
        kprint_label("el ", el);
        kpanic("expected to run in el1");
    };
    if i64.sys_get(.ID_AA64ISAR0_EL1).shift_right_logical(20).bit_and(0b0010) == 0 {
        kprint("WARNING: atomic instructions are not implemented on this cpu (qemu: try -cpu cortex-a76)\n");
    };
    if i64.sys_get(.ICC_SRE_EL1).bit_and(1) == 0 {
        kprint("WARNING: ICC_SRE_EL1 (system register interface for GIC) is disabled\n");
    };
    
    // TODO: maybe better to use this to only save on interrupt if they're actually being used by that process
    cpacr := i64.sys_get(.CPACR_EL1);
    i64.sys_set(.CPACR_EL1, cpacr.bit_or(0b11.shift_left(20)));  // FPEN
    
    // EL0PCTEN: allow userspace to read CNTVCT_EL0 and CNTFRQ_EL0
    cntctl := i64.sys_get(.CNTKCTL_EL1);
    i64.sys_set(.CNTKCTL_EL1, cntctl.bit_or(2));
};

find_physical_memory :: fn(self: DT.Iter) []u8 = {
    self&.consume_root();
    self&.find_node(.BEGIN_NODE, fn(it) => it.name.starts_with("memory"))
        || kpanic("no memory node");
    it := self&.find_node(.PROP, fn(it) => it.name == "reg")
        || kpanic("devicetree: no memory.reg");
    (ptr = u8.ptr_from_int(it.read_2x32(0)), len = it.read_2x32(2))
}

Virt :: import("@/examples/os/drivers/virtio.fr");
Pci :: import("@/examples/os/drivers/pci.fr");
VirtConsole :: import("@/examples/os/drivers/console/virtio.fr");
VirtFuse :: import("@/examples/os/drivers/fs/virtio.fr");

alloc_task :: fn(tt: *TranslationTable) *Task = {
    each kernel.tasks& { it |
        if Task.State.cas(it.state&, .Free, .Suspended) == .Free {
            it.slice(1).set_zeroed();
            it.state = .Suspended;
            it.thread_id = i64.atomic_inc(kernel.next_task_to_spawn&);
            it.mem.root = tt;
            return it;
        };
    };
    kpanic("too many tasks running")
}

init_root_task :: fn(tt: *TranslationTable, elf_bytes: []u8, device_tree_bytes: []u8) void = {
    task := alloc_task(tt);
    gpr := task.mcontext.gpr&;
    gpr[0] = launder_to_user_pointer(u8.raw_from_ptr(elf_bytes.ptr));
    gpr[1] = elf_bytes.len;
    gpr[2] = {
        size := device_tree_bytes.len.align_to(kernel.page_size);
        mem := vmap_reserve(task.mem&, size / kernel.page_size);
        sys_set(*Task, .TPIDR_EL1, task);
        mem := u8.check_user_write(mem, size);
        mem.slice(0, device_tree_bytes.len).copy_from(device_tree_bytes);
        u8.int_from_ptr(mem.ptr)
    };
    gpr[31] = {
        stack_size := 1.shift_left(franca_required_stack_bits);
        s := vmap_reserve(task.mem&, stack_size / kernel.page_size);
        // stack grows left, so start at the right of the allocation. 
        s += stack_size;
        s
    };
    task.mcontext.elr = launder_to_user_pointer(user_exec);
    task.state = .Runnable;
}

fn kill_task(task: *Task) void = {
    task.state = .Uninit;
    // TODO: deallocate its pages and stuff
    //       (but for now everything is thread-like not process-like)
    if task.thread_id == 1 {
        stop();
    };
    @if(task.thread_id == 0) kpanic("tried to kill idle_task");
    @if(task.exit_futex[] != task.thread_id.trunc()) kprint("WARN: corrupt exit_futex");
    task.exit_futex[] = 0;
    futex_wake(u32.int_from_ptr(task.exit_futex), MAX_i32);
    task.state = .Free;
    sys_set(*Task, .TPIDR_EL1, kernel.idle_task&.current());
}

get_current_task :: fn() *Task = 
    sys_get(*Task, .TPIDR_EL1);

fn futex_wake(addr: i64, val: i64) i64 = {
    // TODO: unfair and :slow, maybe i want a queue per futex
    n := 0;
    i := 0;
    cap := kernel.tasks&.len();
    while => i < cap && n < val {
        it := kernel.tasks&[i]&;
        n += int(i64.cas(it.futex_wait&, addr, 0) == addr);
        i += 1;
    };
    n
}

init_interrupt_vector :: fn() void #use(Arm) = {
    InterruptVector :: Array(Array(u32, 32), 16);
    interrupt_handlers := aligned_static(InterruptVector, 2048);
    // TODO: have a way to ask for the global to have the right alignment
    // TODO: have a more official way to ask for it to be in executable memory
    
    // this assumes we're starting in el1 and don't have to set any interrupt masks, etc.
    
    // TODO: idk if i have to enable the caches somehow but i figure this can't hurt
    cc :: import("@/lib/sys/process.fr").aarch64_clear_instruction_cache;
    clear_cache :: AsmFunctionArmOnly(fn(beg: rawptr, end: rawptr) void = (), cc);
    
    // this is a pain because AsmFunction doesn't have a way to do relocations.
    // but since this runs before memory protection is set up, its fine to patch the jump offsets. 
    
    // each slot of the InterruptVector is limited to 32 instructions, 
    // just branch to a seperate function where the size doesn't matter. 
    int_callee := @as(rawptr) interrupt_trampoline;
    each interrupt_handlers { it |
        // TODO: tell it which one this is somehow
        src := u32.raw_from_ptr(it.index(0));
        it[0] = b(ptr_diff(src, int_callee) / 4, 0);
        clear_cache(src, src.offset(128));
    };
    
    // :patch interrupt_trampoline calls syscall_func
    i := @run interrupt_trampoline_code.index_of(arm_nop).unwrap();
    i += 1;  // TODO: really shouldn't force a bti because this super error prone
    src := u32.ptr_from_raw(int_callee).offset(i);
    dest := u32.ptr_from_raw(@as(rawptr) syscall_func);
    // with_link=0, the handler is expected to call return_to_user
    src[] = b(ptr_diff(src, dest), 0);
    clear_cache(int_callee, int_callee.offset(128));
    
    sys_set(*InterruptVector, .VBAR_EL1, interrupt_handlers);
    @run assert_eq(size_of(InterruptVector), 0x800);
}

// EL1 Virtual Timer. INTID defined by the Server Base System Architecture 
// device tree: timer.interrupts[4] + 16 (Private Peripheral Interrupts)
// TODO: how do yo we know it's the last one in devicetree.timer.interrupts?
timer_irq :: 16 + 0xb;
init_timer :: fn(gic: Gic.Self) void = {
    // any priority/ICFGR is fine.
    gic.set_group1(timer_irq);
    gic.enable(timer_irq);
    reset_countdown();  // BEFORE
    i64.sys_set(.CNTV_CTL_EL0, 1);  // turn it on
}

Gic :: import("@/examples/os/drivers/gic3.fr");
Pl011 :: import("@/examples/os/drivers/console/pl011.fr");

reset_countdown :: fn() void = {
    frequency := i64.sys_get(.CNTFRQ_EL0);
    i64.sys_set(.CNTV_TVAL_EL0, frequency / 200);
}

Arm :: import("@/backend/arm64/bits.fr");
interrupt_trampoline :: AsmFunctionArmOnly(fn() void = (), interrupt_trampoline_code);
interrupt_trampoline_code :: {
    #use(Arm);
    out := u32.list(64, ast_alloc());
    emit :: fn(inst) => out&.push(inst);
    
    // return_to_user_impl left sp_el1 pointing at the Task
    // TODO: is it a problem if it's not 16 aligned?
    
    @debug_assert_eq(SavedRegisters.offset_of(.gpr), 0);
    range(0, 16) { r |
        a, b := (r*2, r*2+1);
        off := r * 2;
        emit(stp_so(.X64, a, b, sp, @as(i7) off));
    };
    emit(add_im(.X64, x0, sp, 0, 0));  
    
    // since syscall_func never returns, it never unwinds its stack frame, so manually reset sp at the beginning. 
    // (extra needed now that sp_el1 stores the task while in userspace)
    off := Task.offset_of(.kernel_sp) / 8;
    @debug_assert_lt(off, 1.shift_left(12));
    emit(ldr_uo(.X64, x1, x0, off));
    emit(add_im(.X64, sp, x1, 0, 0));
    
    @assert_lt(SavedRegisters.offset_of(.fpr) / 16 + 32, 1.shift_left(6));
    range(0, 16) { r |
        a, b := (r*2, r*2+1);
        off := r * 2 + SavedRegisters.offset_of(.fpr) / 16;
        emit(f_stp_so(FType.Q128, a, b, x0, @as(i7) off));
    };
    
    // save special registers
    emit(mrs(x1, .ELR_EL1)); 
    emit(mrs(x2, .SPSR_EL1)); 
    emit(mrs(x3, .SP_EL0)); 
    off := SavedRegisters.offset_of(.elr);
    @debug_assert_eq(off, SavedRegisters.offset_of(.spsr) - 8);
    off /= 8;
    emit(stp_so(.X64, x1, x2, x0, @as(i7) off));
    @debug_assert_eq(SavedRegisters.offset_of(.gpr), 0);
    emit(str_uo(.X64, x3, x0, @as(i7) sp));
    
    // call syscall_func(task = x0)
    emit(arm_nop);  // :patch
    @debug_assert_eq(out.items().index_of(arm_nop).unwrap(), out.len - 1, "patch must be first nop");
    out.items()
};

return_to_current_user :: fn() Never = {
    task := get_current_task();
    @if(task.state != .Suspended) kpanic("expected to be suspended");
    task.state = .Running;
    return_to_user(task);
}

return_to_user :: fn(new_task: *Task) Never = {
    @if(new_task.state != .Running) kpanic("caller should set state");
    new_task.kernel_sp = kernel.interrupt_stack&.current()[].end_pointer();
    kernel.syscall_lock&.maybe_unlock();
    return_to_user_impl(new_task)
}

// read SavedRegisters
return_to_user_impl :: AsmFunctionArmOnly(fn(task_x0: *Task) Never = (), {
    #use(Arm);
    out := u32.list(64, ast_alloc());
    emit :: fn(inst) => out&.push(inst);
    
    // leave SP_EL1 pointing to a unique place per core so when interrupt_trampoline_code
    // needs to save a register it doesn't stomp anything.
    // doesn't need to point to a real stack because its reset below
    emit(add_im(.X64, sp, x0, 0, 0)); 
    emit(msr(.TPIDR_EL1, x0)); 
    
    // restore special registers
    off := SavedRegisters.offset_of(.elr);
    @debug_assert_eq(off, SavedRegisters.offset_of(.spsr) - 8);
    off /= 8;
    emit(ldp_so(.X64, x1, x2, x0, @as(i7) off));
    @debug_assert_eq(SavedRegisters.offset_of(.gpr), 0);
    emit(ldr_uo(.X64, x3, x0, @as(i7) sp));
    emit(msr(.ELR_EL1, x1)); 
    emit(msr(.SPSR_EL1, x2)); 
    emit(msr(.SP_EL0, x3)); 
    // TODO: set TTBR0_EL1
    // (because each task should have its own address space)
    
    range(0, 16) { r |
        a, b := (r*2, r*2+1);
        off := r * 2 + SavedRegisters.offset_of(.fpr) / 16;
        emit(f_ldp_so(FType.Q128, a, b, x0, @as(i7) off));
    };
    
    // restore general registers. 
    // rev so x0 is last to be restored so it can be used as indexer
    // note: x31 here is xzr not sp.
    range_rev(0, 16) { r |
        a, b := (r*2, r*2+1);
        off := r * 2;
        emit(ldp_so(.X64, a, b, x0, @as(i7) off));
    };
    
    // jump to ELR_EL1 = task.mcontext.elr
    emit(eret);  
    
    out.items()
});

// TODO: instead of using the timer to wake and poll for runnable tasks, 
//       have other core send an interrupt when making a new task runnable?
return_to_scheduler :: fn() Never = {
    get_current_task()[].state = .Runnable;
    
    task := choose_task_to_run();
    // if the guy before you used most of their time slice and then did a syscall that yielded,
    // the next guy should still get a full time slice, 
    // otherwise you could starve people by controlling the order processes get spawned. 
    reset_countdown();
    return_to_user(task)
}

choose_task_to_run :: fn() *Task = {
    cap := kernel.tasks&.len();
    i := kernel.next_task_to_run&.current();
    now := i64.sys_get(.CNTVCT_EL0);
    range(0, cap) { _ |
        task := kernel.tasks&[i[]]&;
        i[] = i[].add(1).mod(cap);
        if task.futex_wait == 0 && task.sleep_until.ule(now) && Task.State.cas(task.state&, .Runnable, .Running) == .Runnable {
            return task;
        };
    };
    task := kernel.idle_task&.current();
    task.state = .Running;
    task
}

stop :: fn() Never = {
    kprint_label("spin_lock_ticks ", kernel.spin_lock_ticks);
    kprint("goodbye!\n");
    PSCI'system_off()
}

aligned_static :: fn($T: Type, $need_align: i64) *T #generic #inline = {
    need_size :: size_of(T);
    it := @static(Array(u8, need_size + need_align));
    tt := int_from_ptr(@type it[], it);
    start := tt; 
    end := start + size_of(@type it[]);
    tt := tt.ualign_to(need_align);
    if tt + need_size > end {
        kprint("aligned_static: fucked");
    };
    T.ptr_from_int(tt)
}

// all the barriers for good luck
barrier :: @AsmFunctionArmOnly(fn() void = ()) => (
    @bits(0b11010101000000110011, 0b1111, 0b101, 0b11111),  // DMB SY
    @bits(0b11010101000000110011, 0b1111, 0b100, 0b11111),  // DSB SY
    @bits(0b11010101000000110011, 0b1111, 0b110, 0b11111),  // ISB SY
    ret(),
);

user_exec :: fn(elf_bytes: []u8, devicetree: *u8) void = {
    _, c := create_and_set_unsafe_environment();
    c.panic_hook = print_and_exit1;
    c.prefer_syscalls = true;
    c.current_os = 1;
    c.temporary_allocator_i = Arena'stack_alloc(1.shift_left(16));
    c.general_allocator = c.temporary_allocator_i;
    // TODO: its sad that i have to commit new pages for the bytes copied 
    //       for Load commands but not accessed by the running program.
    import("@/examples/elf_loader.fr")'load_elf_file(elf_bytes, (
        args = @slice(@as(CStr) (ptr = devicetree)),
        env = empty(),
    ));
}

Task :: @struct {
    // mcontext must be the first field because the interrupt handler starts saving at TPIDR_EL1
    mcontext: SavedRegisters;  
    mem: AddressSpace;
    kernel_sp: *u8;
    t: ThreadConfig #use;
    signal_stack: i64;
    thread_id: i64;
    futex_wait: i64;  // *u32
    sleep_until: i64;
    state: State;
    State :: @enum(u32) (Uninit, Runnable, Running, Suspended, Free);
};

SavedRegisters :: MContext;
// for now im using the same handler for every slot of the InterruptVector
syscall_func :: fn(task: *Task) void = {
    return :: ();  // must do it via return_to_user
    
    prev := Task.State.cas(task.state&, .Running, .Suspended);
    set_dynamic_context(kernel.env);
    @if(prev != .Running) kpanic("interrupted but not running");
    on_this_core := identical(task.kernel_sp, kernel.interrupt_stack&.current()[].end_pointer());
    @if(!on_this_core) kpanic("running on other core");
    args := task.mcontext&;
    
    // these being in SavedRegisters isn't important because i don't restore them 
    // but it seems convient to have one context thing you can pass around.
    // TODO: if i keep these do it in the assembly part instead of as a bunch of calls here. 
    args.esr = i64.sys_get(.ESR_EL1);
    args.far = i64.sys_get(.FAR_EL1);
    intid := i64.sys_get(.ICC_IAR1_EL1).bit_and(0xfff);
    intid0 := i64.sys_get(.ICC_IAR0_EL1).bit_and(0xfff);
    if intid0 != Gic.null_interrupt_id {
        kpanic("intid0");
    };
    
    if intid == timer_irq {
        reset_countdown();  // BEFORE
        i64.sys_set(.ICC_EOIR1_EL1, intid);
        return_to_scheduler();
    };
    
    kernel.syscall_lock&.lock();
    ::if_opt(Pl011.Self, i64);
    irq_uart: i64 = if(kernel.uart, fn(it) => it.irq, => -1);
    if intid == irq_uart {
        uart := kernel.uart.unwrap();
        uart := uart.base;
        //                     TMIS           RXMIS
        if u32.volatile(uart + 0x040).bit_and(0b10000) == 0 {
            kpanic("got uart interrupt but not ready to receive");
        };
        // the read clears the bit  ^
        byte: u8 = u32.volatile(uart).trunc();
        
        // always the init task! not whatever task was interrupted. 
        init := kernel.tasks&[0]&;
        state := init.state;
        state2 := Task.State.cas(init.state&, state, .Suspended);
        @if(state != state2 || !@is(state, .Suspended, .Runnable)) kpanic("TODO: input while init running on other core");
        @if(init.signal_stack_buf.len == 0) kpanic("got input before init signal handler");
        push_signal_context(init);
        init.mcontext.gpr&[1] = intid;
        init.mcontext.gpr&[2] = byte.zext();  // HACK
        init.state = .Running;
        // run the signal handler BEFORE acknowledging the signal by setting ICC_EOIR1_EL1 (its done in sig_return). 
        // if interrupts are being spammed (ie you pasted some text), this lets them be processed 
        // in queue order instead of stack order.  
        // also since sig_return sets ICC_EOIR1_EL1, it would be wrong to do so here anyway because
        // then you'd do it twice and could miss an interrupt. 
        return_to_user(init);
    };
    
    exception_class := args.esr.shift_right_logical(26).bit_and(0b111111);
    
    if intid == Gic.null_interrupt_id {    
        if exception_class == 0b100100 {
            vaddr := args.far.align_back(kernel.page_size);
            
            // TODO: don't create a L2 table for it if its just a junk pointer 
            pte := index_page_table(task.mem&, vaddr);
            if commit_if_reserved(task.mem&, vaddr, pte) {
                return_to_current_user();
            };

            // problem is if the above late commit happens on one core and then another core 
            // tries to access the memory, the tlb might have cached that that entry was invalid. 
            // TODO: idk if there's a cleaner way to make clear_tlb propagate to all the cores
            is_translation_fault := args.esr.bit_and(0b111100) == 0b000100;  // ie. not a permission fault
            if is_translation_fault && pte.map.get(.tag) == 0b11 {
                clear_tlb(vaddr);
                return_to_current_user();
            };
            
            kprint("data abort from el0\n");
            kprint_label("fault address: ", args.far);
            kprint_label("ip: ", args.elr);
            inst := u32.ptr_from_int(args.elr)[];
            kprint_label("instruction: ", inst.zext());
            kprint_label("spsr: ", args.spsr);
            kprint_label("esr: ", args.esr);
            send_signal(task);  // give userspace a chance to print a stack trace
        };
        
        if exception_class == 0b100101 {
            // TODO: its fine for user to pass pointer to memory they haven't touched yet (implicitly zeroed)
            //       to a syscall, so should do the auto commit here as well.  
            kprint_label("fault address: ", args.far);
            kprint_label("ip: ", args.elr);
            kpanic("data abort from el1");
        };
        
        if exception_class == 0b011000 {
            inst := u32.ptr_from_int(args.elr)[];
            kprint("trapped system instruction\n");
            kprint_label("instruction: ", inst.zext());
            send_signal(task);
        };
        
        if exception_class == 0b111100 {
            inst := u32.ptr_from_int(args.elr)[];
            context := inst.shift_right_logical(5).bit_and(0xFFFF);
            kprint_label("executed brk: ", context.zext());
            kprint_label("ip: ", args.elr);
            send_signal(task);
        };
    
        is_svc := exception_class == 0b010101;
        if is_svc {
            syscall_number := args.gpr&[8];
            if syscall_number != args.gpr&[16] {
                tpanic("perform_syscall always uses both x8 and x16");
            };
            // note: not iterating over the array at runtime so it doesn't need data relocations so the kernel is PIC
            calls :: @run import("@/examples/os/kernel/syscalls.fr")'generate();
            inline_for calls { $it |
                callee, n := @run it[];
                if syscall_number == n {
                    args.gpr&[0] = callee(args.gpr&[0], args.gpr&[1], args.gpr&[2], args.gpr&[3], args.gpr&[4], args.gpr&[5]);
                    return_to_current_user();
                }
            };

            // unknown syscall
            send_signal(task);
        };
        
        kprint_label("FAR_EL1: ", args.far);
        kprint_label("ESR_EL1: ", args.esr);
        kprint_label("SPSR_EL1: ", args.spsr);
        kprint_label("ELR_EL1: ", args.elr);
            kprint_label("exception class: ", exception_class);
            if exception_class == 0 {
                inst := u32.ptr_from_int(args.elr)[];
                kprint_label("instruction: ", inst.zext());
            };
            
        send_signal(task);
    };
    
    handled := false;
    each kernel.devices& { console |
        if intid == console.intid {
            console.check_interrupt(intid);
            handled = true;
        };
    };
    
    if !handled { 
        kprint_label("intid: ", intid);
        kpanic("unknown interrupt");
    };
    
    i64.sys_set(.ICC_EOIR1_EL1, intid);
    if task.thread_id == 0 {
        return_to_scheduler();  
    };
    return_to_current_user();
};

show_registers :: fn(task: *Task) void = {
    args := task.mcontext&;
    out := u8.list(2048, Arena'stack_alloc(2048));
    @fmt(out&, "=== task: %, core: % ===\n", task.thread_id, current_core_index());
    range(0, 32) { i |
        @fmt(out&, "%: %, ", i, args.gpr&[i]);
        if i.add(1).mod(8) == 0 {
            @fmt(out&, "\n");
        };
    };
    @fmt(out&, "spsr: %, ", args.spsr);
    @fmt(out&, "elr: %, ", args.elr);
    @fmt(out&, "inst: %, ", u32.ptr_from_int(args.elr)[].zext());
    @fmt(out&, "esr: %, ", args.esr);
    @fmt(out&, "far: %, ", args.far);
    @fmt(out&, "\n");
    kprint(out.items());
} 

push_signal_context :: fn(task: *Task) void = {
    @if(task.state != .Suspended) kpanic("push_signal_context to running task");
    task.futex_wait = 0;
    task.sleep_until = 0;
    if task.signal_stack_buf.len == 0 {
        tpanic(task, "no signal handler")
    };
        ctx := task.mcontext&;
        signal_stack_left, signal_stack_right := task.t&.sigstack();
        if ctx.gpr&[31] > signal_stack_left && ctx.gpr&[31] < task.signal_stack {
            // when sending a nested signal, update for whatever stack frames have been pushed in user space. 
            task.signal_stack = ctx.gpr&[31];
        };
        
        task.signal_stack -= size_of(SavedRegisters);
        saved := SavedRegisters.ptr_from_int(task.signal_stack);
        saved[] = ctx[];
        ctx.elr = bit_cast_unchecked(SignalHandler.F, i64, task.signal_handler);
        ctx.gpr&[0] = task.signal_stack;
        ctx.gpr&[1] = Gic.null_interrupt_id;
        ctx.gpr&[2] = 0;
        ctx.gpr&[31] = task.signal_stack;
        ctx.gpr&[30] = launder_to_user_pointer(user_unwind_signal);
        // HACK
        // give some buffer in case it gets another signal while pushing the frame,
        // as long as this is more than handle_signal's stack frame, it's fine. 
        // i don't have a test that requires this if i delay ICC_EOIR1_EL1 until sig_return,
        // but i still think you need to do it, its just a very tight race window. 
        task.signal_stack -= 2048;  
        
        if task.signal_stack < signal_stack_left {
            tpanic(task, "too many nested signals");
        };
}

fn sigstack(t: *ThreadConfig) Ty(i64, i64) = 
    (u8.int_from_ptr(t.signal_stack_buf.ptr), u8.int_from_ptr(t.signal_stack_buf.end_pointer()));
    
SyscallFunc :: @FnPtr(x0: i64, x1: i64, x2: i64, x3: i64, x4: i64, x5: i64) i64;

send_signal :: fn(task: *Task) Never = {
    if task.thread_id == 0 {
        @if(kernel.tasks&.items().contains_address(task)) kpanic("non-idle_task has id 0");
        kpanic("send_signal to idle task");
    };
    push_signal_context(task);
    return_to_scheduler()
}

launder_to_user_pointer :: fn(a: rawptr) i64 #noinline = {
    i := int_from_rawptr(a) - u8.int_from_ptr(kernel.kernel_image.ptr);
    @if(i >= kernel.kernel_image.len) kpanic("bad launder");
    u8.int_from_ptr(kernel.user_image.ptr) + i
}

user_unwind_signal :: @AsmFunctionArmOnly(fn(intid_x0: i64, ctx_x1: *MContext) Never = ()) => (
    add_im(.X64, x2, sp, 0, 0),
    movz(.X64, x16, 0xFFFF, Hw.Left16),
    movk(.X64, x16, 0x0004, Hw.Left0),
    mov(.X64, x8, x16),
    svc,  // sig_return
    // :end_user_unwind_signal
);

user_virtq_poll :: fn(it: *UQueue, _: i64) void = {
    args: Array(i64, 6) = (UQueue.int_from_ptr(it), 0, 0, 0, 0, 0);
    _ := Syscall'perform_syscall(args&, 0xFFFF0007);
}

Crash :: import("@/lib/crash_report.fr");

PSCI :: @struct {
    system_off :: fn() Never = {
        // not listed in the devicetree's psci section which is strange (cpu_off is there but doesn't do what i want). 
        // Arm Power State Coordination Interface. Version 1.3 issue F.b
        SYSTEM_OFF :: 0x84000008;
        f(SYSTEM_OFF);
        f :: @AsmFunctionArmOnly(fn(n: i64) Never = ())
            => (hvc, ret());
    }
    
    cpu_on :: fn(cpu: i64, callee: @FnPtr(ctx: *u8) void, ctx: *u8) void = {
        CPU_ON :: 0xC4000003;  // TODO: get this from device tree? it has a specific value in the pdf tho. 
        f(CPU_ON, cpu, callee, ctx);
        f :: @AsmFunctionArmOnly(fn(n: i64, cpu: i64, callee: @FnPtr(ctx: *u8) void, ctx: *u8) void = ()) 
            => (hvc, ret());
    }
};

wake_secondary_cores :: fn(dt: DT.Iter) void = {
    barrier();
    dt&.consume_root();
    dt&.find_node(.BEGIN_NODE, fn(it) => it.name == "cpus") || {
        kprint("missing devicetree.cpus");
        return()
    };
    
    // TODO: check #address-cells = 1
    loop {
        dt&.find_node(.BEGIN_NODE, fn(it) => it.name.starts_with("cpu@")) 
            || return();
        // TODO: check enable-method = "psci";
        node := dt&.find_node(.PROP, fn(it) => it.name == "reg")
            .expect("missing devicetree.cpus.cpu@#.reg");
        mpidr: i64 = node.read_32(0).zext();
        if mpidr != 0 {
            stack := kernel.physical&.map_contiguous(1.shift_left(16), 1);
            @assert(size_of(DefaultContext) < stack.len);
            p := rawptr.reinterpret_bytes(stack);
            p[p.len - 2] = cpu_on_main;
            sp := stack.end_pointer().offset(-16);
            PSCI'cpu_on(mpidr, cpu_on_trampoline, sp);
        };
        dt&.skip_current_object();
    };
}

// can't pass cpu_on_main to cpu_on directly because need to set sp first. 
cpu_on_trampoline :: @AsmFunctionArmOnly(fn(new_sp: *u8) void = ()) => (
    0xd50041bf, // msr SPSel, #0x1; probably already the case but just make sure. 
    add_im(.X64, sp, x0, 0, 0),
    ldr_uo(.X64, x0, x0, 0),
    br(x0, 0),
    b(0, 0),
);

cpu_on_main :: fn() void = {
    i := current_core_index();
    
    check_system_registers();
    init_interrupt_vector();
    gic := kernel.gic;
    init(gic);
    init_timer(gic);
    task := kernel.idle_task&.current();
    set_mmu_reg(task.mem.root.entries&.items());
    
    // needs a value because return_to_user writes to task.state
    sys_set(*Task, .TPIDR_EL1, task);  
    return_to_scheduler();
    unreachable();
}

current_core_index :: fn() i64 = 
    i64.sys_get(.MPIDR_EL1).bit_and(0xFF);

fn tpanic(msg: Str) Never = 
    tpanic(get_current_task(), msg);

fn tpanic(task: *Task, msg: Str) Never = {
    kprint("killed ");
    kscary_log(task.thread_id);
    kprint(": ");
    kprint(msg);
    kprint("\n");
    kill_task(task);
    return_to_scheduler()
}

kpanic :: fn(msg: Str) Never = {
    kprint("we're so fucked\n===\n");
    kprint(msg);
    kprint("\n===\ni repeat, we're so fucked\n");
    PSCI'system_off()
};

kprint_label :: fn(msg: Str, value: i64) void = {
    kprint(msg);
    kscary_log(value);
    kprint("\n");
}

spin :: (@AsmFunctionArmOnly(fn() Never = ()) => (
    wfi,
    b(-1, 0),
));

kprint :: fn(s: Str) void = {
    if kernel.uart& { it |
        for(s, fn(c) => u32.volatile(it.base, c.zext()));
        return();
    };
    each kernel.devices& { it |
        if it.label&.get_cstr().ends_with(".console") {
            it := bit_cast_unchecked(*Virt.Device, *Virt.Device, it);
            it.console_write(s);
            return();
        };
    };

    // for early debugging before reading the device tree, just yolo hope its a pl011 at hardcoded qemu address, 
    for(s, fn(c) => u32.volatile(0x09000000, c.zext()));
}

// TODO: the volatile helpers should go elsewhere

// TODO: can't let it try to bake the junk address or it crashes in bake_relocatable_constant :compilerbug
fn volatile($T: Type, addr: i64, value: T) void #noinline #generic = {
    bit_cast_unchecked(i64, *T, addr)[] = value;
}

fn volatile($T: Type, addr: i64) T #noinline #generic = {
    bit_cast_unchecked(i64, *T, addr)[]
}

fn volatile(p: ~T) Deref(T) #where(fn(T) => get_type_info_ref(T).is(.Ptr)) #noinline = {
    p[]
}

fn volatile(p: ~T, value: Deref(T)) void #where(fn(T) => get_type_info_ref(T).is(.Ptr)) #noinline = {
    p[] = value;
}

fn vol_2x32(p: *u32) i64 =
    p.vol().zext().bit_or({ p := p.offset(1); p.vol() }.zext().shift_left(32));

fn vol_2x32(p: *u32, val: i64) void = {
    p.vol(val.trunc());
    p := p.offset(1);
    p.vol(val.shift_right_logical(32).trunc());
}

vol :: volatile;

fn kscary_log(i: i64) void = {
    buf := @uninitialized Array(u8, 24);
    buf := buf&.items();
    len := write_int_to_buffer(buf, i);
    kprint(buf.slice(0, len));
}

// TODO: this is a neat demo but it would be more simple to just make the syntax such that it doesn't resolve
//       eagarly which might just mean using `fn() = {}` instead of `fn() => ()` but then its annoying to get 
//       the tuple out of the block... idk this whole operation is kinda dumb anyway. 
// this is convoluted to trick it into resolving the expression in the context of #use(arm/bits)
fn AsmFunctionArmOnly(signeture: FatExpr, code: FatExpr) FatExpr #macro = {
    signeture := FuncId.const_eval(signeture);
    code_fid := FuncId.const_eval(code);
    code := get_function_ast(code_fid, false, false, false, false);
    ::tagged(@type code.body);
    @debug_assert(code.body&.is(.Normal));

    code.body.Normal = @{ @const_slice(@[code.body.Normal]) };
    unresolve_scopes(code.body.Normal&);
    
    code.resolve_scope = FuncId.scope_of(fn() = {
        // the body of `code` will be able to see any constants declared here.
        #use(Arm);
    });
    // TODO: makes sense that you need this,
    //       but you shouldn't panic without it, should be an error instead of 
    //       `panic! Unreachable unless you set the load factor to 100%`
    code.unset_flag(.ResolvedBody);
    
    code := Slice(u32).const_eval(@{ @[@literal code_fid]() });
    @literal AsmFunctionArmOnly(signeture, code)
}

// time to do some sketchy shit do da do da
fn unresolve_scopes(expr: *FatExpr) void = {
    #use("@/compiler/walk_ast.fr");
    #use("@/compiler/ast_external.fr");
    UnResolve :: @struct();
    ::WalkAst(UnResolve, void);
    
    // TODO: `@ref(@as(UnResolve) ())` ambigous overload for FatExpr -> FatExpr. :compilerbug?
    unresolve: UnResolve = ();  
    walk_expr(unresolve&, expr);

    fn handle_expr(self: *UnResolve, expr: *FatExpr) Result(DoMore, void) = {
        @match(expr.expr&) {
            fn GetVar(it) => {
                expr.expr = (GetNamed = it.name);
            }
            fn UndeclaredVar(it) => {
                expr.expr = (GetNamed = it.name);
            }
            fn Block(it) => {
                it.flags = it.flags.bit_and(bit_not(1.shift_left(@as(i64) BlockFlags.Resolved)));
                it.scope = NOSCOPE;
            }
            @default => ();
        };
        (Ok = .Continue)
    }
    fn handle_stmt(self: *UnResolve, stmt: *FatStmt) Result(DoMore, void) = (Ok = .Continue);
    fn handle_type(self: *UnResolve, type: *LazyType) Result(DoMore, void) = (Ok = .Continue);
    fn handle_func(self: *UnResolve, func: *Func) Result(DoMore, void) = (Ok = .Continue);
    fn handle_pattern(self: *UnResolve, pattern: *Pattern) Result(DoMore, void) = (Ok = .Continue);
}

#use("@/examples/os/libkernel.fr");
#use("@/lib/sys/process.fr");  // TODO: maybe sys_get/sys_set should go somewhere else

SpinMutex :: @struct(thread: i64);

fn lock(self: *SpinMutex) void = {
    my_thread := current_core_index() + 1;
    @if(self.thread == my_thread) kpanic("relocked mutex");
    while => i64.cas(self.thread&, 0, my_thread) != 0 {
        i64.atomic_inc(kernel.spin_lock_ticks&);
        // spin
    };
    barrier();
}

fn maybe_unlock(self: *SpinMutex) void = {
    barrier();
    my_thread := current_core_index() + 1;
    _ := i64.cas(self.thread&, my_thread, 0);
}

fn OnePerCore($T: Type) Type = {
    Self :: @struct(items: Array(T, 8));
    
    fn current(self: *Self) *T = 
        self.items&.index(current_core_index());
    
    Self
}
