
main :: fn() void = {
    sane_main();
}

sane_main :: fn() void = {
    _, c := create_and_set_unsafe_environment();
    c.panic_hook = kpanic;
    kprint("These are the first words I have to say.\nThat's probably why they took so long to write.\n");
    
    print_el();
    setup_interrupts();
    setup_virtual_memory();
    kprint("done setup\n");
    
    xxx := (@as(rawptr) user_func);
    do_eret(789, xxx, 0, 512*1024*1024);
    kpanic("eret returned");
}

print_el :: fn() void = {
    kprint("Current Exception Level: ");
    el := i64.sys_get(.CurrentEL);
    c := "0".char() + el.shift_right_logical(2);
    putchar(c.trunc());
    kprint("\n");
}

setup_interrupts :: fn() void = {
    interrupt_base := align_the_thing(interrupt_handlers, 2048, size_of(InterruptVector));
    interrupt_handlers := InterruptVector.ptr_from_raw(interrupt_base);
    // TODO: have a way to ask for the global to have the right alignment
    // TODO: have a more official way to ask for it to be in executable memory

    // this assumes we're starting in el1 and don't have to set any interrupt masks, etc.
    #use(Arm);
    
    // TODO: idk if i have to enable the caches somehow but i figure this can't hurt
    cc :: import("@/lib/sys/process.fr").aarch64_clear_instruction_cache;
    clear_cache :: AsmFunctionArmOnly(fn(beg: rawptr, end: rawptr) void = (), cc);
    
    // this is a pain because AsmFunction doesn't have a way to do relocations.
    // but since this runs before memory protection is set up, its fine to patch the jump offsets. 
    
    // each slot of the InterruptVector is limited to 32 instructions, 
    // just branch to a seperate function where the size doesn't matter. 
    int_callee := @as(rawptr) interrupt_trampoline;
    each interrupt_handlers { it |
        // TODO: tell it which one this is somehow
        src := u32.raw_from_ptr(it.index(0));
        it[0] = b(ptr_diff(src, int_callee) / 4, 0);
        clear_cache(src, src.offset(128));
    };
    
    // :patch interrupt_trampoline calls syscall_func
    i := @run interrupt_trampoline_code.index_of(arm_nop).unwrap();
    i += 1;  // TODO: really shouldn't force a bti because this super error prone
    src := u32.ptr_from_raw(int_callee).offset(i);
    dest := u32.ptr_from_raw(@as(rawptr) syscall_func);
    src[] = b(ptr_diff(src, dest), 1);
    clear_cache(int_callee, int_callee.offset(128));
    
    sys_set(*InterruptVector, .VBAR_EL1, interrupt_handlers);
    @run assert_eq(size_of(InterruptVector), 0x800);
    
    // magic to make the timer actually work:    
    // these are memory mapped but have names you can look up as registers in the pdf 
    // TODO: make struct for these
    // TODO: make it work with ,gic-version=3 
    u32.volatile(gicd, 1);  // GICD_CTLR: enable distributor
    u32.volatile(gicc, 1);  // GICC_CTLR: enable cpu interface controller
    u32.volatile(gicc + 4, 255);  // GICC_PMR: priority mask register (255 is low, 0 is high)
    // now for the specific interrupt
    set_field(timer_irq, gicd + 0x100, 1, 1); // GICD_ISENABLER enable
    
    // start timer
    frequency := i64.sys_get(.CNTFRQ_EL0);
    i64.sys_set(.CNTP_TVAL_EL0, frequency);  // count down from the frequency, so wait 1 second
    i64.sys_set(.CNTP_CTL_EL0, 1);  // turn it on
    kprint_label("frequency: ", frequency);
};

set_field :: fn(interrupt_id: i64, addr: i64, bit_width: i64, value: u32) void #inline = {
    mask: u32 = 1.shift_left(bit_width) - 1;
    value := value.bit_and(mask);
    count_per_word := 32.umod(bit_width);
    shift := interrupt_id.umod(count_per_word) * bit_width;
    addr += interrupt_id.udiv(count_per_word);
    old := u32.volatile(addr);
    new := old.bit_and(mask.shift_left(shift).bit_not()).bit_or(value.shift_left(shift));
    u32.volatile(addr, new);
}

// generic interrupt controller
// TODO: device tree?
gicd :: 0x08000000;
gicc :: 0x08010000;
// EL1 Physical Timer. INTID defined by the Server Base System Architecture (doesn't show in device tree?)
timer_irq :: 30;

Arm :: import("@/backend/arm64/bits.fr");
// TODO: floats
interrupt_trampoline :: AsmFunctionArmOnly(fn() void = (), interrupt_trampoline_code);
interrupt_trampoline_code :: {
    #use(Arm);
    out := u32.list(64, ast_alloc());
    emit :: fn(inst) => out&.push(inst);
    
    // write SavedRegisters
    emit(sub_im(Bits.X64, sp, sp, 64, 0));  // leave space for special registers
    range_rev(0, 16) { r |
        a, b := (r*2, r*2+1);
        emit(stp_pre(.X64, a, b, sp, @as(i7) -2));
    };
    // TODO: kernel use a different stack so userspace can't see what's left there
    //       (don't forget 31 in stp_pre is xzr not sp)
    // call syscall_func
    emit(add_im(Bits.X64, x0, sp, 0, 0));
    emit(arm_nop);  // :patch
    @debug_assert_eq(out.items().index_of(arm_nop).unwrap(), out.len - 1, "patch must be first nop");
    
    // read SavedRegisters
    range(0, 16) { r |
        a, b := (r*2, r*2+1);
        emit(ldp_post(.X64, a, b, sp, @as(i7) 2));
    };
    emit(add_im(Bits.X64, sp, sp, 64, 0));
    
    // branch to ELR_EL1 which was set by the interrupt gods
    emit(eret);  
    
    out.items()
};

InterruptVector :: Array(Array(u32, 32), 16);
interrupt_handlers :: @static(Array(u8, size_of(InterruptVector) * 2));

align_the_thing :: fn(it: ~T, need_align: i64, need_size: i64) rawptr #where = {
    tt := int_from_ptr(@type it[], it);
    start := tt; 
    end := start + size_of(@type it[]);
    tt := tt.ualign_to(need_align);
    if tt + need_size > end {
        kprint("fucked\n!!!\n");
    };
    rawptr_from_int(tt)
}

setup_virtual_memory :: fn() void = {
    // translation tables must be aligned to the size of the table.
    tt := align_the_thing(translation_table, 1.shift_left(16), 8192*8);
    tt := i64.ptr_from_raw(tt).slice(8192);
    // TODO: check ID_AA64MMFR0_EL1 to make sure the granule size i want is supported
    
    parange := i64.sys_get(.ID_AA64MMFR0_EL1).bit_and(0b111);
    kprint_label("ID_AA64MMFR0_EL1: ", i64.sys_get(.ID_AA64MMFR0_EL1));
    kprint("--\n");
    
    // make sure its turned off before we start fucking with the tables
    barrier();
    i64.sys_set(.SCTLR_EL1, 0);
    barrier();
    
    // Memory Attribute Indirection Register
    // each slot is 1 byte, describes a caching behaviour, 
    // page table entries contain an index of which slot to use.
    // [0] = normal, [1..=7] = device nGnRnE
    i64.sys_set(.MAIR_EL1, 0xff);
    
    // Translation Table Base Register
    // "Together, the granule and the size of the virtual address space control the starting level of address translation."
    // 64KB granule, level 2 table has 512MB per entry, indexed by bits 41:29, 
    // so with a 42 bit address space, theres only one level of table (a Level Two table) because theres no bits left over.  
    nbit := 42;
    p_addr: u64 = 0;
    block_size: u64 = 512*1024*1024;
    each tt { it |
        // low bits are attributes, the address is always aligned so its low bits are zero and don't matter. 
        device := p_addr < 0x40000000;
        idx := int(device);
        // block type, mair index, permissions, sharable, access flag
        attr: BlockEntry = (repr = p_addr);
        attr&.set(.tag, 0b01);
        attr&.set(.idx, idx);
        // ?? regardless of UXN/PXN, virtual memory that's writable from el0 cannot be executable from el1 ??
        //                       // e0 e1
        if !device {
            attr&.set(.ap, 0b00);  // -- RW
        } else {
            attr&.set(.ap, 0b01);  // RW RW
        };
        //attr&.set(.ap, 0b10);  // -- R-
        //attr&.set(.ap, 0b11);  // R- R-
        attr&.set(.sh, 0b11);
        attr&.set(.af, 0b1);
        
        it[] = attr.repr.bitcast();
        p_addr += block_size;
    };
    // remap the first chunk of physical as the second chunk of virtual as R+W for el0. 
    // so el1 can execute that memory by its real address and el0 can use it by this new fake address. 
    xxx := tt[(0x40000000/block_size.bitcast()) + 1]&;
    attr: BlockEntry = (repr = xxx[].bitcast());
    attr&.set(.ap, 0b01);
    attr.repr -= block_size;
    xxx[] = attr.repr.bitcast();
    barrier();

    sys_set(*i64, .TTBR0_EL1, tt.ptr);
    
    tcr := TcrEl1.zeroed();
    sctlr := SctlrEl1.zeroed();
    
    // note: tg0 and tg1 have different encodings
    tcr&.set(.tg0, 0b01);  // Granule size for the TTBR0_EL1 = 64KB
    tcr&.set(.ips, parange);
    tcr&.set(.t0sz, 64 - nbit);
    tcr&.set(.ha, 0);

    sctlr&.set(.EE, 0);  // little endian
    sctlr&.set(.M, 1);  // address translation enabled
    sctlr&.set(.nTWI, 1);
    // TODO: make sure to turn on I: "instruction access Cacheability control"
    
    kprint_label("TCR_EL1: ", tcr.repr.bitcast());
    kprint_label("SCTLR_EL1: ", sctlr.repr.bitcast());
    TcrEl1.sys_set(.TCR_EL1, tcr);
    barrier();
    SctlrEl1.sys_set(.SCTLR_EL1, sctlr);
    barrier();
    kprint("it works!\n");
}

#use("@/lib/bit_fields.fr");

TcrEl1 :: @bit_fields(
    t0sz := 6,
    res0_epd0_irgn0_orgn0 := 8,
    tg0 := 2,
    t1sz := 6,
    a1_epd1_irgn1_orgn1_sh1 := 8,
    tg1 := 2,
    ips := 3,
    res0_as_tbi0_tbi1 := 4,
    ha := 1,
    _ := 24,
);

SctlrEl1 :: @bit_fields(
    M := 1,
    ____ := 15,
    nTWI := 1,
    _ := 6,
    span := 1,
    ___ := 1,
    EE := 1,
    __ := 38,
);

BlockEntry :: @bit_fields(
    tag := 2,
    idx := 3,
    ns := 1,
    ap := 2,
    sh := 2,
    af := 1,
    _ := 53,
);

translation_table :: @static(Array(i64, 8192*2));

// all the barriers for good luck
barrier :: @AsmFunctionArmOnly(fn() void = ()) => (
    @bits(0b11010101000000110011, 0b1111, 0b101, 0b11111),  // DMB SY
    @bits(0b11010101000000110011, 0b1111, 0b100, 0b11111),  // DSB SY
    @bits(0b11010101000000110011, 0b1111, 0b110, 0b11111),  // ISB SY
    ret(),
);

user_func :: fn(arg: i64) void = {
    kprint("inside user_func\n");
    kprint_label("user argument is: ", arg);
    // print_el();  // you can tell you're in el0 because reading CurrentEL is an illegal instruction
    
    result := do_syscall(123);
    kprint_label("syscall result is: ", result);
    
    if false {
        echo();
    };
    spin();
};

echo :: fn() void = {
    prev := 0;
    loop {
        i: i64 = getchar().zext();
        if i != prev {
            prev = i;
            kprint_label("input: ", i);
        }
    };
}

SavedRegisters :: @struct {
    gpr: Array(i64, 32);
    // why are we being interrupted?
    esr: i64;  // exception syndrome
    far: i64;  // fault address
    // how do we return after the interrupt?
    elr: i64;  // eret link register
    spsr: i64; // saved program status
    _: Array(u32, 4);
};

// for now im using the same handler for every slot of the InterruptVector
syscall_func :: fn(args: *SavedRegisters) void = {
    kprint("inside syscall_func\n");
    
    // these being in SavedRegisters isn't important because i don't restore them 
    // but it seems convient to have one context thing you can pass around.
    // TODO: if i keep these do it in the assembly part instead of as a bunch of calls here. 
    args.esr = i64.sys_get(.ESR_EL1);
    args.far = i64.sys_get(.FAR_EL1);
    
    // saving and restoring these only matters if i allow nested interrupts. 
    // TODO: maybe i want that so unset PSTATE I/F/A mask at some point?
    //       its also not clear to me what happens if i fault while in EL1,
    //       cause if im not the hypervisor i can't control whether those go to me or not, right? 
    args.elr = i64.sys_get(.ELR_EL1);
    args.spsr = i64.sys_get(.SPSR_EL1);
    
    intid := u32.volatile(gicc + 0x000C);  // GICC_IAR: Interrupt Acknowledge Register
    kprint_label("intid: ", intid.zext());
    
    if intid == 1023 {
        arg := args.gpr&[0];
        kprint_label("FAR_EL1: ", args.far);
        kprint_label("ESR_EL1: ", args.esr);
        kprint_label("SPSR_EL1: ", args.spsr);
        print_el();
        kprint_label("syscall argument is: ", arg);
        args.gpr&[0] = arg * 2;
    };
    
    if intid == timer_irq {
        kprint("timer fired!\n");
        frequency := i64.sys_get(.CNTFRQ_EL0);
        i64.sys_set(.CNTP_TVAL_EL0, frequency);
        
        u32.volatile(gicc + 0x0010, intid);  // GICC_EOIR: End of Interrupt Register
    };
    
    i64.sys_set(.ELR_EL1, args.elr);
    i64.sys_set(.SPSR_EL1, args.spsr);
};

Crash :: import("@/lib/crash_report.fr");

do_syscall :: @AsmFunctionArmOnly(fn(arg: i64) i64 = ()) => (
    svc,
    ret(), 
);

sys_get :: fn($T: Type, $name: Arm.SystemRegister) T #generic = {
    @run @ct_assert(size_of(T) == 8, @source_location(), "type must fit in a register");
    #use(Arm);
    AsmFunctionArmOnly(fn() T = (), @const_slice(
        mrs(x0, name),
        ret(), 
    ))()
}

sys_set :: fn($T: Type, $name: Arm.SystemRegister, value: T) void #generic = {
    @run @ct_assert(size_of(T) == 8, @source_location(), "type must fit in a register");
    #use(Arm);
    AsmFunctionArmOnly(fn(value: T) void = (), @const_slice(
        msr(name, x0),
        ret(), 
    ))(value);
}

do_eret :: @AsmFunctionArmOnly(fn(result: i64, callee: rawptr, _xx2: i64, xx3: i64) i64 = ()) => (
    // keep the same stack
    // , but mapped differently because everything is a massive pain in the ass
    add_im(Bits.X64, x2, sp, 0, 0),
    add_sr(Bits.X64, x2, x2, x3, .LSL, 0),
    add_sr(Bits.X64, x1, x1, x3, .LSL, 0),
    add_sr(Bits.X64, fp, fp, x3, .LSL, 0),
    
    msr(.SP_EL0, x2),
    // set eret destination
    msr(.ELR_EL1, x1),
    // jump to callee but in el0
    eret,
    // unreachable
    wfi,
    b(0, 0),
);

// TODO: make calling panic() work
kpanic :: fn(msg: Str) Never = {
    kprint("we're so fucked\n===\n");
    kprint(msg);
    kprint("\n===\ni repeat, we're so fucked\n");
    spin()
};

kprint_label :: fn(msg: Str, value: i64) void = {
    kprint(msg);
    kscary_log(value);
    kprint("\n");
}

spin :: fn() Never = {
    loop {
        (@AsmFunctionArmOnly(fn() void = ()) => (
            wfi,
            ret(), 
        ))();
    };
}

kprint :: fn(s: Str) void = 
    for(s, putchar);

// TODO: read the magic number from the device tree thingy?
uart :: 0x09000000;
putchar :: fn(c: u8) void = {
    u8.volatile(uart, c);
}

getchar :: fn() u8 = {
    u8.volatile(uart)
}

// TODO: can't let it try to bake the junk address or it crashes in bake_relocatable_constant :compilerbug
fn volatile($T: Type, addr: i64, value: T) void #noinline #generic = {
    bit_cast_unchecked(i64, *T, addr)[] = value;
}

fn volatile($T: Type, addr: i64) T #noinline #generic = {
    bit_cast_unchecked(i64, *T, addr)[]
}

fn kscary_log(i: i64) void = {
    buf := @uninitialized Array(u8, 40);
    if i != 0 && i == -i { // ugly twos compliment :hack
        kprint("-9223372036854775808");
        return();
    };
    operator_index :: index_unchecked;
    len := 0;
    inner :: fn(i: i64, buf: []u8, len: *i64) void = {
        if i < 0 {
            buf[len[]] = "-".ascii();
            len[] += 1;
            inner(-i, buf, len);
        } else {
            is_digit := i >= 0 && i < 10;
            if is_digit {
                buf[len[]] = i.trunc() + 48;
                len[] += 1;
            } else {
                inner(i / 10, buf, len);
                buf[len[]] = i.mod(10).trunc() + 48;
                len[] += 1;
            };
        };
    };
    inner(i, buf&.items(), len&);
    kprint(@as(Str) (ptr = buf&.as_ptr(), len = len));
}

// TODO: this is a neat demo but it would be more simple to just make the syntax such that it doesn't resolve
//       eagarly which might just mean using `fn() = {}` instead of `fn() => ()` but then its annoying to get 
//       the tuple out of the block... idk this whole operation is kinda dumb anyway. 
// this is convoluted to trick it into resolving the expression in the context of #use(arm/bits)
fn AsmFunctionArmOnly(signeture: FatExpr, code: FatExpr) FatExpr #macro = {
    signeture := FuncId.const_eval(signeture);
    code_fid := FuncId.const_eval(code);
    code := get_function_ast(code_fid, false, false, false, false);
    ::tagged(@type code.body);
    @debug_assert(code.body&.is(.Normal));

    code.body.Normal = @{ @const_slice(@[code.body.Normal]) };
    unresolve_scopes(code.body.Normal&);
    
    code.resolve_scope = FuncId.scope_of(fn() = {
        // the body of `code` will be able to see any constants declared here.
        #use(Arm);
    });
    // TODO: makes sense that you need this,
    //       but you shouldn't panic without it, should be an error instead of 
    //       `panic! Unreachable unless you set the load factor to 100%`
    code.unset_flag(.ResolvedBody);
    
    code := Slice(u32).const_eval(@{ @[@literal code_fid]() });
    @literal AsmFunctionArmOnly(signeture, code)
}

// time to do some sketchy shit do da do da
fn unresolve_scopes(expr: *FatExpr) void = {
    #use("@/compiler/walk_ast.fr");
    #use("@/compiler/ast_external.fr");
    UnResolve :: @struct();
    ::WalkAst(UnResolve, void);
    
    // TODO: `@ref(@as(UnResolve) ())` ambigous overload for FatExpr -> FatExpr. :compilerbug?
    unresolve: UnResolve = ();  
    walk_expr(unresolve&, expr);

    fn handle_expr(self: *UnResolve, expr: *FatExpr) Result(DoMore, void) = {
        @match(expr.expr&) {
            fn GetVar(it) => {
                expr.expr = (GetNamed = it.name);
            }
            fn UndeclaredVar(it) => {
                expr.expr = (GetNamed = it.name);
            }
            fn Block(it) => {
                it.flags = it.flags.bit_and(bit_not(1.shift_left(@as(i64) BlockFlags.Resolved)));
                it.scope = NOSCOPE;
            }
            @default => ();
        };
        (Ok = .Continue)
    }
    fn handle_stmt(self: *UnResolve, stmt: *FatStmt) Result(DoMore, void) = (Ok = .Continue);
    fn handle_type(self: *UnResolve, type: *LazyType) Result(DoMore, void) = (Ok = .Continue);
    fn handle_func(self: *UnResolve, func: *Func) Result(DoMore, void) = (Ok = .Continue);
    fn handle_pattern(self: *UnResolve, pattern: *Pattern) Result(DoMore, void) = (Ok = .Continue);
}

fn AsmFunctionArmOnly(signeture: FuncId, code: []u32) FuncId #fold = {
    fid := AsmFunction(signeture, code, fn(out: *List(u8)) => (), fn(out: *List(u8)) => (), empty());
    _ := get_function_ast(fid, true, true, true, false);
    fid
}

// TODO: these should go somewhere else
fn stack_alloc($bytes: i64) Alloc #inline /*semantic*/ = {
    buf := @uninitialized Array(u8, bytes + 16);
    self: FixedBufAlloc = (buf = buf&.items(), i = 0);
    // TODO: should be able to ask for higher aligned stack slot
    i := u8.int_from_ptr(self.buf.ptr);
    self.i += i.ualign_to(16) - i;
    self&.borrow()
}

FixedBufAlloc :: @struct {
    buf: []u8;
    i: i64;
    impl :: fn(self: rawptr, action: Alloc.Action, ptr: rawptr, count: i64, align: i64) []u8 = {
        self := FixedBufAlloc.ptr_from_raw(self);
        if action == Alloc.Action.Allocate {
            self.i = self.i.ualign_to(align);
            start := self.i;
            self.i += count;
            return(self.buf.slice(start, self.i));
        };
        empty()
    };
    
    fn borrow(self: *FixedBufAlloc) Alloc = 
        (data = FixedBufAlloc.raw_from_ptr(self), vptr = impl);
};
