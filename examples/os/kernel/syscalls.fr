write :: fn(fd: i64, buf: i64, len: i64) i64 #n(64) = {
    kprint(u8.ptr_from_int(buf).slice(len));
    len
}

mmap :: fn(kernel: *Kernel, task: *Task, addr: i64, len: i64, prot: i64, flags: i64, fd: i64, offset: i64) i64 #n(222) = {
    if addr != 0 || fd.intcast() != -1 || offset != 0 {
        tpanic("non-trivial mmap");
    };
    pages := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    result_addr := vmap_reserve(kernel, task.mem&, pages);
    barrier();  // TODO: theres an instruction to kill the tlb or something
    if result_addr == 0 {
        tpanic("vmap_reserve can't return null");
    };
    result_addr
}

mprotect :: fn(addr: i64, len: i64, prot: i64) i64 #n(226) = {
    // TODO: do the protection. for now everything is RWX so it doesn't matter. 
    barrier(); // TODO: theres an instruction to kill the tlb or something
    0
}

munmap :: fn(kernel: *Kernel, task: *Task, addr: i64, len: i64) i64 #n(215) = {
    addr := addr.align_back(PAGE_SIZE);
    page_count := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    if addr + page_count * PAGE_SIZE >= kernel.next_vaddr || addr < (512*1024*1024) * 4 || page_count < 0 {
        tpanic("bad munmap");
    };
    vunmap(kernel, task.mem&, addr, page_count);
    barrier();  // TODO: theres an instruction to kill the tlb or something
    0
}

exit :: fn(task: *Task, status: i64) i64 #n(94) = {
    if status != 0 {
        kprint_label("called exit: ", status);
    };
    if status == 81 {  // :temporarystacktraceonexithack
        task.mcontext.esr = 0;
        send_signal(task);
    };
    kill_task(task);
    0
}

futex :: fn(kernel: *Kernel, task: *Task, addr: i64, op: i64, val: i64, timeout: i64) i64 #n(98) = {
    @if(timeout != 0) tpanic("TODO: futex timeout");
    // TODO: check that `addr` is committed and readable
    pte := index_page_table(kernel, task.mem&, addr.align_back(PAGE_SIZE));
    @if(pte.map.repr == 0) tpanic("futex unmapped address");
    
    @switch(op) {
        @case(0) => {  // wait
            // TODO: gets more complicated when i have multiple cores
            // TODO: what happens if `addr` is shared memory thats mapped to different virtual addresses?
            p := u32.ptr_from_int(addr);
            barrier();
            sleep := p[] == val.trunc();
            if sleep {
                task.futex_wait = addr;
            };
            int(sleep) - 1
        };
        @case(1) => {  // wake
            @if(val <= 0) return(0);
            // TODO: unfair and :slow, maybe i want a queue per futex
            n := 0;
            each kernel.tasks& { it |
                if it.futex_wait == addr {
                    it.futex_wait = 0;
                    n += 1;
                    @if(n >= val) return(n);
                };
            };
            n
        };
        @default => tpanic("bad futex op");
    }
}

yield :: fn(task: *Task) i64 #n(0xFFFF0002) = {
    // return_to_scheduler always cycles through all the tasks so don't need to do anything here. 
    0
}

sig_action :: fn(task: *Task, callee: i64, buf: i64, len: i64) i64 #n(0xFFFF0003) = {
    // TODO: check that buf[..+len] is writable by task
    prev := task.signal_handler;
    task.signal_handler = callee;
    task.signal_stack_left = buf;
    task.signal_stack = buf + len;
    prev
}

sig_return :: fn(task: *Task, intid: i64, sp: i64) i64 #n(0xFFFF0004) = {
    elr := 5*4 + 4/*bti*/ + launder_to_user_pointer(user_unwind_signal); // :end_user_unwind_signal
    @if(task.mcontext.elr != elr) tpanic("sig_return is #once");
    @if(task.signal_stack != sp) tpanic("poorly nested sig_return");
    saved := SavedRegisters.ptr_from_int(sp);
    if intid != 1023 {
        i64.sys_set(.ICC_EOIR1_EL1, intid);
    };
    // TODO: probably shouldn't expose all the bits of spsr mutably to user space but do want to store them
    task.mcontext = saved[];
    task.signal_stack += size_of(SavedRegisters);
    task.mcontext.gpr&[0]
}

spawn_thread :: fn(kernel: *Kernel, parent: *Task, ctx: i64) i64 #n(0xFFFF0005) = {
    ctx := SavedRegisters.ptr_from_int(ctx);
    child := kernel.alloc_task(parent.mem.root);
    if u32.ptr_from_int(ctx.elr)[] != Arm.branch_target_marker {
        tpanic("spawn thread expected bti");
    };
    child.mcontext.gpr = ctx.gpr;
    child.mcontext.elr = ctx.elr;
    child.mcontext.fpr = ctx.fpr;
    child.active = true;
    child.thread_id
}

// TODO: have a more efficient waitpid than calling this in a loop
check_thread :: fn(kernel: *Kernel, id: i64) i64 #n(0xFFFF0006) = {
    each kernel.tasks& { it |
        if it.thread_id == id {
            return it.active.not().int();
        }
    };
    1
}

generate :: fn() []Ty(@FnPtr(task: *Task) void, i64) = {
    results := Ty(@FnPtr(task: *Task) void, i64).list(ast_alloc());
    s := import("@/examples/os/kernel/syscalls.fr");
    for get_constants(s) { name |
        continue :: local_return;
        impl_fid := get_constant(FuncId, s, name).unwrap();
        impl_func := get_function_ast(impl_fid, true, true, true, false);
        loc := impl_func.loc;
        
        i := impl_func.annotations&.index_of(fn(it) => it.name == @symbol n)
            || continue();
        syscall_number := const_eval(i64)(impl_func.annotations[i].args);
        
        fr := current_compiler_context();
        func := empty_fn(impl_func.name, loc);
        func.resolve_scope = s; // :WhatScope
        fill_bindings(func&, fr, @slice(*Task));
        wrapper_fid := fr.intern_func(func&);
        func := get_function_ast(wrapper_fid, true, true, false, false);
        
        the_task := func.arg.bindings[0].name.Var;
        the_task: FatExpr = (expr = (GetVar = the_task), ty = UnknownType, done = false, loc = loc);
        args := FatExpr.list(ast_alloc());
        i := 0;  // choosing this index gets more complicated once its not just for arm
        each impl_func.arg.bindings& { it |
            @switch(it.ty.Finished) {
                @case(*Task) => args&.push(the_task);
                @case(*Kernel) => args&.push(@{ kernel() });
                @case(i64) => {
                    args&.push(@{ @[the_task].mcontext.gpr&[@[@literal i]] });
                    i += 1;
                };
                @default => panic("invalid syscall argument type");
            };
        };
        @debug_assert_le(i, 8, "too many syscall arguments");
        
        args: FatExpr = if args.len == 1 {
            args[0]
        } else {
            (expr = (Tuple = args.as_raw()), ty = UnknownType, done = false, loc = loc)
        };
        func.body = (Normal = @{ 
            x0: i64 = @[@literal impl_fid](@[args]);
            @[the_task].mcontext.gpr&[0] = x0;
        });
        
        callee := fr.get_jitted(wrapper_fid);
        callee := bit_cast_unchecked(rawptr, @FnPtr(task: *Task) void, callee);
        push(results&, (callee, syscall_number));
    };
    results.items()
}
