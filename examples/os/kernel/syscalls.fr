write :: fn(fd: i64, buf: i64, len: i64) i64 #n(64) = {
    kprint(u8.ptr_from_int(buf).slice(len));
    len
}

mmap :: fn(task: *Task, addr: i64, len: i64, prot: i64, flags: i64, fd: i64, offset: i64) i64 #n(222) = {
    if addr != 0 || fd.intcast() != -1 || offset != 0 {
        tpanic("non-trivial mmap");
    };
    PAGE_SIZE := kernel.page_size;
    pages := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    result_addr := vmap_reserve(task.mem&, pages);
    barrier();  // TODO: theres an instruction to kill the tlb or something
    if result_addr == 0 {
        tpanic("vmap_reserve can't return null");
    };
    result_addr
}

mprotect :: fn(addr: i64, len: i64, prot: i64) i64 #n(226) = {
    // TODO: do the protection. for now everything is RWX so it doesn't matter. 
    barrier(); // TODO: theres an instruction to kill the tlb or something
    0
}

munmap :: fn(task: *Task, addr: i64, len: i64) i64 #n(215) = {
    PAGE_SIZE := kernel.page_size;
    addr := addr.align_back(PAGE_SIZE);
    page_count := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    if addr + page_count * PAGE_SIZE >= kernel.next_vaddr || addr < u8.int_from_ptr(kernel.ram.end_pointer()) || page_count < 0 {
        tpanic("bad munmap");
    };
    vunmap(task.mem&, addr, page_count);
    barrier();  // TODO: theres an instruction to kill the tlb or something
    0
}

exit :: fn(task: *Task, status: i64) i64 #n(94) = {
    if status != 0 {
        kprint_label("called exit: ", status);
    };
    if status == 81 {  // :temporarystacktraceonexithack
        task.mcontext.esr = 0;
        send_signal(task);
    };
    kill_task(task);
    return_to_scheduler()  // yield
}

futex :: fn(task: *Task, addr: i64, op: i64, val: i64, timeout: i64) i64 #n(98) = {
    @if(timeout != 0) tpanic("TODO: futex timeout");
    // TODO: check that `addr` is committed and readable
    pte := index_page_table(task.mem&, addr.align_back(kernel.page_size));
    @if(pte.map.repr == 0) tpanic("futex unmapped address");
    
    @switch(op) {
        @case(0) => {  // wait
            // TODO: gets more complicated when i have multiple cores
            // TODO: what happens if `addr` is shared memory thats mapped to different virtual addresses?
            p := u32.ptr_from_int(addr);
            barrier();
            sleep := p[] == val.trunc();
            if sleep {
                task.futex_wait = addr;
                task.mcontext.gpr&[0] = 0;
                return_to_scheduler();  // yield
            };
            -1
        };
        @case(1) => {  // wake
            @if(val <= 0) return(0);
            futex_wake(addr, val)
        };
        @default => tpanic("bad futex op");
    }
}

yield :: fn(task: *Task) i64 #n(0xFFFF0002) = {
    task.mcontext.gpr&[0] = 0;
    return_to_scheduler()
}

sig_action :: fn(task: *Task, new: i64, old: i64) i64 #n(0xFFFF0003) = {
    old := SignalHandler.ptr_from_int(old);
    old.callee = bit_cast_unchecked(i64, @type old.callee, task.signal_handler);
    old.stack.ptr = u8.ptr_from_int(task.signal_stack_left);
    // TODO: this length is wrong if its currently in a signal handler (which it will be since its doing a syscall)
    old.stack.len = task.signal_stack - task.signal_stack_left;
    
    // TODO: check that buf[..+len] is writable by task
    new := SignalHandler.ptr_from_int(new);
    task.signal_handler = bit_cast_unchecked(@type new.callee, i64, new.callee);
    buf := u8.int_from_ptr(new.stack.ptr);
    task.signal_stack_left = buf;
    task.signal_stack = buf + new.stack.len;
    
    0
}

sig_return :: fn(task: *Task, intid: i64, sp: i64) i64 #n(0xFFFF0004) = {
    elr := 5*4 + 4/*bti*/ + launder_to_user_pointer(user_unwind_signal); // :end_user_unwind_signal
    @if(task.mcontext.elr != elr) tpanic("sig_return is #once");
    @if(task.signal_stack != sp) tpanic("poorly nested sig_return");
    saved := SavedRegisters.ptr_from_int(sp);
    if intid != 1023 {
        i64.sys_set(.ICC_EOIR1_EL1, intid);
    };
    // TODO: probably shouldn't expose all the bits of spsr mutably to user space but do want to store them
    task.mcontext = saved[];
    task.signal_stack += size_of(SavedRegisters);
    task.mcontext.gpr&[0]
}

spawn_thread :: fn(parent: *Task, ctx: i64, exit_futex: i64) i64 #n(0xFFFF0005) = {
    ctx := SavedRegisters.ptr_from_int(ctx);
    child := alloc_task(parent.mem.root);
    if u32.ptr_from_int(ctx.elr)[] != Arm.branch_target_marker {
        tpanic("spawn thread expected bti");
    };
    child.mcontext.gpr = ctx.gpr;
    child.mcontext.elr = ctx.elr;
    child.mcontext.fpr = ctx.fpr;
    child.exit_futex = exit_futex;
    child.active = true;
    u32.ptr_from_int(exit_futex)[] = child.thread_id.trunc();
    child.thread_id
}

virtq_poll :: fn(task: *Task, it: i64) i64 #n(0xFFFF0007) = {
    it := UQueue.ptr_from_int(it);
    q := Virt.Queue.ptr_from_raw(it.system_handle);
    pipe := it.pipe&;
    need_wake := 0;
    
    // q.used -> p.done
    while => q.device_idx != q.device.head.idx&.vol() && pipe.done.claimed.bouba - pipe.done.committed.kiki != pipe.done.ring.len.trunc() {
        i := q.device_idx.zext().mod(q.device.ring.len);
        q.device_idx += 1;
        used := q.device.ring[i];
        desc := q.desc[used.id.zext()];

        j := pipe.work.committed.kiki;
        while => j != pipe.work.claimed.kiki && pipe.work.ring[j.zext().mod(pipe.work.ring.len)].system_id != used.id {
            j += 1;
        };
        if j == pipe.work.claimed.kiki {
            tpanic("done: currupt UQueue.Entry id");
        };
        // TODO: debug check that the addresses match
        if j != pipe.work.claimed.kiki - 1 {
            tpanic("TODO: allow completing virtio requests out of order");
        };
        
        it := pipe.work.ring[j.zext().mod(pipe.work.ring.len)];
        pipe.work.committed.kiki += 1;
        it.n.bytes_written = desc.len;
        it.system_id = MAX_u32;
        
        // TODO: be more careful about being atomic? but there's only one kernel thread so its fine. 
        // pipe.push(it) but without syscalls
        pipe.done.claimed.bouba += 1;
        pipe.done.ring[pipe.done.claimed.bouba.zext().sub(1).umod(pipe.done.ring.len)] = it;
        pipe.done.committed.bouba += 1;
        need_wake += 1;
    };
    
    // p.work -> q.avail
    while => pipe.work.claimed.kiki != pipe.work.claimed.bouba {
        pipe.work.claimed.kiki += 1;
        i := pipe.work.claimed.kiki.zext().sub(1).umod(pipe.work.ring.len);
        // note: `it` is left in the pipe, claimed but uncommitted, until the device finishes the request.  
        it := pipe.work.ring[i]&;
        if it.system_id != MAX_u32 {
            tpanic("work: corrupt UQueue.Entry id");
        };
        read_n := it.n.bufs_readable;
        for it.bufs { it |
            q.push_virt(it, @if(read_n > 0, 0, Virt.DESC_F_WRITE));
            read_n -= 1;
        };
        it.system_id = q.driver_chain_start.expect("driver_chain_start").zext();
        q.publish_chain();
    };
    
    if need_wake != 0 {
        futex_wake(u32.int_from_ptr(pipe.done.committed.bouba&), need_wake);
        futex_wake(u32.int_from_ptr(pipe.work.committed.kiki&), need_wake);
    };
    
    0
}

virtq_create :: fn(it: i64, p: i64, l: i64) i64 #n(0xFFFF0008) = {
    it := UQueue.ptr_from_int(it);
    label := u8.ptr_from_int(p).slice(l);
    kprint(label);
    kprint("\n");
    if kernel.fuse& { device |
        it.system_handle = Virt.Queue.raw_from_ptr(device.device.queues.index(VirtFuse.REQUEST));
        return 0;
    };
    -1
}

generate :: fn() []Ty(@FnPtr(task: *Task) void, i64) = {
    results := Ty(@FnPtr(task: *Task) void, i64).list(ast_alloc());
    s := import("@/examples/os/kernel/syscalls.fr");
    for get_constants(s) { name |
        continue :: local_return;
        impl_fid := get_constant(FuncId, s, name).unwrap();
        impl_func := get_function_ast(impl_fid, true, true, true, false);
        loc := impl_func.loc;
        
        i := impl_func.annotations&.index_of(fn(it) => it.name == @symbol n)
            || continue();
        syscall_number := const_eval(i64)(impl_func.annotations[i].args);
        
        fr := current_compiler_context();
        func := empty_fn(impl_func.name, loc);
        func.resolve_scope = s; // :WhatScope
        fill_bindings(func&, fr, @slice(*Task));
        wrapper_fid := fr.intern_func(func&);
        func := get_function_ast(wrapper_fid, true, true, false, false);
        
        the_task := func.arg.bindings[0].name.Var;
        the_task: FatExpr = (expr = (GetVar = the_task), ty = UnknownType, done = false, loc = loc);
        args := FatExpr.list(ast_alloc());
        i := 0;  // choosing this index gets more complicated once its not just for arm
        each impl_func.arg.bindings& { it |
            @switch(it.ty.Finished) {
                @case(*Task) => args&.push(the_task);
                @case(i64) => {
                    args&.push(@{ @[the_task].mcontext.gpr&[@[@literal i]] });
                    i += 1;
                };
                @default => panic("invalid syscall argument type");
            };
        };
        @debug_assert_le(i, 8, "too many syscall arguments");
        
        args: FatExpr = if args.len == 1 {
            args[0]
        } else {
            (expr = (Tuple = args.as_raw()), ty = UnknownType, done = false, loc = loc)
        };
        func.body = (Normal = @{ 
            x0: i64 = @[@literal impl_fid](@[args]);
            @[the_task].mcontext.gpr&[0] = x0;
        });
        
        callee := fr.get_jitted(wrapper_fid);
        callee := bit_cast_unchecked(rawptr, @FnPtr(task: *Task) void, callee);
        push(results&, (callee, syscall_number));
    };
    results.items()
}
