// this is used when stdout console is a pl011 instead of a virtq. 
debug_write :: fn(buf: i64, len: i64) i64 #n(0xFFFF0009) = {
    kprint(u8.check_user_read(buf, len));
    0
}

mmap :: fn(addr: i64, len: i64, prot: i64, flags: i64, fd: i64, offset: i64) i64 #n(222) = {
    task := get_current_task();
    if addr != 0 || fd.intcast() != -1 || offset != 0 {
        tpanic("non-trivial mmap");
    };
    PAGE_SIZE := kernel.page_size;
    pages := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    result_addr := vmap_reserve(task.mem&, pages);
    if result_addr == 0 {
        tpanic("vmap_reserve can't return null");
    };
    result_addr
}

mprotect :: fn(addr: i64, len: i64, prot: i64) i64 #n(226) = {
    check_user_remap(addr, len);  // TODO: merge this loop with the one for doing the work
    // TODO: do the protection. for now everything is RWX so it doesn't matter. 
    0
}

munmap :: fn(addr: i64, len: i64) i64 #n(215) = {
    task := get_current_task();
    PAGE_SIZE := kernel.page_size;
    addr := addr.align_back(PAGE_SIZE);
    page_count := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    check_user_remap(addr, len);  // TODO: merge this loop with the one for doing the work
    if addr + page_count * PAGE_SIZE >= kernel.next_vaddr || addr < u8.int_from_ptr(kernel.ram.end_pointer()) || page_count < 0 {
        tpanic("bad munmap");
    };
    vunmap(task.mem&, addr, page_count);
    0
}

exit :: fn(status: i64) i64 #n(94) = {
    task := get_current_task();
    if status == 81 {  // :temporarystacktraceonexithack
        task.mcontext.esr = 0;
        send_signal(task);
    };
    kill_task(task);
    return_to_scheduler()  // yield
}

futex :: fn(addr: i64, op: i64, val: i64, timeout: i64) i64 #n(98) = {
    @if(timeout != 0) tpanic("TODO: futex timeout");
    p := u32.check_user_read(addr);
    
    @switch(op) {
        @case(0) => {  // wait
            // TODO: gets more complicated when i have multiple cores
            // TODO: what happens if `addr` is shared memory thats mapped to different virtual addresses?
            sleep := p[] == val.trunc();
            if sleep {
                task := get_current_task();
                task.futex_wait = addr;
                task.mcontext.gpr&[0] = 0;
                return_to_scheduler();  // yield
            };
            -1
        };
        @case(1) => {  // wake
            @if(val <= 0) return(0);
            futex_wake(addr, val)
        };
        @default => tpanic("bad futex op");
    }
}

yield :: fn() i64 #n(0xFFFF0002) = {
    task := get_current_task();
    task.mcontext.gpr&[0] = 0;
    return_to_scheduler()
}

sig_action :: fn(new: i64, old: i64) i64 #n(0xFFFF0003) = {
    task := get_current_task();
    @if(task.thread_id != 1) tpanic("only init needs to sig_action, others do it on spawn");
    old := SignalHandler.check_user_write(old);
    old.callee = task.signal_handler;
    old.stack = task.signal_stack_buf;
    
    new := SignalHandler.check_user_read(new)[];
    task.signal_handler = new.callee;
    buf := u8.int_from_ptr(new.stack.ptr);
    u8.check_user_write(buf, new.stack.len);
    task.signal_stack_buf = new.stack;
    task.signal_stack = u8.int_from_ptr(task.signal_stack_buf.end_pointer());
    0
}

sig_return :: fn(intid: i64, ctx: i64, sp: i64) i64 #n(0xFFFF0004) = {
    task := get_current_task();
    elr := 5*4 + 4/*bti*/ + launder_to_user_pointer(user_unwind_signal); // :end_user_unwind_signal
    signal_stack_left, signal_stack_right := task.t&.sigstack();
    bad := false
        || task.mcontext.elr != elr      // #once
        || sp < signal_stack_left        // oob
        || sp > signal_stack_right       // oob
        || sp < task.signal_stack        // over estimate of current position
        || ctx != sp                     // returned the wrong one (poorly nested)
    ;
    @if(bad) tpanic("invalid sig_return");
    // don't need to check_user_read because sig_action does and just checked its in bounds of signal_stack
    // TODO: probably shouldn't expose all the bits of spsr mutably to user space but do want to store them
    task.mcontext = SavedRegisters.ptr_from_int(sp)[];
    if intid != 1023 {
        i64.sys_set(.ICC_EOIR1_EL1, intid);
    };
    task.signal_stack = sp;
    task.signal_stack += size_of(SavedRegisters);
    task.mcontext.gpr&[0]
}

spawn_thread :: fn(ctx: i64, t: i64) i64 #n(0xFFFF0005) = {
    parent := get_current_task();
    ctx := SavedRegisters.check_user_read(ctx)[];
    t := ThreadConfig.check_user_read(t)[];
    if u32.ptr_from_int(ctx.elr)[] != Arm.branch_target_marker {
        tpanic("spawn thread expected bti");
    };
    if ctx.esr != 0 || ctx.spsr != 0 || ctx.far != 0 {
        tpanic("spawn_thread: don't try to set esr,spsr,far");
    };
    u32.check_user_write(u32.int_from_ptr(t.exit_futex));
    @if(t.exit_futex[] != MAX_u32) tpanic("exit_futex[]");
    child := alloc_task(parent.mem.root);
    child.mcontext = ctx;
    signal_stack_left, signal_stack_right := t&.sigstack();
    u8.check_user_write(signal_stack_left, signal_stack_right - signal_stack_left);
    child.t = t;
    child.signal_stack = signal_stack_right;
    child.exit_futex[] = child.thread_id.trunc();
    child.active = true;
    child.thread_id
}

virtq_poll :: fn(it: i64) i64 #n(0xFFFF0007) = {
    task := get_current_task();
    it := UQueue.check_user_write(it);
    if !do_virtq_poll(it) {
        // TODO: it sucks that i have to spin instead of waiting on a futex
        //       because i don't get interrupts for the fuse stuff. presumably its a skill issue.
        //       regardless, yielding if it failed makes it a bit less offensive. 
        //yield(task);
    };
    1
}

fn do_virtq_poll(it: *UQueue) bool = {
    if bit_cast_unchecked(@type it.poll, i64, it.poll) != user_virtq_poll.launder_to_user_pointer() {
        // note: this isn't as strong a check as the one in sig_return because it doesn't check elr. 
        // not a big deal if i want to get rid of this to make it more flexible, 
        // but for now its an easy to detect memory corruption.
        tpanic("virtq_poll is #once");
    };

    q := Virt.Queue.ptr_from_raw(it.system_handle);
    pipe := it.pipe&;
    made_progress := false;
    need_wake := 0;
    
    // TODO: there's a bit where it tells me if it wrapped so you can tell if it processed everything or nothing if the ring was full    
    
    // q.used -> p.done
    while => q.device_idx != q.device.head.idx&.vol() && pipe.done.claimed.bouba - pipe.done.committed.kiki != pipe.done.ring.len.trunc() {
        i := q.device_idx.zext().mod(q.device.ring.len);
        q.device_idx += 1;
        used := q.device.ring[i];
        desc := q.desc[used.id.zext()];

        j := pipe.work.committed.kiki;
        while => j != pipe.work.claimed.kiki && pipe.work&[j].system_id != used.id {
            j += 1;
        };
        if j == pipe.work.claimed.kiki {
            tpanic("done: currupt UQueue.Entry id");
        };
        // TODO: debug check that the addresses match
        //       (phys in desc with virt in bufs)
        if j != pipe.work.committed.kiki {
            tpanic("TODO: allow completing virtio requests out of order");
        };
        
        it := pipe.work&[j];
        pipe.work.committed.kiki += 1;
        it.n.bytes_written = used.len;
        it.system_id = MAX_u32;
        
        // TODO: be more careful about being atomic? but there's only one kernel thread so its fine. 
        // pipe.push(it) but without syscalls
        i := u32.atomic_inc(pipe.done.claimed.bouba&);
        pipe.done&[i] = it;
        pipe.done.committed.bouba += 1;
        need_wake += 1;
        made_progress = true;
    };
    
    // p.work -> q.avail
    while => pipe.work.claimed.kiki != pipe.work.claimed.bouba {
        i := u32.atomic_inc(pipe.work.claimed.kiki&);
        // note: `it` is left in the pipe, claimed but uncommitted, until the device finishes the request.  
        it := pipe.work&[i]&;
        if it.system_id != MAX_u32 {
            tpanic("work: corrupt UQueue.Entry id");
        };
        read_n := it.n.bufs_readable;
        for it.bufs { it |
            // TODO: really i should be check_user_(read, write) here, but meh.
            //       if i do that i also have to be careful to do it in the context of the person who
            //       owns the queue, not the task that  was interrupted if we're in Virt.check_interrupt
            q.push_virt(it, @if(read_n > 0, 0, Virt.DESC_F_WRITE));
            read_n -= 1;
        };
        it.system_id = q.driver_chain_start.expect("driver_chain_start").zext();
        q.publish_chain();
        made_progress = true;
    };
    
    if need_wake != 0 {
        futex_wake(u32.int_from_ptr(pipe.done.committed.bouba&), need_wake);
        futex_wake(u32.int_from_ptr(pipe.work.committed.kiki&), need_wake);
    };
    
    made_progress
}

virtq_create :: fn(out: i64, p: i64, l: i64, q_index: i64) i64 #n(0xFFFF0008) = {
    task := get_current_task();
    // not a problem to remove this. its just a debugging aid for now. 
    out := UQueue.check_user_write(out);
    label := u8.check_user_read(p, l);
    each kernel.devices& { it |
        if it.label&.get_cstr() == label {
            q := it.queues.index(q_index);
            // only allow virtq_create once per Virt.Queue
            @if(q.user.is_some()) return(-1);
            // happens if kernel code is using the Virt.Queue directly already
            @if(q.device_idx != q.driver_idx) return(-2);
            out.pipe.work&.set_all(q.device_idx.zext());
            out.pipe.done&.set_all(q.driver_idx.zext());
            
            out.system_handle = Virt.Queue.raw_from_ptr(q);
            out.poll = bit_cast_unchecked(i64, @type out.poll, user_virtq_poll.launder_to_user_pointer());
            q.user = (Some = out);
            return 0;
        }
    };
    -1
}

// translate a virtual address range to its backing physical address ranges. 
virt_translate :: fn(addr: i64, len: i64, out_addr: i64, out_len: i64) i64 #n(0xFFFF000A) = {
    task := get_current_task();
    // translating requires write permission because you can use virtio to write to physical memory addresses. 
    //     which is still not at all secure because you can put any random numbers in there 
    //     so this has to be replaced eventually, but for now i'm using memory 
    //     protection to catch bugs not to sandbox untrusted code, so it's fine. 
    virtual := u8.check_user_write(addr, len);
    out := Slice(u8).check_user_write(out_addr, out_len);
    i := 0;
    Virt'iter_phys_pages(task.mem&, virtual) { phys |
        if i < out.len {
            out[i] = phys;
        };
        i += 1;
    };
    i
}

generate :: fn() []Ty(SyscallFunc, i64) = {
    results := Ty(SyscallFunc, i64).list(ast_alloc());
    s := import("@/examples/os/kernel/syscalls.fr");
    for get_constants(s) { name |
        continue :: local_return;
        impl_fid := get_constant(FuncId, s, name).unwrap();
        impl_func := get_function_ast(impl_fid, true, true, true, false);
        loc := impl_func.loc;
        
        i := impl_func.annotations&.index_of(fn(it) => it.name == @symbol n)
            || continue();
        syscall_number := const_eval(i64)(impl_func.annotations[i].args);
        @ct_assert(results.find(fn(it) => it._1 == syscall_number).is_none(), impl_func.loc, "duplicate");
        
        fr := current_compiler_context();
        i := 0;
        each impl_func.arg.bindings& { it |
            @switch(it.ty.Finished) {
                @case(i64) => {
                    i += 1;
                };
                @case(void) => ();
                @default => @ct_assert(false, impl_func.loc, "invalid syscall argument type");
            };
        };
        @debug_assert_le(i, 6, "too many syscall arguments");
        
        callee := fr.get_jitted(impl_fid);
        callee := bit_cast_unchecked(rawptr, SyscallFunc, callee);
        push(results&, (callee, syscall_number));
    };
    results.items()
}
