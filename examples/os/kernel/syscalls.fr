write :: fn(fd: i64, buf: i64, len: i64) i64 #n(64) = {
    kprint(u8.ptr_from_int(buf).slice(len));
    len
}

read :: fn(fd: i64, buf: i64, len: i64) i64 #n(63) = {
    @if(fd != 0) return(-1);
    dest := u8.ptr_from_int(buf).slice(len);
    count := 0;
    // TODO
    count
}

mmap :: fn(kernel: *Kernel, addr: i64, len: i64, prot: i64, flags: i64, fd: i64, offset: i64) i64 #n(222) = {
    pages := len.ualign_to(PAGE_SIZE).udiv(PAGE_SIZE);
    result_addr := vmap_reserve(kernel, kernel.tasks&[0].mem&, pages);
    barrier();  // TODO: theres an instruction to kill the tlb or something
    if result_addr == 0 {
        kpanic("vmap_reserve can't return null");
    };
    result_addr
}

mprotect :: fn(addr: i64, len: i64, prot: i64) i64 #n(226) = {
    // TODO: do the protection. for now everything is RWX so it doesn't matter. 
    barrier(); // TODO: theres an instruction to kill the tlb or something
    0
}

munmap :: fn(addr: i64, len: i64) i64 #n(215) = {
    // TODO: dont just leak it. also unmap any physical pages that were committed. 
    0
}

exit :: fn(task: *Task, status: i64) i64 #n(94) = {
    kprint_label("called exit: ", status);
    if status == 81 {  // :temporarystacktraceonexithack
        task.mcontext.esr = 0;
        send_signal(task);
    };
    kill_task(task);
    0
}

futex :: fn(addr: i64, futex_op: i64, val: i64, timeout: i64) i64 #n(98) = {
    // TODO
    0
}

// TODO: new address space
spawn :: fn(kernel: *Kernel, buf: i64, len: i64) i64 #n(0xFFFF0001) = {
    //elf_bytes := u8.ptr_from_int(buf).slice(len);
    
    task := kernel.alloc_task();
    tt := kernel.idle_task.mem.root;
    init_task(task, user_exec, tt);
    task.mcontext.gpr&[0] = buf;
    task.mcontext.gpr&[1] = len;
    
    0
}

yield :: fn(task: *Task) i64 #n(0xFFFF0002) = {
    // return_to_scheduler always cycles through all the tasks so don't need to do anything here. 
    0
}

sig_action :: fn(task: *Task, callee: i64, save_space: i64) i64 #n(0xFFFF0003) = {
    prev := task.signal_handler;
    task.signal_handler = callee;
    task.signal_mcontext = save_space;
    prev
}

sig_return :: fn(task: *Task, saved: i64) i64 #n(0xFFFF0004) = {
    saved := SavedRegisters.ptr_from_int(saved);
    task.mcontext.gpr = saved.gpr;
    task.mcontext.elr = saved.elr;
    task.signal_depth -= 1;
    task.mcontext.gpr&[0]
}

spawn_thread :: fn(kernel: *Kernel, parent: *Task, ctx: i64) i64 #n(0xFFFF0005) = {
    ctx := SavedRegisters.ptr_from_int(ctx);
    child := kernel.alloc_task();
    child.mem = parent.mem;
    child.mcontext.gpr = ctx.gpr;
    child.mcontext.elr = ctx.elr;
    child.kernel_sp = kernel.interrupt_stack.end_pointer();
    child.active = true;
    child.thread_id
}

// TODO: have a more efficient waitpid than calling this in a loop
check_thread :: fn(kernel: *Kernel, id: i64) i64 #n(0xFFFF0006) = {
    each kernel.tasks& { it |
        if it.thread_id == id {
            return it.active.not().int();
        }
    };
    1
}

generate :: fn() []Ty(@FnPtr(task: *Task) void, i64) = {
    results := Ty(@FnPtr(task: *Task) void, i64).list(ast_alloc());
    s := import("@/examples/os/kernel/syscalls.fr");
    for get_constants(s) { name |
        continue :: local_return;
        impl_fid := get_constant(FuncId, s, name).unwrap();
        impl_func := get_function_ast(impl_fid, true, true, true, false);
        loc := impl_func.loc;
        
        i := impl_func.annotations&.index_of(fn(it) => it.name == @symbol n)
            || continue();
        syscall_number := const_eval(i64)(impl_func.annotations[i].args);
        
        fr := current_compiler_context();
        func := empty_fn(impl_func.name, loc);
        func.resolve_scope = s; // :WhatScope
        fill_bindings(func&, fr, @slice(*Task));
        wrapper_fid := fr.intern_func(func&);
        func := get_function_ast(wrapper_fid, true, true, false, false);
        
        the_task := func.arg.bindings[0].name.Var;
        the_task: FatExpr = (expr = (GetVar = the_task), ty = UnknownType, done = false, loc = loc);
        args := FatExpr.list(ast_alloc());
        i := 0;  // choosing this index gets more complicated once its not just for arm
        each impl_func.arg.bindings& { it |
            @switch(it.ty.Finished) {
                @case(*Task) => args&.push(the_task);
                @case(*Kernel) => args&.push(@{ kernel() });
                @case(i64) => {
                    args&.push(@{ @[the_task].mcontext.gpr&[@[@literal i]] });
                    i += 1;
                };
                @default => panic("invalid syscall argument type");
            };
        };
        @debug_assert_le(i, 8, "too many syscall arguments");
        
        args: FatExpr = if args.len == 1 {
            args[0]
        } else {
            (expr = (Tuple = args.as_raw()), ty = UnknownType, done = false, loc = loc)
        };
        func.body = (Normal = @{ 
            x0: i64 = @[@literal impl_fid](@[args]);
            @[the_task].mcontext.gpr&[0] = x0;
        });
        
        callee := fr.get_jitted(wrapper_fid);
        callee := bit_cast_unchecked(rawptr, @FnPtr(task: *Task) void, callee);
        push(results&, (callee, syscall_number));
    };
    results.items()
}
