
Physical :: @struct {
    remaining: []u8;
    free: *Free;
    page_size: i64;
    // TODO: have a scheme that represents contiguous chunks better
    Free :: @rec @struct(link: *Free);
};

fn map(self: *Physical, n: i64) *Physical.Free = {
    ::ptr_utils(@type self.free[]);
    if self.free.is_null() {
        self.free = self.new();
    };
    
    head := self.free;
    prev := head;
    while => n > 0 && !self.free.is_null() {
        prev = self.free;
        self.free = self.free.link;
        n -= 1;
    };
    while => n > 0 {
        it := self.new();
        prev.link = it;
        prev = it;
        n -= 1;
    };
    prev.link = zeroed(*Physical.Free);
    head
}

fn unmap_one(self: *Physical, it: *Physical.Free) void = {
    it.link = self.free;
    self.free = it;
}

fn new(self: *Physical) *Physical.Free = {
    buf := self.map_contiguous(1);
    it := bit_cast_unchecked(*u8, *Physical.Free, buf.ptr);
    it.link = zeroed(*Physical.Free);
    it
}

// TODO: when called directly (not from new()), this should reuse memory
fn map_contiguous(self: *Physical, n: i64) []u8 = {
    n *= self.page_size;
    @if(self.remaining.len < n) kpanic("oom");
    mem := self.remaining.slice(0, n);
    self.remaining = self.remaining.rest(n);
    mem
}

// TODO: permissions
// TODO: spread out the virtual address space more so there's gaps where you fault if you read past the end of a page
fn vmap_reserve(kernel: *Kernel, addrs: *AddressSpace, page_count: i64) i64 = {
    kernel.pages_reserved += page_count;
    vaddr := kernel.next_vaddr;
    @if(page_count < 0) kpanic("-pages");
    @if(vaddr < 0) kpanic("voom");
    kernel.next_vaddr += page_count * kernel.page_size;
    kernel.next_vaddr += 1.shift_left(16);  // bit of padding as a guard page
    {
        vaddr := vaddr;
        range(0, page_count) { _ |
            pte := index_page_table(kernel, addrs, vaddr);
            pte.res&.set(.tag, 0b00);
            pte.res&.set(.reserved, 0b1);            
            vaddr += kernel.page_size;
        };
    };
    
    vaddr
}

// :SLOW
fn vunmap(kernel: *Kernel, addrs: *AddressSpace, vaddr: i64, page_count: i64) void = {
    range(0, page_count) { _ |
        kernel.pages_reserved_freed += 1;
        pte := index_page_table(kernel, addrs, vaddr);
        @switch(pte.map.get(.tag)) {
            @case(0b00) => if pte.res.repr == 0 {
                kpanic("unmapped a page that was already unmapped");  // todo: just log
            };
            @case(0b11) => {
                kernel.pages_committed_freed += 1;
                phys := get_pointer(kernel.page_index_bits, pte[], Physical.Free);
                phys.link = kernel.physical.free;
                kernel.physical.free = phys;
            };
            @default => kpanic("invalid page table");
        };
        pte.res.repr = 0;
        vaddr += kernel.page_size;
    };
}

// this is called by the interrupt handler the first time you try to access a new page
fn vmap_one(kernel: *Kernel, addrs: *AddressSpace, vaddr: i64, phys: *Physical.Free, device: bool) void = {
    kernel.pages_committed += 1;
    pte := index_page_table(kernel, addrs, vaddr);
    if pte.map.repr != 0 {
        kpanic("stomp mapping");
    };
    
    p_addr: u64 = Physical.Free.int_from_ptr(phys).bitcast();
    attr: PageTableEntry = (repr = p_addr);
    attr&.set(.tag, 0b11); // since its a single page, its not 0b01=block
    attr&.set(.idx, int(device));
    attr&.set(.ap, 0b01);  // RW RW
    attr&.set(.sh, 0b11);
    attr&.set(.af, 0b1);
    attr&.set(.pxn, 0b1);  // the kernel code is fixed. never add more. (redundant if it's el0 writable but can't hurt)
    
    pte.map = attr;
    
    // zero fill incase its an old page being reused. 
    bit_cast_unchecked(*Physical.Free, *u8, phys).slice(kernel.page_size).set_bytes(0);
}

fn index_page_table(kernel: *Kernel, addrs: *AddressSpace, vaddr: i64) *PTE = {
    bits := kernel.page_index_bits;
    level := bits.len - 1;
    
    tt := addrs.root;
    while => level != 1 {
        L := make_level_index(bits[level] - 1, bits[level - 1]);
        
        i := L.get(vaddr);
        e := tt.entries&[i].map;
        if e.repr == 0 {
            //table_size := 1.shift_left(L.len);  // TODO: use this to map the right amount
            // this is the first page on this chunk, need to make a next level table for it. 
            mem := kernel.physical&.map_contiguous(size_of(TranslationTable) / kernel.page_size);
            // stored address points to the next level of table
            attr: PageTableEntry = (repr = u8.int_from_ptr(mem.ptr).bitcast());
            attr&.set(.tag, 0b11);  // table entry
            // attrs don't matter here, set for individual pages latter. 
    
            e = attr;
            tt.entries&[i].map = e;
        } else {
            if e.get(.tag) != 0b11 {
                kpanic("L2 table not a table");
            };
        };
        
        tt = get_pointer(kernel.page_index_bits, (map = e), TranslationTable);
        level -= 1;
    };
    L := make_level_index(bits[level] - 1, bits[level - 1]);
    i := L.get(vaddr);
    tt.entries&[i]&
}

fn get_pointer(bits: []i64, it: PTE, $T: Type) *T #generic = {
    TableField := make_level_index(bits[bits.len - 1], bits[0]);
    it := TableField.get(it.map).shift_left(bits[0]);
    it := T.ptr_from_int(it);
    it
} 

AddressSpace :: @struct {
    root: *TranslationTable;  // TTBR0_EL1
};

// the table controls the range [vbase..+step*8192].
// step depends on its level in the tree, vbase depends on its position in the tree. 
TranslationTable :: @struct(entries: Array(PTE, 8192));

BitRange :: @struct(off: i64, len: i64);
fn get(range: BitRange, addr: i64) i64 = {
    mask := 1.shift_left(range.len) - 1;
    addr.shift_right_logical(range.off).bit_and(mask)
}

fn get(range: BitRange, addr: PageTableEntry) i64 = 
    get(range, addr.repr.bitcast());

fn make_level_index(hi: i64, lo: i64) BitRange =
    (off = lo, len = hi - lo + 1);

fn map_identity(kernel: *Kernel, mem: *AddressSpace, mem: []u8, device: bool) void = {
    // TODO: if its in physical memory, make sure its not in remaining/free?
    vaddr := u8.int_from_ptr(mem.ptr);
    end := vaddr + mem.len;
    while => vaddr < end {
        vmap_one(kernel, mem, vaddr, Physical.Free.ptr_from_int(vaddr), device);      
        vaddr += kernel.page_size;
    };
}

setup_virtual_memory :: fn() *TranslationTable = {
    kernel := kernel();
    page_size := kernel.page_size;
    kprint_label("page size = ", page_size);
    
    // translation tables must be aligned to the size of the table.
    mem: AddressSpace = (root = {
        // TODO: not all page tables are full size when not using 64kb pages
        tt := kernel.physical&.map(size_of(TranslationTable) / page_size);
        tt.link = zeroed(*Physical.Free);
        bit_cast_unchecked(*Physical.Free, *TranslationTable, tt)
    });
    tt := mem.root.entries&.items();
    
    mmfr := Aa64Mmfr.sys_get(.ID_AA64MMFR0_EL1);
    // make sure its turned off before we start fucking with the tables
    barrier();
    i64.sys_set(.SCTLR_EL1, 0);
    barrier();
    
    // Memory Attribute Indirection Register
    // each slot is 1 byte, describes a caching behaviour, 
    // page table entries contain an index of which slot to use.
    // [0] = normal, [1..=7] = device nGnRnE
    i64.sys_set(.MAIR_EL1, 0xff);
    
    // TODO: shouldn't it be this: kernel.page_index_bits[kernel.page_index_bits.len - 1];
    nbit := 48; 
    
    p_addr: u64 = 0;
    block_size: u64 = 512*1024*1024;
    // TODO: explicitly map pages for the devices (read addresses from device tree) instead of a massive chunk like this. 
    while => p_addr < block_size * 4 {
    if p_addr < block_size * 3 {
        // low bits are attributes, the address is always aligned so its low bits are zero and don't matter. 
        device := p_addr < 0x40000000;
        idx := int(device);
        attr: PageTableEntry = (repr = p_addr);
        attr&.set(.tag, 0b11);
        attr&.set(.idx, idx);
        // ?? regardless of UXN/PXN, virtual memory that's writable from el0 cannot be executable from el1 ??
        attr&.set(.ap, 0b00);  // -- RW
        attr&.set(.sh, 0b11);
        attr&.set(.af, 0b1);
        if device {
            attr&.set(.pxn, 0b1);
        };
        
        pte := index_page_table(kernel, mem&, p_addr.bitcast());
        pte.map = attr;
    } else {
    // TODO: copy the part for user space instead of letting it write to the kernel part!!
    // remap the first chunk of physical as the second chunk of virtual as R+W for el0. 
    // so el1 can execute that memory by its real address and el0 can use it by this new fake address. 
        old_pte := index_page_table(kernel, mem&, (p_addr - block_size).bitcast())[];
        pte := index_page_table(kernel, mem&, p_addr.bitcast());
        old_pte.map&.set(.ap, 0b01);
        old_pte.map&.set(.pxn, 0b1);  // redundant but can't hurt
        pte[] = old_pte;
    };
        
        p_addr += page_size.bitcast();
    };
    
    // unmap the low addresses to make sure null pointers fault. 
    unmapped := 1.shift_left(26);
    range(0, unmapped / page_size) { i |
        index_page_table(kernel, mem&, i * page_size)[].map.repr = 0;
    };
    
    sys_set(*PTE, .TTBR0_EL1, tt.ptr);
    
    tcr := TcrEl1.zeroed();
    sctlr := SctlrEl1.zeroed();
    
    // note: tg0 and tg1 have different encodings
    @if(page_size == 1.shift_left(16)) tcr&.set(.tg0, 0b01);  // Granule size for the TTBR0_EL1 = 64KB
    @if(page_size == 1.shift_left(14)) tcr&.set(.tg0, 0b10);
    @if(page_size == 1.shift_left(12)) tcr&.set(.tg0, 0b00);
    
    tcr&.set(.ips, mmfr.get(.PARange));
    tcr&.set(.t0sz, 64 - nbit);
    tcr&.set(.ha, 0);
    tcr&.set(.TBI0, 1);  // TopByteIgnore just seems friendly

    sctlr&.set(.EE, 0);  // little endian
    sctlr&.set(.M, 1);  // address translation enabled
    sctlr&.set(.nTWI, 1);
    sctlr&.set(.I, 1);
    sctlr&.set(.C, 1);
    // when FEAT_PAN is implemented and SPAN=0, when you take an exception to el1, 
    // PSTATE.PAN will be set and PrivledgedAccessNever means 
    // "Disables privileged read and write accesses to addresses accessible at EL0"
    sctlr&.set(.span, 1);
    sctlr&.set(.UCI, 1);  // allow cache mantinence at el0
    sctlr&.set(.UCT, 1);  // allow CTR_EL0 at el0
    
    if tracing {
    kprint_label("TCR_EL1: ", tcr.repr.bitcast());
    kprint_label("SCTLR_EL1: ", sctlr.repr.bitcast());
    };
    TcrEl1.sys_set(.TCR_EL1, tcr);
    barrier();
    SctlrEl1.sys_set(.SCTLR_EL1, sctlr);
    barrier();
    
    mem.root
}

choose_page_size :: fn(kernel: *Kernel) void = {
    kernel.page_index_bits = page_index_bits();
    n := 1.shift_left(kernel.page_index_bits[0]);
    kernel.physical.page_size = n;
    kernel.page_size = n;
}

page_index_bits :: fn() []i64 = {
    // (1 << page_index_bits[$i][0]) == page_size
    it :: @const_slice(
        @const_slice(12, 21, 30, 39, 48),  // 4kb
        @const_slice(14, 25, 36, 47, 48),  // 16kb
        @const_slice(16, 29, 42, 52),      // 64kb
    );
    // @run so no relocs needed

    ::[][]i64;
    mmfr := Aa64Mmfr.sys_get(.ID_AA64MMFR0_EL1);
    @if(mmfr.get(.TGran64) == 0) return(@run it[2]);
    @if(mmfr.get(.TGran16) != 0) return(@run it[1]);  // encoding is different, != is not a mistake
    @if(mmfr.get(.TGran4) == 0) return(@run it[0]);
    unreachable();
}

Aa64Mmfr :: @bit_fields(
    PARange := 4,
    _ := 16,
    TGran16 := 4,
    TGran64 := 4,
    TGran4 := 4,
    __ := 32,
);

TcrEl1 :: @bit_fields(
    t0sz := 6,
    res0_epd0_irgn0_orgn0 := 8,
    tg0 := 2,
    t1sz := 6,
    a1_epd1_irgn1_orgn1_sh1 := 8,
    tg1 := 2,
    ips := 3,
    res0_as := 2,
    TBI0 := 1,
    TBI1 := 1,
    ha := 1,
    _ := 24,
);

SctlrEl1 :: @bit_fields(
    M := 1,
    A := 1,
    C := 1,
    _____ := 9,
    I := 1,
    ____ := 2,
    UCT := 1,
    nTWI := 1,
    _ := 6,
    span := 1,
    ___ := 1,
    EE := 1,
    UCI := 1,
    __ := 37,
);

PTE :: @union(
    map: PageTableEntry,
    res: PagePlaceholder,
);

PageTableEntry :: @bit_fields(
    tag := 2,  // 0b00 = invalid, 0b01 = block, 0b11 = page or next table
    idx := 3,  // index in MAIR_EL1
    ns := 1,
    ap := 2,  // access permission (el0 el1; 0b00 = -- RW, 0b01 = RW RW, 0b10 = -- R-, 0b11 = R- R-)
    sh := 2,  // shareable
    af := 1,  // access flag
    
    // physical address of next table or page base.
    // (shifted; the low attribute bits are naturally ignored because the address is always aligned to 4k or higher)
    value := 42,  

    pxn := 1,   // privileged execute never
    uxn := 1,   // unprivileged execute never
    soft := 4,  // "reserved for software use"
    _ := 5,
);

// when PageTableEntry.tag=0b00, its always invalid so i can put whatever i want in the other bits. 
// so when you mmap, just reserve a range of virtual memory and remember the flags it was mapped with, 
// and the first time you try to access it and fault, allocate a zero filled physical page. 
PagePlaceholder :: @bit_fields(
    tag := 2,
    reserved := 1,
    _ := 61,
);

#use("@/lib/bit_fields.fr");
