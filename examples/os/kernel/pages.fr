
Physical :: @struct {
    remaining: []u8;
    free: *Free;
    page_size: i64;
    // TODO: have a scheme that represents contiguous chunks better
    Free :: @rec @struct(link: *Free);
};

fn map(self: *Physical, n: i64) *Physical.Free = {
    ::ptr_utils(@type self.free[]);
    if self.free.is_null() {
        self.free = self.new();
    };
    
    head := self.free;
    prev := head;
    while => n > 0 && !self.free.is_null() {
        prev = self.free;
        self.free = self.free.link;
        n -= 1;
    };
    while => n > 0 {
        it := self.new();
        prev.link = it;
        prev = it;
        n -= 1;
    };
    prev.link = zeroed(*Physical.Free);
    head
}

fn unmap_one(self: *Physical, it: *Physical.Free) void = {
    it.link = self.free;
    self.free = it;
}

fn new(self: *Physical) *Physical.Free #inline = {
    buf := self.map_contiguous(self.page_size, 1);
    it := bit_cast_unchecked(*u8, *Physical.Free, buf.ptr);
    it.link = zeroed(*Physical.Free);
    it
}

// TODO: when called directly (not from new()), this should reuse memory
fn map_contiguous(self: *Physical, n: i64, align: i64) []u8 #inline = {
    if align > self.page_size {
        base := u8.int_from_ptr(self.remaining.ptr);
        waste := base.ualign_to(align) - base;
        self.remaining = self.remaining.rest(waste);  // TODO: add the gap to .free
    };
    
    n = n.align_to(self.page_size);
    @if(self.remaining.len < n) kpanic("oom");
    mem := self.remaining.slice(0, n);
    self.remaining = self.remaining.rest(n);
    mem
}

// TODO: permissions
fn vmap_reserve(addrs: *AddressSpace, page_count: i64) i64 = {
    kernel.pages_reserved += page_count;
    vaddr := kernel.next_vaddr;
    @if(page_count < 0) kpanic("-pages");
    @if(vaddr < 0) kpanic("voom");
    kernel.next_vaddr += page_count * kernel.page_size;
    kernel.next_vaddr += 1.shift_left(16);  // bit of padding as a guard page
    {
        vaddr := vaddr;
        range(0, page_count) { _ |
            pte := index_page_table(addrs, vaddr);
            pte.res&.set(.tag, 0b00);
            pte.res&.set(.reserved, 0b1);            
            vaddr += kernel.page_size;
        };
    };
    
    vaddr
}

// :SLOW
fn vunmap(addrs: *AddressSpace, vaddr: i64, page_count: i64) void = {
    range(0, page_count) { _ |
        kernel.pages_reserved_freed += 1;
        pte := index_page_table(addrs, vaddr);
        @switch(pte.map.get(.tag)) {
            @case(0b00) => if pte.res.repr == 0 {
                kpanic("unmapped a page that was already unmapped");  // todo: just log
            };
            @case(0b11) => {
                kernel.pages_committed_freed += 1;
                phys := get_pointer(kernel.page_index_bits, pte[], *Physical.Free);
                phys.link = kernel.physical.free;
                kernel.physical.free = phys;
            };
            @default => kpanic("invalid page table");
        };
        pte.res.repr = 0;
        vaddr += kernel.page_size;
    };
}

// TODO: use flags instead of crazy bools
// this is called by the interrupt handler the first time you try to access a new page
fn vmap_one(addrs: *AddressSpace, vaddr: i64, phys: *Physical.Free, user: bool, pxn: bool, device: bool, zerofill: bool) void = {
    kernel.pages_committed += 1;
    pte := index_page_table(addrs, vaddr);
    if pte.map.repr != 0 {
        kprint_label("vaddr ", vaddr);
        kprint_label("paddr ", Physical.Free.int_from_ptr(phys));
        kpanic("stomp mapping");
    };
    
    p_addr: u64 = Physical.Free.int_from_ptr(phys).bitcast();
    attr: PageTableEntry = (repr = p_addr);
    attr&.set(.tag, 0b11); // since its a single page, its not 0b01=block
    attr&.set(.idx, int(device));
    if user {
        attr&.set(.ap, 0b01);  // RW RW
    } else {
        attr&.set(.ap, 0b00);  // -- RW
    };
    attr&.set(.sh, 0b11);
    attr&.set(.af, 0b1);
    if pxn {
        attr&.set(.pxn, 0b1);  // the kernel code is fixed. never add more. (redundant if it's el0 writable but can't hurt)
    };
    
    pte.map = attr;
    
    // zero fill incase its an old page being reused. 
    if zerofill {
        bit_cast_unchecked(*Physical.Free, *u8, phys).slice(kernel.page_size).set_bytes(0);
    }
}

fn index_page_table(addrs: *AddressSpace, vaddr: i64) *PTE = {
    bits := kernel.page_index_bits;
    level := bits.len - 1;
    
    tt := addrs.root;
    while => level != 1 {
        L := make_level_index(bits[level] - 1, bits[level - 1]);
        
        i := L.get(vaddr);
        e := tt.entries&[i].map;
        if e.repr == 0 {
            //table_size := 1.shift_left(L.len);  // TODO: use this to map the right amount
            // this is the first page on this chunk, need to make a next level table for it. 
            mem := kernel.physical&.map_contiguous(size_of(TranslationTable), size_of(TranslationTable));
            mem.set_zeroed();
            // stored address points to the next level of table
            attr: PageTableEntry = (repr = u8.int_from_ptr(mem.ptr).bitcast());
            attr&.set(.tag, 0b11);  // table entry
            // attrs don't matter here, set for individual pages latter. 
    
            e = attr;
            tt.entries&[i].map = e;
        } else {
            if e.get(.tag) != 0b11 {
                kpanic("L2 table not a table");
            };
        };
        
        tt = get_pointer(kernel.page_index_bits, (map = e), *TranslationTable);
        level -= 1;
    };
    L := make_level_index(bits[level] - 1, bits[level - 1]);
    i := L.get(vaddr);
    tt.entries&[i]&
}

// TODO: does doing this from vunmap,vmap_reserve,map_identity matter? it makes qemu -accel tcg much slower
fn clear_tlb(va: i64) void = {
    i64.sys_set(.TLBI_VALE1IS, va.shift_right_logical(12));
}

fn get_pointer(bits: []i64, it: PTE, $T: Type) T #generic = {
    TableField := make_level_index(bits[bits.len - 1], bits[0]);
    it := TableField.get(it.map).shift_left(bits[0]);
    bit_cast_unchecked(i64, T, it)
} 

fn v_to_p(mem: *AddressSpace, vaddr: i64) i64 = {
    @if(!kernel.mmu_enabled) return(vaddr);
    v_base := vaddr.align_back(kernel.page_size);
    pte := index_page_table(mem, v_base)[];
    if pte.map.get(.tag) != 0b11 {
        kpanic("v_to_p: unmapped");
    };
    p_base := get_pointer(kernel.page_index_bits, pte, i64);
    p_base + (vaddr - v_base)
}

AddressSpace :: @struct {
    root: *TranslationTable;  // TTBR0_EL1
};

// the table controls the range [vbase..+step*8192].
// step depends on its level in the tree, vbase depends on its position in the tree. 
TranslationTable :: @struct(entries: Array(PTE, 8192));

BitRange :: @struct(off: i64, len: i64);
fn get(range: BitRange, addr: i64) i64 = {
    mask := 1.shift_left(range.len) - 1;
    addr.shift_right_logical(range.off).bit_and(mask)
}

fn get(range: BitRange, addr: PageTableEntry) i64 = 
    get(range, addr.repr.bitcast());

fn make_level_index(hi: i64, lo: i64) BitRange =
    (off = lo, len = hi - lo + 1);

fn map_identity(addrs: *AddressSpace, mem: []u8, user: bool, pxn: bool, device: bool) void = {
    // TODO: if its in physical memory, make sure its not in remaining/free?
    vaddr := u8.int_from_ptr(mem.ptr);
    end := vaddr + mem.len;
    while => vaddr < end {
        vmap_one(addrs, vaddr, Physical.Free.ptr_from_int(vaddr), user, pxn, device, false);      
        vaddr += kernel.page_size;
    };
}

setup_virtual_memory :: fn(ram: []u8, pci: ?Pci.MemoryRegion) *TranslationTable = {
    page_size := kernel.page_size;
    
    // translation tables must be aligned to the size of the table.
    mem: AddressSpace = (root = {
        // TODO: not all page tables are full size when not using 64kb pages
        mem := kernel.physical&.map_contiguous(size_of(TranslationTable), size_of(TranslationTable));
        mem.set_zeroed();
        bit_cast_unchecked(*u8, *TranslationTable, mem.ptr)
    });
    tt := mem.root.entries&.items();
    
    mmfr := Aa64Mmfr.sys_get(.ID_AA64MMFR0_EL1);
    // make sure its turned off before we start fucking with the tables
    barrier();
    i64.sys_set(.SCTLR_EL1, 0);
    barrier();
    
    // Memory Attribute Indirection Register
    // each slot is 1 byte, describes a caching behaviour, 
    // page table entries contain an index of which slot to use.
    // [0] = normal, [1..=7] = device nGnRnE
    i64.sys_set(.MAIR_EL1, 0xff);
    
    // TODO: shouldn't it be this: kernel.page_index_bits[kernel.page_index_bits.len - 1];
    nbit := 48; 
    
    // map everything below ram as device memory for kernel
    map_identity(mem&, u8.ptr_from_int(0).slice(u8.int_from_ptr(ram.ptr)), false, true, true);
    // TODO: only kimage should be kernel executable
    // map the whole ram into kernel memory
    map_identity(mem&, ram, false, false, false);
    
    if pci& { pci |
        if u8.int_from_ptr(pci.config.ptr) > u8.int_from_ptr(ram.ptr) {
            @if(u8.int_from_ptr(pci.config.ptr) < u8.int_from_ptr(ram.end_pointer())) kpanic("TODO: pci config overlaps ram");
            map_identity(mem&, pci.config, false, true, true);
        } else {
            @if(u8.int_from_ptr(pci.config.end_pointer()) > u8.int_from_ptr(ram.ptr)) kpanic("TODO: pci config overlaps ram");
        };
        // TODO: catch mistakes by only mapping the parts of bar that are allocated
        if u8.int_from_ptr(pci.bar.ptr) > u8.int_from_ptr(ram.ptr) {
            @if(u8.int_from_ptr(pci.bar.ptr) < u8.int_from_ptr(ram.end_pointer())) kpanic("TODO: pci bar overlaps ram");
            map_identity(mem&, pci.bar, false, true, true);
        } else {
            @if(u8.int_from_ptr(pci.bar.end_pointer()) > u8.int_from_ptr(ram.ptr)) kpanic("TODO: pci bar overlaps ram");
        };
    };
    
    // map a copy of the init image into user memory
    // TODO: make it virtual instead of physical
    kernel.user_image = kernel.physical&.map_contiguous(kernel.kernel_image.len, 1);
    kernel.user_image.copy_from(kernel.kernel_image);
    range(0, kernel.user_image.len / kernel.page_size) { i |
        pte := index_page_table(mem&, u8.int_from_ptr(kernel.user_image.ptr) + i * page_size);
        pte.map&.set(.pxn, 1);  // redundant but can't hurt
        pte.map&.set(.ap, 0b01);  // RW RW
    };
    // TODO: unmap the kernel copy of elf_bytes so that physical memory can be reused
    
    kernel.pages_committed = 0;
    kernel.pages_reserved = 0;
    
    // unmap the low addresses to make sure null pointers fault. 
    unmapped := 1.shift_left(20);
    range(0, unmapped / page_size) { i |
        index_page_table(mem&, i * page_size)[].map.repr = 0;
    };
    
    sys_set(*PTE, .TTBR0_EL1, tt.ptr);
    
    tcr := TcrEl1.zeroed();
    sctlr := SctlrEl1.zeroed();
    
    // note: tg0 and tg1 have different encodings
    @if(page_size == 1.shift_left(16)) tcr&.set(.tg0, 0b01);  // Granule size for the TTBR0_EL1 = 64KB
    @if(page_size == 1.shift_left(14)) tcr&.set(.tg0, 0b10);
    @if(page_size == 1.shift_left(12)) tcr&.set(.tg0, 0b00);
    
    tcr&.set(.ips, mmfr.get(.PARange));
    tcr&.set(.t0sz, 64 - nbit);
    tcr&.set(.ha, 0);
    tcr&.set(.TBI0, 1);  // TopByteIgnore just seems friendly

    sctlr&.set(.EE, 0);  // little endian
    sctlr&.set(.M, 1);  // address translation enabled
    sctlr&.set(.nTWI, 1);
    sctlr&.set(.I, 1);
    sctlr&.set(.C, 1);
    // when FEAT_PAN is implemented and SPAN=0, when you take an exception to el1, 
    // PSTATE.PAN will be set and PrivledgedAccessNever means 
    // "Disables privileged read and write accesses to addresses accessible at EL0"
    sctlr&.set(.span, 1);
    sctlr&.set(.UCI, 1);  // allow cache mantinence at el0
    sctlr&.set(.UCT, 1);  // allow CTR_EL0 at el0
    
    if tracing {
    kprint_label("TCR_EL1: ", tcr.repr.bitcast());
    kprint_label("SCTLR_EL1: ", sctlr.repr.bitcast());
    };
    TcrEl1.sys_set(.TCR_EL1, tcr);
    barrier();
    SctlrEl1.sys_set(.SCTLR_EL1, sctlr);
    barrier();
    kernel.mmu_enabled = true;
    kprint("mmu on\n");
    
    mem.root
}

choose_page_size :: fn() void = {
    kernel.page_index_bits = page_index_bits();
    n := 1.shift_left(kernel.page_index_bits[0]);
    kernel.physical.page_size = n;
    kernel.page_size = n;
}

page_index_bits :: fn() []i64 = {
    // (1 << page_index_bits[$i][0]) == page_size
    it :: @const_slice(
        @const_slice(12, 21, 30, 39, 48),  // 4kb
        @const_slice(14, 25, 36, 47, 48),  // 16kb
        @const_slice(16, 29, 42, 52),      // 64kb
    );
    // @run so no relocs needed

    ::[][]i64;
    mmfr := Aa64Mmfr.sys_get(.ID_AA64MMFR0_EL1);
    @if(mmfr.get(.TGran64) == 0) return(@run it[2]);
    @if(mmfr.get(.TGran16) != 0) return(@run it[1]);  // encoding is different, != is not a mistake
    @if(mmfr.get(.TGran4) == 0) return(@run it[0]);
    unreachable();
}

Aa64Mmfr :: @bit_fields(
    PARange := 4,
    _ := 16,
    TGran16 := 4,
    TGran64 := 4,
    TGran4 := 4,
    __ := 32,
);

TcrEl1 :: @bit_fields(
    t0sz := 6,
    res0_epd0_irgn0_orgn0 := 8,
    tg0 := 2,
    t1sz := 6,
    a1_epd1_irgn1_orgn1_sh1 := 8,
    tg1 := 2,
    ips := 3,
    res0_as := 2,
    TBI0 := 1,
    TBI1 := 1,
    ha := 1,
    _ := 24,
);

SctlrEl1 :: @bit_fields(
    M := 1,
    A := 1,
    C := 1,
    _____ := 9,
    I := 1,
    ____ := 2,
    UCT := 1,
    nTWI := 1,
    _ := 6,
    span := 1,
    ___ := 1,
    EE := 1,
    UCI := 1,
    __ := 37,
);

PTE :: @union(
    map: PageTableEntry,
    res: PagePlaceholder,
);

PageTableEntry :: @bit_fields(
    tag := 2,  // 0b00 = invalid, 0b01 = block, 0b11 = page or next table
    idx := 3,  // index in MAIR_EL1
    ns := 1,
    ap := 2,  // access permission (el0 el1; 0b00 = -- RW, 0b01 = RW RW, 0b10 = -- R-, 0b11 = R- R-)
    sh := 2,  // shareable
    af := 1,  // access flag
    
    // physical address of next table or page base.
    // (shifted; the low attribute bits are naturally ignored because the address is always aligned to 4k or higher)
    value := 42,  

    pxn := 1,   // privileged execute never
    uxn := 1,   // unprivileged execute never
    soft := 4,  // "reserved for software use"
    _ := 5,
);

// when PageTableEntry.tag=0b00, its always invalid so i can put whatever i want in the other bits. 
// so when you mmap, just reserve a range of virtual memory and remember the flags it was mapped with, 
// and the first time you try to access it and fault, allocate a zero filled physical page. 
PagePlaceholder :: @bit_fields(
    tag := 2,
    reserved := 1,
    _ := 61,
);

#use("@/lib/bit_fields.fr");
