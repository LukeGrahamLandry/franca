
Physical :: @struct {
    remaining: []u8;
    free: *Free;
    // TODO: have a scheme that represents contiguous chunks better
    Free :: @rec @struct(link: *Free);
};

fn map(self: *Physical, n: i64) *Physical.Free = {
    ::ptr_utils(@type self.free[]);
    if self.free.is_null() {
        self.free = self.new();
    };
    
    head := self.free;
    prev := head;
    while => n > 0 && !self.free.is_null() {
        prev = self.free;
        self.free = self.free.link;
        n -= 1;
    };
    while => n > 0 {
        it := self.new();
        prev.link = it;
        prev = it;
        n -= 1;
    };
    prev.link = zeroed(*Physical.Free);
    head
}

fn unmap_one(self: *Physical, it: *Physical.Free) void = {
    it.link = self.free;
    self.free = it;
}

fn new(self: *Physical) *Physical.Free = {
    @if(self.remaining.len < PAGE_SIZE) kpanic("oom");
    it := bit_cast_unchecked(*u8, *Physical.Free, self.remaining.ptr);
    self.remaining = self.remaining.rest(PAGE_SIZE);
    it.link = zeroed(*Physical.Free);
    it
}

// TODO: permissions
// TODO: spread out the virtual address space more so there's gaps where you fault if you read past the end of a page
fn vmap_reserve(kernel: *Kernel, addrs: *AddressSpace, page_count: i64) i64 = {
    kernel.pages_reserved += page_count;
    vaddr := kernel.next_vaddr;
    @if(page_count < 0) kpanic("-pages");
    @if(vaddr < 0) kpanic("voom");
    kernel.next_vaddr += page_count * PAGE_SIZE;
    {
        vaddr := vaddr;
        range(0, page_count) { _ |
            pte := index_page_table(kernel, addrs, vaddr);
            pte.res&.set(.tag, 0b00);
            pte.res&.set(.reserved, 0b1);            
            vaddr += PAGE_SIZE;
        };
    };
    
    vaddr
}

// this is called by the interrupt handler the first time you try to access a new page
fn vmap_one(kernel: *Kernel, addrs: *AddressSpace, vaddr: i64, phys: *Physical.Free) void = {
    kernel.pages_committed += 1;
    pte := index_page_table(kernel, addrs, vaddr);
    if pte.map.repr != 0 {
        kpanic("stomp mapping");
    };
    
    p_addr: u64 = Physical.Free.int_from_ptr(phys).bitcast();
    attr: PageTableEntry = (repr = p_addr);
    attr&.set(.tag, 0b11); // since its a single page, its not 0b01=block
    attr&.set(.idx, 0);  // normal
    attr&.set(.ap, 0b01);  // RW RW
    attr&.set(.sh, 0b11);
    attr&.set(.af, 0b1);
    attr&.set(.pxn, 0b1);  // the kernel code is fixed. never add more. (redundant if it's el0 writable but can't hurt)
    
    pte.map = attr;
    
    // zero fill incase its an old page being reused. 
    bit_cast_unchecked(*Physical.Free, *u8, phys).slice(PAGE_SIZE).set_bytes(0);
}

fn index_page_table(kernel: *Kernel, addrs: *AddressSpace, vaddr: i64) *PTE = {
    L2 := make_level_index(41, 29);
    L3 := make_level_index(28, 16);
    TableField := make_level_index(48, 12);
    
    tt_2 := addrs.root&;
    i_2 := L2.get(vaddr);
    e_2 := tt_2.entries&[i_2].map;
    if e_2.repr == 0 {
        // this is the first page on this 512mb chunk, need to make a next level table for it. 
        mem := kernel.physical&.map(1);
        mem.link = zeroed(*Physical.Free);
        // stored address points to the L3 table
        attr: PageTableEntry = (repr = Physical.Free.int_from_ptr(mem).bitcast());
        attr&.set(.tag, 0b11);  // table entry
        // attrs don't matter here, set for individual pages latter. 

        e_2 = attr;
        tt_2.entries&[i_2].map = e_2;
    } else {
        if e_2.get(.tag) != 0b11 {
            kpanic("L2 table not a table");
        };
    };
    
    tt_3 := TableField.get(e_2).shift_left(12);
    tt_3 := TranslationTable.ptr_from_int(tt_3);
    i_3 := L3.get(vaddr);

    if i_3.shift_left(16).bit_or(i_2.shift_left(29)) != vaddr {
        kprint_label("vaddr ", vaddr);
        kpanic("unaligned page vaddr");
    };
    
    tt_3.entries&[i_3]&
}

PAGE_SIZE :: 1.shift_left(16);

AddressSpace :: @struct {
    root: *TranslationTable;  // TTBR0_EL1
};

// the table controls the range [vbase..+step*8192].
// step depends on its level in the tree, vbase depends on its position in the tree. 
TranslationTable :: @struct(entries: Array(PTE, 8192));

BitRange :: @struct(off: u8, len: u8);
fn get(range: BitRange, addr: i64) i64 = {
    mask := 1.shift_left(range.len.zext()) - 1;
    addr.shift_right_logical(range.off.zext()).bit_and(mask)
}

fn get(range: BitRange, addr: PageTableEntry) i64 = 
    get(range, addr.repr.bitcast());

fn make_level_index(hi: u8, lo: u8) BitRange =
    (off = lo, len = hi - lo + 1);

setup_virtual_memory :: fn() *TranslationTable = {
    translation_table :: @static(Array(i64, 8192*2));
    // translation tables must be aligned to the size of the table.
    tt := align_the_thing(translation_table, 1.shift_left(16), 8192*8);
    tt := i64.ptr_from_raw(tt).slice(8192);
    // TODO: check ID_AA64MMFR0_EL1 to make sure the granule size i want is supported
    
    parange := i64.sys_get(.ID_AA64MMFR0_EL1).bit_and(0b111);
    
    // make sure its turned off before we start fucking with the tables
    barrier();
    i64.sys_set(.SCTLR_EL1, 0);
    barrier();
    
    // Memory Attribute Indirection Register
    // each slot is 1 byte, describes a caching behaviour, 
    // page table entries contain an index of which slot to use.
    // [0] = normal, [1..=7] = device nGnRnE
    i64.sys_set(.MAIR_EL1, 0xff);
    
    // Translation Table Base Register
    // "Together, the granule and the size of the virtual address space control the starting level of address translation."
    // 64KB granule, level 2 table has 512MB per entry, indexed by bits 41:29, 
    // so with a 42 bit address space, theres only one level of table (a Level Two table) because theres no bits left over.  
    nbit := 42;
    p_addr: u64 = 0;
    block_size: u64 = 512*1024*1024;
    each tt.slice(0, 4) { it |
        // low bits are attributes, the address is always aligned so its low bits are zero and don't matter. 
        device := p_addr < 0x40000000;
        idx := int(device);
        // block type, mair index, permissions, sharable, access flag
        attr: PageTableEntry = (repr = p_addr);
        attr&.set(.tag, 0b01);
        attr&.set(.idx, idx);
        // ?? regardless of UXN/PXN, virtual memory that's writable from el0 cannot be executable from el1 ??
        if !device {
            attr&.set(.ap, 0b00);  // -- RW
        } else {
            attr&.set(.ap, 0b01);  // RW RW
        };
        attr&.set(.sh, 0b11);
        attr&.set(.af, 0b1);
        
        it[] = attr.repr.bitcast();
        p_addr += block_size;
    };
    each tt.rest(4) { it |
        it[] = 0;
    };
    // TODO: should unmap the low addresses to make sure null pointers fault. 
    // TODO: copy the part for user space instead of letting it write to the kernel part!!
    // remap the first chunk of physical as the second chunk of virtual as R+W for el0. 
    // so el1 can execute that memory by its real address and el0 can use it by this new fake address. 
    xxx := tt[(0x40000000/block_size.bitcast()) + 1]&;
    attr: PageTableEntry = (repr = xxx[].bitcast());
    attr&.set(.ap, 0b01);
    attr.repr -= block_size;
    xxx[] = attr.repr.bitcast();
    barrier();

    sys_set(*i64, .TTBR0_EL1, tt.ptr);
    
    tcr := TcrEl1.zeroed();
    sctlr := SctlrEl1.zeroed();
    
    // note: tg0 and tg1 have different encodings
    tcr&.set(.tg0, 0b01);  // Granule size for the TTBR0_EL1 = 64KB
    tcr&.set(.ips, parange);
    tcr&.set(.t0sz, 64 - nbit);
    tcr&.set(.ha, 0);
    tcr&.set(.TBI0, 1);  // TopByteIgnore just seems friendly

    sctlr&.set(.EE, 0);  // little endian
    sctlr&.set(.M, 1);  // address translation enabled
    sctlr&.set(.nTWI, 1);
    sctlr&.set(.I, 1);
    sctlr&.set(.C, 1);
    // when FEAT_PAN is implemented and SPAN=0, when you take an exception to el1, 
    // PSTATE.PAN will be set and PrivledgedAccessNever means 
    // "Disables privileged read and write accesses to addresses accessible at EL0"
    sctlr&.set(.span, 1);
    
    if tracing {
    kprint_label("TCR_EL1: ", tcr.repr.bitcast());
    kprint_label("SCTLR_EL1: ", sctlr.repr.bitcast());
    };
    TcrEl1.sys_set(.TCR_EL1, tcr);
    barrier();
    SctlrEl1.sys_set(.SCTLR_EL1, sctlr);
    barrier();
    
    kernel := kernel();
    bit_cast_unchecked(*i64, *TranslationTable, tt.ptr)
}

#use("@/lib/bit_fields.fr");

TcrEl1 :: @bit_fields(
    t0sz := 6,
    res0_epd0_irgn0_orgn0 := 8,
    tg0 := 2,
    t1sz := 6,
    a1_epd1_irgn1_orgn1_sh1 := 8,
    tg1 := 2,
    ips := 3,
    res0_as := 2,
    TBI0 := 1,
    TBI1 := 1,
    ha := 1,
    _ := 24,
);

SctlrEl1 :: @bit_fields(
    M := 1,
    A := 1,
    C := 1,
    _____ := 9,
    I := 1,
    ____ := 3,
    nTWI := 1,
    _ := 6,
    span := 1,
    ___ := 1,
    EE := 1,
    __ := 38,
);

PTE :: @union(
    map: PageTableEntry,
    res: PagePlaceholder,
);

PageTableEntry :: @bit_fields(
    tag := 2,  // 0b00 = invalid, 0b01 = block, 0b11 = page or next table
    idx := 3,  // index in MAIR_EL1
    ns := 1,
    ap := 2,  // access permission (el0 el1; 0b00 = -- RW, 0b01 = RW RW, 0b10 = -- R-, 0b11 = R- R-)
    sh := 2,  // shareable
    af := 1,  // access flag
    
    // physical address of next table or page base.
    // (shifted; the low attribute bits are naturally ignored because the address is always aligned to 4k or higher)
    value := 42,  

    pxn := 1,   // privledged execute never
    uxn := 1,   // unprivledged execute never
    soft := 4,  // "reserved for software use"
    _ := 5,
);

// when PageTableEntry.tag=0b00, its always invalid so i can put whatever i want in the other bits. 
// so when you mmap, just reserve a range of virtual memory and remember the flags it was mapped with, 
// and the first time you try to access it and fault, allocate a zero filled physical page. 
PagePlaceholder :: @bit_fields(
    tag := 2,
    reserved := 1,
    _ := 61,
);
