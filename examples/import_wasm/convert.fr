
// convert a wasm module into a Qbe.Module. 
// an important constraint is that anything unique to an instantiation 
// (imports, global/table storage) must be passed around at runtime,
// not baked into the code. this allows compiling once an instantiating 
// multiple times without a linking step. 

Loaded :: @rec @struct {
    datas: []Data;
    elems: []Elem;
    global_cls: []Qbe.Cls;  // including imports
    init_global_values: []Rt.GlobalValue;  // not including imports
    init_table_sizes: []i64;  // not including imports
    rt_needed: RtNeeded;
    counts: Rt.Counts;
    functions: []Qbe.Sym;  // [padding for imports, ...exports]
    function_types: []Wasm.TypeIdx;
};

RtNeeded :: StaticBitSet(collect_runtime_calls().len());
Data :: @struct(id: Qbe.Sym, size: i64, active_at: ?i64);
Elem :: @struct(functions: []i64, active_at: ?@struct(table: i64, offset: i64));

fn load(m: *Qbe.Module, w: *WasmModule, a: Alloc) Loaded = {
    ::require_layout_ready(LoadWasm);
    counts := zeroed Rt.Counts;
    counts&[.imports] = w.import_count&[.Func];
    counts&[.tables] = w.tables.len + w.import_count&[.Table];
    counts&[.runtime] = RtNeeded.capacity;
    counts&[.globals] = w.globals.len + w.import_count&[.Global];
    counts&[.local_globals] = w.globals.len;
    counts&[.local_tables] = w.tables.len;
    
    global_cls := a.alloc_uninit(Qbe.Cls, counts&[.globals]);
    {
        i := 0;
        each w.imports& { import |
            @if_let(import.desc&) fn Global(it) => {
                global_cls[i] = it.ref_type.cls();
                i += 1;
            };
        };
        @debug_assert_eq(i, w.import_count&[.Global]);
    };

    functions, function_types, function_names := declare_functions(m, w, a);
    
    loaded: Loaded = (
        rt_needed = zeroed(RtNeeded),
        counts = counts,
        functions = functions,
        function_types = function_types,
        init_global_values = a.alloc_init(Rt.GlobalValue, w.globals.len) { local_i |
            global := w.globals.index(local_i);
            value, k := const_eval_wasm_expr(global.init_expr.items());
            global_cls[w.import_count&[.Global] + local_i] = k;
            @debug_assert_eq(k, global.type.cls());
            (i64 = value)
        },
        global_cls = global_cls,
        init_table_sizes = a.alloc_init(i64, w.tables.len) { i |
            w.tables[i].limits.min.zext()
        },
        elems = a.alloc_init(Elem, w.elements.len) { i |
            elem := w.elements.index(i);
            functions := a.alloc_init(i64, elem.funcs.len, fn(i) => elem.funcs[i].id.zext());
            (functions = functions, active_at = @if(elem.active_offset_expr.len == 0, .None, {
                off, k := const_eval_wasm_expr(elem.active_offset_expr.items());
                @assert(k == .Kl || k == .Kw, "expected integer not % for elem[%] active_offset_expr", k, i);
                @assert(elem.table.id.zext() < counts&[.tables], "active elem has oob table index");
                (Some = (table = elem.table.id.zext(), offset = off))
            }))
        },
        datas = a.alloc_init(Data, w.data.len) { i |
            data := w.data.index(i);
            expr := data.active_offset_expr;
            off: ?i64 = @if(expr.len == 0, .None, {
                off, k := const_eval_wasm_expr(expr.items());
                @debug_assert_eq(k, .Kw, "offset expression");
                @debug_assert_eq(data.active_mem.id, 0, "TODO: multiple memories");
                (Some = off)
            });
            id := m.intern(@tfmt("data%@W", i));
            m.Qbe'backend'compile_dat(template = (Bytes = data.bytes.items()), id = id, relocations = empty());
            (id = id, size = data.bytes.len, active_at = off)
        },
    );
    
    range(0, w.code.len) { i |
        translate_func(w, m, loaded&, function_names, i);
    };
    
    loaded
};

fn declare_functions(m: *Qbe.Module, w: *WasmModule, a: Alloc) Ty([]Qbe.Sym, []Wasm.TypeIdx, []Str) = {
    count := w.functions.len + w.import_count&[.Func];
    ids := a.alloc_uninit(Qbe.Sym, count); // :Leak
    types := a.alloc_uninit(Wasm.TypeIdx, count);
    names := a.alloc_zeroed(Str, count);
    f_i := 0;
    each w.imports& { import |
        @if_let(import.desc&) fn Function(id) => {
            ids[f_i] = Qbe.no_symbol_S;  // called through Rt.Instance instead
            types[f_i] = id[];
            names[f_i] = @tfmt("%@%", import.name.items(), import.module.items());
            f_i += 1;
        };
    };
    range(0, w.functions.len) { i |
        id := m.intern(@tfmt("F%@W", f_i));;
        ids[f_i] = id;
        names[f_i] = m.str(id);
        types[f_i] = w.functions[i];
        f_i += 1;
    };
    @debug_assert_eq(count, f_i);    
    
    // actually, if it's an export it's nicer to use the real names. TODO: same thing for globals
    each w.exports& { it |
        if it.type == .Func && it.id.zext() >= w.import_count&[.Func] {
            id := m.intern(it.name.items());
            ids[it.id.zext()] = id;
            names[it.id.zext()] = m.str(id);
        };
    };
    
    (ids, types, names)
}

fn const_eval_wasm_expr(expr: []u8) Ty(i64, Qbe.Cls) = {
    o := @as(Wasm.Inst) expr[0];
    v, n, k := @match(o) {
        fn F32_Const() => (u32.cast_front_unaligned(expr.rest(1)).zext(), 4, .Ks);
        fn F64_Const() => (i64.cast_front_unaligned(expr.rest(1)), 8, .Kd)
        @default => {
            @assert(@is(o, .I32_Const, .I64_Const), "TODO: complex expr %", o);
            v, r, ok := read_leb128_signed(expr.rest(1));
            @debug_assert(ok, "bad constant expression");
            (v, expr.len - r.len - 1, @if(o == .I32_Const, Qbe.Cls.Kw, .Kl))
        };
    };
    @debug_assert_eq(expr[1 + n], Wasm.Inst.End.raw(), "TODO: complex expr (expected End after %)", o);
    (v, k)
}

// the rest of this file handles compiling one wasm function body to one Qbe.Fn. 
// stack -> ssa, locals -> mutable temporaries, structured control flow -> cfg, block params -> phi nodes. 

LoadWasm :: @struct {
    m: *Qbe.Module;
    w: *WasmModule;
    src: []u8;
    cursor: i64;
    function_names: []Str;
    
    stack: List(Qbe.Ref);
    blocks: List(BlockEntry);
    locals: []Qbe.Ref;  // TODO: we can't store a 128 bit vector in a temporary 
    f: *Qbe.Fn;
    local_rt_header: Qbe.Ref;
    local_memory_base: Qbe.Ref;
    return_types: []Wasm.ValType;
    link: **Qbe.Blk;
    
    loaded: *Loaded #use;
};

fn translate_func(w: *WasmModule, m: *Qbe.Module, loaded: *Loaded, function_names: []Str, local_func_i: i64) void = {
    mark := mark_temporary_storage();
    f := @ref Qbe.Fn.zeroed();
    f.default_init(m);
    
    global_func_i := local_func_i + w.import_count&[.Func];
    f.lnk.id = loaded.functions[global_func_i];
    type_index := loaded.function_types[global_func_i];
    signature := w.types[type_index.id.zext()]&;
    code := w.code.index(local_func_i);
    
    self: LoadWasm = (
        m = m,
        w = w,
        f = f,
        loaded = loaded,
        function_names = function_names,
        src = code.insts.items(),
        locals = temp().alloc_uninit(Qbe.Ref, code.total_local_count + signature.arg.len),
        blocks = list(temp()),
        stack = list(temp()),
        local_rt_header = f.newtmp("wasm_globals", .Kl),
        local_memory_base = f.newtmp("memory", .Kl),
        cursor = 0,
        return_types = signature.ret.items(),
        link = f.start&,
    );
    self&.translate_func(code, signature);
    Qbe'backend'compile_fn(f);
    reset_temporary_storage(mark);
}

fn translate_func(self: *LoadWasm, code: *Wasm.Func, signature: *Wasm.FuncType) void = {
    self.f.emit(.par, .Kl, self.local_rt_header, QbeNull, QbeNull);
    // parameters come in as locals not on the stack
    range(0, signature.arg.len) { i |
        k := signature.arg[i].cls();
        self.locals[i] = self.f.newtmp(@tfmt("L%", i), k);
        self.f.emit(.par, k, self.locals[i], QbeNull, QbeNull);
    };
    
    @run @assert_eq(offset_of(Rt.Instance, .memory), 0, "memory must be the first field of instance");
    memory_ptr := self.f.newtmp("memory", .Kl);
    self.f.emit(.load, .Kl, memory_ptr, self.local_rt_header, QbeNull);
    self.f.emit(.load, .Kl, self.local_memory_base, memory_ptr, QbeNull);
    
    {
        i := signature.arg.len;
        for code.locals& { local |
            k := local.type.cls();
            range(0, local.count) { _ |
                self.locals[i] = self.f.newtmp(@tfmt("L%", i), k);
                // zero if undefined. sad that this adds a bunch of junk for the backend to remove. 
                // TODO: profile self-compile to make sure it's not a big deal
                self.f.emit(.copy, k, self.locals[i], Qbe.ConZero, QbeNull);
                i += 1;
            };
        };
        @debug_assert_eq(self.locals.len, i);
    };
    // note: more alloc8 instructions are added to f.start as needed
    
    // there's an implicit block around the body of the function
    // which you can br with the same effect as Return.
    self.blocks&.push(current = self.link_new_block(), continue = Qbe.Blk.ptr_from_int(0), break = Qbe.Blk.ptr_from_int(0), loop = false);
    self.push_block(empty(), self.return_types, false);
    
    self.emit_the_code();
    
    // implicit fallthrough return at the end of the function. 
    @if(self.get_blk()[].jmp.type == .Jxxx)
    if self.stack.len >= self.return_types.len {
        self.emit_return();
    } else {
        self.get_blk()[].jmp.type = .hlt;
    };
    
    self.f.lnk.export = true; // TODO: kind depends if this is the final thing. if it's aot into a franca program, you want to to TOFKALTO
}

Rt :: import("@/examples/import_wasm/runtime.fr");
BlockEntry :: @struct(continue: *Qbe.Blk, break: *Qbe.Blk, current: *Qbe.Blk, loop: bool);

// doing this iteratively instead of recursively is cute because you can't run out of callstack 
// on pathological functions but the BlockEntry/IfNode bookkeeping is super confusing :(
fn emit_the_code(self: *LoadWasm) void = {
    IfNode :: @struct(src: *Qbe.Blk, block_type: i64 /* needed for op else */);
    if_stack := list(IfNode, temp());
    while => self.cursor < self.src.len {
        continue :: local_return;
        
        start_cursor := self.cursor;
        b := self.src[self.cursor];
        self.cursor += 1;
        inst := @as(Wasm.Inst) b;
        
        if self.get_blk()[].jmp.type != .Jxxx && !@is(inst, .End, .Unreachable, .Nop, .Else) {
            // the block has unreachable code
            self.cursor -= 1;
            self.skip_current_instruction();
            continue();
        };
        
        // Single byte unary/binary instructions are found in the table. 
        encoding := lookup_wasm_encoding(b);
        if b == 0xC4 {
            encoding = (kind = .unary, op = .extsw, cls = .Kl);
        };
        
        if encoding.kind != .other {
            a0, a1, r := (QbeNull, QbeNull, QbeNull);
            @match(encoding.kind) {
                fn other() => unreachable();
                fn unary() => {
                    if encoding.op == .truncl {
                        encoding.op = .copy;
                    };
                    r = self.f.newtmp("w", encoding.cls);
                    a := self.stack[self.stack.len - 1]&;
                    a0 = a[];
                    a[] = r;
                };
                fn binary() => {
                    r = self.f.newtmp("w", encoding.cls);
                    a1 = self.stack[self.stack.len - 1];
                    b := self.stack[self.stack.len - 2]&;
                    self.stack.len -= 1;
                    a0 = b[];
                    b[] = r;
                };
                fn load() => {
                    _align := self.read_u();
                    offset := self.read_u();
                    addr := self.stack&.pop().expect("stack for load addr");
                    addr := self.compute_address(addr, offset.bitcast());
                    
                    result := self.f.newtmp("load", encoding.cls);
                    self.stack&.push(result);
                    a0 = addr; a1 = QbeNull; r = result; 
                };
                fn store() => {
                    _align := self.read_u();
                    offset := self.read_u();
                    value := self.stack&.pop().expect("stack for store value");
                    addr := self.stack&.pop().expect("stack for store addr");
                    addr := self.compute_address(addr, offset.bitcast());
                    encoding.cls = .Kw;  // HACK: undo "this being Kl is subtle"
                    a0 = value; a1 = addr; r = QbeNull; 
                };
            };
            
            self.f.emit(encoding.op, encoding.cls, r, a0, a1);
            continue();
        }; // Otherwise it's a more complicated instruction. 
        
        @match(inst) {
            fn I32_Const() => self.stack&.push(self.f.getcon(self.read_s()));
            fn I64_Const() => self.stack&.push(self.f.getcon(self.read_s()));
            fn F64_Const() => self.push_float(u64);
            fn F32_Const() => self.push_float(u32);
            fn LocalGet()  => self.local_inst(1);
            fn LocalSet()  => self.local_inst(-1);
            fn LocalTee()  => self.local_inst(0);
            // TODO: this is unfortunate code to generate but i don't know how you do better without knowing that tables are immutable. 
            //       if max=min then at least you know they don't resize and you could save an indirection. 
            fn CallIndirect() => {
                type_index := self.read_u();
                table := self.read_u();
                index := self.stack&.pop().expect("call index on stack");
                externref := self.table_get(table, index);
                r := self.temporaries(3);
                self.f.emit(.add, .Kl, r[2], externref, self.f.getcon(8));
                self.f.emit(.load, .Kl, r[0], externref, Qbe.Null);
                self.f.emit(.load, .Kl, r[1], r[2], Qbe.Null);
                self.emit_call_from_stack(r[0], r[1], (id = type_index.trunc()));
            }
            fn TableGet() => {
                table := self.read_u();
                index := self.stack&.pop().expect("table index on stack");
                r := self.table_get(table, index);
                self.stack&.push(r);
            }
            fn TableSet() => {
                table := self.read_u();
                value := self.stack&.pop().unwrap();
                index := self.stack&.pop().unwrap();
                table_slot := self.compute_table_slot(table, index);
                self.f.emit(.blit0, .Kw, QbeNull, value, table_slot);
                self.f.emit(.blit1, .Kw, Qbe.Null, INT(size_of(Rt.Callable)), Qbe.Null);
            }
            fn Call() => {
                callee := self.read_index(Wasm.FuncIdx);
                callee := callee.id.zext();
                type_index := self.function_types[callee];
                
                if callee < self.counts&[.imports] {
                    off := Rt'Instance'compute_offset(self.counts&, .imports, callee);
                    r := self.temporaries(4);
                    r[0] = self.f.newtmp(self.function_names[callee], .Kl);
                    self.f.emit(.add, .Kl, r[3], self.local_rt_header, self.f.getcon(off));  // *Callable
                    self.f.emit(.add, .Kl, r[2], r[3], self.f.getcon(8));
                    self.f.emit(.load, .Kl, r[0], r[3], Qbe.Null);
                    self.f.emit(.load, .Kl, r[1], r[2], Qbe.Null);
                    self.emit_call_from_stack(r[0], r[1], type_index);
                } else {
                    callee := self.f.symcon(self.functions[callee]);
                    self.emit_call_from_stack(callee, self.local_rt_header, type_index);
                };
            }
            fn Return() => self.emit_return();
            fn GlobalSet() => {
                i: i64 = self.read_u().bitcast();
                g := self.compute_global_slot(i);
                val := self.stack&.pop().expect("enough stack");
                k := self.global_cls[i];
                self.f.emit(k.store_op(), .Kw, QbeNull, val, g); 
            }
            fn GlobalGet() => {
                i: i64 = self.read_u().bitcast();
                g := self.compute_global_slot(i);
                k := self.global_cls[i];
                val := self.f.newtmp("global_get", k);
                self.stack&.push(val);
                self.f.emit(.load, k, val, g, QbeNull);
            }
            fn VectorPrefix() => panic("Vector instructions are not supported (0xFD prefix)");
            fn Nop() => ();
            fn Unreachable() => if self.get_blk()[].jmp.type == .Jxxx {
                self.get_blk()[].jmp.type = .hlt;
            };
            fn Loop()  => self.push_block(self.read_s(),  true);
            fn Block() => self.push_block(self.read_s(), false);
            fn End()   => self.end_block(true);
            fn If() => {
                block_type := self.read_s();
                old := self.get_blk();
                old.jmp = (type = .jnz, arg = self.stack&.pop().expect("arg on stack for If"));
                if_stack&.push(src = old, block_type = block_type); 
                self.push_block(block_type, false);
                continuation := self.blocks[self.blocks.len - 1].break;
                old.s1 = self.get_blk();
                old.s2 = continuation; // used if no else
            }
            fn Else() => {
                break := self.blocks[self.blocks.len - 1].break;
                self.end_block(true);
                if_node := if_stack&.pop()
                    || @panic("else outside if (in $%)", self.f.name());
                self.phi_to_stack(if_node.src);
                new := self.link_new_block();
                self.blocks&.push(continue = new, break = break, current = new, loop = false);
                @debug_assert_eq(if_node.src.jmp.type, .jnz);
                if_node.src.s2 = self.get_blk();
            }
            fn Br() => {
                src := self.get_blk();
                src.jmp.type = .jmp;
                src.s1 = self.parse_branch_target();
                self.stack_to_phi(src, src.s1);
            }
            fn BrIf() => {
                src := self.get_blk();
                src.jmp = (type = .jnz, arg = self.stack&.pop().expect("arg on stack for BrIf"));
                // TODO: only clone the part that gets popped. 
                stack := self.stack.items().clone(temp());
                src.s1 = self.parse_branch_target();
                self.stack_to_phi(src, src.s1);
                
                // If the condition is false, fallthrough and keep parsing instructions into a new block 
                // (without consuming anything from the stack). don't need phis since this is the only entry point. 
                src.s2 = self.link_new_block();
                current := self.blocks[self.blocks.len - 1].current&;
                self.f.copy_instructions_from_scratch_reversed_which_means_forwards(current[]);
                current[] = src.s2;
                self.stack = stack;
            }
            fn BrTable() => {
                src := self.get_blk();
                inspect := self.stack&.pop().expect("arg for br_table");
                r := self.f.newtmp("switch", .Kl);
                self.f.emit(.extuw, .Kl, r, inspect, Qbe.Null);
                n: i64 = self.read_u().bitcast();
                b := temp().alloc_init(*Qbe.Blk, n + 1, fn(_) => self.parse_branch_target());
                
                case := self.f.new_switch(n, src, r, b[b.len - 1]);
                ss := self.f.switches[case]&;
                
                // TODO: only clone the part that gets popped. 
                stack := self.stack.items().clone(temp());
                enumerate b.slice(0, b.len - 1) { i, s |
                    s := s[];
                    // careful not to call stack_to_phi multiple times if default used explicitly
                    // TODO: same applies to other labels being reused
                    if !identical(s, ss.default) {
                        self.stack_to_phi(src, s);
                        self.stack = stack;
                        push(ss.cases&, ss.case_count&, (s, i));
                    }
                };
                self.stack_to_phi(src, ss.default);
            }
            fn Drop() => {
                self.stack&.pop().expect("stack for store drop");
            }
            fn SelectK() => {
                a := @slice(self.stack[self.stack.len - 2], self.stack[self.stack.len - 3]);
                k := Qbe.Cls.Kl;
                for a { a |
                    if self.f.get_tmp(a) { t |
                        k = t.cls;
                    };
                };
                self.do_select(k);
            }
            fn Select() => {
                count := self.read_u();
                @assert_eq(count, 1, "TODO: multi-value");
                k := cls(@as(Wasm.ValType) @as(u8) self.src[self.cursor]);
                self.cursor += 1;
                self.do_select(k);
            }
            fn AtomicPrefix() => {
                opcode := self.read_u();
                _align := self.read_u();
                offset: u64 = @if(opcode == 3, 0, self.read_u());
                
                k: Qbe.Cls = @switch(opcode) {
                    @case(72) => .Kw;
                    @case(73) => .Kl;
                    @default => {
                        self.choose_runtime_call(start_cursor);
                        continue()
                    };
                };
                
                new := self.stack&.pop().expect("stack for cas-new");
                old := self.stack&.pop().expect("stack for cas-old");
                p := self.stack&.pop().expect("stack for cas-p");
                p := self.compute_address(p, offset.bitcast());
                prev := self.f.newtmp("cas-prev", k);
                
                self.f.emit(.cas0, .Kw, QbeNull, p, QbeNull);
                self.f.emit(.cas1, k, prev, old, new);
                self.stack&.push(prev);
            }
            fn PrefixFC() => {
                opcode := @as(u64) self.read_u();
                if opcode == 10 {  // memory.copy
                    @debug_assert(self.src[self.cursor] == 0 && self.src[self.cursor + 1] == 0);
                    self.cursor += 2;
                    n := self.stack&.pop().unwrap();
                    src := self.stack&.pop().unwrap();
                    dest := self.stack&.pop().unwrap();
                    src := self.compute_address(src, 0);
                    dest := self.compute_address(dest, 0);
                    if self.f.get_int(n) { n |
                        // TODO: this is wrong! you're allowed overlap
                        self.f.emit(.blit0, .Kw, QbeNull, src, dest);
                        self.f.emit(.blit1, .Kw, Qbe.Null, INT(n), Qbe.Null);
                    } else {
                        panic("TODO: non-constant memory.copy");
                    }
                    continue();
                };
                self.choose_runtime_call(start_cursor);
            }
            fn I32_EqZero() => self.eqz(.ceqw);
            fn I64_EqZero() => self.eqz(.ceql);
            @default => self.choose_runtime_call(start_cursor);
        }
    }
}

fn do_select(self: *LoadWasm, k: Qbe.Cls) void = {
    @assert(k.is_int(), "TODO: select on floats");
    result := self.f.newtmp("select", k);
    cond := self.stack&.pop().expect("stack for sel-cond");
    f := self.stack&.pop().expect("stack for sel-false");
    t := self.stack&.pop().expect("stack for sel-true");
    self.f.emit(.sel0, .Kw, QbeNull, cond, QbeNull);
    self.f.emit(.sel1, k, result, t, f);
    self.stack&.push(result);
}

fn choose_runtime_call(self: *LoadWasm, start: i64) void = {
    it, rt_i, indices := self.match_runtime_call(start)
        || @panic("unhandled instruction %\n(at byte index % of %)", 
            self.src[start], start, self.m.str(self.f.lnk.id));
    // found the right instruction so emit a call to the function in Rt

    args := self.stack.items().rest(self.stack.len - it.stacked.len);
    externrefs := Qbe.Ref.list(temp());
    i := 0;
    for it.match { it |
        continue :: local_return;
        @if_let(it) fn int(it) => {
            var := @match(it) {
                fn memory() => {
                    @assert_eq(indices[i], 0, "TODO: multiple memories");
                    memory_ptr := self.f.newtmp("memory", .Kl);
                    self.f.emit(.load, .Kl, memory_ptr, self.local_rt_header, QbeNull);
                    memory_ptr
                }
                fn table() => {
                    off := Rt'Instance'compute_offset(self.counts&, .tables, indices[i]);
                    r := self.temporaries(2);
                    self.f.emit(.add, .Kl, r[0], self.local_rt_header, self.f.getcon(off));  // **Table
                    self.f.emit(.load, .Kl, r[1], r[0], QbeNull);  // *Table
                    r[1]
                }
                fn data() => self.f.symcon(self.datas[indices[i]].id);
                fn memarg() => {
                    _align, off := (indices[i], indices[i + 1]);
                    args[0] = self.compute_address(args[0], off);
                    i += 2;
                    continue()
                }
            };
            externrefs&.push(var);
            i += 1;
        }
    };
    
    off := Rt'Instance'compute_offset(self.counts&, .runtime, rt_i);
    callee, b := (self.f.newtmp(it.name, .Kl), self.f.newtmp("rt_slot", .Kl));
    self.f.emit(.add, .Kl, b, self.local_rt_header, self.f.getcon(off));  // *rawptr
    self.f.emit(.load, .Kl, callee, b, QbeNull);  // rawptr
    
    self.f.emit(.arg, .Kl, QbeNull, self.local_rt_header, QbeNull);
    for externrefs { it |
        self.f.emit(.arg, .Kl, QbeNull, it, QbeNull);
    };
    range(0, args.len) { i |
        self.f.emit(.arg, it.stacked[i], QbeNull, args[i], QbeNull);
    };
    self.stack.len -= it.stacked.len;
    if it.result == .Kx {
        self.f.emit(.call, .Kw, Qbe.Null, callee, Qbe.Null);
    } else {
        result := self.f.newtmp(it.name, it.result);
        self.f.emit(.call, it.result, result, callee, Qbe.Null);
        self.stack&.push(result);
    };
}

fn match_runtime_call(self: *LoadWasm, start: i64) ?Ty(*InstSeq, i64, []i64) = {
    indices := i64.list(temp());
    enumerate collect_runtime_calls() { i, it |
        continue :: local_return;
        self.cursor = start;
        indices&.clear();
        for it.match { it |
            @match(it) {
                fn byte(it) => {
                    @if(self.src[self.cursor] != it) continue();
                    self.cursor += 1;
                };
                fn int(it) => range(0, 1 + int(it == .memarg)) { _ |
                    indices&.push(self.read_u().bitcast());
                };
            };
        };
        self.rt_needed&.set(i);
        return(Some = (it, i, indices.items()));
    };
    .None
}
        
InstSeq :: @struct {
    Arg :: @enum(u8) (memory, table, data, memarg);
    Inst :: @tagged(byte: u8, int: Arg);
    callee: rawptr;
    match: []Inst;
    stacked: []Qbe.Cls;
    result: Qbe.Cls;
    name: Str;
};

// TODO: i want #fold to mean the return value is cached without the extra @run
//       so it's fine that i call this in like 4 places. :compiler
fn collect_runtime_calls() []InstSeq #fold = @run {
    out := InstSeq.list(ast_alloc());
    fr := current_compiler_context();
    for get_constants(Rt) { name |
        continue :: local_return;
        val, ty := get_constant(Rt, name) || continue();
        @if(ty != FuncId) continue();
        fid := FuncId.ptr_from_raw(val)[];
        func := get_function_ast(fid, true, true, true, false);
        i := func.annotations&.index_of(fn(it) => it.name == @symbol wasm)
            || continue();
        args := func.annotations[i].args&.items();
        insts := InstSeq.Inst.list(args.len, ast_alloc());
        const_args := 1 /*base_pointer*/;
        each args { it |
            if it.expr&.is(.ContextualField) {
                it[] = @{ @as(InstSeq.Arg) @[it[]] };
            };
            it[] = compile_ast(it[]);
            @switch(it.ty) {
                @case(u8) => insts&.push(byte = u8.const_eval(it[]));
                @case(InstSeq.Arg) => {
                    value := InstSeq.Arg.const_eval(it[]);
                    insts&.push(int = value);
                    const_args += int(value != .memarg);
                };
                @default => compile_error("invalid type", it.loc);
            };
        };
        stacked := Qbe.Cls.list(ast_alloc());
        for fr.arg_types(func.finished_arg.unwrap()).rest(const_args) { ty |
            stacked&.push(fr_ty_to_cls(ty));
        };
        out&.push(
            callee = fr.get_jitted(fid), 
            match = insts.items(), 
            stacked = stacked.items(),
            result = fr_ty_to_cls(func.finished_ret.unwrap()),
            name = fr.get_string(name),
        );
    };
    out.items()
};
#use("@/compiler/ast_external.fr");

fr_ty_to_cls :: fn(ty: Type) Qbe.Cls = @switch(ty) {
    @case(i32) => .Kw;
    @case(u32) => .Kw;
    @case(i64) => .Kl;
    @case(f32) => .Ks;
    @case(f64) => .Kd;
    @case(void) => .Kx;
    @default => .Kl;
};

fn eqz(self: *LoadWasm, o: Qbe.O) void = {
    result := self.f.newtmp("eqz", .Kw);
    value := self.stack&.pop().unwrap();
    self.f.emit(o, .Kw, result, value, Qbe.ConZero);
    self.stack&.push(result);
}

fn table_get(self: *LoadWasm, table_index: u64, index_in_table_W: Qbe.Ref) Qbe.Ref = {
    table_slot := self.compute_table_slot(table_index, index_in_table_W);
    result := self.f.newtmp("table.get", .Kl);
    // TODO: this will have to change when i want to support returning (func/extern)ref
    self.f.start.push(make_ins(.alloc8, .Kl, result, self.f.getcon(size_of(Rt.Callable)), Qbe.Null));
    self.f.emit(.blit0, .Kw, QbeNull, table_slot, result);
    self.f.emit(.blit1, .Kw, Qbe.Null, INT(size_of(Rt.Callable)), Qbe.Null);
    result
}

fn compute_table_slot(self: *LoadWasm, table_index: u64, index_in_table_W: Qbe.Ref) Qbe.Ref = {
    table_var        := self.compute_slot(table_index.bitcast(), .Table, .tables, .local_tables);
    index_in_table_L := self.f.newtmp("Iindex", .Kl);
    table_slot       := self.f.newtmp("Islot", .Kl);
    table_offset     := self.f.newtmp("Ioff", .Kl);
    table_base       := self.f.newtmp("Ibase", .Kl);

    @run @assert_eq(offset_of(Rt.Table, .ptr), 0);    
    self.f.emit(.load, .Kl, table_base, table_var, QbeNull);  // *Callable
    self.f.emit(.extuw, .Kl, index_in_table_L, index_in_table_W, QbeNull);
    self.f.emit(.mul, .Kl, table_offset, index_in_table_L, self.f.getcon(size_of(Rt.Callable)));
    self.f.emit(.add, .Kl, table_slot, table_base, table_offset);
    table_slot
}

// TODO: cache these per function in the same way i do for the memory base address
fn compute_global_slot(self: *LoadWasm, i: i64) Qbe.Ref = {
    self.compute_slot(i, .Global, .globals, .local_globals)
} 

fn compute_slot(self: *LoadWasm, i: i64, A: Wasm.ImportType, B: Fields(Rt.View), C: Fields(Rt.View)) Qbe.Ref = {
    n := self.w.import_count&[A];
    if i < n {
        off := Rt'Instance'compute_offset(self.counts&, B, i);
        r := self.temporaries(2);
        self.f.emit(.add, .Kl, r[0], self.local_rt_header, self.f.getcon(off));  // **T
        self.f.emit(.load, .Kl, r[1], r[0], QbeNull);  // *T
        return r[1];
    };
    off := Rt'Instance'compute_offset(self.counts&, C, i - n);
    r := self.f.newtmp("", .Kl);
    self.f.emit(.add, .Kl, r, self.local_rt_header, self.f.getcon(off));  // *T
    r
} 

fn parse_branch_target(self: *LoadWasm) *Qbe.Blk = {
    depth: i64 = self.read_u().bitcast();
    it := self.blocks[self.blocks.len - depth - 1];
    @if(it.loop, it.continue, it.break)
}

fn push_float(self: *LoadWasm, $Bits: Type) void = {
    v := Bits.cast_front_unaligned(self.src.rest(self.cursor));
    value: i64 = v.int();
    self.cursor += Bits.size_of();
    self.stack&.push(self.f.getcon(value));
}

fn stack_to_phi(self: *LoadWasm, src: *Qbe.Blk, dest: *Qbe.Blk) void = {
    for_phi dest { p |
        p.push(src, self.stack&.pop() || @panic("missing value for phi % -> % %", src.id, dest.id, { printfn(self.f, self.f.globals.debug_out); ""}));
    }
}

fn decode_block_type(self: *LoadWasm, block_type: i64) Ty([]Wasm.ValType, []Wasm.ValType) = {
    if block_type >= 0 {
        ty := self.w.types[block_type]&;
        return(ty.arg.items(), ty.ret.items());
    };
    if block_type == preleb(.EmptyBlock).intcast() {
        return(empty(), empty());
    };
    
    // :SLOW
    values :: get_cases(Wasm.ValType);
    each values { it |
        if block_type == preleb(it[]).intcast() {
            return(empty(), (ptr = it, len = 1));
        };
    };
    
    @panic("invalid block type %", block_type)
}

fn get_blk(self: *LoadWasm) *Qbe.Blk =
    self.blocks[self.blocks.len - 1].current;

fn push_block(self: *LoadWasm, block_type: i64, loop: bool) void = {
    arg, ret := self.decode_block_type(block_type);
    push_block(self, arg, ret, loop)
}

fn push_block(self: *LoadWasm, arg: []Wasm.ValType, ret: []Wasm.ValType, loop: bool) void = {
    ::DerefEq(Wasm.ValType);
    old := self.blocks[self.blocks.len - 1]&;
    new := self.link_new_block();
    init_empty_phis(self.f, new, arg);
    self.stack_to_phi(old.current, new);
    self.f.copy_instructions_from_scratch_reversed_which_means_forwards(old.current);
    continuation := self.link_new_block();
    init_empty_phis(self.f, continuation, ret);  // break's inputs are the new block's outputs
    if old.current.jmp.type == .Jxxx {
        old.current.jmp.type = .jmp;
        old.current.s1 = new;
    }
    old.current = continuation;
    self.phi_to_stack(new);
    self.blocks&.push(continue = new, break = continuation, current = new, loop = loop);
    
    init_empty_phis :: fn(f: *Qbe.Fn, b: *Qbe.Blk, arg_types: []Wasm.ValType) void = 
        for(arg_types, fn(ty) => new_phi(f, b, ty.cls(), 1));
}

fn end_block(self: *LoadWasm, fallthrough: bool) void = {
    it := self.blocks&.pop().expect("well nested End instructions");
    self.f.copy_instructions_from_scratch_reversed_which_means_forwards(it.current);
    if it.current.jmp.type == .Jxxx {
        self.stack_to_phi(it.current, it.break);
        it.current.jmp.type = .jmp;
        it.current.s1 = it.break;
    }
    it.current = zeroed(*Qbe.Blk);
    self.blocks[self.blocks.len - 1].current = it.break;
    if fallthrough {
        self.phi_to_stack(it.break);
    }
}

fn phi_to_stack(self: *LoadWasm, b: *Qbe.Blk) void = {
    ::import("@/backend/wasm/isel.fr");
    for_phi_rev_TODOSLOW b { p |
        self.stack&.push(p.to);
    }
}

fn link_new_block(self: *LoadWasm) *Qbe.Blk = {
    b := newblk();
    b.id = self.f.nblk;
    self.f.nblk += 1;
    self.link[] = b; 
    self.link = b.link&;
    b
}

fn compute_address(self: *LoadWasm, addr: Qbe.Ref, offset: i64) Qbe.Ref = {
    f := self.f;
    r0 := f.newtmp("a", .Kl);
    r1 := f.newtmp("a", .Kl);
    f.emit(.extuw, .Kl, r0, addr, QbeNull);
    f.emit(.add, .Kl, r1, self.local_memory_base, r0);
    if(offset == 0, => return(r1));
    r2 := f.newtmp("a", .Kl);
    f.emit(.add, .Kl, r2, r1, f.getcon(offset));
    r2
}

fn local_inst(self: *LoadWasm, delta: i64) void = {
    i    := self.read_u();
    var  := self.locals[i.bitcast()];
    k := self.f.get_temporary(var)[].cls;
    ::if(Qbe.Ref);
    dest := if(delta == 1, => self.f.newtmp(@tfmt("getL%", i), k), => var);
    src  := if(delta == 1, => var, => self.stack[self.stack.len - 1]);
    self.stack&.reserve(abs(delta));
    self.stack.len += delta;
    if delta == 1 {
        @debug_assert(self.stack.len > 0, "tried to access empty stack in local_inst");
        self.stack[self.stack.len - 1] = dest;
    };
    self.f.emit(.copy, k, dest, src, QbeNull);
}

fn emit_return(self: *LoadWasm) void = {
    @debug_assert(self.get_blk()[].jmp.type == .Jxxx, "tried to return from terminated block");
    @switch(self.return_types.len) {
        @case(0) => {
            self.get_blk()[].jmp.type = .ret0;
        };
        @case(1) => {
            k := self.return_types[0].cls();
            self.get_blk()[].jmp = (type = k.retk(), arg = self.stack[self.stack.len - 1]);
            self.stack.len -= 1;
            self.f.ret_cls = k;
        };
        @default => panic("TODO: handle multiple returns");
    };
}

fn emit_call_from_stack(self: *LoadWasm, callee: Qbe.Ref, header: Qbe.Ref, type: Wasm.TypeIdx) void = {
    type := self.w.types[type.id.zext()]&;
    @assert_le(
        type.arg.len, self.stack.len, 
        "not enough stack for call to % %", callee, 
        @if(rtype(callee) == .RCon, self.m.str(self.f.get_constant(callee)[].sym), ""),
    );

    self.f.emit(.arg, .Kl, QbeNull, header, QbeNull);
    range(0, type.arg.len) { i |
        ref := self.stack[self.stack.len - type.arg.len + i];
        k := type.arg[i].cls();
        self.f.emit(.arg, k, QbeNull, ref, QbeNull);
    };
    self.stack.len -= type.arg.len;
    
    @switch(type.ret.len) {
        @case(0) => {
            self.f.emit(.call, .Kw, QbeNull, callee, QbeNull);
        };
        @case(1) => {
            k := type.ret[0].cls();
            ref := self.f.newtmp("ret", k);
            self.f.emit(.call, k, ref, callee, QbeNull);
            self.stack&.push(ref);
        };
        @default => panic("TODO: handle multiple returns");
    };
}

fn temporaries(self: *LoadWasm, $n: i64) []Qbe.Ref #inline/*semantic*/ = {
    r := @uninitialized Array(Qbe.Ref, n);
    each r& { it | it[] = self.f.newtmp("", .Kl); };
    r&.items()
}

fn read_u(self: *LoadWasm) u64 = {
    bytes := self.src.slice(self.cursor, self.src.len);
    value, rest, ok := read_leb128_unsigned(bytes);
    @assert(ok, "Invalid leb128 number");
    self.cursor += bytes.len - rest.len;
    value
}

fn read_s(self: *LoadWasm) i64 = {
    bytes := self.src.slice(self.cursor, self.src.len);
    value, rest, ok := read_leb128_signed(bytes);
    @assert(ok, "Invalid leb128 number");
    self.cursor += bytes.len - rest.len;
    value
}

fn read_index(self: *LoadWasm, $T: Type) T #generic = {
    (id = self.read_u().trunc())
}

// todo: unfortunate to need so much extra code for this. 
//       when there's unreachable code at the end of a block, 
//       don't want to emit anything and it doesn't have to typecheck 
//       but since instructions are variable width, they can't be skipped without decoding.
fn skip_current_instruction(self: *LoadWasm) void = {
    start := self.cursor;
    b := self.src[self.cursor];
    self.cursor += 1;
    inst := @as(Wasm.Inst) b;
    
    encoding := lookup_wasm_encoding(b);
    if encoding.kind != .other {
        if @is(encoding.kind, .load, .store) {
            self.read_u();
            self.read_u();
        };
        return();
    };
    
    if @is(inst, .LocalGet, .LocalSet, .LocalTee, .GlobalGet, .GlobalSet, .TableGet, .TableSet, .Call, .Br, .BrIf, .Select, .I32_Const, .I64_Const, .Loop, .Block, .If) {
        self.read_u();
        return();
    };

    if @is(inst, .CallIndirect) {
        self.read_u();
        self.read_u();
        return();
    };
    
    if @is(inst, .Return, .Nop, .Unreachable, .End, .Else, .Drop, .I32_EqZero, .I64_EqZero, .SelectK) {
        return();
    }
    
    @match(inst) {
        fn F64_Const() => { 
            self.cursor += 8; 
            return();
        };
        fn F32_Const() => { 
            self.cursor += 4; 
            return();
        };
        fn AtomicPrefix() => {
            opcode := self.read_u();
            if opcode == 72 || opcode == 73 {
                self.read_u();
                self.read_u();
                return();
            };
        }
        fn PrefixFC() => if self.read_u() == 10 {  // memory.copy
            self.cursor += 2;
            return();
        };
        fn BrTable() => {
            range(0, self.read_u().bitcast() + 1) { _ |
                self.read_u();
            };
            return();
        }
        @default => ();
    };
    self.match_runtime_call(start)
        || @panic("unknown instruction in unreachable code can't be decoded. %\n(at byte index % of %)", 
            self.src[start], start, self.m.str(self.f.lnk.id));
}

#use("@/lib/collections/bit_set.fr");
QbeNull :: Qbe.Null;
Qbe :: import("@/backend/ir.fr");
#use("@/examples/import_wasm/parse.fr");
preleb :: import("@/backend/wasm/isel.fr").wasm_type_preleb;
