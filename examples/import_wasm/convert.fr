//! === WARNING === This is not a sandbox! === WARNING ===

#use("@/backend/lib.fr");
#use("@/examples/import_wasm/parse.fr");

// TODO: this could be hella faster: 
//       - there's a wasteful prepass that splits up bits of the module into a big data structure. 
//       - the parser+wasm->ir could be on a different thread from the ir->asm 
//         (i have the machinery to do this already because the Franca compiler works that way)
//         + you could compile multiple functions in parallel (but that would need more work).
//       - the number of bounds checks the parser does per byte is a bit sad. 
//       - could inline more if we pre-sorted based on callgraph,
//         but maybe the expectation with wasm is that the first compiler did that already?

// TODO: validation
// TODO: support aot. needs extra work because of import_wasm/runtime.fr
// TODO: provide standard c api so could run other people's tests? https://github.com/WebAssembly/wasm-c-api/blob/main/include/wasm.h

LoadWasm :: @rec @struct(
    m: *QbeModule,
    w: *WasmModule,
    src := "",
    cursor := 0,
    module_name := "module",
    global_cls: []Qbe.Cls = empty(),
    functions: []Qbe.Sym = empty(),  // [padding for imports, ...exports]
    function_types: []Wasm.TypeIdx = empty(),
    counts := zeroed Rt.Counts,
    function_names: []Str = empty(),
    
    datas: []Data = empty(),
    elems: []Elem = empty(),
    init_global_values: []Rt.GlobalValue = empty(),  // not including imports
    init_table_sizes: []i64 = empty(),  // not including imports
    
    stack: List(Qbe.Ref),
    blocks: List(BlockEntry),
    locals: []Qbe.Ref,  // TODO: we can't store a 128 bit vector in a temporary 
    f := zeroed(*Qbe.Fn),
    local_rt_header := QbeNull,
    local_memory_base := Qbe.Null,
    return_types: []Wasm.ValType = empty(),
    link := zeroed(**Qbe.Blk),
    alloc: Alloc,
    
    log_local_func_i := 0,
);

Data :: @struct(id: Qbe.Sym, size: i64, active_at: ?i64);
Elem :: @struct(functions: []i64, active_at: ?@struct(table: i64, offset: i64));

fn init(m: *QbeModule, w: *WasmModule, a: Alloc) LoadWasm = (
    m = m,
    w = w,
    stack = list(temp()),
    locals = empty(),
    blocks = list(temp()),
    alloc = a,
);

fn mangle_name(it: *Wasm.Import) Str = 
    @tfmt("%^%", it.name.items(), it.module.items());  // :NamespaceChar

fn const_eval_wasm_expr(expr: []u8) Ty(i64, Qbe.Cls) = {
    o := @as(Wasm.Inst) expr[0];
    ::enum(Wasm.Inst);
    @assert(@is(o, .I32_Const, .I64_Const), "TODO: complex expr");
    v, _, ok := read_leb128_signed(expr.rest(1));
    @debug_assert(ok, "bad constant expression");
    (v, @match(o) {
        fn I32_Const() => .Kw;
        fn I64_Const() => .Kl;
        fn F32_Const() => .Ks;
        fn F64_Const() => .Kd;
        @default => @panic("unhandled op in const_eval_wasm_expr");
    })
} 

// :NamespaceChar
fn private_symbol(self: *LoadWasm, name: Str) Qbe.Sym = {
    self.m.intern(@tfmt("%__^%", name, self.module_name))
}

preleb :: import("@/backend/wasm/isel.fr").wasm_type_preleb;

fn load_module(self: *LoadWasm) void = {
    // :Leak
    self.declare_functions();
    self.declare_globals();
    self.declare_tables();
    self.declare_datas();
    
    range(0, self.w.code.len) { i |
        mark := mark_temporary_storage();
        f := Qbe.Fn.zeroed();
        self.translate_func(i, f&);
        run_qbe_passes(f&);  // :ThisIsWhereWeEmit
        reset_temporary_storage(mark);
    };
};

fn translate_func(self: *LoadWasm, local_func_i: i64, f: *Qbe.Fn) void = {
    code := self.w.code.index(local_func_i);
    global_func_i := local_func_i + self.w.import_count&[.Func];
    self.f = f;
    self.f.default_init(self.m);
    
    f.lnk.id = self.functions[global_func_i];
    
    type_index := self.function_types[global_func_i];
    
    signature := self.w.types[type_index.id.zext()]&;
    self.locals = temp().alloc_uninit(Qbe.Ref, code.total_local_count + signature.arg.len);
    self.blocks = list(temp());
    self.stack = list(temp());
    self.local_rt_header = self.f.newtmp("wasm_globals", .Kl);
    self.local_memory_base = self.f.newtmp("memory", .Kl);
    
    self.f.emit(.par, .Kl, self.local_rt_header, QbeNull, QbeNull);
    // parameters come in as locals not on the stack
    range(0, signature.arg.len) { i |
        k := signature.arg[i].cls();
        self.locals[i] = self.f.newtmp(@tfmt("L%", i), k);
        self.f.emit(.par, k, self.locals[i], QbeNull, QbeNull);
    };
    
    @run @assert_eq(offset_of(Rt.Instance, .memory), 0, "memory must be the first field of instance");
    memory_ptr := self.f.newtmp("memory", .Kl);
    self.f.emit(.load, .Kl, memory_ptr, self.local_rt_header, QbeNull);
    self.f.emit(.load, .Kl, self.local_memory_base, memory_ptr, QbeNull);
    
    {
        i := signature.arg.len;
        for code.locals& { local |
            k := local.type.cls();
            range(0, local.count) { _ |
                self.locals[i] = self.f.newtmp(@tfmt("L%", i), k);
                i += 1;
            };
        };
        @debug_assert_eq(self.locals.len, i);
    };
    
    self.src = code.insts.items();
    self.log_local_func_i = local_func_i;
    self.cursor = 0;
    
    self.link = self.f.start&;
    // you can't br targtting the outer scope of the function so only `current` matters. 
    self.blocks&.push(current = self.link_new_block(), continue = Qbe.Blk.ptr_from_int(0), break = Qbe.Blk.ptr_from_int(0), loop = false);
    self.push_block(preleb(.EmptyBlock).intcast(), false);
    
    self.return_types = signature.ret.items();
    self.emit_the_code();
    
    // implicit fallthrough return at the end of the function. 
    ::enum(Qbe.J);
    @if(self.get_blk()[].jmp.type == .Jxxx)
    if self.stack.len >= self.return_types.len {
        self.emit_return();
    } else {
        self.get_blk()[].jmp.type = .hlt;
    };
    
    self.f.lnk.export = true; // TODO: kind depends if this is the final thing. if it's aot into a franca program, you want to to TOFKALTO
}

fn declare_functions(self: *LoadWasm) void = {
    a := self.alloc;
    count := self.w.functions.len + self.w.import_count&[.Func];
    self.functions = a.alloc_uninit(Qbe.Sym, count); // :Leak
    self.function_types = a.alloc_uninit(Wasm.TypeIdx, count);
    self.function_names = a.alloc_zeroed(Str, count);
    f_i := 0;
    each self.w.imports& { import |
        @if_let(import.desc&) fn Function(id) => {
            self.functions[f_i] = Qbe.no_symbol_S;  // called through Rt.Instance instead
            self.function_types[f_i] = id[];
            self.function_names[f_i] = import.name.items();
            f_i += 1;
        };
    };
    self.counts&[.imports] = f_i;
    range(0, self.w.functions.len) { i |
        id := self.private_symbol(@tfmt("F%", f_i));;
        self.functions[f_i] = id;
        self.function_names[f_i] = self.m.str(id);
        self.function_types[f_i] = self.w.functions[i];
        f_i += 1;
    };
    @debug_assert_eq(count, f_i);    
    
    // actually, if it's an export it's nicer to use the real names. TODO: same thing for globals
    each self.w.exports& { it |
        ::enum(@type it.type);
        if it.type == .Func && it.id.zext() >= self.w.import_count&[.Func] {
            // :NamespaceChar
            id := self.m.intern(@tfmt("%^%", it.name.items(), self.module_name));
            self.functions[it.id.zext()] = id;
            self.function_names[it.id.zext()] = self.m.str(id);
        };
    };
}

fn declare_globals(self: *LoadWasm) void = {
    a := self.alloc;
    count := self.w.globals.len + self.w.import_count&[.Global];
    self.global_cls = a.alloc_uninit(Qbe.Cls, count);
    self.counts&[.globals] = count;
    
    i := 0;
    each self.w.imports& { import |
        @if_let(import.desc&) fn Global(it) => {
            self.global_cls[i] = it.ref_type.cls();
            i += 1;
        };
    };
    
    self.init_global_values = a.alloc_init(Rt.GlobalValue, self.w.globals.len) { local_i |
        global := self.w.globals.index(local_i);
        value, k := const_eval_wasm_expr(global.init_expr.items());
        self.global_cls[i] = k;
        i += 1;
        @debug_assert_eq(k, global.type.cls());
        (i64 = value)
    };
    @debug_assert_eq(i, count);
}

fn declare_tables(self: *LoadWasm) void = {
    a := self.alloc;
    count := self.w.tables.len + self.w.import_count&[.Table];
    self.counts&[.tables] = count;
    
    self.init_table_sizes = a.alloc_init(i64, self.w.tables.len) { i |
        self.w.tables[i].limits.min.zext()
    };
    
    self.elems = a.alloc_init(Elem, self.w.elements.len) { i |
        elem := self.w.elements.index(i);
        functions := a.alloc_init(i64, elem.funcs.len, fn(i) => elem.funcs[i].id.zext());
        (functions = functions, active_at = @if(elem.active_offset_expr.len == 0, .None, {
            off, k := const_eval_wasm_expr(elem.active_offset_expr.items());
            @assert(k == .Kl || k == .Kw, "expected integer not % for elem[%] active_offset_expr", k, i);
            @assert(elem.table.id.zext() < count, "active elem has oob table index");
            (Some = (table = elem.table.id.zext(), offset = off))
        }))
    };
}   

fn declare_datas(self: *LoadWasm) void = {
    a := self.alloc;    
    count := self.w.data.len;
    self.datas = a.alloc_uninit(Data, count);
    
    enumerate self.w.data { i, data | 
        expr := data.active_offset_expr;
        off: ?i64 = @if(expr.len == 0, .None, {
            off, k := const_eval_wasm_expr(expr.items());
            @debug_assert_eq(k, .Kw, "offset expression");
            @debug_assert_eq(data.active_mem.id, 0, "TODO: multiple memories");
            (Some = off)
        });
        id := self.private_symbol(@tfmt("data%", i));
        self.m.emit_data(template = (Bytes = data.bytes.items()), id = id, relocations = empty());
        self.datas[i] = (id = id, size = data.bytes.len, active_at = off);
    };
}   

Rt :: import("@/examples/import_wasm/runtime.fr");
BlockEntry :: @struct(continue: *Qbe.Blk, break: *Qbe.Blk, current: *Qbe.Blk, loop: bool);

fn emit_the_code(self: *LoadWasm) void = {
    IfNode :: @struct(src: *Qbe.Blk, block_type: i64 /* needed for op else */, stack: []Qbe.Ref);
    if_stack := list(IfNode, temp());
    while => self.cursor < self.src.len {
        continue :: local_return;
        
        start_cursor := self.cursor;
        b := self.src[self.cursor];
        self.cursor += 1;
        inst := @as(Wasm.Inst) b;
        ::enum(Wasm.Inst);
        
        ::enum(Qbe.J);
        if self.get_blk()[].jmp.type != .Jxxx && !@is(inst, .End, .Unreachable, .Nop) {
            printfn(self.f, self.f.globals.debug_out);
            @panic("more instructions @% which has already been terminated. (TODO: this happens when there is unreachable code after a BR instruction but that is likely not what you wanted anyway)", self.get_blk()[].id);
        };
        
        // Single byte unary/binary instructions are found in the table. 
        encoding := lookup_wasm_encoding(b);
        
        ::enum(@type encoding.kind);
        if encoding.kind != .other {
            a0, a1, r := (QbeNull, QbeNull, QbeNull);
            @match(encoding.kind) {
                fn other() => unreachable();
                fn unary() => {
                    ::enum(Qbe.O);
                    if encoding.op == .truncl {
                        encoding.op = .copy;
                    };
                    r = self.f.newtmp("wasm", encoding.cls);
                    a := self.stack[self.stack.len - 1]&;
                    a0 = a[];
                    a[] = r;
                };
                fn binary() => {
                    r = self.f.newtmp("wasm", encoding.cls);
                    a1 = self.stack[self.stack.len - 1];
                    b := self.stack[self.stack.len - 2]&;
                    self.stack.len -= 1;
                    a0 = b[];
                    b[] = r;
                };
                fn load() => {
                    align := self.read_u();
                    offset := self.read_u();
                    addr := self.stack&.pop().expect("stack for load addr");
                    addr := self.compute_address(addr, offset.bitcast());
                    
                    result := self.f.newtmp("load", encoding.cls);
                    self.stack&.push(result);
                    a0 = addr; a1 = QbeNull; r = result; 
                };
                fn store() => {
                    align := self.read_u();
                    offset := self.read_u();
                    value := self.stack&.pop().expect("stack for store value");
                    addr := self.stack&.pop().expect("stack for store addr");
                    addr := self.compute_address(addr, offset.bitcast());
                    a0 = value; a1 = addr; r = QbeNull; 
                };
            };
            
            self.f.emit(encoding.op, encoding.cls, r, a0, a1);
            continue();
        }; // Otherwise it's a more complicated instruction. 
        
        @match(inst) {
            fn I32_Const() => self.stack&.push(self.f.getcon(self.read_s()));
            fn I64_Const() => self.stack&.push(self.f.getcon(self.read_s()));
            fn F64_Const() => self.push_float(u64);
            fn F32_Const() => self.push_float(u32);
            fn LocalGet()  => self.local_inst(1);
            fn LocalSet()  => self.local_inst(-1);
            fn LocalTee()  => self.local_inst(0);
            // TODO: this is unfortunate code to generate but i don't know how you do better without knowing that tables are immutable. 
            //       if max=min then at least you know they don't resize and you could save an indirection. 
            fn CallIndirect() => {
                type_index := self.read_u();
                table := self.read_u();
                index := self.stack&.pop().expect("call index on stack");
                externref := self.table_get(table, index);
                r := temp().alloc_init(Qbe.Ref, 3, fn(_) => self.f.newtmp("", .Kl));
                self.f.emit(.add, .Kl, r[2], externref, self.f.getcon(8));
                self.f.emit(.load, .Kl, r[0], externref, Qbe.Null);
                self.f.emit(.load, .Kl, r[1], r[2], Qbe.Null);
                self.emit_call_from_stack(r[0], r[1], (id = type_index.trunc()));
            }
            fn TableGet() => {
                table := self.read_u();
                index := self.stack&.pop().expect("table index on stack");
                r := self.table_get(table, index);
                self.stack&.push(r);
            }
            fn TableSet() => {
                table := self.read_u();
                value := self.stack&.pop().unwrap();
                index := self.stack&.pop().unwrap();
                table_slot := self.compute_table_slot(table, index);
                self.f.emit(.blit0, .Kw, QbeNull, value, table_slot);
                self.f.emit(.blit1, .Kw, Qbe.Null, INT(size_of(Rt.Callable)), Qbe.Null);
            }
            fn Call() => {
                callee := self.read_index(Wasm.FuncIdx);
                callee := callee.id.zext();
                type_index := self.function_types[callee];
                
                if callee < self.counts&[.imports] {
                    off := Rt'Instance'compute_offset(self.counts&, .imports, callee);
                    r := temp().alloc_init(Qbe.Ref, 4, fn(_) => self.f.newtmp("", .Kl));
                    r[0] = self.f.newtmp(self.function_names[callee], .Kl);
                    self.f.emit(.add, .Kl, r[3], self.local_rt_header, self.f.getcon(off));  // *Callable
                    self.f.emit(.add, .Kl, r[2], r[3], self.f.getcon(8));
                    self.f.emit(.load, .Kl, r[0], r[3], Qbe.Null);
                    self.f.emit(.load, .Kl, r[1], r[2], Qbe.Null);
                    self.emit_call_from_stack(r[0], r[1], type_index);
                } else {
                    callee := self.f.symcon(self.functions[callee]);
                    self.emit_call_from_stack(callee, self.local_rt_header, type_index);
                };
            }
            fn Return() => self.emit_return();
            fn GlobalSet() => {
                i: i64 = self.read_u().bitcast();
                g := self.compute_global_slot(i);
                val := self.stack&.pop().expect("enough stack");
                k := self.global_cls[i];
                self.f.emit(k.store_op(), .Kw, QbeNull, val, g); 
            }
            fn GlobalGet() => {
                i: i64 = self.read_u().bitcast();
                g := self.compute_global_slot(i);
                k := self.global_cls[i];
                val := self.f.newtmp("global_get", k);
                self.stack&.push(val);
                self.f.emit(.load, k, val, g, QbeNull);
            }
            fn VectorPrefix() => panic("Vector instructions are not supported (0xFD prefix)");
            fn Nop() => ();
            fn Unreachable() => if self.get_blk()[].jmp.type == .Jxxx {
                self.get_blk()[].jmp.type = .hlt;
            };
            fn Loop()  => self.push_block(self.read_s(),  true);
            fn Block() => self.push_block(self.read_s(), false);
            fn End()   => self.end_block(true);
            fn If() => {
                block_type := self.read_s();
                old := self.get_blk();
                old.jmp = (type = .jnz, arg = self.stack&.pop().expect("arg on stack for If"));
                stack := self.stack.items().clone(temp()).items();
                if_stack&.push(src = old, block_type = block_type, stack = stack); 
                self.push_block(block_type, false);
                old.s1 = self.get_blk();
            }
            fn Else() => {
                self.end_block(false);
                if_node := if_stack&.pop().expect("Else only inside If");
                self.stack = assume_owned(if_node.stack, temp());
                self.push_block(if_node.block_type, false);   // TODO: feels like this wont work. i just don't generate if/else yet
                if_node.src.s2 = self.get_blk();
            }
            fn Br() => {
                src := self.get_blk();
                src.jmp.type = .jmp;
                src.s1 = self.parse_branch_target();
                self.stack_to_phi(src, src.s1);
            }
            fn BrIf() => {
                src := self.get_blk();
                src.jmp = (type = .jnz, arg = self.stack&.pop().expect("arg on stack for BrIf"));
                // TODO: only clone the part that gets popped. 
                stack := self.stack.items().clone(temp());
                src.s1 = self.parse_branch_target();
                self.stack_to_phi(src, src.s1);
                
                // If the condition is false, fallthrough and keep parsing instructions into a new block 
                // (without consuming anything from the stack). don't need phis since this is the only entry point. 
                src.s2 = self.link_new_block();
                current := self.blocks[self.blocks.len - 1].current&;
                self.f.copy_instructions_from_scratch_reversed_which_means_forwards(current[]);
                current[] = src.s2;
                self.stack = stack;
            }
            fn Drop() => {
                self.stack&.pop().expect("stack for store drop");
            }
            fn Select() => {
                count := self.read_u();
                @assert_eq(count, 1, "TODO: multi-value");
                k := cls(@as(Wasm.ValType) @as(u8) self.src[self.cursor]);
                self.cursor += 1;
                @assert(k.is_int(), "TODO: select on floats");

                result := self.f.newtmp("select", k);
                cond := self.stack&.pop().expect("stack for sel-cond");
                f := self.stack&.pop().expect("stack for sel-false");
                t := self.stack&.pop().expect("stack for sel-true");
                self.f.emit(.sel0, .Kw, QbeNull, cond, QbeNull);
                self.f.emit(.sel1, k, result, t, f);
                self.stack&.push(result);
            }
            fn AtomicPrefix() => {
                opcode := self.read_u();
                align := self.read_u();
                offset: u64 = @if(opcode == 3, 0, self.read_u());
                
                k: Qbe.Cls = @switch(opcode) {
                    @case(72) => .Kw;
                    @case(73) => .Kl;
                    @default => {
                        self.choose_runtime_call(start_cursor);
                        continue()
                    };
                };
                
                new := self.stack&.pop().expect("stack for cas-new");
                old := self.stack&.pop().expect("stack for cas-old");
                p := self.stack&.pop().expect("stack for cas-p");
                p := self.compute_address(p, offset.bitcast());
                prev := self.f.newtmp("cas-prev", k);
                
                self.f.emit(.cas0, .Kw, QbeNull, p, QbeNull);
                self.f.emit(.cas1, k, prev, old, new);
                self.stack&.push(prev);
            }
            fn PrefixFC() => {
                opcode := @as(u64) self.read_u();
                if opcode == 10 {  // memory.copy
                    @debug_assert(self.src[self.cursor] == 0 && self.src[self.cursor + 1] == 0);
                    self.cursor += 2;
                    n := self.stack&.pop().unwrap();
                    src := self.stack&.pop().unwrap();
                    dest := self.stack&.pop().unwrap();
                    src := self.compute_address(src, 0);
                    dest := self.compute_address(dest, 0);
                    if self.f.get_int(n) { n |
                        // TODO: this is wrong! you're allowed overlap
                        self.f.emit(.blit0, .Kw, QbeNull, src, dest);
                        self.f.emit(.blit1, .Kw, Qbe.Null, INT(n), Qbe.Null);
                    } else {
                        panic("TODO: non-constant memory.copy");
                    }
                    continue();
                };
                self.choose_runtime_call(start_cursor);
            }
            fn I32_EqZero() => self.eqz(.ceqw);
            fn I64_EqZero() => self.eqz(.ceql);
            @default => self.choose_runtime_call(start_cursor);
        }
    }
}

fn choose_runtime_call(self: *LoadWasm, start: i64) void = {
    it, indices := self.match_runtime_call(start)
        || @panic("unhandled instruction %\n(at byte index % of % = global function %)", 
            self.src[start], start, self.m.str(self.f.lnk.id), self.log_local_func_i + self.w.import_count&[.Func]);
    // found the right instruction so emit a call to the function in Rt

    args := self.stack.items().rest(self.stack.len - it.stacked.len);
    externrefs := Qbe.Ref.list(temp());
    i := 0;
    for it.match { it |
        continue :: local_return;
        ::enum(InstSeq.Arg);
        @if_let(it) fn int(it) => {
            var := @match(it) {
                fn memory() => {
                    @assert_eq(indices[i], 0, "TODO: multiple memories");
                    memory_ptr := self.f.newtmp("memory", .Kl);
                    self.f.emit(.load, .Kl, memory_ptr, self.local_rt_header, QbeNull);
                    memory_ptr
                }
                fn table() => {
                    off := Rt'Instance'compute_offset(self.counts&, .tables, indices[i]);
                    r := temp().alloc_init(Qbe.Ref, 2, fn(_) => self.f.newtmp("", .Kl));
                    self.f.emit(.add, .Kl, r[0], self.local_rt_header, self.f.getcon(off));  // **Table
                    self.f.emit(.load, .Kl, r[1], r[0], QbeNull);  // *Table
                    r[1]
                }
                fn data() => self.f.symcon(self.datas[indices[i]].id);
                fn memarg() => {
                    _align, off := (indices[i], indices[i + 1]);
                    args[0] = self.compute_address(args[0], off);
                    i += 2;
                    continue()
                }
            };
            externrefs&.push(var);
            i += 1;
        }
    };
    
    self.f.emit(.arg, .Kl, QbeNull, self.local_rt_header, QbeNull);
    for externrefs { it |
        self.f.emit(.arg, .Kl, QbeNull, it, QbeNull);
    };
    range(0, args.len) { i |
        self.f.emit(.arg, it.stacked[i], QbeNull, args[i], QbeNull);
    };
    self.stack.len -= it.stacked.len;
    callee := self.f.getcon(bit_cast_unchecked(rawptr, i64, it.callee));
    if it.result == .Kx {
        self.f.emit(.call, .Kw, Qbe.Null, callee, Qbe.Null);
    } else {
        result := self.f.newtmp(it.name, it.result);
        self.f.emit(.call, it.result, result, callee, Qbe.Null);
        self.stack&.push(result);
    };
}

fn match_runtime_call(self: *LoadWasm, start: i64) ?Ty(*InstSeq, []i64) = {
    indices := i64.list(temp());
    each collect_runtime_calls() { it |
        continue :: local_return;
        self.cursor = start;
        indices&.clear();
        for it.match { it |
            @match(it) {
                fn byte(it) => {
                    @if(self.src[self.cursor] != it) continue();
                    self.cursor += 1;
                };
                fn int(it) => range(0, 1 + int(it == .memarg)) { _ |
                    indices&.push(self.read_u().bitcast());
                };
            };
        };
        return(Some = (it, indices.items()));
    };
    .None
}
        
InstSeq :: @struct {
    Arg :: @enum(u8) (memory, table, data, memarg);
    Inst :: @tagged(byte: u8, int: Arg);
    callee: rawptr;
    match: []Inst;
    stacked: []Qbe.Cls;
    result: Qbe.Cls;
    name: Str;
};

fn collect_runtime_calls() []InstSeq #fold = {
    out := InstSeq.list(ast_alloc());
    fr := current_compiler_context();
    for get_constants(Rt) { name |
        continue :: local_return;
        val, ty := get_constant(Rt, name) || continue();
        @if(ty != FuncId) continue();
        fid := FuncId.ptr_from_raw(val)[];
        func := get_function_ast(fid, true, true, true, false);
        i := func.annotations&.index_of(fn(it) => it.name == @symbol wasm)
            || continue();
        args := func.annotations[i].args&.items();
        insts := InstSeq.Inst.list(args.len, ast_alloc());
        const_args := 1 /*base_pointer*/;
        ::enum(InstSeq.Arg);
        each args { it |
            if it.expr&.is(.ContextualField) {
                it[] = @{ @as(InstSeq.Arg) @[it[]] };
            };
            it[] = compile_ast(it[]);
            @switch(it.ty) {
                @case(u8) => insts&.push(byte = u8.const_eval(it[]));
                @case(InstSeq.Arg) => {
                    value := InstSeq.Arg.const_eval(it[]);
                    insts&.push(int = value);
                    const_args += int(value != .memarg);
                };
                @default => compile_error("invalid type", it.loc);
            };
        };
        stacked := Qbe.Cls.list(ast_alloc());
        #use("@/compiler/ast_external.fr");
        for fr.arg_types(func.finished_arg.unwrap()).rest(const_args) { ty |
            stacked&.push(fr_ty_to_cls(ty));
        };
        out&.push(
            callee = fr.get_jitted(fid), 
            match = insts.items(), 
            stacked = stacked.items(),
            result = fr_ty_to_cls(func.finished_ret.unwrap()),
            name = fr.get_string(name),
        );
    };
    out.items()
};

fr_ty_to_cls :: fn(ty: Type) Qbe.Cls = @switch(ty) {
    @case(i32) => .Kw;
    @case(u32) => .Kw;
    @case(i64) => .Kl;
    @case(f32) => .Ks;
    @case(f64) => .Kd;
    @case(void) => .Kx;
    @default => .Kl;
};

fn eqz(self: *LoadWasm, o: Qbe.O) void = {
    result := self.f.newtmp("eqz", .Kw);
    value := self.stack&.pop().unwrap();
    self.f.emit(o, .Kw, result, value, Qbe.ConZero);
    self.stack&.push(result);
}

fn table_get(self: *LoadWasm, table_index: u64, index_in_table_W: Qbe.Ref) Qbe.Ref = {
    table_slot := self.compute_table_slot(table_index, index_in_table_W);
    result := self.f.newtmp("table.get", .Kl);
    // TODO: this will have to change when i want to support returning (func/extern)ref
    self.f.emit(.alloc8, .Kl, result, self.f.getcon(size_of(Rt.Callable)), Qbe.Null);
    self.f.emit(.blit0, .Kw, QbeNull, table_slot, result);
    self.f.emit(.blit1, .Kw, Qbe.Null, INT(size_of(Rt.Callable)), Qbe.Null);
    result
}

fn compute_table_slot(self: *LoadWasm, table_index: u64, index_in_table_W: Qbe.Ref) Qbe.Ref = {
    off := Rt'Instance'compute_offset(self.counts&, .tables, table_index.bitcast());
    r := temp().alloc_init(Qbe.Ref, 2, fn(_) => self.f.newtmp("", .Kl));
    self.f.emit(.add, .Kl, r[0], self.local_rt_header, self.f.getcon(off));  // **Table
    self.f.emit(.load, .Kl, r[1], r[0], QbeNull);  // *Table

    table_var        := r[1];
    index_in_table_L := self.f.newtmp("Iindex", .Kl);
    table_slot       := self.f.newtmp("Islot", .Kl);
    table_offset     := self.f.newtmp("Ioff", .Kl);
    table_base       := self.f.newtmp("Ibase", .Kl);

    @run @assert_eq(offset_of(Rt.Table, .ptr), 0);    
    self.f.emit(.load, .Kl, table_base, table_var, QbeNull);  // *Callable
    self.f.emit(.extuw, .Kl, index_in_table_L, index_in_table_W, QbeNull);
    self.f.emit(.mul, .Kl, table_offset, index_in_table_L, self.f.getcon(size_of(Rt.Callable)));
    self.f.emit(.add, .Kl, table_slot, table_base, table_offset);
    table_slot
}

// TODO: cache these per function in the same way i do for the memory base address
fn compute_global_slot(self: *LoadWasm, i: i64) Qbe.Ref = {
    off := Rt'Instance'compute_offset(self.counts&, .globals, i);
    r := temp().alloc_init(Qbe.Ref, 2, fn(_) => self.f.newtmp("", .Kl)); 
    self.f.emit(.add, .Kl, r[0], self.local_rt_header, self.f.getcon(off));  // **GlobalValue
    self.f.emit(.load, .Kl, r[1], r[0], QbeNull);  // *GlobalValue
    r[1]
} 

fn parse_branch_target(self: *LoadWasm) *Qbe.Blk = {
    depth: i64 = self.read_u().bitcast();
    it := self.blocks[self.blocks.len - depth - 1];
    @if(it.loop, it.continue, it.break)
}

fn push_float(self: *LoadWasm, $Bits: Type) void = {
    v := Bits.cast_front_unaligned(self.src.rest(self.cursor));
    value: i64 = v.int();
    self.cursor += Bits.size_of();
    self.stack&.push(self.f.getcon(value));
}

fn stack_to_phi(self: *LoadWasm, src: *Qbe.Blk, dest: *Qbe.Blk) void = {
    for_phi dest { p |
        p.push(src, self.stack&.pop() || @panic("missing value for phi % -> % %", src.id, dest.id, { printfn(self.f, self.f.globals.debug_out); ""}));
    }
}

fn decode_block_type(self: *LoadWasm, block_type: i64) Ty([]Wasm.ValType, []Wasm.ValType) = {
    if block_type >= 0 {
        ty := self.w.types[block_type]&;
        return(ty.arg.items(), ty.ret.items());
    };
    if block_type == preleb(.EmptyBlock).intcast() {
        return(empty(), empty());
    };
    
    // :SLOW
    values :: get_cases(Wasm.ValType);
    each values { it |
        if block_type == preleb(it[]).intcast() {
            return(empty(), (ptr = it, len = 1));
        };
    };
    
    @panic("invalid block type %", block_type)
}

fn get_blk(self: *LoadWasm) *Qbe.Blk =
    self.blocks[self.blocks.len - 1].current;

fn push_block(self: *LoadWasm, block_type: i64, loop: bool) void = {
    arg, ret := self.decode_block_type(block_type);
    ::enum(Wasm.ValType);  ::DerefEq(Wasm.ValType);
    
    old := self.blocks[self.blocks.len - 1]&;
    new := self.link_new_block();
    init_empty_phis(self.f, new, arg);
    self.stack_to_phi(old.current, new);
    self.f.copy_instructions_from_scratch_reversed_which_means_forwards(old.current);
    continuation := self.link_new_block();
    init_empty_phis(self.f, continuation, ret);  // break's inputs are the new block's outputs
    if old.current.jmp.type == .Jxxx {
        old.current.jmp.type = .jmp;
        old.current.s1 = new;
    }
    old.current = continuation;
    self.phi_to_stack(new);
    self.blocks&.push(continue = new, break = continuation, current = new, loop = loop);
    
    init_empty_phis :: fn(f: *Qbe.Fn, b: *Qbe.Blk, arg_types: []Wasm.ValType) void = 
        for(arg_types, fn(ty) => new_phi(f, b, ty.cls(), 1));
}

fn end_block(self: *LoadWasm, fallthrough: bool) void = {
    it := self.blocks&.pop().expect("well nested End instructions");
    self.f.copy_instructions_from_scratch_reversed_which_means_forwards(it.current);
    if it.current.jmp.type == .Jxxx {
        self.stack_to_phi(it.current, it.break);
        it.current.jmp.type = .jmp;
        it.current.s1 = it.break;
    }
    it.current = zeroed(*Qbe.Blk);
    self.blocks[self.blocks.len - 1].current = it.break;
    if fallthrough {
        self.phi_to_stack(it.break);
    }
}

fn phi_to_stack(self: *LoadWasm, b: *Qbe.Blk) void = {
    for_phi_rev_TODOSLOW b { p |
        self.stack&.push(p.to);
    }
}

fn link_new_block(self: *LoadWasm) *Qbe.Blk = {
    b := newblk();
    b.id = self.f.nblk;
    self.f.nblk += 1;
    self.link[] = b; 
    self.link = b.link&;
    b
}

fn compute_address(self: *LoadWasm, addr: Qbe.Ref, offset: i64) Qbe.Ref = {
    f := self.f;
    r0 := f.newtmp("a", .Kl);
    r1 := f.newtmp("a", .Kl);
    f.emit(.extuw, .Kl, r0, addr, QbeNull);
    f.emit(.add, .Kl, r1, self.local_memory_base, r0);
    if(offset == 0, => return(r1));
    r2 := f.newtmp("a", .Kl);
    f.emit(.add, .Kl, r2, r1, f.getcon(offset));
    r2
}

fn local_inst(self: *LoadWasm, delta: i64) void = {
    i    := self.read_u();
    var  := self.locals[i.bitcast()];
    k := self.f.get_temporary(var)[].cls;
    ::if(Qbe.Ref);
    dest := if(delta == 1, => self.f.newtmp(@tfmt("getL%", i), k), => var);
    src  := if(delta == 1, => var, => self.stack[self.stack.len - 1]);
    self.stack&.reserve(abs(delta));
    self.stack.len += delta;
    if delta == 1 {
        @debug_assert(self.stack.len > 0, "tried to access empty stack in local_inst");
        self.stack[self.stack.len - 1] = dest;
    };
    self.f.emit(.copy, k, dest, src, QbeNull);
}

fn emit_return(self: *LoadWasm) void = {
    ::enum(Qbe.Cls);
    @debug_assert(self.get_blk()[].jmp.type == .Jxxx, "tried to return from terminated block");
    @switch(self.return_types.len) {
        @case(0) => {
            self.get_blk()[].jmp.type = .ret0;
        };
        @case(1) => {
            k := self.return_types[0].cls();
            self.get_blk()[].jmp = (type = k.retk(), arg = self.stack[self.stack.len - 1]);
            self.stack.len -= 1;
            self.f.ret_cls = k;
        };
        @default => panic("TODO: handle multiple returns");
    };
}

fn emit_call_from_stack(self: *LoadWasm, callee: Qbe.Ref, header: Qbe.Ref, type: Wasm.TypeIdx) void = {
    type := self.w.types[type.id.zext()]&;
    @assert_le(
        type.arg.len, self.stack.len, 
        "not enough stack for call to % %", callee, 
        @if(rtype(callee) == .RCon, self.m.str(self.f.get_constant(callee)[].sym), ""),
    );

    self.f.emit(.arg, .Kl, QbeNull, header, QbeNull);
    range(0, type.arg.len) { i |
        ref := self.stack[self.stack.len - type.arg.len + i];
        k := type.arg[i].cls();
        self.f.emit(.arg, k, QbeNull, ref, QbeNull);
    };
    self.stack.len -= type.arg.len;
    
    @switch(type.ret.len) {
        @case(0) => {
            self.f.emit(.call, .Kw, QbeNull, callee, QbeNull);
        };
        @case(1) => {
            k := type.ret[0].cls();
            ref := self.f.newtmp("ret", k);
            self.f.emit(.call, k, ref, callee, QbeNull);
            self.stack&.push(ref);
        };
        @default => panic("TODO: handle multiple returns");
    };
}
