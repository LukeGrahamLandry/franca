//! This converts the legacy bytecode into the new ir. 
//! It was used in the interim time when the frontend couldn't generate the new ir directly. 

#include_std("compiler/worker.fr");  // for emit_constant
#include_std("compiler/pool.fr");

EmitQbe2 :: @struct(
    comp: CompCtx,
    b: QbeBuilder,
    block_args: List(Qbe.Ref),
    indirect_return_slot := Qbe.Ref.zeroed(),
    out: BucketArray(u8),
    current: BbId,
    next_var: i64 = 0,
    return_value_type_sizes: BitSet,
    current_has_indirect_return := false,
    os: Os,
    signeture := PrimSig.zeroed(), // TODO: HACK   also now redundant except that you don't always have a body. 
    body := zeroed(*FnBody), // :EVIL
    pair_type_id := 99999999, // filled by init
    context := QbeNull,
    implicit_context: bool,
    my_node := QbeNull,
    env_node := QbeNull,
    arch: Arch,
    $Val := Qbe.Ref, // TODO: const can't be first (doesn't parse).  -- Jun 14
);

// TODO: dependency problem so can't call this `init`
fn new(comp: CompCtx, alloc: Alloc, arch: Arch, os: Os, m: rawptr) EmitQbe2 = {
    gpa := libc_allocator;
    s: EmitQbe2 = (
        comp = comp, 
        b = (
            globals = QbeModule.ptr_from_raw(m),
            temporaries = list(gpa),
            constants = list(gpa),
            blocks = list(gpa),
            aggragate_return_index = .None,
        ), 
        block_args = list(temp()), 
        out = init(12, alloc), 
        current = BbId.zeroed(), 
        return_value_type_sizes = empty(), 
        os = os,
        arch = arch,
        implicit_context = comp.get_build_options()[].implicit_context_runtime,
    );
    f: []Qbe.Field = @slice(@as(Qbe.Field) (type = .Fl, len = 8.trunc()), @as(Qbe.Field) (type = .Fl, len = 8.trunc()));
    s.pair_type_id = s.b&.struct_type(f, 16, 3);
    s
}

fn finished(self: *EmitQbe2, fid: FuncId) void = {
    name: List(u8) = list(temp()); 
    self.fmt_fn_name(fid, name&);
    func := self.comp.get_function(fid);
    self.b.lnk.no_inline = func.get_flag(.NoInline);
    self.build(name.items(), self.comp.get_log_types(fid));
    self.body = zeroed(*FnBody); // :EVIL     but just avoiding confusion. 
    self.context = QbeNull;
    self.my_node = QbeNull;
}; 

fn add_code_bytes(self: *EmitQbe2, name: Str, bytes: []u8) void = {
    id := self.b.globals.intern(name);
    self.b.globals.add_code_bytes(id, bytes);  
}

fn emit_function_husk(self: *EmitQbe2, fid: FuncId, signeture: PrimSig, $emit_body: @Fn() void) void = {
    //@println("Emitting F% %", fid.to_index(), self.comp.get_string(self.comp.get_function(fid)[].name));
    emit_body();
}

fn emit_entry_points_and_debug_info(gen: *CodeGen(EmitQbe2), comp: CompCtx, fns: []FuncId, entry: ProgramEntry) void = {
    self := gen.backend&;

    @match(entry) {
        fn TestRunnerMain() => {
            fake_decl_name := "test_runner_main";
            entry := self.b&.push_block();
            self.current = entry;
            env := self.b&.push_inst(self.current, .pare, .Kl, QbeNull, QbeNull);
            _ := self.b&.push_inst(self.current, .par, .Kl, QbeNull, QbeNull);
            _ := self.b&.push_inst(self.current, .par, .Kl, QbeNull, QbeNull);
            // TODO: warn if fns has duplicates
            for fns {f|
                callee := self.inst_func_ref(f);
                self.b&.push_inst0(self.current, .arge, .Kl, env, QbeNull);
                self.b&.push_inst0(self.current, .call, .Kw, callee, QbeNull);
            };
            self.b&.end_block(self.current, (type = .retl, arg = self.b&.push_literal(0)));
            self.build(fake_decl_name, "");
            
            // now actually call it
            self.init_runtime_entry_point(fake_decl_name);
        };
        fn GiveMeTheCodeAndGiveItToMeRaw() => ();
        fn ExportWithNames() => {
            // TODO: warn if fns has duplicates
            for fns { fid |
                gen.backend&.write_export_bounce(fid);
            };
        };
        fn WrapMain() => {
            @assert(fns.len == 1, ".WrapMain expected a single entry point");
            self.init_runtime_entry_point(self.fn_name_str(fns[0]));
        }
    };
    
    gen.loop_emit_constants();
    
    m := self.b.globals;
    chunks := {self.comp.vtable.finish_qbe_module}(QbeModule.raw_from_ptr(m));
    for chunks { c |
        self.out&.push_bucket(maybe_uninit = c, len = c.len, gpa = temp());
    };
}

fn init_runtime_entry_point(self: *EmitQbe2, usermain_name: Str) void = {
    env := self.comp.get_comptime_env();
    runtime_init := env.runtime_init.expect("declaration of franca_runtime_init");
    
    self.b.lnk.export = true;
    entry := self.b&.push_block();
    self.current = entry;

    argc := self.b&.push_inst(self.current, .par, .Kl, QbeNull, QbeNull);
    argv := self.b&.push_inst(self.current, .par, .Kl, QbeNull, QbeNull);
    user_main := self.b&.push_symbol(usermain_name);
    runtime_init := self.inst_func_ref(runtime_init);
    self.b&.push_inst0(self.current, .arg, .Kl, argc, QbeNull);
    self.b&.push_inst0(self.current, .arg, .Kl, argv, QbeNull);
    self.b&.push_inst0(self.current, .arg, .Kl, user_main, QbeNull);
    self.b&.push_inst0(self.current, .call, .Kw, runtime_init, QbeNull);
    self.b&.end_block(self.current, (type = .ret0, arg = QbeNull));
    self.build("main", "");
}

fn emit_bounce_fn(self: *EmitQbe2, impl_name_callee: Str, fake_decl_name: Str, signeture: PrimSig, private: bool) void = {
    self.b.lnk.export = !private;
    self.context = QbeNull;
    self.my_node = QbeNull;
    
    entry := self.b&.push_block();
    self.current = entry;
    self.block_args = list(0, temp());
    
    params: List(Qbe.Ref) = list(temp());
    for signeture.args { prim | 
        params&.push(self.b&.push_parameter(qbe_real_class(prim)));
    };
    
    self.current_has_indirect_return = signeture.first_arg_is_indirect_return;
    if signeture.first_arg_is_indirect_return {
        size := self.b&.push_literal(@as(i64) signeture.return_value_bytes.zext());
        self.indirect_return_slot = self.b&.push_inst(entry, .alloc8, .Kl, size, QbeNull);
        params&.insert(0, self.indirect_return_slot);
        // :set_aggragate_return_index
        self.b.aggragate_return_index = (Some = self.b&.opaque_type_slow(signeture.return_value_bytes.zext(), 3));
    };
    if signeture.ret2.is_some() {
        self.indirect_return_slot = self.b&.push_inst(entry, .alloc8, .Kl, self.b&.push_literal(16), QbeNull);
    };
    
    callee := self.b&.push_symbol(impl_name_callee);
    self.signeture = signeture;
    ret := self.inst_call(params.items(), signeture, callee, true); // TODO: :Context?
    
    self.inst_return(ret);
    
    self.build(fake_decl_name, "");
}

// TODO: copy-paste from DynamicImport but reversed so its a pain. 
fn write_export_bounce(self: *EmitQbe2, fid: FuncId) void = {
    func := self.comp.get_function(fid);
    export_name := self.comp.get_string(func.name);
    internal_name: List(u8) = list(temp()); 
    self.fmt_fn_name(fid, internal_name&);
    
    sig := self.comp.prim_sig(func).unwrap();
    self.emit_bounce_fn(internal_name.items(), export_name, sig, false);
}

// TODO: we'd rather do constants gradually instead of all at the end?
//     current system is garbage because you always have to build up a bunch of fixups when generating code. 
//     but maybe it's worth waiting for nwe emit_ir instead of caring about fixing it now.  -- Nov 28
fn emit_constant(self: *EmitQbe2, id: BakedVarId) void = {
    mark := mark_temporary_storage();
    out := Qbe.Dat.list(temp());
    self.comp.emit_constant(self.b.globals, id, out&);
    each out { it |
        self.b.globals.new_emit_data(it);
    };
    reset_temporary_storage(mark);
}

// :DumbNameAlias
// TODO: we do it this way because we don't know if we've already emitted a reference to the __1234 name.
//       For our own backend here at least, it would be easy to add support for aliases where you just let it deal with resolving
//       the symbols to the same thing. or fix it properly but like meh.  

fn emit_special(self: *EmitQbe2, f_id: FuncId, body: *FuncImpl, func: *Func, bc: *FnBody, pending: *List(FuncId)) bool = {
    comp := self.comp;
    @match(body) {
        fn Normal(_) => { panic("ICE: empty body but expr"); };
        fn Redirect(f) => {
            // :DumbNameAlias
            // TODO: this is fucking stupid :SLOW
            // TODO: assert that 'f' is flagged for being emitted?
            //       currently just trusting that emit_bc replaced all actual uses. 
            //       could also just have the front end check if body is redirect before adding to callees list. -- Jun 27
            
            pending.push(f[]);
            
            // :sema_regression :ExtraRedirectShim
            assert(f[] != f_id, "redirect to yourself");
            new_name := @format("%", (self, f[])) temp();
            old_name := @format("%", (self, f_id)) temp();
            self.emit_bounce_fn(new_name.items(), old_name.items(), bc.signeture, true);
            return(true);
        }
        fn Merged(parts) => { 
            each(parts[].items()) {check: *FuncImpl| 
                if(self.emit_special(f_id, check, func, bc, pending), => return(true));
            };
        }
        fn DynamicImport(name) => { 
            // :DumbNameAlias
            // TODO: this is fucking stupid :SLOW
            // TODO: do i need to hackily do the same for ComptimeAddr and hope for the best? ideally the forntend would handle that instead. 
            import_name := comp.check_link_rename(name[], func, (arch = self.arch, os = self.os));
            self.forward_declare(bc.signeture, false, import_name);
            name := @format("%", (self, bc.func)) temp();
            self.emit_bounce_fn(import_name, name.items(), bc.signeture, true);
            return(true);
        }
        fn TargetOsSplit(it) => {
            // :DumbNameAlias
            if it.os == self.os {
                // TODO: :copy-paste from emit_special:redirect
                pending.push(it.fid);
                assert(it.fid != f_id, "redirect to yourself");
                new_name := @format("%", (self, it.fid)) temp();
                old_name := @format("%", (self, f_id)) temp();
                self.emit_bounce_fn(new_name.items(), old_name.items(), bc.signeture, true);
                return(true);
            };
        }
        @default fn() void => return(false);
    };
    false
}

fn forward_declare(self: *EmitQbe2, sig: PrimSig, private: bool, name: Str) void = {
    // Qbe doesn't need external functions to be declared. The call instruction has the type information. 
}

fn emit_special_asm(self: *EmitQbe2, body: *FuncImpl, func: *Func, bc: *FnBody, target: *TargetEnv) bool = {
    arch := target.arch;
    comp := self.comp;
    @match(body) {
        (fn Merged(parts) => { 
            each(parts[].items()) {check: *FuncImpl| 
                if(self.emit_special_asm(check, func, bc, target), => return(true));
            };
        });
        (fn JittedAarch64(code) => { 
            if(arch.ne(Arch.aarch64), => { return(false); });
            self.add_code_bytes(self.fn_name_str(bc.func), code.items().interpret_as_bytes());
            return(true);
        });
        fn X86AsmBytes(code) => { // :copy-paste
            if target.arch != .x86_64 { 
                return(false); 
            };
            self.add_code_bytes(self.fn_name_str(bc.func), code.items());
            return(true);
        }
        @default fn() void => return(false);
    };
    false
}

fn setup(self: *EmitQbe2, body: *FnBody, vars_out: *List(Qbe.Ref)) void = {
    entry := self.b&.push_block();
    opts := self.comp.get_build_options();
    self.signeture = body.signeture;
    self.body = body;
    params: List(Qbe.Ref) = list(temp());
    self.context = QbeNull;
    self.my_node = QbeNull;
    if body.context && self.implicit_context {
        self.context = self.b&.push_inst((id = 0), .pare, .Kl, QbeNull, QbeNull);
    };
    for body.signeture.args { prim | 
        params&.push(self.b&.push_parameter(qbe_real_class(prim)));
    };
    do_debug := self.comp.get_build_options()[].debug_info;
    
    // TODO: waste for ssa vars, they just get set to the value. -- Jun 14
    enumerate body.vars.items() { i, ty |
        op := Qbe.O.alloc4;
        align := 4;
        if(ty.align > align.trunc(), => { align = 8; op = .alloc8; });
        if(ty.align > align.trunc(), => { align = 16; op = .alloc16; });
        
        size := self.b&.push_literal(@as(i64) ty.size.zext());
        
        prefix := "v";
        if do_debug && body.var_names.len > i {
            name := body.var_names[i];
            if !(name.raw() == 0) {
                prefix = self.comp.get_string(name);
            };
        };
        
        v := self.b&.push_temp(.Kl, prefix);
        self.b&.push_inst(entry, op, .Kl, v, size, QbeNull);
        vars_out.push(v);
    };
    // TODO: use phi nodes cause this is stupid but that's such a pain in the ass. 
    // TODO: actually qbe vars are mutable so could just use those. 
    // I use these for block arguments. 
    max_args := 0;
    each body.blocks.items() { b |
        max_args = max_args.max(b.arg_prims.len);
    };
    self.block_args = list(max_args, temp());
    range(0, max_args) {i|
        v := self.b&.push_inst(entry, .alloc8, .Kl, self.b&.push_literal(8), QbeNull);
        self.block_args&.push(v);
    };
    
    block_count := 0;
    each body.blocks { b |
        block_count += int(!b.insts[0]&.is(.NoCompile));
    };
    
    // >5 so don't trace tiny functions (like say... get_dynamic_context which you have to call in init before you've setup the first node)
    if opts.runtime_stack_trace && self.context != QbeNull && body.vars.len > 5 && block_count > 1 {
        self.my_node = self.b&.push_inst(entry, .alloc8, .Kl, self.b&.push_literal(12), QbeNull);
        env_node_ref := self.b&.push_inst(entry, .add, .Kl, self.context, self.b&.push_literal(8));
        self.env_node = self.b&.push_inst(entry, .load, .Kl, env_node_ref, QbeNull);
        self.b&.push_inst0(entry, .storel, .Kw, self.env_node, self.my_node);  // store prev
        self.b&.push_inst0(entry, .storel, .Kw, self.my_node, env_node_ref);  // update ctx
        func_id := self.b&.push_inst(entry, .add, .Kl, self.my_node, self.b&.push_literal(8));
        self.b&.push_inst0(entry, .storew, .Kw, self.b&.push_literal(body.func.as_index()), func_id);
    };
    
    self.current_has_indirect_return = body.signeture.first_arg_is_indirect_return;
    shift := if body.signeture.first_arg_is_indirect_return {
        size := self.b&.push_literal(@as(i64) body.signeture.return_value_bytes.zext());
        self.indirect_return_slot = self.b&.push_inst(entry, .alloc8, .Kl, size, QbeNull);
        self.b&.push_inst0(entry, .storel, .Kw, self.indirect_return_slot, self.block_args[0]);
        // :set_aggragate_return_index
        self.b.aggragate_return_index = (Some = self.b&.opaque_type_slow(body.signeture.return_value_bytes.zext(), 3));
        1
    }{| 0 };
    
    if body.signeture.ret2.is_some() {
        self.indirect_return_slot = self.b&.push_inst(entry, .alloc8, .Kl, self.b&.push_literal(16), QbeNull);
    };
    
    enumerate body.signeture.args {i, ty|
        // :StoreAlwaysKw
        self.b&.push_inst0(entry, qbe_store(ty[]), .Kw, params[i], self.block_args[i + shift]);
    };
    
    // It's crimes to jump to the start block so don't use that as our entry block. 
    self.b&.end_block(entry, (type = .jmp, target1 = (id = 1)));
    
    // :AllOurBlocksUpFront
    // The backend might not go to blocks in order but we want BbIds to match. 
    // And we need to add more later for switches (and not have those indices mess us up)
    while => self.b.blocks.len <= body.blocks.len + 1 {
        self.b&.push_block();
    };
}

fn fmt_fn_name(self: *EmitQbe2, f: FuncId, out: *List(u8)) void = 
    out.push_all(self.comp.fmt_fn_name(f));

fn fn_name_str(self: *EmitQbe2, f: FuncId) Str = {
    name: List(u8) = list(temp()); 
    self.fmt_fn_name(f, name&);
    name.items()
}

fn display(self: Ty(*EmitQbe2, FuncId), out: *List(u8)) void = {
    self._0.fmt_fn_name(self._1, out);
}

// TODO: are the float cmp ordered or unordered? 
fn inst_intrinsic(self: *EmitQbe2, args: []Qbe.Ref, op: Intrinsic) []Qbe.Ref = {
    bin :: fn(self: *EmitQbe2, args: [] Qbe.Ref, op: Qbe.O, out: Qbe.Cls) [] Qbe.Ref = {
        a := temp().alloc(EmitQbe2.Val, 1);
        a.ptr[] = self.b&.push_inst(self.current, op, out, args[0], args[1]);
        a
    };
    cast :: fn(self: *EmitQbe2, args: []Qbe.Ref, ty: Prim, op: Qbe.O) []Qbe.Ref = {
        a := temp().alloc(EmitQbe2.Val, 1);
        a.ptr[] = self.b&.push_inst(self.current, op, qbe_real_class(ty), args[0], QbeNull);
        a
    };

    @match(op) {
        fn Add() => self.bin(args, .add, .Kl);
        fn Sub() => self.bin(args, .sub, .Kl);
        fn Mul() => self.bin(args, .mul, .Kl);
        fn Div() => self.bin(args, .div, .Kl);
        fn Eq() => self.bin(args, .ceql, .Kl);
        fn Ne() => self.bin(args, .cnel, .Kl);
        fn Ge() => self.bin(args, .csgel, .Kl);
        fn Le() => self.bin(args, .cslel, .Kl);
        fn Gt() => self.bin(args, .csgtl, .Kl);
        fn Lt() => self.bin(args, .csltl, .Kl);
        // sketch aliasing because walk_bc is gonna copy the slice back onto itself but it seems to be fine. -- Jul 24  
        fn IntToPtr() => args.slice(0, 1); // no-op 
        fn PtrToInt() => args.slice(0, 1); // no-op
        fn ShiftLeft()            => self.bin(args, .shl, .Kl);
        fn ShiftRightLogical()    => self.bin(args, .shr, .Kl);
        fn ShiftRightArithmetic() => self.bin(args, .sar, .Kl);
        fn BitOr()  => self.bin(args, .or, .Kl);
        fn BitAnd() => self.bin(args, .and, .Kl);
        fn BitXor() => self.bin(args, .xor, .Kl);
        fn FAdd() => self.bin(args, .add, .Kd);
        fn FSub() => self.bin(args, .sub, .Kd);
        fn FMul() => self.bin(args, .mul, .Kd);
        fn FDiv() => self.bin(args, .div, .Kd);
        fn FEq() => self.bin(args, .ceqd, .Kl);
        fn FNe() => self.bin(args, .cned, .Kl);
        fn FGe() => self.bin(args, .cged, .Kl);
        fn FLe() => self.bin(args, .cled, .Kl);
        fn FGt() => self.bin(args, .cgtd, .Kl);
        fn FLt() => self.bin(args, .cltd, .Kl);
        
        // I'm not quite sure what these should do. 
        // 
        fn Trunc64To32() => self.cast(args, .I32, .copy);  // args.slice(0, 1); // extuw
        fn Trunc64To16() => self.cast(args, .I32, .extuh); // args.slice(0, 1);
        fn Trunc64To8()  => self.cast(args, .I32, .extub); // args.slice(0, 1);
        fn Trunc32To16() => self.cast(args, .I32, .extuh); // args.slice(0, 1);
        fn Trunc32To8()  => self.cast(args, .I32, .extub); // args.slice(0, 1);
        fn Trunc16To8()  => self.cast(args, .I32, .extub); // args.slice(0, 1);
        
        fn SignExtend32To64() => self.cast(args, .I64, .extsw);
        fn ZeroExtend32To64() => self.cast(args, .I64, .extuw);
        fn ZeroExtend16To64() => self.cast(args, .I64, .extuw);
        fn ZeroExtend8To64()  => self.cast(args, .I64, .extuw);
        fn ZeroExtend16To32() => args.slice(0, 1); // no-op 
        fn ZeroExtend8To32()  => args.slice(0, 1); // no-op 
        fn ZeroExtend8To16()  => args.slice(0, 1); // no-op 
        
        fn IntToFloatValue() => self.cast(args, .F64, .sltof); 
        fn FloatToIntValue() => self.cast(args, .I64, .dtosi); 
        fn IntToFloatBits()  => self.cast(args, .F64, .cast); 
        fn FloatToIntBits()  => self.cast(args, .I64, .cast); 
        fn ShrinkFloat()     => self.cast(args, .F32, .truncd); 
        fn GrowFloat()       => self.cast(args, .F64, .exts); 
        fn BitNot() => {
            a := temp().alloc(EmitQbe2.Val, 1);
            a.ptr[] = self.b&.push_inst(self.current, .xor, .Kl, args[0], self.b&.push_literal(-1));
            a
        }
        fn GetContext() => {
            @debug_assert(self.implicit_context);
            @debug_assert(self.context != QbeNull, "Cannot used GetContext in function without context.");
            args[0] = self.b&.push_inst(self.current, .copy, .Kl, self.context, QbeNull);
            args
        }
        fn SetContext() => {
            @debug_assert(self.implicit_context);
            if self.context == QbeNull {
                self.context = self.b&.push_inst(self.current, .copy, .Kl, args[0], QbeNull); 
            } else {
                self.b&.push_inst(self.current, .copy, .Kl, self.context, args[0], QbeNull); 
            };
            args[0] = QbeConZero;
            args
        }
        @default => @panic("ICE: unhandled qbe inst_intrinsic %", op);
    }
}

fn inst_call(self: *EmitQbe2, args: Slice(Qbe.Ref), sig: PrimSig, f: FuncId, tail: bool, _loc: ?Span, context: bool) Slice(EmitQbe2.Val) = {
    name := self.fn_name_str(f);
    callee := self.b&.push_symbol(name);
    self.inst_call(args, sig, callee, context)
}

fn inst_call(self: *EmitQbe2, args: Slice(Qbe.Ref), sig: PrimSig, callee: Qbe.Ref, context: bool) Slice(EmitQbe2.Val) = {
    self.b.leaf = false;
    out_vals: List(Qbe.Ref) = list(temp());
    return_value := Qbe.Ref.zeroed();
    return_type := Qbe.Cls.zeroed();
    maybe_struct_return_type := Qbe.Ref.zeroed();
    if sig.ret1 { fst: Prim |
        if sig.ret2 { snd: Prim | // two
            return_value = self.get_var(.P64);
            return_type = .Kl;
            maybe_struct_return_type = ref(.RType, self.pair_type_id);
            out_vals&.push(self.get_var(fst));
            out_vals&.push(self.get_var(snd));
        } else { // one
            return_value = self.get_var(fst);
            out_vals&.push(return_value);
            return_type = qbe_real_class(fst);
        };
    } else { // void or indirect
        if sig.first_arg_is_indirect_return {
            return_type = .Kl;
            return_value = self.get_var(.P64);
            size: i64 = sig.return_value_bytes.zext();
            type_id := self.b&.opaque_type_slow(size, 3);
            maybe_struct_return_type = ref(.RType, type_id);
        };
        // TODO: push call
    };
    
    shift := if(sig.first_arg_is_indirect_return, => 1, => 0);
    @debug_assert_eq(args.len, sig.args.len.add(shift), "mismatch args");
    
    if self.context != QbeNull && context {
        self.b&.push_inst0(self.current, .arge, .Kl, self.context, QbeNull);
    };
    range(0, sig.args.len) {i|
        ty := sig.args[i];
        v := args[i + shift];
        self.b&.push_argument(self.current, qbe_real_class(ty), v);
    };
    
    self.b&.push_inst(self.current, .call, return_type, return_value, callee, maybe_struct_return_type);
    if sig.ret2 { t2 | // two
        t1 := sig.ret1.unwrap();
        self.b&.push_inst(self.current, qbe_load(t1), qbe_real_class(t1), out_vals[0], return_value, QbeNull);
        // TODO: for now i always use (i64, i64) but that probably needs to change (ie floats!). 
        offset_to_second_field := self.b&.push_literal(@as(i64) 8);
        struct_out_2 := self.b&.push_inst(self.current, .add, .Kl, return_value, offset_to_second_field);
        self.b&.push_inst(self.current, qbe_load(t2), qbe_real_class(t2), out_vals[1], struct_out_2, QbeNull);
    };
    if sig.first_arg_is_indirect_return {
        size: i64 = sig.return_value_bytes.zext();
        self.b&.push_blit(self.current, args[0], return_value, size);
    };
    out_vals.items()
}

fn inst_trap(self: *EmitQbe2) void = {
    self.b&.end_block(self.current, (type = .hlt));
}

fn inst_call_ptr(self: *EmitQbe2, args: Slice(Qbe.Ref), sig: PrimSig, ptr: EmitQbe2.Val, _loc: ?Span, context: bool) Slice(EmitQbe2.Val) = {
    self.inst_call(args, sig, ptr, context)
}

fn inst_offset(self: *EmitQbe2, ptr: EmitQbe2.Val, bytes: i64) EmitQbe2.Val = {
    bytes := self.b&.push_literal(bytes);
    self.b&.push_inst(self.current, .add, .Kl, ptr, bytes)
}

fn inst_literal(self: *EmitQbe2, value: i64, ty: Prim) EmitQbe2.Val = {
    @match(ty) {    
        fn F64() => self.b&.push_literal(@as(f64) value.bitcast()); 
        fn F32() => self.b&.push_literal(@as(f32) value.trunc().bitcast());
        @default => self.b&.push_literal(value);
    }
}

fn inst_store(self: *EmitQbe2, addr: EmitQbe2.Val, value: EmitQbe2.Val, ty: Prim) void = {
    // :StoreAlwaysKw
    self.b&.push_inst0(self.current, qbe_store(ty), .Kw, value, addr);
}

fn inst_copy(self: *EmitQbe2, from: EmitQbe2.Val, to: EmitQbe2.Val, bytes: u16) void = {
    self.b&.push_blit(self.current, to, from, bytes.zext()); 
    
    // Just an option for debugging to make sure overlapping isn't the problem. 
    //self.b&.push_argument(self.current, .Kl, to);
    //self.b&.push_argument(self.current, .Kl, from);
    //self.b&.push_argument(self.current, .Kl, self.b&.push_literal(@as(i64) bytes.zext()));
    //self.b&.push_inst(self.current, .call, .Kw, QbeNull,  self.b&.push_symbol("memmove".sym().c_str()), QbeNull);
}

fn inst_func_ref(self: *EmitQbe2, fid: FuncId) EmitQbe2.Val = {
    self.b&.push_symbol(self.fn_name_str(fid))
}

fn inst_jump_if(self: *EmitQbe2, cond: EmitQbe2.Val, true: BbId, false: BbId, args: Slice(EmitQbe2.Val)) void = {
    assert(args.len.eq(0), "i dont use this yet");
    self.b&.end_block(self.current, (type = .jnz, arg = cond, target1 = (id = true.id + 1), target2 = (id = false.id + 1)));
}

fn inst_jump(self: *EmitQbe2, always: BbId, args: Slice(EmitQbe2.Val)) void = {
    enumerate(args){i, arg|
        ty := self.body.blocks[always.id.zext()].arg_prims[i];  // TODO: ask qbe for the type instead of smuggling the whole body here
        r := arg[];
        store_op := qbe_store(ty);
        if rtype(r) == .RTmp {
            @debug_assert(r.val() >= Qbe.Tmp0, "why do you think you have a register?");
            t := self.b.temporaries.index(r.val() - Qbe.Tmp0)[].cls;
            store_op = store_ops[t.raw().zext()];
        };
        self.b&.push_inst0(self.current, store_op, .Kw, r, self.block_args[i]);
    };
    self.b&.end_block(self.current, (type = .jmp, target1 = (id = always.id + 1)));
}

fn is_kw_tmp(self:  *EmitQbe2, r: Qbe.Ref) bool = {
   if rtype(r) == .RTmp {
        @debug_assert(r.val() >= Qbe.Tmp0, "why do you think you have a register?");
        t := self.b.temporaries.index(r.val() - Qbe.Tmp0)[].cls;
        if t == .Kw {
            return(true);
        };
    };
    false
}

fn inst_switch(self: *EmitQbe2, info: *RawList(SwitchPayload), inspected: Qbe.Ref, uid: i64) void = {
    cmp_op := @if(is_kw_tmp(self, inspected), Qbe.O.ceqw, Qbe.O.ceql);
    // :AllOurBlocksUpFront
    normal_branches, default := info.decode_switch();
    default := default.expect("switch node has default");
    if normal_branches.len > 4 {
        sort :: quicksort(SwitchPayload, fn(a, b) => a.value > b.value);
        sort(normal_branches);
        binary_search_switch(self, normal_branches, inspected, default);
        return();
    };
    linear_search_switch(self, normal_branches, inspected, default);
    self.current.id = 999;
}

fn binary_search_switch(self: *EmitQbe2, cases: []SwitchPayload, inspected: Qbe.Ref, default: BbId) void = {
    if cases.len < 3 {
        linear_search_switch(self, cases, inspected, default);
        return();
    };
    mid := cases.len/2;
    high_block := self.b&.push_block();
    low_block  := self.b&.push_block();
    case := cases[mid];
    
    value := self.b&.push_literal(case.value);
    cmp_op := @if(is_kw_tmp(self, inspected), Qbe.O.cslew, Qbe.O.cslel);
    ok := self.b&.push_inst(self.current, cmp_op, .Kw, inspected, value);
    self.b&.end_block(self.current, (type = .jnz, arg = ok, target1 = low_block, target2 = high_block));
    
    self.current = high_block;
    binary_search_switch(self, cases.slice(0, mid), inspected, default);
    
    // include mid
    self.current = low_block;
    binary_search_switch(self, cases.slice(mid, cases.len), inspected, default);
}

fn linear_search_switch(self: *EmitQbe2, cases: []SwitchPayload, inspected: Qbe.Ref, default: BbId) void = {
    cmp_op := @if(is_kw_tmp(self, inspected), Qbe.O.ceqw, Qbe.O.ceql);
    next_block := self.b&.push_block();
    for cases { case | 
        value := self.b&.push_literal(case.value);
        ok := self.b&.push_inst(self.current, cmp_op, .Kw, inspected, value);
        self.b&.end_block(self.current, (type = .jnz, arg = ok, target1 = (id = case.block.id + 1), target2 = next_block));
        self.current = next_block;
        next_block = self.b&.push_block();
    };
    self.b&.end_block(self.current, (type = .jmp, target1 = (id = default.id + 1)));
}

fn inst_return(self: *EmitQbe2, args: Slice(EmitQbe2.Val)) void = {
    if self.context != QbeNull && self.my_node != QbeNull {
        env_node_ref := self.b&.push_inst(self.current, .add, .Kl, self.context, self.b&.push_literal(8));
        self.b&.push_inst0(self.current, .storel, .Kl, self.env_node, env_node_ref);  // reset prev
    };
    
    @switch(args.len){
        @case(0) => {
            if self.current_has_indirect_return {
                self.b&.end_block(self.current, (type = .retc, arg = self.indirect_return_slot));
                // :set_aggragate_return_index
            } else {
                self.b&.end_block(self.current, (type = .ret0));
            };
        };
        @case(1) => {
            // TODO: this should be the same? does llvm mess with it somehow? 
            //k := qbe_real_class(self.signeture.ret1.unwrap());
            //op: Qbe.J = @as(Qbe.J) @as(i64) Qbe.J.xxx.raw().zext() + k.raw().zext() + 1;
            op := qbe_ret(self.signeture.ret1.unwrap());
            self.b&.end_block(self.current, (type = op, arg = args[0]));
        };
        @case(2) => {
            t1 := self.signeture.ret1.unwrap();
            t2 := self.signeture.ret2.unwrap();
            self.b&.push_inst0(self.current, qbe_store(t1), .Kw, args[0], self.indirect_return_slot);
            // TODO: for now i always use (i64, i64) but that probably needs to change (ie floats!). 
            offset_to_second_field := self.b&.push_literal(@as(i64) 8);
            struct_out_2 := self.b&.push_inst(self.current, .add, .Kl, self.indirect_return_slot, offset_to_second_field);
            self.b&.push_inst0(self.current, qbe_store(t2), .Kw, args[1], struct_out_2);
            self.b&.end_block(self.current, (type = .retc, arg = self.indirect_return_slot));
            self.b.aggragate_return_index = (Some = self.pair_type_id);
        };
        @default fn() => unreachable();
    };
}

fn move_to_block(self: *EmitQbe2, block: *BasicBlock, ip: BbId) Slice(EmitQbe2.Val) = {
    // :AllOurBlocksUpFront
    self.current = (id = ip.id + 1); // off by one for entry block
    args: List(EmitQbe2.Val) = list(block.arg_prims.len, temp());
    
    enumerate block.arg_prims { i, ty |
        v := self.b&.push_inst(self.current, qbe_load(ty[]), qbe_real_class(ty[]), self.block_args[i], QbeNull);
        args&.push(v);
    };
    
    args.items()
}

fn var(ty: Prim, id: i64) Qbe.Ref = {
    (ty = ty, data = (Var = id))
}

// TODO: need to sign extend sometimes. 
fn inst_load(self: *EmitQbe2, addr: EmitQbe2.Val, ty: Prim) EmitQbe2.Val = {
    // note: cls of the dest is not the same as the cls in the load opcode. 
    self.b&.push_inst(self.current, qbe_load(ty), qbe_real_class(ty), addr, QbeNull)
}

fn inst_global(self: *EmitQbe2, id: BakedVarId) EmitQbe2.Val = {
    self.b&.push_symbol(@tfmt("g%", id.id))
}

fn get_var(self: *EmitQbe2, ty: Prim) Qbe.Ref = {
    self.b&.push_temp(qbe_real_class(ty), "v")
}

fn qbe_real_class(ty: Prim) Qbe.Cls = {
    @match(ty) {
        fn P64() => .Kl;
        fn I64() => .Kl;
        fn I32() => .Kw;
        fn I16() => .Kw;
        fn I8() =>  .Kw;
        fn F64() => .Kd;
        fn F32() => .Ks;
    }
}

fn qbe_ret(ty: Prim) Qbe.J = {
    @match(ty) {
        fn P64() => .retl;
        fn I64() => .retl;
        fn I32() => .retw;
        fn I16() => .retw;
        fn I8() =>  .retw;
        fn F64() => .retd;
        fn F32() => .rets;
    }
}

fn qbe_fake_class(ty: Prim) Qbe.Cls = {
    @match(ty) {
        fn P64() => .Kl;
        fn I64() => .Kl;
        fn I32() => .Kw;
        fn I16() => .Kuh;
        fn I8() =>  .Kub;
        fn F64() => .Kd;
        fn F32() => .Ks;
    }
}

// :StoreAlwaysKw no result type so the instruction must have .Kw
fn qbe_store(ty: Prim) Qbe.O = {
    ::enum(Prim);
    @match(ty) {
        fn P64() => .storel;
        fn I64() => .storel;
        fn I32() => .storew;
        fn I16() => .storeh;
        fn I8() =>  .storeb;
        fn F64() => .stored;
        fn F32() => .stores;
    }
}

// TODO: wrong becuase sometimes you want sign extend
fn qbe_load(ty: Prim) Qbe.O = {
    @match(ty) {
        fn P64() => .load;
        fn I64() => .load;
        fn I32() => .loaduw;
        fn I16() => .loaduh;
        fn I8() =>  .loadub;
        fn F64() => .load;
        fn F32() => .load;
    }
}

//
// TODO: preallocate the arrays since qbe sets maximum sizes for things (and other places in the backend use those numbers). 
// it doesnt have to be fast it just has to work
// it doesnt have to be fast it just has to work
// for now... 
//

QbeBuilder :: @struct(
    globals: *QbeModule,
    temporaries: List(Qbe.Tmp),
    constants: List(Qbe.Con),
    blocks: List(WipBlock),
    lnk := Qbe.Lnk.zeroed(),
    leaf := true,  // TODO: debug assert somewhere that you don't do a call in a leaf. for now we trust the frontend to remember to unset this. 
    aggragate_return_index: ?i64,
);

// TODO: make the Qbe.Fn sooner and just call the functions in util.fr
fn push_temp(b: *QbeBuilder, k: Qbe.Cls, prefix: Str) Qbe.Ref = {
    temp := Qbe.Tmp.zeroed();
    @if(TRACK_IR_NAMES) {
        name := fixed_list(temp.name&.items());
        @fmt(name&, "%%", prefix, b.temporaries.len);
    };
    temp.cls = k;
    temp.slot = -1;
    temp.nuse = 1;
    temp.ndef = 1;
    b.temporaries&.push(temp);
    TMP(b.temporaries.len - 1 + Qbe.Tmp0)
}

// TODO: fix load/store union in emit_bc so you can just pass `get_field_type(Qbe.Con, @symbol bits)`
fn push_literal(b: *QbeBuilder, i: i64) Qbe.Ref = {
    con: Qbe.Con = (type = .CBits, bits = (i = i));
    b.constants&.push(con);
    ref(.RCon, b.constants.len - 1)
}

fn push_literal(b: *QbeBuilder, i: f64) Qbe.Ref = {
    con: Qbe.Con = (type = .CBits, bits = (d = i));
    b.constants&.push(con);
    ref(.RCon, b.constants.len - 1)
}

fn push_literal(b: *QbeBuilder, i: f32) Qbe.Ref = {
    con: Qbe.Con = (type = .CBits, bits = (s = (s = i)));
    b.constants&.push(con);
    ref(.RCon, b.constants.len - 1)
}

fn push_symbol(b: *QbeBuilder, name: Str) Qbe.Ref = {
    con: Qbe.Con = (type = .CAddr, bits = (i = 0));
    con.sym.id = b.globals.intern(name); // TODO: why can't do this inline :FUCKED
    b.constants&.push(con);
    ref(.RCon, b.constants.len - 1)
}

fn push_inst(b: *QbeBuilder, block: BbId, op: Qbe.O, k: Qbe.Cls, out: Qbe.Ref, in1: Qbe.Ref, in2: Qbe.Ref) void = {
    // TODO: why bother
    // if k > .Ksb {| k = .Kw; };
    @debug_assert(k == .Kl || k == .Kw || k == .Ks || k == .Kd, "size must be w/l/s/d for %", op.get_name());
    b.blocks[block.id.zext()].insts&.push(make_ins(op, k, out, in1, in2));
}

// TODO: catch the mistake of calling this instead of push_inst0 for a store (it confuses the backend)
fn push_inst(b: *QbeBuilder, block: BbId, op: Qbe.O, k: Qbe.Cls, in1: Qbe.Ref, in2: Qbe.Ref) Qbe.Ref = {
    out := b.push_temp(k, "v");
    b.push_inst(block, op, k, out, in1, in2);
    out
}

fn push_inst0(b: *QbeBuilder, block: BbId, op: Qbe.O, k: Qbe.Cls, in1: Qbe.Ref, in2: Qbe.Ref) void = {
    b.push_inst(block, op, k, QbeNull, in1, in2);
}

fn push_blit(b: *QbeBuilder, block: BbId, dest: Qbe.Ref, src: Qbe.Ref, size: i64) void = {
    if size < 0 || size >= 0x10000000 {
        panic("invalid blit size");
    };
    // TODO: why not just put the size in the out slot? 
    //       maybe its about avoiding a branchy special case to check that its actually a register. 
    b.push_inst(block, .blit0, .Kw, QbeNull, src, dest); // note: flipped from memcpy
    b.push_inst(block, .blit1, .Kw, QbeNull, small_int_for_blit(size), QbeNull);
}
        
// Parameters are represented as instructions at the beginning of the @start block. 
fn push_parameter(b: *QbeBuilder, arg_type: Qbe.Cls) Qbe.Ref = {
    @debug_assert(b.blocks.len == 1, "push args at the beginning of the start block");
    // TODO: check that only args pushed so far just to be helpful. 
    @debug_assert(arg_type != .Kc, "TODO: I don't support struct typed args yet because emit_bc throws away the info (but that needs to be fixed to implement the abi correctly)");
    // if (arg_type == .Kc) *curi = (Ins){Oparc, Kl, r, {TYPE(ty)}};
    
    is_small_for_abi := arg_type >= Qbe.Cls.Ksb;
    base_type := if(is_small_for_abi, => Qbe.Cls.Kw, => arg_type);
    op := if(is_small_for_abi, => offset_by_class(Qbe.O.parsb, arg_type), => Qbe.O.par);
    value_out := b.push_temp(base_type, "v");
    block: BbId = (id = 0);
    b.push_inst(block, op, base_type, value_out, QbeNull, QbeNull);
    value_out
}

// Each argument to a call is represented by an instruction. Don't forget to `push_inst(call)` at the end. 
fn push_argument(b: *QbeBuilder, block: BbId, arg_type: Qbe.Cls, value: Qbe.Ref) void = {
    @debug_assert(arg_type != .Kc, "TODO: I don't support struct typed args yet because emit_bc throws away the info (but that needs to be fixed to implement the abi correctly)");
    is_small_for_abi := arg_type >= Qbe.Cls.Ksb;
    base_type := if(is_small_for_abi, => Qbe.Cls.Kw, => arg_type);
    op := if(is_small_for_abi, => offset_by_class(Qbe.O.argsb, arg_type), => Qbe.O.arg);
    b.push_inst(block, op, base_type, QbeNull, value, QbeNull);
}

fn offset_by_class(op: Qbe.O, k: Qbe.Cls) Qbe.O = {
    // TODO: make this not painful
    a: i64 = k.raw().zext();
    b: i64 = Qbe.Cls.Ksb.raw().zext();
    c: i64 = op.raw().zext();
    d := c + a - b;
    @as(Qbe.O) @as(i32) d.intcast()
}

// TODO: type for typeid
// TODO: the front end should cache these since it has to track types anyway. and then we wouldn't need to do this scan. 
//       they probably don't have to be uniqued anyway, i think we just care about the structure for the abi. 
fn opaque_type_slow(b: *QbeBuilder, size: i64, align_log2: i64) i64 = {
    // :SLOW
    n := b.globals.number_of_types;
    enumerate b.globals.types[].slice(0, n) { i, type |  // :ThreadSafety
        if type.is_dark && type.size == size && type.align_log2.zext() == align_log2 {
            return(i);
        };
    };
    b.globals.types.grow(n + 1);
    type := b.globals.types[n]&;
    type[] = Qbe.Typ.zeroed();
    type.is_dark = true;
    type.align_log2 = align_log2.intcast();
    type.size = size;
    type.nunion = 1; // don't forget this or life is ass 
    b.globals.number_of_types += 1;
    @if(TRACK_IR_NAMES) {
        name := fixed_list(type.name&.items());
        @fmt(name&, "t%s%a%", n, size, align_log2);
    };
    n
}

// TODO: could do the size+align calc here but my front end already does it for now.  that means you have to put FPad in too!!!
//       but i barely use this for now. TODO: ill need to improve that when i want to follow the abi better. 
fn struct_type(b: *QbeBuilder, fields: []Qbe.Field, size: i64, align_log2: i64) i64 = { // :ThreadSafety
    n := b.globals.number_of_types;
    b.globals.types.grow(n + 1);
    type := b.globals.types[n]&;
    type[] = Qbe.Typ.zeroed();
    type.fields = b.globals.new_long_life(fields.len + 1);
    type.align_log2 = align_log2.intcast();
    type.size = size;
    enumerate fields { i, f | 
        type.fields[i] = f[];
    };
    type.nunion = 1;// don't forget this or life is ass (it caused a 0 op, `no match for (null)(w)`)
    type.fields[fields.len] = (type = .FEnd, len = 0);
    b.globals.number_of_types += 1;
    @if(TRACK_IR_NAMES) {
        name := fixed_list(type.name&.items());
        @fmt(name&, "t%", n);
    };
    n
}

fn push_block(b: *QbeBuilder) BbId = {
    // zero init terminator is Qbe.J.xxx
    b.blocks&.push(insts = list(b.blocks.gpa), jmp = QbeTerminator.zeroed());
    // TODO: set name for debug
    (id = b.blocks.len.sub(1).trunc())
}

fn end_block(b: *QbeBuilder, bb: BbId, jmp: QbeTerminator) void = {
    block := b.blocks[bb.id.zext()]&;
    @debug_assert(block.jmp.type == .Jxxx, "tried to close block % twice", bb.id);
    block.jmp = jmp;
}

fn build(self: *EmitQbe2, name: Str, logging: Str) void = {
    b := self.b&;
   	m := b.globals;
    mark := mark_temporary_storage();
    a := temp();
    curf := a.box(Qbe.Fn);
    curf.default_init(m); 
   	// curf.rpo = 0; filled in by later pass
   	// curf.vararg = false; // TODO
    // TODO: dont copy :SLOW
    curf.ntmp += b.temporaries.len.intcast();
    curf.tmp&.grow(curf.ntmp.zext());
    dest := curf.tmp.slice(Qbe.Tmp0, b.temporaries.len + Qbe.Tmp0);
    dest.copy_from(b.temporaries.items());
    curf.ncon = b.constants.len.intcast();
    curf.con = new_copy(b.constants.items());
    @debug_assert_eq(curf.nmem, 0);
    //@println("% temps. % blocks.", curf.ntmp.zext() - Qbe.Tmp0, b.blocks.len);
    
   	curf.leaf = b.leaf;
    curf.lnk = b.lnk;
    curf.lnk.id = b.globals.intern(name);
   	curf.retty = (b.aggragate_return_index || @as(i64) -1).intcast();
   	
    if b.blocks.len == 0 {
        panic("empty function");
    };
    if name.len > 80 {
        panic("TODO: function name cannot be longer than 80 characters");
    };
    @if(TRACK_IR_NAMES) {
        name_dest := curf.name&.items().slice(0, name.len);
        name_dest.copy_from(name);
    };
    
    qbe_blocks: List(Qbe.Blk) = list(b.blocks.len, a); // stable!
   	curf.nblk = b.blocks.len.intcast();
   	
    curf.start = qbe_blocks.index_unchecked(0);
    enumerate b.blocks { i, wip |
        if wip.jmp.type == .Jxxx {
            wip.jmp.type = .hlt;  // TODO: report error and force the front end to be precise and not have empty blocks. 
        };
        qbe_blocks&.push(Qbe.Blk.zeroed());
        block := qbe_blocks[i]&;
        block.id = i.intcast(); // TODO: probably don't need this because cfg sets it? but qbe parser does it so just being safe for now.  
    
        // TODO: dont copy :SLOW
        idup(block, wip.insts.maybe_uninit.ptr, wip.insts.len);
        if i == b.blocks.len - 1 {
            block.link = Qbe.Blk.ptr_from_int(0);
        } else {
            block.link = qbe_blocks.index_unchecked(i + 1);
        };
        block.jmp.type = wip.jmp.type;
        block.jmp.arg = wip.jmp.arg;
        if wip.jmp.target1.id != 0 {
            block.s1 = qbe_blocks.index_unchecked(wip.jmp.target1.id.zext());
        };
        if wip.jmp.target2.id != 0 {
            block.s2 = qbe_blocks.index_unchecked(wip.jmp.target2.id.zext());
        };
        
        @if(TRACK_IR_NAMES) {
            name := fixed_list(block.name&.items());
            @fmt(name&, "b%", i);
        };
    };
    
    if b.globals.debug["P".char()] {
        write(b.globals.debug_out, "\n> After parsing:\n");
        printfn(curf, b.globals.debug_out);
    };
    m.set_debug_types(logging, true);
    {self.comp.vtable.run_qbe_passes}(Qbe.Fn.raw_from_ptr(curf));
    m.set_debug_types(logging, false);
    reset_temporary_storage(mark);
    
    b.temporaries&.clear();
    b.constants&.clear();
    b.blocks&.clear();
    b.aggragate_return_index = .None;
    b.constants&.push(type = .CBits, bits = (i = 0xdeaddead)); // QbeUndef
    b.constants&.push(type = .CBits, bits = (i = 0));          // QbeConZero
    ::assert_eq(FIXED_CONST_COUNT, 2);
}

QbeTerminator :: @struct(
    arg := QbeNull,
    target1 := BbId.zeroed(),
    target2 := BbId.zeroed(),
    type: Qbe.J,
);

WipBlock :: @struct(
    insts: List(Qbe.Ins),
    jmp: QbeTerminator,
);
