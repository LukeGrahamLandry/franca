#include_std("compiler/ast_external.fr");  
#include_std("examples/old_backend/emit_bc.fr");
#include_std("examples/old_backend/llvm.fr");
#include_std("examples/old_backend/aarch64.fr");
#include_std("examples/old_backend/x64.fr");  
#include_std("examples/old_backend/jit.fr");  
#include_std("examples/old_backend/walk_bc.fr");
#include_std("examples/old_backend/from_bc.fr");

BcBackend :: @struct(
    jitted: Jitted,
    bytecodes: BucketArray(FnBody),
    // TODO: just always use .Aot and make this a set?
    saved_bytecodes: HashMap(*FnBody, ExecStyle),
    free_bytecodes: List(*FnBody),
);

FnBody :: @struct(  
    blocks: RsVec(BasicBlock),
    // TODO: would be nice if i could sort these so less alignment padding on my backends. 
    //       but thats a pain because then you need to map indexes.
    vars: RsVec(VarSlotType),
    var_names: RsVec(Symbol),
    when: ExecStyle,
    hash: i64 = 0,
    signeture: PrimSig,
    func: FuncId,
    name: Symbol,
    context := true,
    switch_payloads: RsVec(RsVec(SwitchPayload)),  // indexed by Bc.Switch
    sig_payloads: RsVec(PrimSig),  // indexed by `sig` field of Bc.CallDirect and Bc.CallFnPtr
);
VarSlotType :: @struct(size: u16, align: u16);
SwitchPayload :: @struct(value: i64, block: BbId);

Bc :: @tagged(
    CallDirect: @struct(sig: u32, f: FuncId, tail: bool, context: bool),         // <args:m> -> <ret:n>
    CallFnPtr: @struct(sig: u32, context: bool),                                 // <ptr:1> <args:m> -> <ret:n>
    PushConstant: @struct(value: i64, ty: Prim),                  // _ -> <v:1>
    JumpIf: @struct(true_ip: BbId, false_ip: BbId, slots: u16),   // <args:slots> <cond:1> -> !
    Goto: @struct(ip: BbId, slots: u16),                          // <args:slots> -> !
    GetNativeFnPtr: FuncId,                                       // _ -> <ptr:1>
    Load: Prim,                                                   // <ptr:1> -> <?:n>
    StorePost: Prim,                                              // <?:n> <ptr:1> -> _
    StorePre: Prim,                                               // <ptr:1> <?:n> -> _
    AddrVar: @struct(id: u16),                                    // _ -> <ptr:1>
    SaveSsa: @struct(id: u16, ty: Prim),                          // <p:1> -> _
    LoadSsa: @struct(id: u16),                                    // _ -> <p:1>
    IncPtrBytes: i64,                                             // <ptr:1> -> <ptr:1>
    PeekDup: u16,                                                 // <x:1> <skip:n> -> <x:1> <skip:n> <x:1>,
    CopyBytesToFrom: u16,                                         // <to_ptr:1> <from_ptr:1> -> _
    LastUse: @struct(id: u16),                                    // _ -> _
    Unreachable,                                                  // _ -> !
    NoCompile,
    PushGlobalAddr: BakedVarId,
    Snipe: u16,
    Ret0, // big return uses this too because code has already written to indirect return address.
    Ret1: Prim,
    Ret2: Ty(Prim, Prim),
    Nop,
    Intrinsic: Intrinsic,
    Switch: u32,
    RotateForImmediateCallPtr,
);
::enum(Intrinsic);

PrimSig :: @struct(
    args: Slice(Prim) = empty(),
    ret1: ?Prim,
    ret2: ?Prim,
    return_value_bytes: u16 = 0,
    first_arg_is_indirect_return: bool = false,
    no_return: bool = false,
    arg_int_count: u8 = 0, // todo: remoove 
);

BbId :: @struct(id: u16);
BasicBlock :: @struct(
    insts: RsVec(Bc),
    debug: RsVec(Span), // TODO: use this if self.debug
    arg_prims: Slice(Prim),
    incoming_jumps: u16,
    clock: u16,
);

fn push_block(body: *FnBody, alloc: Alloc) void = {
    body.blocks&.push((
        insts = empty(),
        debug = empty(),
        arg_prims = empty(),
        incoming_jumps = 0,
        clock = 0,
    ), alloc);
}

fn create(c: CompCtx) *BcBackend = {
    a := c.get_alloc();
    b := a.box(BcBackend);
    b[] = (
        // TODO: inline Jitted.new() here once this is the only place we call it. 
        jitted = new(1.shift_left(28)), // Its just virtual memory right? I really don't want to ever run out of space and need to change the address.
        bytecodes = init(10, a),
        saved_bytecodes = init(a),
        free_bytecodes = list(a),
    );
    b
}

fn destroy(self: *BcBackend, c: CompCtx) void = {
    page_allocator.dealloc(u32, self.jitted.mmapped);
}

// TODO: use #target_arch but that's not a thing yet.
fn jit_asm(self: *BcBackend, c: CompCtx, f: FuncId, body: *FnBody, comptime_arch: Arch) CRes(Ty([]u8, []FuncId)) = {
    (Ok = @match(comptime_arch) {
        fn aarch64() => {
            a: EmitArm64 = new(c.data.cast(), self, body);
            (a&.compile(f), a.forward_calls.items())
        }
        fn x86_64() => {
            a: EmitX64 = new(c.data.cast(), self, body);
            (a&.compile(f), a.forward_calls.items())
        }
        fn wasm32() => panic("unreachable: compiler doesn't support wasm yet");
    })
}

fn jit_shim(self: *BcBackend, c: CompCtx, fid: FuncId, handler: rawptr) CRes(*FnBody) = {
    create_jit_shim(c, self, fid, handler)
}

// get_fn_callable, get_fn_old
fn get_jitted(self: *BcBackend, c: CompCtx, f: FuncId, make_exec: bool) ?rawptr = {
    addr := get_already_jitted(self, c, f);
    if(make_exec && addr.is_some(), => self.jitted&.bump_dirty());
    addr
}

fn asm_bytes(bc: *BcBackend, c: CompCtx, fid: FuncId, bytes: []u8) rawptr = {
    bc.jitted&.mark_start(fid);
    bc.jitted&.push_bytes_asm(bytes);
    code := bc.jitted&.save_current(fid);
    u32.raw_from_ptr(code.ptr)
}

fn prim_sig(c: CompCtx, func: *Func) Result(PrimSig, *CompileError) = {
    f_ty := func.finished_ty().unwrap();  // TODO: return error
    c.prim_sig(f_ty)
}

/////////////////////////////////////
/// Primitive Type Representation ///
// 
// The distinction between backend types and front end types is important. 
// There are different registers for storing integers/pointers vs floats so the backend really does need to know which everything is. 
// It was much more confusing before when I was keeping a mask of which parts were floats and only had one type of float/int. 
// But the current implementation is more complicated than it needs to be because I wasn't sure what was important at the time. 
// 

// This (prim_sig implementation) is painful. 
// At least the interface is fairly thin. ~almost~ only emit_bc needs to deal with creating prim_sigs.
// but every backend needs to remember to handle indirect returns specially every time they touch one. 
//
// I treat the indirect seperately because the backend wants to put it in a special register. 
// But then for the block args, you do want the indirect ptr because the bc expects it. 
// And then compctx is weird because the front end doesn't want to insert it into the arguments, 
// so its not as part of the function type, its in the calling convention. -- Jul 5
//
// :ConfusingPrims These slices always have (P64) before them so you can offset backwards for indirect returns. 
//
fn prim_sig(program: CompCtx, f_ty: FnType) CRes(PrimSig) = {
    @err_assert(!f_ty.arg.is_unknown() && !f_ty.ret.is_unknown(), "unknown fn types") return;
    ret := program.get_info(f_ty.ret);

    sig: PrimSig = (
        ret1 = .None,
        ret2 = .None,
        return_value_bytes = ret.stride_bytes,
        first_arg_is_indirect_return = ret.size_slots > @as(u16) 2.trunc(),
        no_return = f_ty.ret.is_never(),
    ); // TODO: if you typeo make this a '}' need to have a good error message. -- Jul 6

    @switch(ret.size_slots) {
        @case(0) => ();
        @case(1) => {
            sig.ret1 = program.prim(f_ty.ret);
        };
        @case(2) => {
            a, b := @try(program.prim_pair(f_ty.ret)) return;
            sig.ret1 = (Some = a);
            sig.ret2 = (Some = b);
        };
        @default => {
            // Note: not adding indirect pointer to sig because its handled sperately. TODO: that's kinda confusing.
        };
    };

    args: List(Prim) = list(program.get_alloc());
    // :ConfusingPrims
    args&.push(.P64); // for ret

    found_arity := 0;
    // TODO: compiler bug? what push_arg am i possibly shadowing?????? -- Jul 6
    push_arg__wtf :: fn(ty: Type) void => {
        found_arity += 1;
        info := program.get_info(ty);
        if info.pass_by_ref {
            args&.push(.P64);
        } else {
            for program.flat_tuple_types(ty) { t | 
                args&.push(program.prim(t).expect("arg prim"));
            };
        };
    };

    done := false;
    // TODO: if i always collapsed tuples of the same to array this would need different handling.
    @if_let(program.get_type(f_ty.arg)) 
        (fn Struct(f) => {
            if f.is_tuple {
                done = true;
                // TODO: :const_field_fix (not done on rust side either so add test for this!)
                each f.fields { f |
                    push_arg__wtf(f.ty);
                };
            };
        });
    if !done {
        push_arg__wtf(f_ty.arg);
    };

    // TODO: decide what i want a tuple to be. is it a real type you can have in memory or is it the thing that multiple arguments are?
    //       those ideas should extend to return values instead of them being special.
    //       should have a spread operator for calling a function on a tuple of arguments? like 'a := (1, 2); f(..a)'
    // @assert_eq(found_arity, f_ty.arity, "TODO: fn(a: Ty(i64, i64))");

    // :ConfusingPrims
    prims := args.items();
    prims.ptr = prims.ptr.offset(1);
    prims.len -= 1;
    
    sig.args = prims;
    
    sig.arg_int_count = 0;
    for sig.args { p |
        if !p.is_float() {
            sig.arg_int_count += 1;
        };
    };
    
    (Ok = sig)
}

fn prim_pair(self: CompCtx, ty: Type) CRes(Ty(Prim, Prim)) = {
    types := self.flat_tuple_types(ty); // TODO: don't allocate
    @debug_assert_eq(types.len(), 2);
    a : Prim = @unwrap(self.prim(types[0]), "non-prim") return;
    b := @unwrap(self.prim(types[1]), "non-prim") return;
    (Ok = @as(Ty(Prim, Prim)) (a, b))
}

// note: this just gives empty for unit so you can't offset it like :ConfusingPrims
fn get_primatives(self: CompCtx, ty: Type) [] Prim = {
    if(ty.is_unit() || ty.is_never(), => return(empty()));
    slots := self.get_info(ty).size_slots();
    flat_types := self.flat_tuple_types(ty);
    types: List(Prim) = list(self.get_alloc());
    // :ConfusingPrims
    types&.push(.P64);  // for ret
    types&.push(.P64);  // for #ct
    
    for flat_types { t |
        if !t.is_unit().or(t.is_never()) {
            types&.push(self.prim(t).expect("tuple part to be prim"));
        };
    };
    // :ConfusingPrims
    prims := types.items();
    prims.ptr = prims.ptr.offset(2);
    prims.len -= 2;
    prims
}

fn decode_sig(self: *FnBody, i: u32) *PrimSig = { // Note: not stable if you push again!!
    self.sig_payloads&.index(i.zext())
} 

fn decode_switch(cases: *RsVec(SwitchPayload)) Ty([]SwitchPayload, ?BbId) = {
    has_default := cases.last().unwrap()[].value == -1;
    ::if(Ty([]SwitchPayload, ?BbId));
    if(has_default) {
        (cases.items().slice(0, cases.len - 1), (Some = cases.last().unwrap()[].block))
    } else {
        (cases.items(), .None)
    }
}

// TODO: make the other enum varient builtin so you can use it and still override eq. because my order independence doesn't really work. 
// TODO: unhandled switch case if i try to make this u8. becuase of comptime BakedEntry?
Prim :: @enum(i64) (I8, I16, I32, I64, F64, F32, P64);
::enum(Prim);

fn prim(self: CompCtx, ty: Type) ?Prim = {
    p: Prim = @match(self.get_type(ty)) {
        fn F64()     => .F64;
        fn F32()     => .F32;
        fn Bool()    => .I8 ;
        fn VoidPtr() => .P64;
        fn FnPtr(_)  => .P64;
        fn Ptr(_)    => .P64;
        fn Fn(_)     => .I32;
        fn Label(_)  => .I32;
        fn Int(int) => 
            @switch(int.bit_count) { 
                @case(8)  => .I8 ;
                @case(16) => .I16;
                @case(32) => .I32;
                @default  => .I64;  // TODO: :fake_ints_too_big
            };
        fn Struct(f) => {
            if f.fields.len - f.const_field_count.zext() == 1 {
                fst := f.fields[0]&;
                assert(fst.kind != .Const, "TODO: first field const");
                if self.slot_count(fst.ty) == 1 {
                    return(self.prim(fst.ty));
                };
            };
            return(.None)
        }
        fn Tagged(f) => {
            each f.cases { c |
                if !c._1.is_unit() {
                    return(.None);
                };
            };
            .I64 // TODO: :tag_is_always_i64
        }
        fn Named(f) => return(self.prim(f._0));
        fn Enum(f) => return(self.prim(f.raw));
        fn Array(f) => {
            if f.len == 1 {
                return(self.prim(f.inner));
            };
            return(.None)
        }
        @default => return(.None);
    };
    (Some = p)
}
