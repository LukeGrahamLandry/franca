//! Converts simple ASTs into my bytecode-ish format.
//! All comptime execution for a function is finished before it reaches this phase.
//! - Flatten nested expressions to stack operations.
//! - Convert control flow to explicit basic blocks.
//! - Bind non-local return labels.
//! - Reduce variable usage to register sized loads/stores.
//! - Convert large arguments/returns to references and remap signatures to use only register sized types.

ENABLE_DEDUPLICATION :: false; // TODO: something's making a cycle on qbe_frontend

// note: bc structs are in lib/driver_api.fr

//
// TODO: less pain
// - versions of @assert that return a compile error instead of crashing. 
// - struct init fields needing to be in order is super dumb!
// - @sanity(.DebugAsserts)? 
// - access fields on rvalues
// - index operator looks for the wrong overload when used in nested expressions
// - deal with infinite loops if you try to put from_raw in fn tagged because it triggers for the tag of ?i64 but then the raw of that needs to return a ?i64, it needs to be like @rec but just @rec doesnt wokr 
// - TODO: it seems the compiler segfaults trying to cope with this line?????????! -- Jul 7 //ty := binding.ty&.unwrap();

EmitBc :: @struct(
    result: *FnBody,
    program: CompCtx,
    last_loc: Span,
    locals: List(List(u16)),
    var_lookup: HashMap(Var, u16),
    inlined_return_addr: HashMap(LabelId, ReturnAddr),
    is_ssa_var: BitSet,
    result_addr_depth: i64 = 0,
    out_alloc: Alloc,
    current_block: BbId, 
    clock: u16,
    want_log: bool,
    debug: bool,
    dyn_call_context: bool,
);

fn hash(s: *LabelId) i64 #redirect(*u32, i64);

ReturnAddr :: @struct(
    block: BbId,
    result_loc: ResultLoc,
    store_res_ssa_inst: ?Ty(BbId, i64),
    res_ssa_id: ?u16,
    used: bool,
    result_addr_depth: i64,
);

ResultLoc :: @enum(i64) (PushStack, ResAddr, Discard);
::enum(ResultLoc);

fn emit_bc(comp: CompCtx, bc: *BcBackend, f: FuncId, when: ExecStyle) CRes(*FnBody) = {
    if when == .Aot {
        // :bake_relocatable_value
        {comp.vtable.check_for_new_aot_bake_overloads}(comp.data); 
    };
    func := comp.get_function(f);
    bc.jitted&.extend_blanks(f);
    opts := comp.get_build_options();
    self: EmitBc = (
        result = FnBody.ptr_from_int(0), // EVIL. filled in soon! 
        program = comp,
        last_loc = func.loc,
        locals = list(temp()),
        var_lookup = init(temp()),
        inlined_return_addr = init(temp()),
        is_ssa_var = empty(),
        // Note: you can't use a free-ing allocator here because of the shit i pull with :ConfusingPrims
        // but for deduplication you want the functions to live forever anyway
        out_alloc = comp.get_alloc(),  
        current_block = (id = 0),
        clock = 0,
        want_log = func.get_flag(.LogBc),
        debug = opts.debug_info,
        dyn_call_context = if(when == .Aot, => opts.implicit_context_runtime, => opts.implicit_context_comptime),
    );
    //@println("emit_bc % % %", when, f, comp.pool.get(func.name));
    @debug_assert(func.get_flag(.EnsuredCompiled), "fn not compiled?");
    // TODO: this should be true
    //if func.finished_bytecode_pointer.is_some() {
    //    old := func.finished_bytecode_pointer.unwrap();
    //    old := FnBody.ptr_from_int(old);
    //    if old.when == when {
    //        //@println("compiled twice");
    ////        //return(Ok = old);
    //    };
    //};
    
    //if when == .Jit && !func.get_flag(.SyntheticImmEval) {
    //    @println("jitting % %", f, comp.log_name(f));
    //};

    sig: PrimSig = (ret1 = .None, ret2 = .None);
    
    self.locals&.push(list(temp()));
    body := @try(self.program.empty_fn_body(bc, f, when)) return;
    self.result = body;
    
    res := self&.emit_body(f);
    @debug_assert(body.vars.len <= MAX_u16.zext() && body.blocks.len <= MAX_u16.zext(), "program too big for my tiny brain");
    
    if res&.is_err() {
        res.Err.update_main_span(self.last_loc);
        return(Err = res.Err)
    };
    
    @if(ENABLE_DEDUPLICATION) {
        if body.hash != 0 && comp.get_build_options()[].deduplicate_bytecode {
            if bc.saved_bytecodes&.get_key(body&) { check | 
                if check.func.as_index() != f.as_index() {|  // TODO: this shouldn't happen?
                    // TODO: this is sketchy... maybe? had to think of a test that would hurt it tho. 
                    func.body = (Redirect = check.func); 
                    
                    //old := check.func;
                    //old_func := comp[old]&;
                    
                    //size := 0;
                    //each check.blocks { b |
                    //    size += b.insts.len;
                    //};
                    
                    //@println("% ops saved. % F%_% <- F%_%", size, when,
                    //    old.as_index(), comp.pool.get(old_func.name),
                    //    f.as_index(), comp.pool.get(func.name), 
                    //);
                };
                return(Ok = check);
            };
            
            bc.saved_bytecodes&.insert(body, when);
        };
    };
    body.context = func.get_flag(.YesContext) && !func.get_flag(.NoContext);
    if !body.context {
        func.set_flag(.NoContext);
    };
    (Ok = body)
}

reserved_1to1_sig :: 0;
:: List(Symbol);
fn empty_fn_body(program: CompCtx, bc: *BcBackend, func: FuncId, when: ExecStyle) CRes(*FnBody) = {
    f := program.get_function(func);
    f_ty := @unwrap(f.finished_ty(), "ICE: fn type not ready") return;
    sigs := PrimSig.list(1, program.get_alloc());
    {
        rp := rawptr; // :get_or_create_type
        f_ty: FnType = (arg = rp, ret = rp, arity = 1);
        sig_data := program.prim_sig(f_ty).unwrap();
        @debug_assert_eq(sigs.len, reserved_1to1_sig);
        sigs&.push(sig_data);
    };
    ptr := bc.bytecodes&.push(
        blocks = empty(),
        vars = empty(),
        var_names = empty(),
        when = when,
        signeture = @try(program.prim_sig(f_ty)) return,
        func = func,
        name = f.name,
        switch_payloads = empty(),
        sig_payloads = sigs.rs(),
    );
    
    (Ok = ptr)
}

// TODO: if you have an unclosed string it randomly decides later top level vars are undeclared? -- Jul 7 
fn bind_args(self: *EmitBc, arguments: *Pattern) CRes(i64) #once = {
    arity: u16 = arguments.bindings.len.trunc();

    func := self.program.get_function(self.result.func);
    f_ty := func.finished_ty().expect("known function type");
    arg_ty := f_ty.arg;
    pushed := if(self.result.signeture.first_arg_is_indirect_return, => 1, => 0);
    // reversed because they're on the stack like [0, 1, 2]
    each_rev arguments.bindings { binding |
        continue :: local_return;
        ty := binding.unwrap_ty();
        
        // TODO:? probably fine, i just set to const in closure capture but then shouldn't be adding to vars below.
        // TODO: the frontend needs to remove the 'const' parts from the ast in DeclVarPattern
        ::if_opt(Var, Str);
        @err_assert(binding.kind != .Const, "arg '%' was const!", 
            if(binding.var(), fn(name: Var) => self.program.get_string(name.name), => "_")) return;

        info := self.program.get_info(ty);

        if(info.size_slots == 0, => continue());
        :: if(u16);
        id: u16 = if info.pass_by_ref {
            pushed += 1;
            // TODO: callee make copy if it wants to modify
            self.save_ssa_var()
        } else {
            slots := self.slot_count(ty);
            id := self.add_var(ty);
            @switch(slots) {
                @case(0) => (); 
                @case(1) => {
                    ty := @unwrap(self.program.prim(ty), "expected prim but found %", self.program.log(ty)) return;
                    pushed += 1;
                    self.addr_var(id);
                    self.push(StorePost = ty);
                };
                @case(2) => {
                    types := self.program.flat_tuple_types(ty);
                    offset_2 := align_to(
                        self.program.get_info(types[0]).stride_bytes(),
                        self.program.get_info(types[1]).align_bytes(),
                    );
                    p0 := self.program.prim(types[0]).expect("ret1 prim");
                    p1 := self.program.prim(types[1]).expect("ret2 prim");
                    self.addr_var(id);
                    self.inc_ptr_bytes(offset_2.trunc());
                    self.push(StorePost = p1);
                    self.addr_var(id);
                    self.push(StorePost = p0);
                    pushed += 2;
                };
                @default => panic("ICE: unreachable because large args are passed by reference");
            };
            @as(u16) id
        };

        ::?*List(u16);
        locals := self.locals.last().expect("in block").push(id);
        if binding.var() { name |
            prev := self.var_lookup&.insert(name, id);
            @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program));
        };
    };

    (Ok = pushed)
}

fn compile_for_arg(self: *EmitBc, arg: *FatExpr, arity: usize) CRes(bool) = {
    info := self.program.get_info(arg.ty);
    // If the whole thing is passed in registers, cool, we're done.
    if !info.pass_by_ref {
        // arity isn't always the same as info.size_slots because of two slot structs like slices. 
        @try(self.compile_expr(arg, .PushStack, false)) return; // TODO: tail if its for a ret of the main function?
        return(Ok = false);
    };

    if arity == 1 {
        id := self.add_var(arg.ty);
        self.addr_var(id);
        @try(self.compile_expr(arg, .ResAddr, false)) return;
        self.addr_var(id);
        return(Ok = true);
    };
    
    @assert(arg.expr&.is(.Tuple), "TODO: % %", self.program.log(arg.ty), arity);
    parts := arg.expr.Tuple;
    info := self.program.get_type(arg.ty);
    @assert(info.is(.Struct), "ICE: expected fn args to be tuple");
    // TODO: probably assert(info.Struct.is_tuple) or we're in an odd place. need to be more consistant about what an <arguments> is -- Jul 8
    fields := info.Struct.fields;
    @debug_assert_eq(fields.len(), parts.len(), "ICE?: not enough parts for type in compile_for_arg");

    _pushed := 0;
    // TODO: sad that I can't zip iterators
    enumerate parts { i, val |
        continue :: local_return;
        ty := fields[i].ty;
        info := self.program.get_info(ty);
        // :thiscanthappenyoudhope
        if ty != val.ty {
            @println("% vs %", self.program.log(ty), self.program.log(val.ty));
        };
        if !info.pass_by_ref {
            _pushed += info.size_slots.zext();
            @try(self.compile_expr(val, .PushStack, false)) return;
            continue();
        };
        
        @match(val.expr&) {
            fn Deref(arg) => {
                _pushed += 1;
                
                // TODO: this is probably unsound.
                //       we're assuming that we can defer the load to be done by the callee but that might not be true.
                @try(self.compile_expr(arg[], .PushStack, false)) return;
                continue();
            }
            // it doesn't work if you don't do this which is scary but also i want to get rid of this whole thing soon anyway. 
            fn GetVar(_) => {
                // TODO: this is probably unsound.
                //       we're assuming that we can defer the load to be done by the callee but that might not be true.
                @try(self.addr_macro(val, .PushStack)) return;
                continue();
            }
            // TODO: factor out aot handling from main value handling so can use here too. -- Jun 3
            fn Value(f) => {
                if self.result.when == .Jit {
                    // TODO: this gets super bad if the callee isn't properly copying it because they'll be nmodifying something we think is constant
                    _pushed += 1;
                    self.push(PushConstant = (
                        value = f.bytes&.jit_addr(),  // Note: we know it's pass_by_ref so its large and not stored inline. 
                        ty = .P64,
                    ));
                    continue();
                };
                // else: fallthrough
            }
            @default => (); // fallthrough
        };

        _pushed += 1;
        id := self.add_var(ty);
        self.addr_var(id);
        self.result_addr_depth += 1;
        @try(self.compile_expr(val, .ResAddr, false)) return;
        self.result_addr_depth -= 1;
        self.addr_var(id);  // now we want to pass that pointer to the call, so leave it on the stack.
    };
    (Ok = true)
}

fn store_pre(self: *EmitBc, ty: Type) void = {
    if(ty.is_never(), => self.push(.Unreachable));
    slots := self.slot_count(ty);
    @switch(slots) {
        @case(0) => ();
        @case(1) => {
            ty := self.program.prim(ty).expect("store prim");
            self.push(StorePre = ty);
        };
        @case(2) => {
            types := self.program.flat_tuple_types(ty);
            offset_2 := align_to(
                self.program.get_info(types[0]).stride_bytes(),
                self.program.get_info(types[1]).align_bytes(),
            );
            self.push(PeekDup = 2); // grab the pointer
            self.inc_ptr_bytes(offset_2.trunc());
            self.push(StorePost = self.program.prim(types[1]).expect("store prim!"));
            self.push(StorePre = self.program.prim(types[0]).expect("store prim!"));
        };
        @default => panic("ICE: Tried to store big value.");
    }
}

fn load(self: *EmitBc, ty: Type) CRes(void) = {
    if(ty.is_never(), => self.push(.Unreachable));
    slots := self.slot_count(ty);
    @switch(slots) {
        @case(0) => ();
        @case(1) => {
            ty := self.program.prim(ty).expect("load prim");
            self.push(Load = ty);
        };
        @case(2) => {
            types := self.program.flat_tuple_types(ty);
            offset_2 := align_to(
                self.program.get_info(types[0]).stride_bytes(),
                self.program.get_info(types[1]).align_bytes(),
            );
            self.push(PeekDup = 0);
            self.push(Load = self.program.prim(types[0]).expect("load prim!"));
            self.push(PeekDup = 1);
            self.inc_ptr_bytes(offset_2.trunc());
            self.push(Load = self.program.prim(types[1]).expect("load prim!"));
            self.push(Snipe = 2);
        };
        @default => return(@err("ICE: Tried to load big value of type % (% slots)", self.program.log(ty), slots));
    };
    .Ok
}

fn emit_body(self: *EmitBc, f: FuncId) CRes(void) #once = {
    func := self.program.get_function(f);
    
    // TODO: use choose_impl
    
    if find_impl(func.body&, .Intrinsic) { op |
        // Direct calls will be inlined in emit_bc but someone might be trying to call through a function pointer. 
        // TODO: don't add to callees for direct calls. -- Jul 24
        entry_block := self.push_block(self.result.signeture.args);
        self.push(Intrinsic = op.bc);
        @debug_assert(self.result.signeture.ret2.is_none(), "So far all intrinsics return one thing.");
        self.push(Ret1 = self.result.signeture.ret1.unwrap());
        self.result.hash = 0;
        return(.Ok)
    };
    
    body := find_impl(func.body&, .Normal) || return(.Ok);

    entry_block := self.push_block(empty());
    args_pushed := @try(self.bind_args(func.arg&)) return;
    entry_block_sig := self.result.signeture.args;
    if self.result.signeture.first_arg_is_indirect_return {
        // :ConfusingPrims
        entry_block_sig.ptr = entry_block_sig.ptr.offset(-1);
        entry_block_sig.len += 1;
    };
    @debug_assert_eq(entry_block_sig.len, args_pushed, "ICE: confusing about function arity");
    self.result[entry_block].arg_prims = entry_block_sig;
    ::if(ResultLoc); ::if([]Prim);
    // We represent the indirect return argument as the left-most thing on the stack,
    // so after popping all the args, its at the top and we can emit the thing normally.
    result_location: ResultLoc = if(self.result.signeture.first_arg_is_indirect_return, => .ResAddr, => .PushStack);
    // Note: this is different from the body expr type because of early returns.
    ret := self.program.get_function(f)[].finished_ret.unwrap();
    prims: []Prim = if(self.result.signeture.first_arg_is_indirect_return, => empty(), => self.get_primatives(ret));
    return_block := self.push_block(prims);  // TODO: this is dumb. the body expression will never jump here (even with early returns as they only go to Blocks). 

    self.current_block = entry_block;

    if self.result.signeture.first_arg_is_indirect_return {|  
        self.result_addr_depth += 1;
    };

    // TODO: indirect return tail
    @try(self.compile_expr(
        body,
        result_location,
        !self.result.signeture.first_arg_is_indirect_return,
    )) return;
    
    if self.result.signeture.first_arg_is_indirect_return {
        self.result_addr_depth -= 1;
    };

    // TODO: we never actually use the return_block created here!
    assert(self.result[return_block].incoming_jumps == 0, "sure hope you did that on purpose");
    self.push_to(return_block, .NoCompile);

    self.locals&.pop().expect("outer scope to exist");
    assert(self.locals.is_empty(), "ICE: bc leaked scopes");

    @try(self.emit_return(ret)) return;
    if self.want_log {
        out: List(u8) = list(temp());
        self.program.log_bc(func, self.result, out&);
        println(out.items());
    };
    .Ok
}

fn emit_return(self: *EmitBc, ret: Type) CRes(void) = {
    if !ret.is_never() {
        slots := self.slot_count(ret); // TODO: this is in the sig so don't look it up again. 
        op: Bc = @switch(slots) {
            @case(1) => {
                a := self.program.prim(ret).expect("return prim");
                //debug_assert_eq!(BigOption::Some(a), self.result.signeture.ret1); // TODO: debug_assert. derive eq
                (Ret1 = a)
            };
            @case(2) => {
                a, b := @try(self.program.prim_pair(ret)) return;
                //debug_assert_eq!(BigOption::Some(a), self.result.signeture.ret1); // TODO: debug_assert. derive eq
                //debug_assert_eq!(BigOption::Some(b), self.result.signeture.ret2);
                (Ret2 = (a, b))
            };
            @default => .Ret0; // void or indirect return
        };

        self.push(op);
    } else {
        self.push(.Unreachable);
    };
    .Ok
}

fn log_bc(self: CompCtx, func: *Func, result: *FnBody, out: *List(u8)) void = {
    //@if(BOOTSTRAP_ONLY_MODE) return();
    @fmt(out, "[#log_bc %] %\n", self.get_string(func.name), result.signeture&);
    enumerate result.blocks { i, block | 
        continue :: local_return;
        if block.insts.len == 1 && block.insts.index(0).is(.NoCompile) {
            continue();
        };
        @fmt(out, "  [b %] (%)\n", i, block.arg_prims);
        each block.insts { inst | 
            @fmt(out, "    - %", inst);
            @if_let(inst) fn AddrVar(i) => {
                i: i64 = i.id.zext();
                if result.var_names.len > i {
                    name := result.var_names[i];
                    if !(name.raw() == 0) {
                        @fmt(out, "  // %", self.get_string(name));
                    }
                }
            };
            @fmt(out, "\n");
        };
    };
}

fn emit_runtime_call(
    self: *EmitBc,
    f_ty: FnType,
    arg_expr: *FatExpr,
    result_location: ResultLoc,
    can_tail: bool,
    $do_call: @Fn(sig: *PrimSig, tail: bool) void,
) CRes(void) = { 
    sig := @try(prim_sig(self.program, f_ty)) return;

    :: if(?u16);
    result_var: ?u16 = if sig.first_arg_is_indirect_return && result_location != .ResAddr {
        id := self.add_var(f_ty.ret);
        self.addr_var(id);
        // TODO: :result_addr_depth?
        (Some = id)
    } else {
        .None
    };

    any_by_ref := @try(self.compile_for_arg(arg_expr, f_ty.arity.zext())) return;
    // if any args are pointers, they might be to the stack and then you probably can't tail call.
    // TODO: can do better than this without getting too fancy, function pointers are fine, and anything in constant data is fine (we know if the arg is a Values).
    // TODO: !tail to force it when you know its fine.
    tail := can_tail && !self.program.get_info(f_ty.arg).contains_pointers() && !any_by_ref;
    
    do_call(sig&, tail);
    self.last_loc = arg_expr.loc;
    slots := self.slot_count(f_ty.ret);
    if slots > 0 {
        @match(result_location) {
            fn PushStack() => {
                if sig.first_arg_is_indirect_return {
                    if result_var { id |
                        self.addr_var(id);
                    };
                    @try(self.load(f_ty.ret)) return;
                }
            }
            fn ResAddr() => if(!sig.first_arg_is_indirect_return, => self.store_pre(f_ty.ret));
            fn Discard() => if(!sig.first_arg_is_indirect_return, => self.pop(slots));
        };
    } else {
        if result_location == .ResAddr {
            // pop dest!
            self.pop(1);
        };
    };

    if f_ty.ret.is_never() {
        self.push(.Unreachable);
    };
    
    // :sema_regression this shouldn't need the hint
    @as(CRes(void)) (.Ok)
}

fn mark_variable(self: *EmitBc, name: Var, id: u16) void = {
    if self.want_log.or(self.program.get_build_options()[].debug_info) {
        while => self.result.var_names.len <= id.zext() {
            self.result.var_names&.push(Flag.SYMBOL_ZERO.ident(), self.program.get_alloc());
        };
        self.result.var_names[id.zext()] = name.name;
    };
}

fn compile_stmt(self: *EmitBc, stmt: *FatStmt) CRes(void) = {
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Eval(expr) void => return(self.compile_expr(expr, .Discard, false));
        fn DeclVar(f) void => {
            @assert_ne(f.name.kind, VarType.Const);
            assert(f.ty&.is(.Finished), "type not ready");
            ty := f.ty.Finished;
            id := self.add_var(ty);
            self.mark_variable(f.name, id);
            self.addr_var(id);
            self.result_addr_depth += 1;
            @try(self.compile_expr(f.value&, .ResAddr, false)) return;
            self.result_addr_depth -= 1;
            if(ty.is_never(), => self.push(.Unreachable));
            prev := self.var_lookup&.insert(f.name, id);
            self.locals.last().unwrap().push(id);
            @assert(prev.is_none(), "shadow is still new var");
        }
        fn Set(f) void => return(self.set_deref(f.place&, f.value&));
        fn DeclVarPattern(f) void => {
            // TODO: test for evaluation order
            @if_let(f.value.expr&) fn Tuple(parts) => {
                @assert_eq(parts.len(), f.binding.bindings.len(), "ICE: DeclVarPattern tuple size mismatch");
                enumerate parts { i, value |
                    b := f.binding.bindings[i]&; // TODO: zip
                    @assert_ne(b.kind, .Const);
                    @try(self.do_binding(b.var(), b.unwrap_ty(), value)) return;
                };
                return(.Ok);
            };
            
            if 1 == f.binding.bindings.len() {
                b := f.binding.bindings[0]&;
                return(self.do_binding(b.var(), b.unwrap_ty(), f.value&));
            };
            
            // It's a destructuring (not inlined args)
            // We store the whole value in a stack slot and then save pointers into different offsets of it as thier own variables.
            full_id := self.add_var(f.value.ty);
            self.addr_var(full_id);
            self.result_addr_depth += 1;
            @try(self.compile_expr(f.value&, .ResAddr, false)) return;
            self.result_addr_depth -= 1;
            info := self.program.get_type(f.value.ty);
            @err_assert(info.is(.Struct), "destructure must be tuple") return;
            fields := info.Struct.fields&;
            @assert_eq(fields.len(), f.binding.bindings.len(), "destructure size mismatch");
            enumerate f.binding.bindings.items() { i, b |
                f := fields[i]&; // TODO: zip
                continue :: local_return;
                @err_assert(f.kind != .Const, "TODO: destructuring skip const fields") return;
                
                name := b.var().unwrap();
                self.addr_var(full_id);
                self.inc_ptr_bytes(f.byte_offset.trunc());
                id := self.save_ssa_var();
                self.mark_variable(name, id);
                // TODO: a test that fails if you typo put this line here. -- Jul 4 (see devlog.md)
                // self.addr_var(id);
                self.locals.last().unwrap().push(id);
                prev := self.var_lookup&.insert(name, id);
                @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program));
            };
        };
        fn Noop() void => ();
        // Can't hit DoneDeclFunc because we don't re-eval constants.
        @default => panic("ICE: stmt not desugared");
    };
    // Note: ^early return!
    .Ok
}

fn do_binding(self: *EmitBc, name: ?Var, ty: Type, value: *FatExpr) CRes(void) = {
    id := self.add_var(ty);
    self.addr_var(id);
    self.result_addr_depth += 1;
    @try(self.compile_expr(value, .ResAddr, false)) return;
    self.result_addr_depth -= 1;
    self.locals.last().unwrap().push(id);
    if name { name |
        self.mark_variable(name, id);
        prev := self.var_lookup&.insert(name, id);
        @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program));
    };
    .Ok
}

// This section looks like we could just use WalkAst, 
// but this operation is subtble enough that making control flow more indirect would probably make it more confusing. 
//
// If result_location == .ResAddr, the top of the stack on entry to this function has the pointer where the result should be stored.
fn compile_expr(self: *EmitBc, expr: *FatExpr, result_location: ResultLoc, can_tail: bool) CRes(void) = {
    @debug_assert(!expr.ty.is_unknown(), "Not typechecked: %", self.program.log(expr));
    @debug_assert(self.slot_count(expr.ty).lt(16).or(result_location != .PushStack), "% %", self.program.log(expr), self.program.log(expr.ty));
    self.last_loc = expr.loc;

    expr_ty := expr.ty;
    @match(expr.expr&) {
        fn Cast(v)  => {
            from := self.program.get_info(v.ty).stride_bytes();
            to   := self.program.get_info(expr_ty).stride_bytes();
            @err_assert(from == to, "@as size mismatch: % vs %", self.program.log(v.ty), self.program.log(expr_ty)) return;
            return(self.compile_expr(v[], result_location, can_tail))
        }
        fn Call(f)  => return(self.emit_call(f.f, f.arg, result_location, can_tail));
        fn Block(f) => return(self.emit_block_expr(expr, result_location, can_tail));
        fn Value(f) => return(self.emit_value(f.bytes&, result_location, expr.ty));
        fn If(_)      => return(self.emit_call_if(expr, result_location, can_tail));
        fn Loop(arg)  => return(self.emit_call_loop(arg[]));
        fn GetVar(_)  => {
            if result_location == .Discard {
                return(.Ok);
            };
            @try(self.addr_macro(expr, .PushStack)) return;
            return(self.deref_top_of_stack(result_location, expr_ty))
        };
        fn Addr(arg)  => return(self.addr_macro(arg[], result_location));
        fn StructLiteralP(pattern) => return(self.construct_struct(pattern, expr.ty, result_location));
        fn Slice(arg) => {
            container_ty := arg.ty;
            // Note: number of elements, not size of the whole array value.
            ty, count := @match(arg.expr) {
                fn Tuple(parts) Ty(Type, i64) => { 
                    fst := parts[0];
                    (fst.ty, parts.len) 
                };
                @default => (arg.ty, 1);
            };

            id := self.add_var(container_ty);
            self.addr_var(id);
            self.result_addr_depth += 1;
            @try(self.compile_expr(arg[], .ResAddr, false)) return;
            self.result_addr_depth -= 1;

            self.addr_var(id);
            self.push(PushConstant = (
                value = count,
                ty = .I64,
            ));
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr() => {
                    self.push(PeekDup = 2);
                    self.inc_ptr_bytes(8); // Note: backwards!
                    self.push(StorePost = .I64);
                    self.push(PeekDup = 1);
                    self.push(StorePost = .P64);
                    self.pop(1);
                }
                fn Discard() => self.pop(2);
            };
            ::?*List(u16);
            self.locals.last().unwrap().push(id);
            return(.Ok)
        };
        fn Deref(arg) => {
            @try(self.compile_expr(arg[], .PushStack, false)) return; // get the pointer
            value_type := self.program.unptr_ty(arg.ty).unwrap();
            @debug_assert(expr_ty == value_type, "missing pointer cast");
            return(self.deref_top_of_stack(result_location, expr_ty));
        };
        fn FnPtr(arg) => {
            f := @unwrap(arg[].as_const(), "expected fn for ptr") return;
            f := FuncId.assume_cast(f&)[];
            // For jit, the redirects go in the dispatch table so it doesn't matter,
            // but for emitting c, you need to get the real name.
            f = self.program.follow_redirects(f);
            self.push(GetNativeFnPtr = f);
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr()   => self.push(StorePre = .P64);
                fn Discard()   => self.push(Snipe = 0);
            };
            return(.Ok)
        };
        fn Unreachable() => {
            self.push(.Unreachable);
            return(.Ok)
        }
        fn Uninitialized() => {
            @assert(!expr_ty.is_never(), "call exit() to produce a value of type 'Never'");
            // Wierd special case I have mixed feelings about. should at least set to sentinal value in debug mode.
            // Now I have to not mess up the stack, tell the backend somehow.
            @match(result_location) {
                fn PushStack() => {
                    // for ty in self.program.flat_tuple_types(expr.ty) {
                    //     self.push(Bc::PushConstant {
                    //         value: 0,
                    //         ty: self.program.prim(ty).unwrap(),
                    //     });
                    // }
                    slots := self.slot_count(expr_ty);
                    range(0, slots.zext()) { _ |
                        self.push(PushConstant = (value = 0, ty = .I64));
                        // TODO: wrong prim!
                    };
                }
                fn ResAddr() => {
                    opts := self.program.get_build_options();
                    if opts.zero_init_memory {
                        self.zero_memory_at_top_of_stack(expr_ty);
                    } else {
                        self.push(Snipe = 0); // just pop the res ptr
                    };
                }  
                fn Discard() => ();
            };
            return(.Ok)
        };
        fn Tuple(values) => {
            raw := self.program.raw_type(expr.ty); // TODO: do i need this? 
            @match(self.program.get_type(raw)) {
               fn Struct(f) => {
                    @err_assert(f.layout_done, "ICE: struct layout not ready.") return;
                    @assert_eq(f.fields.len, values.len);
                    // TODO: assert is_tuple or need to skip const fields (if allow const fields update len assertion). 
                    
                    // TODO: sad that i have to write loops like this. 
                    range(0, f.fields.len) { i |
                        f := f.fields[i]&;
                        value := values[i]&;
                        if result_location == .ResAddr {
                            self.push(PeekDup = 0);
                            self.inc_ptr_bytes(f.byte_offset.trunc());
                            self.result_addr_depth += 1;
                        };
                        @try(self.compile_expr(value, result_location, false)) return;
                        if result_location == .ResAddr {
                            self.result_addr_depth -= 1;
                        };
                    };
                }
                fn Array(f) => {
                    @assert_eq(values.len(), f.len.zext());
                    element_size := self.program.get_info(f.inner).stride_bytes();
                    each values { value |
                        if result_location == .ResAddr {
                            self.inc_ptr_bytes(element_size);
                            self.push(PeekDup = 0);
                            self.result_addr_depth += 1;
                        };
                        @try(self.compile_expr(value, result_location, false)) return;
                        if result_location == .ResAddr {
                            self.result_addr_depth -= 1;
                        };
                    }
                }
                @default => {
                    return(@err("Expr::Tuple should have struct type not %. (0/1 element tuple maybe?)", self.program.log(raw)));
                };
            };
            if(result_location == .ResAddr, => { self.pop(1); });
            return(.Ok)
        }
        fn PtrOffset(f) => {
            // TODO: compiler has to emit tagchecks for enums now!!
            @try(self.compile_expr(f.ptr, .PushStack, false)) return;
            self.inc_ptr_bytes(f.bytes.trunc());
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr() => self.push(StorePre = .P64);
                fn Discard() => self.push(Snipe = 0);
            };
            return(.Ok)
        }
        fn Switch(f) => {
            return(self.emit_switch(expr, result_location, can_tail))
        }
        @default => @panic("ICE: didn't desugar: %", self.program.log(expr));
    }
}

fn deref_top_of_stack(self: *EmitBc, result_location: ResultLoc, value_type: Type) CRes(void) = {
    slots := self.slot_count(value_type);
    if slots == 0 {
        @match(result_location) {
            fn ResAddr() => self.pop(2); // pop dest too!
            @default     => self.push(Snipe = 0);
        };
    } else {
        @match(result_location) {
            fn PushStack() => @try(self.load(value_type)) return;
            fn ResAddr() => {
                info := self.program.get_info(value_type);
                @debug_assert(info.is_sized, "unsized!");

                if info.size_slots == 1 {
                    ty := @unwrap(self.program.prim(value_type), "deref prim") return;
                    self.push(Load = ty);
                    self.push(StorePre = ty);
                } else {| // TODO: else if
                    if info.size_slots == 2 && info.stride_bytes == 16 && info.align_bytes == 8 {
                        self.push(PeekDup = 1);
                        self.inc_ptr_bytes(8);
                        self.push(PeekDup = 1);
                        self.inc_ptr_bytes(8);
                        self.push(Load = .I64);
                        self.push(StorePre = .I64);
                        self.push(Load = .I64);
                        self.push(StorePre = .I64);
                    } else {
                        bytes := info.stride_bytes;
                        self.push(CopyBytesToFrom = bytes);
                    };
                };
            }
            fn Discard() => self.push(Snipe = 0);
        };
    };
    if(value_type.is_never(), => self.push(.Unreachable));
    .Ok
}

// consumes the pointer at the top of the stack.
fn zero_memory_at_top_of_stack(self: *EmitBc, expr_ty: Type) void = {
    info := self.program.get_info(expr_ty);
    align := info.align_bytes.min(8);
    chunks := info.stride_bytes / align;
    prim := @switch(align) {
        @case(1) => Prim.I8;
        @case(2) => Prim.I16;
        @case(4) => Prim.I32;
        @case(8) => Prim.I64;
        @default => panic("ICE: strange alignment");
    };
    range(0, chunks.zext()) { _ | 
        self.push(PeekDup = 0); 
        self.push(PushConstant = (value = 0, ty = prim));
        self.push(StorePre = prim);
        self.inc_ptr_bytes(align);
    }; 
    self.push(Snipe = 0); 
}

//// TODO: this would be prettier as tail calls. 
//fn follow_redirects(program: CompilerRs, f_id: FuncId) FuncId = {
//    loop {
//        continue :: local_return;
//        @if_let(program[f_id].body&)
//            fn Redirect(target) => {
//                f_id = target[];
//                continue();
//            };
//        return(f_id);
//    }; // TODO: this should work without this shit -- Jul 87
//    unreachable()
//}

// infinetkly ibnliens itself???:>?f,.A 
//fn follow_redirects(program: CompilerRs, f_id: FuncId) FuncId = {
//    @if_let(program[f_id].body)
//        fn Redirect(target) => {
//            return(program.follow_redirects(target)); // TODO: tail
//        };
//    f_id
//}

fn emit_call(self: *EmitBc, f: *FatExpr, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) CRes(void) #once = {
    caller := self.program.get_function(self.result.func);
    @match(self.program.get_type(f.ty)) {
        fn Fn(f_ty) => {
            f_id := @unwrap(f.as_const(), "ice: tried to call non-const fn %", self.program.log(f)) return;
            f_id := FuncId.assume_cast(f_id&)[];
            func := self.program.get_function(f_id);
            cc := func.cc.unwrap();
            @err_assert(!func.get_flag(.Generic), "tried to emit call to unlowered #generic") return;
            @debug_assert(!func.get_flag(.MayHaveAquiredCaptures), "tried to emit call to unlowered maybe '=>'");
            @assert(cc != .Inline, "ICE: tried to call inlined %", self.program.get_string(func.name));

            // TODO: ideally the redirect should just be stored in the overloadset so you don't have to have the big Func thing every time.
            // TODO: audit: is this f_ty different from the one we just got from the expression type? -- Jul 8
            f_ty := func.finished_ty().unwrap(); // kinda HACK to fix unaligned store? 
            original_f_id := f_id;
            f_id = self.program.follow_redirects(f_id);

            self.emit_runtime_call(f_ty, arg, result_location, can_tail) { sig, tail |
                self.result.mix_hash(f_id.as_index(), 436265);
                callee := self.program.get_function(f_id);
                if find_impl(func.body&, .Intrinsic) { op |
                    self.push(Intrinsic = op.bc);
                    // TODO: you might have setcontext for yourself
                    if op.bc == .GetContext {
                        caller.set_flag(.YesContext);
                    };
                    if tail {
                        @debug_assert(sig.ret2.is_none(), "So far all intrinsics return one thing.");
                        self.push(Ret1 = sig.ret1.unwrap());
                    };
                } else {
                    if !caller.get_flag(.ComptimeOnly) && original_f_id == f_id {
                        // TODO: it would be nice if this was at the beginning and you never tried to emit anything that was comptimeonly
                        //       that requires never adding them to callees and then later realizing you can inline them, which would make sense. -- Aug 29
                        @debug_assert(!(self.result.when == .Aot && callee.get_flag(.ComptimeOnly)), "cannot .Aot .ComptimeOnly % %", f_id, self.program.log(callee));
                    };
                    context := !callee.get_flag(.NoContext);
                    if context {
                        caller.set_flag(.YesContext);
                    };
                    
                    self.push(CallDirect = (sig = self.push_sig(sig[]), f = f_id, tail = tail, context = context));
                    if self.debug {
                        self.debug_location_for_last_op(f.loc);
                    };
                };
            }
        }
        fn FnPtr(f_ty) => {
            caller.set_flag(.YesContext);
            @try(self.compile_expr(f, .PushStack, false)) return;
            will_use_indirect_ret := self.slot_count(f_ty.ty.ret) > 2; // TODO: get this from sig
            if result_location == .ResAddr && will_use_indirect_ret {
                // grab the result pointer to the top of the stack so the layout matches a normal call.
                // however, if the function wants to push stack but we want it to a resaddr, we don't do this here because emit_runtime_call handles it which is kinda HACK.
                self.push(PeekDup = 1);
            };
            // TODO: allow tail calling through pointer depending on the calling convention 
            @try(self.emit_runtime_call(f_ty.ty, arg, result_location, can_tail) { sig, _tail |
                self.push(CallFnPtr = (sig = self.push_sig(sig[]), context = self.dyn_call_context));
                if self.debug {
                    self.debug_location_for_last_op(f.loc);
                };
            }) return;
            if result_location == .ResAddr && will_use_indirect_ret {
                self.push(Snipe = 0); // original ret ptr
            };
            .Ok
        }
        fn Label(ret_ty) => {
            label := @unwrap(f.as_const(), "called label must be const") return;
            return_from := LabelId.assume_cast(label&)[];

            ret := @unwrap(
                self.inlined_return_addr&.get_ptr(return_from),
                "missing return label. forgot '=>' on function?"
            ) return;
            ret.used = true;  // note: updating the one in the map! not a copy
            ret := ret[];
            // result_location is the result of the ret() expression, which is Never and we don't care (because we're going to jump away).
            // But we have to discard it if the old expression was expecting something large. 
            // Because the incoming pointer on the stack will flow through and confuse us later. :early_return_big
            // this would make sense if we were checking result_location, but we're not
            x := (ret.result_loc == .ResAddr); // (result_location.eq(.ResAddr)).or
            if x {
                self.pop(1); 
            };
            // I feel this should be good enough without the above, but no.
            // Since we might have nested expressions that push result pointers, you not only have to re-push ours,
            // you also need to pop off anything in between that the target block isn't expecting. 
            // ie. its important that each entry to a block has the same stack height. 
            diff := self.result_addr_depth - ret.result_addr_depth;
            self.pop(diff.trunc()); 
            
            slots: u16 = @match(ret.result_loc) {
                fn PushStack() => self.slot_count(ret_ty[]);
                fn ResAddr() => {
                    id := ret.res_ssa_id.unwrap();
                    self.push(LoadSsa = (id = id));
                    self.result_addr_depth += 1;
                    0
                }
                fn Discard() => 0;
            };
            // TODO: sometimes can_tail, if you're returning the main function
            @try(self.compile_expr(arg, ret.result_loc, false)) return;
            if ret.result_loc == .ResAddr {
                self.result_addr_depth -= 1;
            };
            self.push(Goto = (ip = ret.block, slots = slots));
            self.result[ret.block].incoming_jumps += 1;
            .Ok
        }
        @default => @panic("ICE: non callable: %", self.program.log(f));
    }
}

fn push_sig(self: *EmitBc, sig: PrimSig) u32 = {
    self.result.sig_payloads&.push(sig, self.out_alloc);
    self.result.sig_payloads.len.sub(1).trunc()
}

fn emit_block_expr(self: *EmitBc, expr: *FatExpr, result_location: ResultLoc, can_tail: bool) CRes(void) #once = {
    block := expr.expr.Block&;
    body := block.body&;
    value := block.result;
    ret_label := block.ret_label;
    self.locals&.push(list(temp()));
    
    // :block_never_unify_early_return_type
    // Note: block_ty can be different from value.ty if the fall through is a Never but there's an early return to the block. 
    block_ty := expr.ty;
    out := self.program.get_info(block_ty);
    @debug_assert(out.size_slots.lt(8).or(result_location != .PushStack), "pushing large value");

    if ret_label { ret_var | 
        entry_block := self.current_block;
        return_block := @match(result_location) {
            fn PushStack() => self.push_block(self.get_primatives(block_ty));
            fn ResAddr()   => self.push_block_empty();
            fn Discard()   => self.push_block_empty();
        };
        ret: ReturnAddr = (
            block = return_block,
            result_loc = result_location,
            store_res_ssa_inst = .None,
            res_ssa_id = .None,
            used = false,
            result_addr_depth = self.result_addr_depth,
        );
        self.current_block = entry_block;
        if result_location == .ResAddr {
            self.push(PeekDup = 0);
            id := self.save_ssa_var();
            ret.res_ssa_id = (Some = id);
            index := self.result[entry_block].insts.len - 1;
            ret.store_res_ssa_inst = (Some = (entry_block, index));
        };

        prev := self.inlined_return_addr&.insert(ret_var, ret);
        @assert(prev.is_none(), "stomped ret var");

        depth := self.result_addr_depth;
        each body { stmt | 
            @try(self.compile_stmt(stmt)) return;
            @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");
        };
        @try(self.compile_expr(value, result_location, can_tail)) return;
        @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");

        ret := self.inlined_return_addr&.remove(ret_var).unwrap();
        if self.result[return_block].incoming_jumps > 0 {
            @debug_assert(ret.used, "expected jumps");
            slots := self.result[return_block].arg_prims.len;
            self.push(Goto = (ip = return_block, slots = slots.trunc()));
            self.result[return_block].incoming_jumps += 1;
            self.current_block = return_block;
        } else {
            @debug_assert(!ret.used, "expected no jumps"); 
            if ret.store_res_ssa_inst { f | 
                block, index := f;
                // if we didn't use it, don't bother asking the backend to save the register with the result address.
                self.result[block].insts[index] = .Nop;
                self.result[block].insts[index - 1] = .Nop;
            };
            self.push_to(return_block, .NoCompile);
        };
    } else {
        // TODO: sometimes the last one can tail, if value is Unit and we're the body of the main function.
        depth := self.result_addr_depth;
        each body { stmt |
            @try(self.compile_stmt(stmt)) return;
            @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");
        };
        @try(self.compile_expr(value, result_location, can_tail)) return;
        @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");
    };

    // TODO: check if you try to let an address to a variable escape from its scope.
    slots := self.locals&.pop().expect("block to have scope");
    for slots { id |
        self.push(LastUse = (id = id ));
    };
    .Ok
}

fn emit_value(self: *EmitBc, value: *Values, result_location: ResultLoc, expr_ty: Type) CRes(void) #once = {
    if(result_location == .Discard, => return(.Ok));
    reader: ReadBytes = (bytes = value.bytes(), i = 0);
    want_emit_by_memcpy := value.len() > 16;  // dont change this to include small things unless you do something about values being stored inline.
    // TODO: you probably want to allow people to overload bake_relocatable_value even if !contains_pointers, but also there's no point. -- Jun 19
    if self.result.when == .Aot && want_emit_by_memcpy.or(self.program.get_info(expr_ty).contains_pointers()) {
        if result_location.eq(.PushStack).or(!want_emit_by_memcpy) {
            out := @try({self.program.vtable.emit_relocatable_constant_body}(self.program.data, value.bytes(), expr_ty, false)) return;
            
            // just going this to get the prims because llvm wants to know what's a pointer. 
            _parts: List(i64) = list(temp());
            info: List(Ty(Prim, u16)) = list(temp());
            @try(self.program.deconstruct_values(expr_ty, reader&, _parts&, (Some = info&))) return;
            @err_assert(info.len == out.len, "length of relocatable_constant_body does not match the number of Prims for %", self.program.log(expr_ty)) return;
            
            enumerate out { i, part | 
                @match(part[]) {
                    fn Num(f) => {
                        self.push(PushConstant = (value = f.value, ty = info[i]._0));
                        self.result.mix_hash(f.value, 123);
                    }
                    fn FnPtr(f) => {
                        f = self.program.follow_redirects(f);
                        self.push(GetNativeFnPtr = f);
                        self.result.mix_hash(f.as_index(), 1237);
                    }
                    fn AddrOf(id) => {
                        self.push(PushGlobalAddr = id);
                        self.result.mix_hash(id.id.zext(), 9765);
                    }
                };
            };
            if result_location == .ResAddr {
                self.store_pre(expr_ty);
            };
        } else {
            @assert_eq(result_location, .ResAddr);
            id := @try(self.program.emit_relocatable_constant(expr_ty, value.bytes())) return;
            self.result.mix_hash(id.id.zext(), 9765);
            self.push(PushGlobalAddr = id);
            info := self.program.get_info(expr_ty);
            self.push(CopyBytesToFrom = info.stride_bytes);
        };
        return(.Ok);
    };

    @match(result_location) {
        fn PushStack() => {
            parts: List(i64) = list(temp());
            info: List(Ty(Prim, u16)) = list(temp());
            @try(self.program.deconstruct_values(
                expr_ty,
                reader&,
                parts&,
                (Some = info&),
            )) return;
            @debug_assert_eq(parts.len(), info.len());
            enumerate parts { i, value | 
                ty, _ := info[i];
                self.result.mix_hash(value[], 1234567);
                self.push(PushConstant = (value = value[], ty = ty));
            };
        }
        fn ResAddr() => {
            if want_emit_by_memcpy {
                // we know the value is big!
                // TODO: HACK
                //       for a constant ast node, you need to load an enum but my deconstruct_values can't handle it.
                //       this solution is extra bad becuase it relies on the value vec not being free-ed
                self.push(PushConstant = (
                    value = value.jit_addr(),
                    ty = .P64,
                ));
                
                // Note: not the same as value.len!!!!!      // TODO: audit: why not? -- Jul 8
                self.push(CopyBytesToFrom = self.program.get_info(expr_ty).stride_bytes());
            } else {
                parts: List(i64) = list(temp());
                offsets: List(Ty(Prim, u16)) = list(temp());
                @try(self.program.deconstruct_values(
                    expr_ty,
                    reader&,
                    parts&,
                    (Some = offsets&),
                )) return;
                @debug_assert_eq(parts.len(), offsets.len());
                enumerate parts { i, value | 
                    ty, offset := offsets[i];
                    self.push(PeekDup = 0);
                    self.inc_ptr_bytes(offset);
                    self.push(PushConstant = (value = value[], ty = ty));
                    self.push(StorePre = ty);
                    self.result.mix_hash(value[], 1234567);
                };
                self.pop(1); // res ptr
            };
        }
        fn Discard() => ();
    };
    .Ok
}

// :PlaceExpr
fn addr_macro(self: *EmitBc, arg: *FatExpr, result_location: ResultLoc) CRes(void) #once = {
    self.last_loc = arg.loc;
    // field accesses should have been desugared.
    @err_assert(arg.expr&.is(.GetVar), "took address of r-value") return;
    var := arg.expr.GetVar;
    // TODO: this shouldn't allow let either but i changed how variable refs work for :SmallTypes
    @assert_ne(var.kind, .Const, "Can only take address of var (not let/const) %", var&.log(self.program));
    id := @unwrap(self.var_lookup&.get(var), "Missing var % (in !addr) \n(missing $ when used in const context or missing '=>'? TODO: better error message)", var&.log(self.program)) return;
    self.addr_var(id);

    @match(result_location) {
        fn PushStack() => ();
        fn ResAddr()   => self.push(StorePre = .P64);
        fn Discard()   => self.push(Snipe = 0);
    };
    .Ok
}

// we never make the temp variable. if the arg is big, caller needs to setup a result location.
fn emit_call_if(self: *EmitBc, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) CRes(void) #once = {
    @err_assert(arg.expr&.is(.If), "ICE: expected if") return;
    parts := arg.expr.If;
    
    @try(self.compile_expr(parts.cond, .PushStack, false)) return; // cond
    if_true, if_false := (parts.if_true, parts.if_false);
 
    out := self.program.get_info(if_true.ty);
    @assert(out.size_slots.lt(4).or(result_location != .PushStack), "ICE: 'if' result too big to go on stack"); // now its the callers problem to deal with this case

    branch_block := self.current_block;
    true_ip := self.push_block_empty();
    @try(self.compile_expr(if_true, result_location, can_tail)) return;
    end_true_block := self.current_block;
    false_ip := self.push_block_empty();
    @try(self.compile_expr(if_false, result_location, can_tail)) return;
    end_false_block := self.current_block;
    
    ::if(u16);
    block_slots := if(result_location == .PushStack, => out.size_slots, => 0);
    prims := if(result_location == .PushStack, => self.get_primatives(if_true.ty), => empty());
    ip := self.push_block(prims);
    self.push_to(branch_block, (JumpIf = (true_ip = true_ip, false_ip = false_ip, slots = 0 )));
    self.push_to(end_true_block, (Goto = (ip = ip, slots = block_slots)));

    self.push_to(end_false_block, (Goto = (ip = ip, slots = block_slots)));
    self.result[ip].incoming_jumps += 2;
    self.result[true_ip].incoming_jumps += 1;
    self.result[false_ip].incoming_jumps += 1;
    self.bump_clock(ip);

    .Ok
}

fn emit_switch(self: *EmitBc, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) CRes(void) #once = {
    @err_assert(arg.expr&.is(.Switch), "ICE: expected switch") return;
    parts := arg.expr.Switch;
    
    if parts.cases.len == 0 {
        // There's only a default; that's not really a switch bro but ok... 
        @try(self.compile_expr(parts.value, .Discard, false)) return;
        @try(self.compile_expr(parts.default, result_location, false)) return;
        return(.Ok);
    };
    
    @try(self.compile_expr(parts.value, .PushStack, false)) return; //  this is the thing we're inspecting!
    prim := @unwrap(self.program.prim(parts.value.ty), "switch on non-prim") return;
    if prim == .I32 {
        self.push(Intrinsic = .ZeroExtend32To64);  
    };
    if prim == .I16 {
        self.push(Intrinsic = .ZeroExtend16To64);  
    };
    entry_block := self.current_block; 
   
    // This is where we rejoin with the value of the whole switch expression. 
    out := self.program.get_info(arg.ty);
    @assert(out.size_slots.lt(4).or(result_location != .PushStack), "ICE: 'switch' result too big to go on stack"); // now its the callers problem to deal with this case
    block_slots := if(result_location == .PushStack, => out.size_slots, => 0);
    prims := if(result_location == .PushStack, => self.get_primatives(arg.ty), => empty());
    end_block := self.push_block(prims);
    self.current_block = entry_block;
    cases: List(SwitchPayload) = list(self.program.get_alloc());
    
    push_case :: fn(value: i64, body: *FatExpr) void => {
        case_block := self.push_block_empty();
        @try(self.compile_expr(body, result_location, false)) return; // TODO: tail call
        end_case_block := self.current_block;
        self.push_to(end_case_block, (Goto = (ip = end_block, slots = block_slots)));
        cases&.push(value = value, block = case_block); 
        self.result[case_block].incoming_jumps += 1;
        self.result[end_block].incoming_jumps += 1;
    };
    
    each parts.cases { f |
        push_case(f._0, f._1&);
    };
   
    // TODO: would it be nicer to have default branch just be the last thing in the ast node too so you could handle them uniformly? -- Jul 26
    push_case(-1, parts.default);
    // TODO: make sure fn neg is #fold
    
    self.current_block = end_block;
    self.push_to(entry_block, (Switch = self.result.switch_payloads.len.trunc()));
    self.result.switch_payloads&.push(cases.rs(), self.program.get_alloc());
     
    self.bump_clock(end_block);
    .Ok
}

fn emit_call_loop(self: *EmitBc, arg: *FatExpr) CRes(void) #once = {
    @debug_assert_eq(arg.ty, void);

    prev_block := self.current_block;
    start_body_block := self.push_block_empty();
    self.current_block = start_body_block;
    self.push_to(
        prev_block,
        (Goto = (ip = start_body_block, slots = 0)),
    );

    @try(self.compile_expr(arg, .Discard, false)) return;
    end_body_block := self.current_block;

    self.push_to(
        end_body_block,
        (Goto = (ip = start_body_block, slots = 0)),
    );
    self.result[start_body_block].incoming_jumps += 2;
    .Ok
}

// :PlaceExpr
fn set_deref(self: *EmitBc, place: *FatExpr, value: *FatExpr) CRes(void) #once = {
    // we care about the type of the pointer, not the value because there might be a cast.
    @match(place.expr&) {
        fn GetVar(_) => @try(self.addr_macro(place, .PushStack)) return;
        // TODO: write a test for pooiinter eval oreder. left hsould come first. -- MAy 7
        fn Deref(arg) => @try(self.compile_expr(arg[], .PushStack, false)) return;
        @default => return(@err("TODO: other `place=e;` :("));
    };
    self.result_addr_depth += 1;
    @try(self.compile_expr(value, .ResAddr, false)) return;
    self.result_addr_depth -= 1;
    .Ok
}

// TODO: rename? also used for tagged. 
fn construct_struct(self: *EmitBc, pattern: *Pattern, requested: Type, result_location: ResultLoc) CRes(void) #once = {
    raw_container_ty := self.program.raw_type(requested);
    slots := self.slot_count(raw_container_ty);
    @debug_assert(slots < 8 || result_location != .PushStack, "too big to put on stack"); 
    
    @match(self.program.get_type(raw_container_ty)) {
        fn Struct(f) => {
            expected := f.fields.len - f.const_field_count.zext();
            if !f.is_union {
                @assert_eq(expected, pattern.bindings.len, "Cannot assign to type % with wrong field count", self.program.log(requested));
            };
            i := 0;
            each f.fields { field | 
                continue :: local_return;
                i += 1;
                if(field.kind == .Const, => continue());
                value := pattern.bindings[i - 1]&;
                
                name := @unwrap(value.ident(), "map literal entry needs name (while initilizing %)", self.program.log(requested)) return;
                field := or find_struct_field(f, name, i - 1) {
                    if f.is_union {
                        continue();
                    };
                    return(@err("field name mismatch (ICE: should be checked by sema)"))
                };
                
                if result_location == .ResAddr {
                    self.push(PeekDup = 0);
                    self.inc_ptr_bytes(field.byte_offset.trunc());
                    self.result_addr_depth += 1;
                };
                expr := value.get_default().unwrap();
                @try(self.compile_expr(expr, result_location, false)) return;
                if result_location == .ResAddr {
                    self.result_addr_depth -= 1;
                };
                if f.is_union { // :SLOW
                    if result_location == .ResAddr {
                        self.pop(1); // res ptr
                    };
                    return(.Ok);
                };
            };
            if result_location == .ResAddr {
                self.pop(1); // res ptr
            };
        }
        // TODO: make this constexpr in compiler (TODO: audit: this comment is from back when i had the interp)
        fn Tagged(f) => {
            //@debug_assert(result_location != .Discard, "todo");
            size := self.slot_count(raw_container_ty);
            @debug_assert_eq(1, pattern.bindings.len, 
                "% is @tagged, value should have one active varient (ICE: should be checked by sema)",
                self.program.log(requested)
            );
            name := pattern.bindings[0].name.unwrap();
            i := f.cases.position(fn(f) => f._0 == name).expect("case name to exist in type");
            payload_size := self.slot_count(f.cases[i]._1);
            if payload_size >= size {
                return(@err("Enum value won't fit."));
            };
            value := pattern.bindings.index(0).get_default().unwrap();
            @match(result_location) {
                fn PushStack() => {
                    self.push(PushConstant = (
                        value = i,
                        ty = .I64,
                    ));
                    @try(self.compile_expr(value, result_location, false)) return;

                    // :tagged_prims_hack
                    // TODO: this is a dumb hack to make the padding have the right prim types for backends that care. :SLOW -- Jun 23
                    //       This fixes test_option/option_small_payload_n/parse on llvm.  
                    // TODO: this is super copy-paste-y now that new sema outputs values for some tagged ctx_field
                    types := self.program.flat_tuple_types(raw_container_ty);
                    types&.ordered_remove(0);
                    range(0, self.slot_count(value.ty).zext()) { _ |
                        types&.ordered_remove(0);
                    };
                    expected_pad := size - (payload_size + 1);
                    @debug_assert_eq(types.len(), expected_pad.zext(), "confused about padding size");
                    // now types is just padding

                    // If this is a smaller varient, pad out the slot.
                    for types { p |
                        ty := @unwrap(self.program.prim(p), "not prim") return;
                        self.push(PushConstant = (value = 0, ty = ty));
                    };
                }
                fn ResAddr() => {
                    self.push(PeekDup = 0);
                    self.push(PushConstant = (
                        value = i,
                        ty = .I64,
                    ));
                    self.push(StorePre = .I64);
                    self.inc_ptr_bytes(8); // TODO: differetn sizes of tag
                    @try(self.compile_expr(value, result_location, false)) return;
                }
                fn Discard() => {
                    @try(self.compile_expr(value, result_location, false)) return;
                }
            };

            // TODO: support explicit uninit so backend doesn't emit code for the padding above.
            //       current system also means weird type stuff where you write ints into units in bc_to_asm.
            //       if we staticlly know the tag value, could only copy the size of the active varient (which would be the general case of leaving it uninit when creating one).
            //       but also eventually we probably want to define and preserve padding so tags could be stored there.
            //       even without that, maybe its a weird undefined behaviour to just drop some of your bytes,
            //       should have compiler settings about being allowed to cast *T -> *[size_of(T)]u8 because that would observe the padding.
            //       I like the idea of granular toggling ub vs optimisations and having those flags as a place to hang comments,
            //       but that adds a lot of extra work testing all the combinations which might not be worth it.
            //       -- Apr 17
            //
        }
        @default => {
            return(@err("struct literal for non-(struct/tagged)"));
        };
    };
    .Ok
}

fn push_block_empty(self: *EmitBc) BbId = {
    self.result.blocks&.push((
        insts = empty(),
        debug = empty(),
        arg_prims = empty(),
        incoming_jumps = 0,
        clock = self.clock,
    ), self.out_alloc);
    b: BbId = (id = self.result.blocks.len().trunc() - 1);
    self.current_block = b;
    b
}

fn push_block(self: *EmitBc, arg_prims: [] Prim) BbId = {
    self.result.blocks&.push((
        insts = empty(),
        debug = empty(),
        arg_prims = arg_prims,
        incoming_jumps = 0,
        clock = self.clock,
    ), self.out_alloc);
    b: BbId = (id = self.result.blocks.len().trunc() - 1);
    self.current_block = b;
    b
}

fn bump_clock(self: *EmitBc, b: BbId) void ={
    self.clock += 1;
    self.result[b].clock = self.clock;
}

fn slot_count(self: *EmitBc, ty: Type) u16 = {
    i := self.program.get_info(ty);
    @debug_assert(i.is_sized, "unsized type");
    i.size_slots 
}

fn push(self: *EmitBc, inst: Bc) void = {
    self.push_to(self.current_block, inst);
}

fn addr_var(ctx: *EmitBc, id: u16) void = {
    if ctx.is_ssa_var&.get(id.zext()) {
        ctx.push(LoadSsa = (id = id));
    } else {
        ctx.push(AddrVar = (id = id));
    };
}

fn save_ssa_var(ctx: *EmitBc) u16 = {
    id := ctx.add_var(rawptr);
    ctx.is_ssa_var&.set(id.zext(), temp()); 
    ctx.push(SaveSsa = (id = id, ty = .P64));
    id
}

fn inc_ptr_bytes(self: *EmitBc, bytes: u16) void = {
    if bytes != 0 {
        self.push(IncPtrBytes = bytes.zext());
    };
}

::tagged(Bc);
fn push_to(self: *EmitBc, b: BbId, inst: Bc) void = {
    //@println("%", inst&.tag());
    self.result.mix_hash(inst&.tag().ordinal(), 9801);
    self.result[b].insts&.push(inst, self.out_alloc);
}

fn debug_location_for_last_op(self: *EmitBc, loc: Span) void = {
    block := self.result[self.current_block]&;
    // TODO: This representation is wasteful because most instructions aren't calls. 
    while => block.debug.len < block.insts.len { // :SLOW should at least reserve
        block.debug&.push(Span.zeroed(), self.out_alloc); 
    };
    block.debug[block.insts.len - 1] = loc;
}

fn mix_hash(self: *FnBody, i: i64, spice: i64) void = {
    self.hash += i;
    self.hash *= spice;
}

fn pop(self: *EmitBc, slots: u16) void = {
    range(0, slots.zext()) { _ | 
        self.push(Snipe = 0);
    }
}

fn index(self: *FnBody, b: BbId) *BasicBlock = {
    self.blocks.index(b.id.zext())
}

fn add_var(ctx: *EmitBc, ty: Type) u16 = {
    info := ctx.program.get_info(ty);
    ctx.result.mix_hash(info.stride_bytes.zext(), 12345);
    ctx.result.vars&.push((size = info.stride_bytes, align = info.align_bytes), ctx.out_alloc);
    i: u16 = ctx.result.vars.len.trunc();
    i - 1
}

fn get_primatives(self: *EmitBc, ty: Type) []Prim #inline = 
    self.program.get_primatives(ty);

fn idx(s: FuncId) i64 = s.to_index().zext();

fn is_float(self: Prim) bool = 
    self.eq(.F64).or(self == .F32);

fn align_to(offset: u16, align: u16) i64 #redirect(Ty(i64, i64), i64);

fn int_count(self: Prim) i64 = 
    if(self.is_float(), => 0, => 1);

fn ret_slots(self: *PrimSig) u16 = {
    ::if(u16);
    if self.ret1.is_some() {
        if(self.ret2.is_some(), => 2, => 1)
    } else {
        0
    }
}


// TODO: be able to derive but skip fields? or take off `func`?
// TODO: (also in the derived version) for aggragates like this with multiple collections, check all the lengths first before iterating. 
//       generalize quick_ne or something maybe? 
fn eq(lhs: **FnBody, rhs: **FnBody) bool = {
    //if(lhs.when != rhs.when, => return(false));
    if(lhs.blocks&.items() != rhs.blocks&.items(), => return(false));
    if(lhs.vars&.items() != rhs.vars&.items(), => return(false));
    // you almost never need this... but when you do you really do! :deduplicate_checks_switch_payloads
    if(lhs.switch_payloads&.items() != rhs.switch_payloads&.items(), => return(false)); 
    // Note: not checking sig_payloads, i think its fine tho becuase we check function ids
    true
}

// we do a rolling hash as we create the FnBody so just return that. 
fn hash(h: *TrivialHasher, s: **FnBody) void = {
    h.hash(s.hash&);
}

:: {
    @if(ENABLE_DEDUPLICATION) {
        fn eq(a: *BasicBlock, b: *BasicBlock) bool = {
            // not comparing debug info, who cares. 
            a.insts& == b.insts& && a.arg_prims& == b.arg_prims& && a.incoming_jumps == b.incoming_jumps && a.clock == b.clock
        }
        AutoEq(Bc);
        AutoEq(PrimSig);
        DerefEq(Prim);
        DerefEq(Intrinsic);
        AutoEq(?Prim);
        AutoEq(SwitchPayload);
        AutoEq(BbId);
        AutoEq(BakedVarId);
        AutoEq(VarSlotType);
        AutoEq(get_variant_type(Bc, Bc.Tag().CallDirect));
        AutoEq(get_variant_type(Bc, Bc.Tag().CallFnPtr));
        AutoEq(get_variant_type(Bc, Bc.Tag().PushConstant));
        AutoEq(get_variant_type(Bc, Bc.Tag().JumpIf));
        AutoEq(get_variant_type(Bc, Bc.Tag().Goto));
        AutoEq(get_variant_type(Bc, Bc.Tag().AddrVar));
        AutoEq(get_variant_type(Bc, Bc.Tag().SaveSsa));
        AutoEq(get_variant_type(Bc, Bc.Tag().LoadSsa));
        AutoEq(get_variant_type(Bc, Bc.Tag().LastUse));
        AutoEq(get_variant_type(Bc, Bc.Tag().Ret2));
    };
    
    H :: TrivialHasher;
    
    DeriveFmt(Bc);
    DeriveFmt(get_variant_type(Bc, Bc.Tag().CallDirect));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().CallFnPtr));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().PushConstant));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().JumpIf));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().AddrVar));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().SaveSsa));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().LoadSsa));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().LastUse));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().Ret2));
    DeriveFmt(get_variant_type(Bc, Bc.Tag().Goto));
    ::display_slice(Prim);
    ::display_slice(BbId);
    
    DeriveFmt(BakedVarId);
    
    fn display(self: *BbId, out: *List(u8)) void = {
        @fmt(out, "B%", self.id);
    }
    
    fn display(self: *u16, out: *List(u8)) void = {
        display(@as(i64) self[].zext(), out);
    }
    
    fn display(self: *PrimSig, out: *List(u8)) void = {
        @fmt(out, "%", self.args);
        if self.no_return {
            @fmt(out, " #never");
        };
        if self.first_arg_is_indirect_return {
            @fmt(out, " #big");
        };
    }
};
    
// TODO: :const_field_fix
// TODO: if you move the ffi dynamic abi call to this language you can remove the rust version of this function. 
/// Take some opaque bytes and split them into ints. So (u8, u8) becomes a vec of two i64 but u16 becomes just one.
fn deconstruct_values(program: CompCtx, ty: Type, bytes: *ReadBytes, out: *List(i64), offsets: ?*List(Ty(Prim, u16))) CRes(void) = {
    info := program.get_info(ty);

    @debug_assert_eq(bytes.i.mod(info.align_bytes.zext()), 0);
    found_len := bytes.bytes.len() - bytes.i;
    @err_assert(
        found_len >= info.stride_bytes.zext(),
        "deconstruct_values of % wants % bytes but found %: %",
        program.log(ty), info.stride_bytes, found_len, {
            bytes := bytes.bytes.slice(bytes.i, bytes.bytes.len);
            out: List(u8) = list(temp());
            for bytes{ b | 
                @fmt(out&, "%, ", @as(i64) b.zext());
            };
            out.items()
        }
    ) return;
    
    single :: fn($T: Type, prim: Prim) void => {
        offset := bytes.i;
        if offsets { offsets |
            offsets.push(@as(Ty(Prim, u16)) (prim, bytes.i.trunc()));
        };
        raw := bytes.read_next(T);
        out.push(@as(i64) raw.int());
    };
    
    @match(program.get_type(ty)) {
        fn Placeholder() => { return(@err("ICE: Unfinished type")); }; // TODO: show the type
        fn Never()       => return(@err("invalid type: Never"));
        fn F64()     => i64.single(.F64);
        fn FnPtr(_)  => i64.single(.P64);
        fn Ptr(_)    => i64.single(.P64);
        fn VoidPtr() => i64.single(.P64);
        fn F32()     => u32.single(.F32);
        fn Fn(_)     => u32.single(.I32);
        fn Label(_)  => u32.single(.I32);
        fn void()    => return(.Ok);
        fn Bool()    => u8.single(.I8);
        fn Tagged(f) => {
            start := bytes.i;
            start_count := out.len;
            i64.single(.I64);
            tag := out.last().unwrap()[];
            @err_assert(tag < f.cases.len, "Invalid tag % in constant", tag) return;
            case := f.cases.index(tag);
            @try(deconstruct_values(program, case._1, bytes, out, offsets)) return;
            padding := (@as(i64) info.stride_bytes.zext()) - (bytes.i - start);
            @err_assert(padding >= 0, "ICE: confused about @tagged size.") return;
            end_count := out.len;
            padding_slots := (@as(i64) info.size_slots.zext()) - (end_count - start_count);
            @err_assert(padding_slots >= 0, "ICE: too many slots????") return;
            
            // :tagged_prims_hack
            // TODO: try to make a test that fails without this. 
            //       ie just replace with 
            //       range(0, padding_slots) { _ |
            //          // TODO: wrong prim
            //          i64.single(.I64);
            //      };
            //      otherwise reduce the copy-paste of the 3 versions of this. 
            types := program.flat_tuple_types(ty);
            types := types.items().slice_last(padding_slots);
            types := types.unwrap();
            // now types is just padding

            // If this is a smaller varient, pad out the slot.
            for types { p |
                ty := @unwrap(program.prim(p), "not prim") return;
                out.push(0);
                if offsets { offsets |
                    // TODO: do we care about the offsets?
                    offsets.push(@as(Ty(Prim, u16)) (ty, bytes.i.trunc()));
                };
            };
            bytes.i += padding;
        }
        fn Enum(f)   => return(deconstruct_values(program, f.raw, bytes, out, offsets));
        fn Named(f)  => return(deconstruct_values(program, f._0, bytes, out, offsets)); // TODO: make Named payload not a tuple or do destructuring here. -- Jul 8
        fn Int(_)    => {
            @switch(program.get_info(ty)[].stride_bytes) {
                @case(1) => u8.single(.I8);
                @case(2) => u16.single(.I16);
                @case(4) => u32.single(.I32);
                @case(8) => i64.single(.I64);
                @default fn(n: u16) => return(@err("ICE: bad int stride %", n));
            };
        }
        fn Array(f) => {
            inner_align := program.get_info(f.inner).align_bytes();
            range(0, f.len.zext()) { _ |
                @debug_assert_eq(bytes.i.mod(inner_align.zext()), 0);
                @try(deconstruct_values(program, f.inner, bytes, out, offsets)) return;
            };
        }
        fn Struct(f) => {
            @err_assert(f.layout_done, "ICE: layout not ready") return;
            @err_assert(!f.is_union, "You can't deconstruct_values of a union because we don't know which varient is active.") return;
            prev := 0;
            size: i64 = program.get_info(ty)[].stride_bytes.zext();
            start := bytes.i;
            for f.fields { t |
                continue :: local_return;
                if t.kind == .Const {
                    continue();
                };
                assert(prev <= t.byte_offset, "ICE: backwards field offset");
                bytes.i = t.byte_offset;
                @try(deconstruct_values(program, t.ty, bytes, out, offsets)) return;
                prev = t.byte_offset;
            };
            bytes.i = start + size; // eat trailing stride padding
        }
    };
    .Ok
}