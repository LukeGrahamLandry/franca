//! #one_ret_pic SAFETY: one return at the end (or HACK), c abi, no relative jumps outside the function, no stack frame. 
//! - means it can be done inline by just chopping off the ret at the end. makes it a bit less insane to use fns for add, etc. 
//! - always inlines, trusts you that its small enough to be worth it. 
//! - still really dumb rn cause i do redundant stack loads/stores. 
// TODO: some consistent way of expressing debug mode bounds checks for div, shift, etc.

#one_ret_pic #aarch64
fn add(a: i64, b: i64) i64 = (
    add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn sub(a: i64, b: i64) i64 = (
    sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn mul(a: i64, b: i64) i64 = (
    madd(Bits.X64, x0, x0, x1, 0b11111),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn div(a: i64, b: i64) i64 = (
    sdiv(Bits.X64, x0, x0, x1),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn shift_left(value: i64, shift_amount: i64) i64 = (
    lslv(Bits.X64, x0, x0, x1),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn bit_or(a: i64, b: i64) i64 = (
    orr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
    ret(),
)!asm;

#one_ret_pic #aarch64 // TODO: actually test
fn bit_not(a: i64) i64 = (
    orn(Bits.X64, x0, x0, 0b11111, Shift.LSL, @as(u6) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn bit_and(a: i64, b: i64) i64 = (
    and_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!asm;

//! IMPORTANT: cond is inverted is backwards because CSINC
fn cmp_with_cond(inv_cond: Cond) Ty(u32, u32, u32) = (
    cmp(Bits.X64, x0, x1),
    cset(Bits.X64, x0, inv_cond),
    ret(),
);

#one_ret_pic #aarch64
fn eq(a: i64, b: i64) bool = cmp_with_cond(Cond.NE)!asm;

#one_ret_pic #aarch64
fn ne(a: i64, b: i64) bool = cmp_with_cond(Cond.EQ)!asm;

#one_ret_pic #aarch64
fn le(a: i64, b: i64) bool = cmp_with_cond(Cond.GT)!asm;

#one_ret_pic #aarch64
fn ge(a: i64, b: i64) bool = cmp_with_cond(Cond.LT)!asm;

#one_ret_pic #aarch64
fn lt(a: i64, b: i64) bool = cmp_with_cond(Cond.GE)!asm;

#one_ret_pic #aarch64
fn gt(a: i64, b: i64) bool = cmp_with_cond(Cond.LE)!asm;

// TODO: this is just the same as add.
// offsets measured in bytes.  
#one_ret_pic #aarch64
fn offset(ptr: rawptr, bytes: i64) rawptr = (
    add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!asm;

//////
/// Floats
// TODO: my macro system should be able to make all these binary operators much neater
//       something like @bin_math_op(add, f64, fadd, FType.D64)
//       There's wierdness around bootstrapping (for ints, prosumably the compiler doesn't need floats),
//       but I do that by precompiling to bit literals anyway so its fine. 
//       Other problem is name resolution, but they can probably just add themselves to the .gen int versions so its fine. 

// TODO: maybe its weird that #bs implies @pub
#aarch64 #one_ret_pic
fn add(a: f64, b: f64) f64 = (
    fadd(FType.D64, x0, x0, x1),
    ret(),
)!asm;

#aarch64 #one_ret_pic
fn sub(a: f64, b: f64) f64 = (
    fsub(FType.D64, x0, x0, x1),
    ret(),
)!asm;

#aarch64 #one_ret_pic
fn mul(a: f64, b: f64) f64 = (
    fmul(FType.D64, x0, x0, x1),
    ret(),
)!asm;

#aarch64 #one_ret_pic
fn div(a: f64, b: f64) f64 = (
    fdiv(FType.D64, x0, x0, x1),
    ret(),
)!asm;

//! IMPORTANT: cond is inverted is backwards because CSINC
fn fcmp_with_cond(inv_cond: Cond) Ty(u32, u32, u32) = (
    fcmp(FType.D64, x0, x1),
    cset(Bits.X64, x0, inv_cond),
    ret(),
);

#aarch64 #one_ret_pic
fn eq(a: f64, b: f64) bool = fcmp_with_cond(Cond.NE)!asm;
#aarch64 #one_ret_pic
fn ne(a: f64, b: f64) bool = fcmp_with_cond(Cond.EQ)!asm;
#aarch64 #one_ret_pic
fn le(a: f64, b: f64) bool = fcmp_with_cond(Cond.GT)!asm;
#aarch64 #one_ret_pic
fn ge(a: f64, b: f64) bool = fcmp_with_cond(Cond.LT)!asm;
#aarch64 #one_ret_pic
fn lt(a: f64, b: f64) bool = fcmp_with_cond(Cond.GE)!asm;
#aarch64 #one_ret_pic
fn gt(a: f64, b: f64) bool = fcmp_with_cond(Cond.LE)!asm;

// TODO: #c_call fn malicious_c_call() Unit;  // stomps all the registers its allowed to. 

#aarch64 #one_ret_pic
fn ptr_to_int(ptr: rawptr) i64 = (0xd65f03c0, 0xd65f03c0)!asm;

#aarch64 #one_ret_pic
fn int_to_ptr(ptr: i64) rawptr = (0xd65f03c0, 0xd65f03c0)!asm;

// preserves the value (not the bit pattern). rounds towards zero. 
#aarch64 #one_ret_pic
fn int(a: f64) i64 = (
    fcvtzs(Bits.X64, FType.D64, x0, x0),
    ret(),
)!asm;

// preserves the value (not the bit pattern). 
#aarch64 #one_ret_pic
fn float(a: i64) f64 = (
    scvtf(Bits.X64, FType.D64, x0, x0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn load(ptr: *u8) u8 = (
    ldrb_uo(x0, x0, @as(u12) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn store(ptr: *u8, value: u8) Unit = (
    strb_uo(x1, x0, @as(u12) 0),
    ret(),
)!asm;

#redirect(*u8, u8) fn load(ptr: *bool) bool;
#redirect(Ty(*u8, u8), Unit) fn store(ptr: *bool, value: bool) Unit;

#one_ret_pic #aarch64
fn load(ptr: *Unit) Unit = (
    ret(),
    ret(),
)!asm;  // TODO: single element tuples in !asm

#one_ret_pic #aarch64
fn store(ptr: *Unit, value: Unit) Unit = (
    ret(),
    ret(),
)!asm;  // TODO: single element tuples in !asm

// preserves the bit pattern (not the value)
#aarch64 #one_ret_pic
fn bitcast(a: f64) i64 = (
    fmov_from(x0, x0),
    ret(),
)!asm;

// preserves the bit pattern (not the value)
#aarch64 #one_ret_pic
fn bitcast(a: i64) f64 = (
    fmov_to(x0, x0),
    ret(),
)!asm;

// TODO: for i32 need sign extend
#one_ret_pic #aarch64
fn load(ptr: *u32) u32 = (
    ldr_uo(Bits.W32, x0, x0, @as(u12) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn store(ptr: *u32, value: u32) Unit = (
    str_uo(Bits.W32, x1, x0, @as(u12) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn load(ptr: *u16) u16 = (
    ldrh_uo(x0, x0, @as(u12) 0),
    ret(),
)!asm;

#one_ret_pic #aarch64
fn store(ptr: *u16, value: u16) Unit = (
    strh_uo(x1, x0, @as(u12) 0),
    ret(),
)!asm;
