//! @bs functions have their bodies translated into a list of opcode integer literals.
//! If you change anything here, you have to `./bootstrap.sh` again.
// TODO: some consistent way of expressing debug mode bounds checks for div, shift, etc.
// TODO: some of the argument names matter because they're hardcoded in emit_rs
/*
const op = @import(lib.codegen.aarch64.instructions);
@import(lib.codegen.aarch64.instructions) const (Bits, Shift);
@import(lib.collections) const (Slice, list, items, push_all);
*/

// TODO fix inline asm
// TODO: copy paste from tests/aarch64_jit but ^
fn lst(ops: Ty(*u32, i64)) = {
    var out: List(u32) = list(ops&[1][]);
    push_all(out!addr, @as(Slice(u32)) ops);
    var out: Slice(u32) = items(out!addr);
    out
}

@bs @c_call
fn add(a: i64, b: i64) i64 = lst((
    add_sr(Bits.X64[], x0, x0, x1, Shift.LSL[], @as(u6) 0),
    ret(),
)!slice)!asm;

@bs @c_call
fn sub(a: i64, b: i64) i64 = lst((
    sub_sr(Bits.X64[], x0, x0, x1, Shift.LSL[], @as(u6) 0),
    ret(),
)!slice)!asm;

@bs @c_call
fn mul(a: i64, b: i64) i64 = lst((
    madd(Bits.X64[], x0, x0, x1, 0b11111),
    ret(),
)!slice)!asm;

@bs @c_call
fn div(a: i64, b: i64) i64 = lst((
    sdiv(Bits.X64[], x0, x0, x1),
    ret(),
)!slice)!asm;

@bs @c_call
fn shift_left(value: i64, shift_amount: i64) i64 = lst((
    lslv(Bits.X64[], x0, x0, x1),
    ret(),
)!slice)!asm;

@bs @c_call
fn bit_or(a: i64, b: i64) i64 = lst((
    orr(Bits.X64[], x0, x0, x1, Shift.LSL[], 0b000000),
    ret(),
)!slice)!asm;

@bs @c_call // TODO: actually test
fn bit_not(a: i64) i64 = lst((
    orn(Bits.X64[], x0, x0, 0b11111, Shift.LSL[], @as(u6) 0),
    ret(),
)!slice)!asm;

@bs @c_call
fn bit_and(a: i64, b: i64) i64 = lst((
    and_sr(Bits.X64[], x0, x0, x1, Shift.LSL[], @as(u6) 0),
    ret(),
)!slice)!asm;

//! IMPORTANT: cond is inverted is backwards because CSINC
fn cmp_with_cond(inv_cond: Cond.T[]) = lst((
    cmp(Bits.X64[], x0, x1),
    cset(Bits.X64[], x0, inv_cond),
    ret(),
)!slice);

@bs @c_call
fn eq(a: i64, b: i64) bool = cmp_with_cond(Cond.NE[])!asm;

@bs @c_call
fn ne(a: i64, b: i64) bool = cmp_with_cond(Cond.EQ[])!asm;

@bs @c_call
fn le(a: i64, b: i64) bool = cmp_with_cond(Cond.GT[])!asm;

@bs @c_call
fn ge(a: i64, b: i64) bool = cmp_with_cond(Cond.LT[])!asm;

@bs @c_call
fn lt(a: i64, b: i64) bool = cmp_with_cond(Cond.GE[])!asm;

@bs @c_call
fn gt(a: i64, b: i64) bool = cmp_with_cond(Cond.LE[])!asm;

@bs @c_call @no_interp 
fn raw_slice(ptr: VoidPtr, first: i64, one_past_last: i64) VoidPtr = lst((
    movz(Bits.X64[], x2, @as(u16) 8, Hw.Left0[]),
    madd(Bits.X64[], x0, x1, x2, x0),
    ret(),
)!slice)!asm;

//////
/// Floats
// TODO: my macro system should be able to make all these binary operators much neater
//       something like @bin_math_op(add, f64, fadd, FType.D64[])
//       There's wierdness around bootstrapping (for ints, prosumably the compiler doesn't need floats),
//       but I do that by precompiling to bit literals anyway so its fine. 
//       Other problem is name resolution, but they can probably just add themselves to the .gen int versions so its fine. 

// TODO: maybe its weird that @bs implies @pub
@c_call @pub @aarch64
fn add(a: f64, b: f64) f64 = lst((
    fadd(FType.D64[], x0, x0, x1),
    ret(),
)!slice)!asm;

@c_call @pub @aarch64
fn sub(a: f64, b: f64) f64 = lst((
    fsub(FType.D64[], x0, x0, x1),
    ret(),
)!slice)!asm;

@c_call @pub @aarch64
fn mul(a: f64, b: f64) f64 = lst((
    fmul(FType.D64[], x0, x0, x1),
    ret(),
)!slice)!asm;

@c_call @pub @aarch64
fn div(a: f64, b: f64) f64 = lst((
    fdiv(FType.D64[], x0, x0, x1),
    ret(),
)!slice)!asm;

// TODO: can't say a return type here because its an ffi Vec<>
fn fcmp_with_cond(inv_cond: Cond.T[]) = lst((
    fcmp(FType.D64[], x0, x1),
    cset(Bits.X64[], x0, inv_cond),
    ret(),
)!slice);

@c_call @pub @aarch64
fn eq(a: f64, b: f64) bool = fcmp_with_cond(Cond.NE[])!asm;
@c_call @pub @aarch64
fn ne(a: f64, b: f64) bool = fcmp_with_cond(Cond.EQ[])!asm;
@c_call @pub @aarch64
fn le(a: f64, b: f64) bool = fcmp_with_cond(Cond.GT[])!asm;
@c_call @pub @aarch64
fn ge(a: f64, b: f64) bool = fcmp_with_cond(Cond.LT[])!asm;
@c_call @pub @aarch64
fn lt(a: f64, b: f64) bool = fcmp_with_cond(Cond.GE[])!asm;
@c_call @pub @aarch64
fn gt(a: f64, b: f64) bool = fcmp_with_cond(Cond.LE[])!asm;

/*
@any_reg
fn sub(a: i64, b: i64) i64 = (
    sub_sr(Bits.X64[], RET!reg, a!reg, b!reg, Shift.LSL[], 0)
)!slice;
*/

// TODO: @c_call fn malicious_c_call() Unit;  // stomps all the registers its allowed to. 
