//! @bs functions have their bodies translated into a list of opcode integer literals.
//! If you change anything here, you have to `./bootstrap.sh` again.
// TODO: some consistent way of expressing debug mode bounds checks for div, shift, etc.
// TODO: some of the argument names matter because they're hardcoded in emit_rs
/*
const op = @import(lib.codegen.aarch64.instructions);
@import(lib.codegen.aarch64.instructions) const (Bits, Shift);
@import(lib.collections) const (Slice, list, items, push_all);*/

// TODO fix inline asm
fn lst(ops: Slice(u32)) Slice(u32) = {
    var out: List(u32) = list(ops.len[]);
    push_all(out!addr, ops);
    var out: Slice(u32) = items(out!addr);
    out
}

@bs @c_call
fn add(a: i64, b: i64) i64 = lst((
    op.add_sr(Bits.X64[], x0, x0, x1, Shift.LSL[], 0),
    op.ret(),
)!slice)!asm;

@bs @c_call
fn sub(a: i64, b: i64) i64 = lst((
    op.sub_sr(Bits.X64[], x0, x0, x1, Shift.LSL[], 0),
    op.ret(),
)!slice)!asm;

@bs @c_call
fn mul(a: i64, b: i64) i64 = lst((
    op.madd(Bits.X64[], x0, x0, x1, 0b11111),
    op.ret(),
)!slice)!asm;

@bs @c_call
fn div(a: i64, b: i64) i64 = lst((
    op.sdiv(Bits.X64[], x0, x0, x1),
    op.ret(),
)!slice)!asm;

@bs @c_call
fn shift_left(value: i64, shift_amount: i64) i64 = lst((
    op.lslv(Bits.X64[], x0, x0, x1),
    op.ret(),
)!slice)!asm;

@bs @c_call
fn bit_or(a: i64, b: i64) i64 = lst((
    op.orr(Bits.X64[], x0, x0, x1, Shift.LSL[], 0),
    op.ret(),
)!slice)!asm;

@bs @c_call // TODO: actually test
fn bit_not(a: i64) i64 = lst((
    op.orn(Bits.X64[], x0, x0, 0b11111, Shift.LSL[], 0),
    op.ret(),
)!slice)!asm;

@bs @c_call
fn bit_and(a: i64, b: i64) i64 = lst((
    op.and_sr(Bits.X64[], x0, x0, x1, Shift.LSL[], 0),
    op.ret(),
)!slice)!asm;

//! IMPORTANT: cond is inverted is backwards because CSINC
fn cmp_with_cond(inv_cond: Cond.T[]) Slice(u32) = lst((
    op.cmp(Bits.X64[], x0, x1),
    op.cset(Bits.X64[], x0, inv_cond),
    op.ret(),
)!slice);

@bs @c_call
fn eq(a: i64, b: i64) bool = cmp_with_cond(Cond.NE[])!asm;

@bs @c_call
fn ne(a: i64, b: i64) bool = cmp_with_cond(Cond.EQ[])!asm;

@bs @c_call
fn le(a: i64, b: i64) bool = cmp_with_cond(Cond.GT[])!asm;

@bs @c_call
fn ge(a: i64, b: i64) bool = cmp_with_cond(Cond.LT[])!asm;

@bs @c_call
fn lt(a: i64, b: i64) bool = cmp_with_cond(Cond.GE[])!asm;

@bs @c_call
fn gt(a: i64, b: i64) bool = cmp_with_cond(Cond.LE[])!asm;

/*
@bs @c_call
fn raw_slice(ptr: VoidPtr, first: i64, one_past_last: i64) VoidPtr = lst((
    op.movz(Bits.X64[], x2, 8, 0),
    op.madd(Bits.X64[], x0, x1, x2, x0),
    op.ret(),
)!slice)!asm;
*/

const OpPtr = VoidPtr;
const YieldArg: Type = (OpPtr, u32);
const RetOp = FnPtr(YieldArg, Unit);

/*
@any_reg
fn sub(a: i64, b: i64) i64 = (
    op.sub_sr(Bits.X64[], RET!reg, a!reg, b!reg, Shift.LSL[], 0)
)!slice;
*/

// TODO: @c_call fn malicious_c_call() Unit;  // stomps all the registers its allowed to. 
