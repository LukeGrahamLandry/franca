//! #bs functions have their bodies translated into a list of opcode integer literals.
//! If you change anything here, you have to `./bootstrap.sh` again.
//! #one_ret_pic SAFETY: one return at the end (or HACK), c abi, no relative jumps outside the function. 
//! - means it can be done inline by just chopping off the ret at the end. makes it a bit less insane to use fns for add, etc. 
//! - always inlines, trusts you that its small enough to be worth it. 
//! - still really dumb rn cause i do redundant stack loads/stores. 
// TODO: some consistent way of expressing debug mode bounds checks for div, shift, etc.
// TODO: some of the argument names matter because they're hardcoded in emit_rs

// TODO fix inline asm
// TODO: copy paste from tests/aarch64_jit but ^
fn lst(var ops: Ty(*u32, i64)) = {
    var out: List(u32) = list(ops&[1][]);
    push_all(out!addr, @as(Slice(u32)) ops);
    var out: Slice(u32) = items(out!addr);
    out
}

#bs #c_call #one_ret_pic
fn add(a: i64, b: i64) i64 = lst((
    add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic
fn sub(a: i64, b: i64) i64 = lst((
    sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic
fn mul(a: i64, b: i64) i64 = lst((
    madd(Bits.X64, x0, x0, x1, 0b11111),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic
fn div(a: i64, b: i64) i64 = lst((
    sdiv(Bits.X64, x0, x0, x1),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic
fn shift_left(value: i64, shift_amount: i64) i64 = lst((
    lslv(Bits.X64, x0, x0, x1),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic
fn bit_or(a: i64, b: i64) i64 = lst((
    orr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic // TODO: actually test
fn bit_not(a: i64) i64 = lst((
    orn(Bits.X64, x0, x0, 0b11111, Shift.LSL, @as(u6) 0),
    ret(),
)!slice)!asm;

#bs #c_call #one_ret_pic
fn bit_and(a: i64, b: i64) i64 = lst((
    and_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!slice)!asm;

//! IMPORTANT: cond is inverted is backwards because CSINC
fn cmp_with_cond(inv_cond: Cond) = lst((
    cmp(Bits.X64, x0, x1),
    cset(Bits.X64, x0, inv_cond),
    ret(),
)!slice);

#bs #c_call #one_ret_pic
fun eq(a: i64, b: i64) bool = cmp_with_cond(Cond.NE)!asm;

#bs #c_call #one_ret_pic
fn ne(a: i64, b: i64) bool = cmp_with_cond(Cond.EQ)!asm;

#bs #c_call #one_ret_pic
fn le(a: i64, b: i64) bool = cmp_with_cond(Cond.GT)!asm;

#bs #c_call #one_ret_pic
fn ge(a: i64, b: i64) bool = cmp_with_cond(Cond.LT)!asm;

#bs #c_call #one_ret_pic
fn lt(a: i64, b: i64) bool = cmp_with_cond(Cond.GE)!asm;

#bs #c_call #one_ret_pic
fn gt(a: i64, b: i64) bool = cmp_with_cond(Cond.LE)!asm;

// offsets measured in bytes. 
#bs #c_call  #one_ret_pic
fun raw_slice(ptr: rawptr, first: i64, one_past_last: i64) rawptr = lst((
    add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
)!slice)!asm;

//////
/// Floats
// TODO: my macro system should be able to make all these binary operators much neater
//       something like @bin_math_op(add, f64, fadd, FType.D64)
//       There's wierdness around bootstrapping (for ints, prosumably the compiler doesn't need floats),
//       but I do that by precompiling to bit literals anyway so its fine. 
//       Other problem is name resolution, but they can probably just add themselves to the .gen int versions so its fine. 

// TODO: maybe its weird that #bs implies @pub
#c_call #aarch64 #one_ret_pic
fn add(a: f64, b: f64) f64 = lst((
    fadd(FType.D64, x0, x0, x1),
    ret(),
)!slice)!asm;

#c_call #aarch64 #one_ret_pic
fn sub(a: f64, b: f64) f64 = lst((
    fsub(FType.D64, x0, x0, x1),
    ret(),
)!slice)!asm;

#c_call #aarch64 #one_ret_pic
fn mul(a: f64, b: f64) f64 = lst((
    fmul(FType.D64, x0, x0, x1),
    ret(),
)!slice)!asm;

#c_call #aarch64 #one_ret_pic
fn div(a: f64, b: f64) f64 = lst((
    fdiv(FType.D64, x0, x0, x1),
    ret(),
)!slice)!asm;

// TODO: can't say a return type here because its an ffi Vec<>
fn fcmp_with_cond(inv_cond: Cond) = lst((
    fcmp(FType.D64, x0, x1),
    cset(Bits.X64, x0, inv_cond),
    ret(),
)!slice);

#c_call #aarch64 #one_ret_pic
fun eq(a: f64, b: f64) bool = fcmp_with_cond(Cond.NE)!asm;
#c_call #aarch64 #one_ret_pic
fn ne(a: f64, b: f64) bool = fcmp_with_cond(Cond.EQ)!asm;
#c_call #aarch64 #one_ret_pic
fn le(a: f64, b: f64) bool = fcmp_with_cond(Cond.GT)!asm;
#c_call #aarch64 #one_ret_pic
fn ge(a: f64, b: f64) bool = fcmp_with_cond(Cond.LT)!asm;
#c_call #aarch64 #one_ret_pic
fn lt(a: f64, b: f64) bool = fcmp_with_cond(Cond.GE)!asm;
#c_call #aarch64 #one_ret_pic
fn gt(a: f64, b: f64) bool = fcmp_with_cond(Cond.LE)!asm;

// TODO: #c_call fn malicious_c_call() Unit;  // stomps all the registers its allowed to. 

#aarch64 #c_call #one_ret_pic
fn ptr_to_int(ptr: rawptr) i64 = (0xd65f03c0, 0xd65f03c0)!asm;

#aarch64 #c_call #one_ret_pic
fn int_to_ptr(ptr: i64) rawptr = (0xd65f03c0, 0xd65f03c0)!asm;

// preserves the value (not the bit pattern). rounds towards zero. 
#c_call #aarch64 #one_ret_pic
fun int(a: f64) i64 = lst((
    fcvtzs(Bits.X64, FType.D64, x0, x0),
    ret(),
)!slice)!asm;

#c_call #one_ret_pic
fn load(ptr: *u8) u8 = lst((
    ldrb_uo(x0, x0, @as(u12) 0),
    ret(),
)!slice)!asm;

#c_call #one_ret_pic
fn store(ptr: *u8, value: u8) Unit = lst((
    strb_uo(x1, x0, @as(u12) 0),
    ret(),
)!slice)!asm;
