
// Calling Slice.copy_from is easier
// Importing this is just so dumb cause like what does your operating system know that I don't? We're all just shuffling bytes around.  

fn copy_no_alias(dest: rawptr, src: rawptr, len: i64) void = {
    copy_overlapping(dest, src, len);  // im not convinced memcpy is that much faster that we should create extra bugs. 
}

// TODO: im hoping the amount slower that my shitty mem___ are than the fancy libc ones
//       will incentivise me to rethink how TargetSplit configuration works. 
//       so maybe try to refrain from turning them back on until there's a system that isn't embarrassing to explain. 
//       :SLOW -- Apr 14, 2025
// TODO: is alignment real?

fn copy_bytes(dest: rawptr, src: rawptr, len: i64) void = {
    //@if(is_linking_libc(), {
    //    memmove :: fn(dest: rawptr, source: rawptr, num: i64) rawptr #libc;
    //    memmove(dest, src, len);
    //}, {
        copy_bytes_static(dest, src, len);
    //});
}

fn copy_overlapping(dest: rawptr, src: rawptr, len: i64) void = 
    copy_bytes(dest, src, len);

// note: backend/opt/simplify.fr pastes the ir of this function for large blits when not linking libc. 
fn copy_bytes_static(dest: rawptr, src: rawptr, count: i64) void = {
    impl :: fn(dest: rawptr, src: rawptr, count: i64, $T: Type, $delta: i64) void #inline = {
        src  := T.ptr_from_raw(src); 
        dest := T.ptr_from_raw(dest); 
        @if(delta == -1) {
            src = src.offset(count-1);
            dest = dest.offset(count-1);
        };
        
        @if(count == 0) return();
        end := src.offset(delta*count);
        dowhile {
            dest[] = src[];
            dest = dest.offset(delta); 
            src = src.offset(delta);
            !identical(src, end)
        };
    };
    big := count.shift_right_logical(3);
    dist := ptr_diff(src, dest);
    
    
    // :dontforgetslowcopy
    // TODO: get rid of this. :SLOW. problem is you can't do unaligned access before the mmu is enabled
    if dist > 0 && dist < count { 
        impl(dest, src, count, u8, -1);
    } else {
        impl(dest, src, count, u8, 1);
    };
    return();
    
    if dist > 0 && dist < count { 
        impl(dest.offset(big * 8), src.offset(big * 8), count.bit_and(7), u8, -1);
        impl(dest, src, big, i64, -1);
    } else {
        impl(dest, src, big, i64, 1);
        impl(dest.offset(big * 8), src.offset(big * 8), count.bit_and(7), u8, 1);
    }
}

fn set_bytes(mem: []u8, to: u8) void = {
    //@if(is_linking_libc(), {
    //    memset :: fn(destination: rawptr, value: u8, count: i64) rawptr #libc;
    //    memset(u8.raw_from_ptr(mem.ptr), to, mem.len);
    //}, {
        expand :: fn(a: u8) i64 = {
            a: i64 = a.zext();
            a = a.bit_or(a.shift_left(8));
            a = a.bit_or(a.shift_left(16));
            a.bit_or(a.shift_left(32))
        };
        
        // Set 8 bytes at a time and then set the rest. 
        big := mem.len.shift_right_logical(3);
        big:  []i64 = (ptr = ptr_cast_unchecked(u8, i64, mem.ptr), len = big);
        
        to2 := expand(to);
        each big { b |
            b[] = to2;
        };
        if mem.len.bit_and(7) != 0 {
            small: []u8 = mem.rest(big.len * 8);
            each small { b |
                b[] = to;
            };
        };
    //});
}


fn ptr_diff(start: rawptr, end: rawptr) i64 = 
    end.int_from_rawptr() - start.int_from_rawptr();

fn offset(ptr: rawptr, bytes: i64) rawptr #ir(.add, .Kl);

fn identical(a: rawptr, b: rawptr) bool #ir(.ceql, .Kl);

// TODO: put this in fn Ptr so you always get them? 
fn ptr_utils($T: Type) void = {
    fn ptr_diff_bytes(start: *T, end: *T) i64 = sub(@as(i64) T.int_from_ptr(end), T.int_from_ptr(start));
    fn offset(ptr: *T, element_count: i64) *T #inline = 
        ptr.offset_bytes(element_count * T.size_of());
        
    fn offset_bytes(ptr: *T, bytes: i64) *T #ir(.add, .Kl);
    fn ptr_diff(start: *T, end: *T) i64 = 
        ptr_diff_bytes(start, end).div(::T.size_of());
    
    fn identical(a: *T, b: *T) bool #ir(.ceql, .Kl);
    
    fn for(first: *T, past_last: *T, $body: @Fn(it: *T) void) void = {
        end := T.int_from_ptr(past_last);
        while(=> end.ne(T.int_from_ptr(first))) {
            body(first);
            first = first.offset(1);
        }
    }
    
    // TODO: replace with niche ?*T
    fn is_null(ptr: *T) bool = 0.eq(T.int_from_ptr(ptr));
    
    fn in_memory_after(after: *T, before: *T) bool #ir(.cugtl, .Kl);
        
    fn between(first: *T, past_last: *T) []T = 
        (ptr = first, len = ptr_diff(first, past_last))
}

// TODO: replace with niche ?rawptr
fn is_null(ptr: rawptr) bool #inline = 
    ptr.int_from_rawptr().eq(0);

fn eq(a: rawptr, b: rawptr) bool #ir(.ceql, .Kl);

fn ne(a: rawptr, b: rawptr) bool #ir(.cnel, .Kl);

fn ptr_diff(start: rawptr, end: *u8) i64 =
    ptr_diff(start, u8.raw_from_ptr(end));

fn ptr_diff(start: *u8, end: rawptr) i64 =
    ptr_diff(u8.raw_from_ptr(start), end);

fn align_to(offset: rawptr, align: i64) rawptr = 
    offset.int_from_rawptr().align_to(align).rawptr_from_int();

fn align_back(offset: rawptr, align: i64) rawptr = 
    offset.int_from_rawptr().align_back(align).rawptr_from_int();

// TODO: make a version of this that checks alignment
/// This function does nothing, it just lies to the typechecker. 
/// It's a bit clunky to type but maybe that's good for morale since its wildly unsafe!
/// SAFETY: the pointer must be aligned correctly for the new type and valid for use as the new size. 
ptr_cast_unchecked :: fn($From: Type, $To: Type, ptr: *From) *To #generic #ir(.copy, .Kl);

raw_from_ptr :: fn($From: Type, ptr: *From) rawptr #generic #ir(.copy, .Kl);
ptr_from_raw :: fn($To: Type, ptr: rawptr) *To #generic #ir(.copy, .Kl);

int_from_ptr :: fn($From: Type, ptr: *From) i64 #generic #ir(.copy, .Kl);
ptr_from_int :: fn($To: Type, ptr: i64) *To #generic #ir(.copy, .Kl);

// TODO: most of the time what you want to do with these is replace the first argument with a rawptr or reverse that. 
//       should provide convience functions for that. 
erase_types_fn :: fn($Arg: Type, $Ret: Type, ptr: FnPtr(Arg, Ret)) rawptr #generic #ir(.copy, .Kl);
assume_types_fn :: fn($Arg: Type, $Ret: Type, ptr: rawptr) FnPtr(Arg, Ret) #generic #ir(.copy, .Kl);

fn zeroed($T: Type) T #generic #inline = {
    @zeroed T
}

fn zeroed(T: FatExpr) FatExpr #macro = {
    @if(IS_BOOTSTRAPPING) return(@{:: {
        t := @uninitialized @[T];
        set_zeroed(@as([]@[T]) (ptr = t&, len = 1));
        t
    }});

    loc := T.loc;
    T := Type.const_eval(T);
    
    size := T.size_of();
    value: Values = @if(size <= 8, 
        (Small = (0, size.trunc())),
        (Big = {
            // note: alloc_raw calls empty() which calls zeroed()
            allocator := ast_alloc();
            s := {allocator.vptr}(allocator.data, .Allocate, rawptr_from_int(0), size, 8/*todo*/);
            s.set_bytes(0);
            s.as_raw_list()
        }));
    expr: FatExpr = (expr = (Value = (bytes = value)), loc = loc, done = true, ty = T);
    expr
};

// Signs aren't real. You just use different operations depending on the type. 
fn bitcast(v: u64) i64 #ir(.copy, .Kl);
fn bitcast(v: i64) u64 #ir(.copy, .Kl);

fn bitcast(a: u32) f32 #ir(.cast, .Ks);
fn bitcast(a: f32) u32 #ir(.cast, .Kw);

fn bitcast(x: i32) u32 #ir(.copy, .Kw);
fn bitcast(x: u32) i32 #ir(.copy, .Kw);

cast_to_bytes :: fn($T: Type, self: *T) Slice(u8) #generic = {
    ptr := ptr_cast_unchecked(T, u8, self);
    slice(ptr, :: T.size_of())
};

bit_cast_unchecked :: fn($From: Type, $To: Type, v: From) To #generic #inline = {
    ptr_cast_unchecked(From, To, v&)[]
};

reinterpret_bytes :: fn($T: Type, bytes: []u8) []T #generic = 
    (ptr = ptr_cast_unchecked(u8, T, bytes.ptr), len = bytes.len / size_of(T));
