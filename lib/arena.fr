ArenaAlloc :: @struct(
    parent: Alloc,
    chunk: ?*ArenaHeader,
    start: i64,
    end: i64,
    base_size: i64,
);

ArenaHeader :: @rec @struct(
    size: i64,
    prev: ?*ArenaHeader,
    next: ?*ArenaHeader,
);

fn init(parent: Alloc, base_size: i64) ArenaAlloc = {
    (parent = parent, chunk = .None, start = 0, end = 0, base_size = base_size)
}

fn new_arena_chunk(allocator: Alloc, size: i64, prev: ?*ArenaHeader) *ArenaHeader = {
    buf := allocator.alloc(u8, size.max(ArenaHeader.size_of())); // TODO: align
    chunk := ptr_cast_unchecked(From = u8, To = ArenaHeader, ptr = buf.ptr);
    chunk[] = (size = buf.len, prev = prev, next = .None);
    chunk
}

arena_allocator_fn :: fn(self: rawptr, action: AllocatorAction, ptr: rawptr, count: i64, align: i64) rawptr = {
    // note: we can't use @match yet... (because of printing i guess?)
    action := @as(i64) action;
    if action == 0 {
        arena_alloc(self, count, align)
    } else {
        // Noop
        rawptr.uninitialized()
    }
};

// Don't try to printf debug in here...
// TODO: make this less derranged. 
arena_alloc :: fn(raw_self: rawptr, size: i64, log_align: i64) rawptr = {
    self := ArenaAlloc.ptr_from_raw(raw_self);
    
    chunk := self.chunk.or() {
        next := new_arena_chunk(self.parent, self.base_size, .None);
        self.start = ArenaHeader.size_of();
        self.end = next.size;
        self.chunk = (Some = next);
        next
    };
    chunk := self.chunk.unwrap();
    
    end  := self.start.add(size);
    extra := end.mod(8);
    if extra != 0 {
        end += extra;
    };
    
    if(end.le(self.end)) {
        ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
        mem := ptr.offset(self.start);
        self.start = end;
        return(u8.raw_from_ptr(mem));
    };
    
    if(chunk.next) {(next) void|
        self.chunk = (Some = next);
        self.start = ArenaHeader.size_of();
        self.end = next.size;
        return(arena_alloc(raw_self, size, log_align));
    };
    
    next_size := size.add(8).max(chunk.size.mul(2));
    new := new_arena_chunk(self.parent, next_size, (Some = chunk));
    chunk.next = (Some = new);
    self.chunk = (Some = new);
    self.start = ArenaHeader.size_of();
    self.end = new.size;
    
    arena_alloc(raw_self, size, log_align)
};

ArenaMarker :: @struct(chunk: ?*ArenaHeader, start: i64);
// TODO: allow taking a mark when chunk is None (before first allocation), even tho thats dumb.  :panic
fn mark(self: *ArenaAlloc) ArenaMarker = (chunk = self.chunk, start = self.start);

// remember, you can't do this if you acceppted an output allocator as a parameter because they might have passed you the temp allocator. 
// # Safety: This invalidates any allocations in self before the corresponding call to mark. 
// Memory is NOT returned to the backing allocator. 
// TODO: safety check mode that writes over with garbage. 
fn reset_retaining_capacity(self: *ArenaAlloc, mark: ArenaMarker) void = {
    if mark.chunk.is_none() {
        self.reset_retaining_capacity();
        return();  
    };
    chunk := mark.chunk.unwrap();
    self.start = mark.start;
    self.end = chunk.size;
    self.chunk = (Some = chunk);
}

// # Safety: This invalidates all allocations in self!
// Memory is NOT returned to the backing allocator. 
fn reset_retaining_capacity(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        while(=> chunk.prev.is_some()) {
            chunk = chunk.prev.unwrap();
        };
        self.start = ArenaHeader.size_of();
        self.end = chunk.size;
        self.chunk = (Some = chunk);
    }
}

// Some memory is returned to the backing allocator. 
// The original capacity is retained and the arena can be reused. 
// # Safety
//     - This invalidates all allocations in self!
fn reset_retaining_original(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        while(=> chunk.prev.is_some()) {
            prev := chunk.prev.unwrap();
            ptr  := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf  := slice(ptr, chunk.size);
            self.parent.dealloc(u8, buf);
            chunk = prev;
        };
        self.chunk = (Some = chunk);
        self.start = ArenaHeader.size_of();
        self.end = chunk.size;
    };
}

// Memory is returned to the backing allocator. 
// # Safety
//     - This invalidates all allocations in self!
fn deinit(self: *ArenaAlloc) void = {
    ::if_opt(*ArenaHeader, bool);
    if self.chunk { chunk |
        loop {
            prev := chunk.prev;
            
            ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf := slice(ptr, chunk.size);
            addr := u8.int_from_ptr(ptr);
            self.parent.dealloc(u8, buf);
            
            @match(prev) {
                fn Some(prev) => {
                    chunk = prev;
                }
                fn None() => {
                    self.chunk = .None;
                    self.start = 0;
                    self.end = 0;
                    return();
                }
            };
        };
    };
    
}

fn current_allocated_size(self: *ArenaAlloc) i64 = {
    size := 0
    if self.chunk { chunk |
        // go all the way back then all the way forward because we might not be at the latest chunk. 
        while(=> chunk.prev.is_some()) {
            chunk = chunk.prev.unwrap();
        };
        
        while(=> chunk.next.is_some()) {
            size += chunk.size;
            chunk = chunk.next.unwrap();
        };
        size += chunk.size;
    };
    size
}

fn borrow(self: *ArenaAlloc) Alloc = {
    self := ArenaAlloc.raw_from_ptr(self);
    (data = self, vptr = arena_allocator_fn)
}

fn bake_relocatable_value(self: *ArenaAlloc) Slice(BakedEntry) = {
    empty: ArenaAlloc = init(self.parent, self.base_size);
    bytes := ArenaAlloc.cast_to_bytes(empty&);
    entries := dyn_bake_relocatable_value(bytes, ArenaAlloc, true);
    entries
}

thread_local :: static; // TODO

__temp_alloc :: @thread_local(ArenaAlloc);
:: {
    // TODO: crippling problem with this type of seperate init, comptime code before us can reference the variable 
    //       and cause its declaration early but that won't make this code run early.  
    __temp_alloc[] = init(page_allocator, 1.shift_left(20));
};

// TODO: have a Alloc.owns_this_memory(ptr) and then can use that as a debug check that output allocator isn't the temp allocator for things that want to consider themselves leaves and reset temp to a mark when they're done. -- Jul 5
fn temp() Alloc = __temp_alloc.borrow();
fn reset_temporary_storage() void = {
    __temp_alloc.reset_retaining_capacity();
}

todo_allocator: Alloc : (data = 0.rawptr_from_int(), vptr = todo_allocator_fn);

// TODO: respect alignment. but im pretty sure malloc always gives you 16 bits and mostly good enough. 
todo_allocator_fn :: fn(_: rawptr, action: AllocatorAction, ptr: rawptr, count: i64, align: i64) rawptr = { 
    action := @as(i64) action;
    ::if(rawptr);
    // Note: we can't use @match yet! because this is comptime ast_alloc rn
    if action == 0 {
        malloc(count)
    } else {
        rawptr.uninitialized()
    }
};
