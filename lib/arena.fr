ArenaAlloc :: @struct(
    parent: Alloc,
    chunk: ?*ArenaHeader,
    start: i64,
    end: i64,
    base_size: i64,
);

ArenaHeader :: @rec @struct(
    size: i64,
    prev: ?*ArenaHeader,
    next: ?*ArenaHeader,
);

fn init(parent: Alloc, base_size: i64) ArenaAlloc = {
    (parent = parent, chunk = .None, start = 0, end = 0, base_size = base_size)
}

fn new_arena_chunk(allocator: Alloc, size: i64, prev: ?*ArenaHeader) *ArenaHeader = {
    buf := allocator.alloc(u8, size.max(ArenaHeader.size_of())); // TODO: align
    chunk := ptr_cast_unchecked(From = u8, To = ArenaHeader, ptr = buf.ptr);
    chunk[] = (size = buf.len, prev = prev, next = .None);
    chunk
}

arena_vtable :: @static(AllocVTable);
:: {
    arena_vtable[] = (alloc = arena_alloc, free = arena_free);
    
    // Don't try to printf debug in here...
    arena_alloc :: fn(raw_self: rawptr, size: i64, log_align: u8) rawptr = {
        self := ArenaAlloc.ptr_from_raw(raw_self);
        
        chunk := self.chunk.or() {|
            next := new_arena_chunk(self.parent, self.base_size, .None);
            self.start = ArenaHeader.size_of();
            self.end = next.size;
            self.chunk = (Some = next);
            next
        };
        chunk := self.chunk.unwrap();
        
        end  := self.start.add(size);
        extra := end.mod(8);
        if extra != 0 {|
            end += extra;
        };
        
        if(end.le(self.end)) {|
            ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            mem := ptr.offset(self.start);
            self.start = end;
            return(u8.raw_from_ptr(mem));
        };
        
        if(chunk.next) {(next) void|
            self.chunk = (Some = next);
            self.start = ArenaHeader.size_of();
            self.end = next.size;
            return(arena_alloc(raw_self, size, log_align));
        };
        
        next_size := size.add(8).max(chunk.size.mul(2));
        new := new_arena_chunk(self.parent, next_size, (Some = chunk));
        chunk.next = (Some = new);
        self.chunk = (Some = new);
        self.start = ArenaHeader.size_of();
        self.end = new.size;
        
        arena_alloc(raw_self, size, log_align)
    };
    arena_free :: fn(_: rawptr, _: Slice(u8), _: u8) void = {
        // No-op
    };
};

ArenaMarker :: @struct(chunk: *ArenaHeader, start: i64);
fn mark(self: *ArenaAlloc) ArenaMarker = (chunk = self.chunk, start = self.start);

// remember, you can't do this if you acceppted an output allocator as a parameter because they might have passed you the temp allocator. 
// # Safety: This invalidates any allocations in self before the corresponding call to mark. 
// Memory is NOT returned to the backing allocator. 
//fn reset_retaining_capacity(self: *ArenaAlloc, mark: ArenaMarker) void = {
//    todo();
//}

// # Safety: This invalidates all allocations in self!
// Memory is NOT returned to the backing allocator. 
fn reset_retaining_capacity(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        while(=> chunk.prev.is_some()) {|
            chunk = chunk.prev.unwrap();
        };
        self.start = ArenaHeader.size_of();
        self.end = chunk.size;
        self.chunk = (Some = chunk);
    }
}

// Some memory is returned to the backing allocator. 
// The original capacity is retained and the arena can be reused. 
// # Safety
//     - This invalidates all allocations in self!
fn reset_retaining_original(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        while(=> chunk.prev.is_some()) {|
            prev := chunk.prev.unwrap();
            ptr  := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf  := slice(ptr, chunk.size);
            self.parent.dealloc(u8, buf);
            chunk = prev;
        };
        self.chunk = (Some = chunk);
    }
    self.start = ArenaHeader.size_of();
    self.end = self.chunk.size;
}

// Memory is returned to the backing allocator. 
// # Safety
//     - This invalidates all allocations in self!
fn deinit(self: *ArenaAlloc) void = {
    ::if_opt(*ArenaHeader, bool);
    if self.chunk { chunk |
        dowhile {|
            prev := chunk.prev;
            
            ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf := slice(ptr, chunk.size);
            self.parent.dealloc(u8, buf);
            
            if(prev) {(prev)|
                chunk = prev;
                true
            }{| false }
        };
    };
    self.chunk = .None;
    self.start = 0;
    self.end = 0;
    
}

fn borrow(self: *ArenaAlloc) Alloc = {
    self := ArenaAlloc.raw_from_ptr(self);
    (data = self, vptr = arena_vtable)
}

fn bake_relocatable_value(self: *ArenaAlloc) Slice(BakedEntry) = {
    empty: ArenaAlloc = init(self.parent, self.base_size);
    bytes := ArenaAlloc.cast_to_bytes(empty&);
    entries := dyn_bake_relocatable_value(bytes, ArenaAlloc, true);
    entries
}

thread_local :: static; // TODO

__temp_alloc :: @thread_local(ArenaAlloc);
:: {
    // TODO: crippling problem with this type of seperate init, comptime code before us can reference the variable 
    //       and cause its declaration early but that won't make this code run early.  
    __temp_alloc[] = init(page_allocator, 1.shift_left(15)); 
};

// TODO: have a Alloc.owns_this_memory(ptr) and then can use that as a debug check that output allocator isn't the temp allocator for things that want to consider themselves leaves and reset temp to a mark when they're done. -- Jul 5
fn temp() Alloc = __temp_alloc.borrow();
fn reset_temporary_storage() void = {
    __temp_alloc.reset_retaining_capacity();
}

todo_allocator: Alloc : (data = 0.rawptr_from_int(), vptr = @static(AllocVTable) {
    libc_alloc :: fn(_: rawptr, size: i64, log_align: u8) rawptr = malloc(size);
    leak :: fn(_: rawptr, buf: Slice(u8), log_align: u8) void = ();
    (alloc = libc_alloc, free = leak)
});
