ArenaAlloc :: @struct(
    parent: Alloc,
    chunk: ?*ArenaHeader,
    start: i64,
    end: i64,
    base_size: i64,
    making_new_chunk: bool,
);

ArenaHeader :: @rec @struct(
    size: i64,
    used := 0, // for debugging but should use in arena_owns as well for more precision. probably worth the 8 bytes, especially since we're wasting 16 on tags anyway. 
    prev: ?*ArenaHeader,
    next: ?*ArenaHeader,
);

fn init(parent: Alloc, base_size: i64) ArenaAlloc #no_context = {
    (parent = parent, chunk = .None, start = 0, end = 0, base_size = base_size, making_new_chunk = false)
}

fn new_arena_chunk(allocator: Alloc, size: i64, prev: ?*ArenaHeader, making_new_chunk: *bool) *ArenaHeader = {
    if making_new_chunk[] {
        scary_log("recursing in new_arena_chunk");
        abort();
    };
    making_new_chunk[] = true;
    buf := allocator.alloc(u8, size.max(ArenaHeader.size_of())); // TODO: align
    making_new_chunk[] = false;
    chunk := ptr_cast_unchecked(From = u8, To = ArenaHeader, ptr = buf.ptr);
    chunk[] = (size = buf.len, prev = prev, next = .None);
    chunk
}

ENABLE_ASK_OWNERSHIP :: false;

arena_allocator_fn :: fn(self: rawptr, action: AllocatorAction, ptr: rawptr, count: i64, align: i64) rawptr = {
    // note: we can't use @match yet... (because of printing i guess?) // TODO: check again now that ast_alloc is in the compiler
    if action == AllocatorAction.Allocate {
        return(arena_alloc(self, count, align));
    };
    @if(ENABLE_ASK_OWNERSHIP) {
        if action == AllocatorAction.DoYouOwnThisPointer {
            return(arena_owns(self, ptr).to_raw());
        };
    };
    
    // Free is Noop
    rawptr.zeroed()
};

// TODO: test this
// TODO: edge case when it was in the current chunk but then freed?
fn arena_owns(self: rawptr, ptr: rawptr) MemoryOwnership #cold = {
    self := ArenaAlloc.ptr_from_raw(self);
    ptr := int_from_rawptr(ptr);
    chunk := or self.chunk {
        return(MemoryOwnership.No)
    };
    
    dowhile {
        start := ArenaHeader.int_from_ptr(chunk);
        end := start + chunk.size;
        if ptr >= start && ptr <= end {
            return(MemoryOwnership.Yes);
        };
        // Can't use @match here!
        if chunk.prev.is_some() {
            chunk = chunk.prev.unwrap();
            true
        } else {
            false
        }   
    };
    dowhile {
        start := ArenaHeader.int_from_ptr(chunk);
        end := start + chunk.size;
        if ptr >= start && ptr <= end {
            return(MemoryOwnership.Free);
        };
        // Can't use @match here!
        if chunk.next.is_some() {
            chunk = chunk.next.unwrap();
            true
        } else {
            false
        }   
    };
    MemoryOwnership.No
}

// Don't try to printf debug in here...
// TODO: make this less derranged. 
arena_alloc :: fn(raw_self: rawptr, size: i64, log_align: i64) rawptr = {
    self := ArenaAlloc.ptr_from_raw(raw_self);
    
    chunk := self.chunk.or() {
        next := new_arena_chunk(self.parent, self.base_size, .None, self.making_new_chunk&);
        self.start = ArenaHeader.size_of();
        self.end = next.size;
        self.chunk = (Some = next);
        next
    };
    chunk := self.chunk.unwrap();
    
    end  := self.start.add(size);
    extra := self.start.mod(8); // TODO: use log_align
    if extra != 0 {
        end += 8 - extra;
        self.start += 8 - extra;
    };
    
    if(end.le(self.end)) {
        ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
        mem := ptr.offset(self.start);
        self.start = end;
        return(u8.raw_from_ptr(mem));
    };
    
    if(chunk.next) {(next) void|
        chunk.used = self.start;
        self.chunk = (Some = next);
        self.start = ArenaHeader.size_of();
        self.end = next.size;
        return(arena_alloc(raw_self, size, log_align));
    };
    
    @debug_assert(size < MAX_ARENA_BUCKET, "allocating so many bytes bro");
    MAX_ARENA_BUCKET :: 1.shift_left(26);
    next_size := size.add(8).max(chunk.size.mul(2)).min(MAX_ARENA_BUCKET);
    new := new_arena_chunk(self.parent, next_size, (Some = chunk), self.making_new_chunk&);
    chunk.next = (Some = new);
    self.chunk = (Some = new);
    self.start = ArenaHeader.size_of();
    self.end = new.size;
    
    arena_alloc(raw_self, size, log_align)
};

ArenaMarker :: @struct(chunk: ?*ArenaHeader, start: i64);
// TODO: allow taking a mark when chunk is None (before first allocation), even tho thats dumb.  :panic
fn mark(self: *ArenaAlloc) ArenaMarker #inline = (chunk = self.chunk, start = self.start);

// remember, you can't do this if you acceppted an output allocator as a parameter because they might have passed you the temp allocator. 
// # Safety: This invalidates any allocations in self before the corresponding call to mark. 
// Memory is NOT returned to the backing allocator. 
// TODO: safety check mode that writes over with garbage. 
fn reset_retaining_capacity(self: *ArenaAlloc, mark: ArenaMarker) void = {
    //self.debug_stomp_after(mark);
    if mark.chunk.is_none() {
        self.reset_retaining_capacity();
        return();  
    };
    chunk := mark.chunk.unwrap();
    self.start = mark.start;
    self.end = chunk.size;
    self.chunk = (Some = chunk);
    chunk.used = self.start;
}

// set all the memory after a mark to 0xABABABA in the hopes of crashing you if you try to use it accidentally.
// you can call this at the top of reset_retaining_capacity to reveal use-after-reset bugs more quickly,
// at the cost of some performance. this should not change the behaviour of correct programs. 
fn debug_stomp_after(self: *ArenaAlloc, mark: ArenaMarker) void = {
    JUNK :: 0xAB;
    stomp_all :: fn(chunk) => stomp_some(chunk, ArenaHeader.size_of());
    stomp_some :: fn(chunk, clear_after) => {
        clear_after = max(clear_after, ArenaHeader.size_of());
        if chunk.used > clear_after {
            memset(ArenaHeader.raw_from_ptr(chunk).offset(clear_after), JUNK, chunk.used - clear_after);
            chunk.used = clear_after;
        };
    };
    if mark.chunk { chunk | 
        ::ptr_utils(ArenaHeader);
        if chunk.identical(self.chunk.unwrap()) {
            chunk.used = self.start;
        };
        stomp_some(chunk, mark.start);
        while => chunk.next { c | 
            chunk = c;
            stomp_all(chunk);
        };
    } else {
        if self.chunk { chunk | 
            chunk.used = self.start;
            stomp_all(chunk);
        };
    }
};

// # Safety: This invalidates all allocations in self!
// Memory is NOT returned to the backing allocator. 
fn reset_retaining_capacity(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        while => chunk.prev { c | 
            chunk = c;
            chunk.used = ArenaHeader.size_of();
        };
        self.start = ArenaHeader.size_of();
        self.end = chunk.size;
        self.chunk = (Some = chunk);
        chunk.used = self.start;
    }
}

// Some memory is returned to the backing allocator. 
// The original capacity is retained and the arena can be reused. 
// # Safety
//     - This invalidates all allocations in self!
fn reset_retaining_original(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        while(=> chunk.prev.is_some()) {
            prev := chunk.prev.unwrap();
            ptr  := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf  := slice(ptr, chunk.size);
            self.parent.dealloc(u8, buf);
            chunk = prev;
        };
        self.chunk = (Some = chunk);
        self.start = ArenaHeader.size_of();
        self.end = chunk.size;
    };
}

// Memory is returned to the backing allocator. 
// # Safety
//     - This invalidates all allocations in self!
fn deinit(self: *ArenaAlloc) void = {
    ::if_opt(*ArenaHeader, bool);
    if self.chunk { chunk |
        loop {
            prev := chunk.prev;
            
            ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf := slice(ptr, chunk.size);
            addr := u8.int_from_ptr(ptr);
            self.parent.dealloc(u8, buf);
            
            @match(prev) {
                fn Some(prev) => {
                    chunk = prev;
                }
                fn None() => {
                    self.chunk = .None;
                    self.start = 0;
                    self.end = 0;
                    return();
                }
            };
        };
    };
    
}

fn current_allocated_size(self: *ArenaAlloc) i64 = {
    size := 0;
    if self.chunk { chunk |
        // go all the way back then all the way forward because we might not be at the latest chunk. 
        while(=> chunk.prev.is_some()) {
            chunk = chunk.prev.unwrap();
        };
        
        while(=> chunk.next.is_some()) {
            size += chunk.size;
            chunk = chunk.next.unwrap();
        };
        size += chunk.size;
    };
    size
}

// doesn't count those after the current chunk (that were allocated before the last reset).
fn current_used_size(self: *ArenaAlloc) i64 = {
    size := 0;
    if self.chunk { chunk |
        size += self.start;
        while => chunk.prev.is_some() {
            chunk = chunk.prev.unwrap();
            size += chunk.used;
        };
        size += chunk.used;
    };
    size
}

fn borrow(self: *ArenaAlloc) Alloc = {
    self := ArenaAlloc.raw_from_ptr(self);
    (data = self, vptr = arena_allocator_fn)
}

fn bake_relocatable_value(self: *ArenaAlloc) Slice(BakedEntry) = {
    empty: ArenaAlloc = init(self.parent, self.base_size);
    bytes := ArenaAlloc.cast_to_bytes(empty&);
    entries := dyn_bake_relocatable_value(bytes, ArenaAlloc, true);
    entries
}

todo_allocator: Alloc : (data = 0.rawptr_from_int(), vptr = todo_allocator_fn);

// TODO: respect alignment. but im pretty sure malloc always gives you 16 bits and mostly good enough. 
todo_allocator_fn :: fn(_: rawptr, action: AllocatorAction, ptr: rawptr, count: i64, align: i64) rawptr = { 
    action := @as(i64) action;
    ::if(rawptr);
    // Note: we can't use @match yet! because this is comptime ast_alloc rn
    if action == 0 {
        malloc(count)
    } else {
        0.rawptr_from_int()
    }
};
