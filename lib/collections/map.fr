// TODO: use a real hashing thing. wyhash is pretty small: https://github.com/wangyi-fudan/wyhash
// TODO: explicit wrapping_op functions. 
fn hash(i: *i64) i64 = i[].mul(345671).add(9765431);
fn hash(s: *Str) i64 = {
    h := 0;
    for(s[]) {c|
        c: i64 = c.zext();
        h += c&.hash().mul(4256465971);
    };
    h
}

/// This is the kind where you get bumped to the next slot if you collide, not the kind where you spill out into a linked list.
/// TODO: reuse the first tombstone slot when inserting after a remove. currently im being dumb and just wasting search time until a resize. 
/// (K: eq + hash + move, V: move)
/// TODO: take hasher as param once i have default args? 
/// TODO: api for getting the real len (not including tombstones)
fn HashMap($K: Type, $V: Type) Type = {
    R :: RawHashMap(K, V);
    Self :: @struct(raw: R, alloc: Alloc, $Raw := R);
    fn init(gpa: Alloc) Self = (raw = init(), alloc = gpa);
    
    fn insert(self: *Self, key: K, value: V) ?V #inline = self.raw&.insert(key, value, self.alloc);
    fn get(self: *Self, key: K) ?V #inline = self.raw&.get(key&);
    fn get_ptr(self: *Self, key: K) ?*V #inline = self.raw&.get_ptr(key&);
    fn get(self: *Self, key: *K) ?V #inline = self.raw&.get(key);
    
    /// SAFETY: the returned pointer is invalid after a call to 'insert' (which may trigger a resize)
    fn get_ptr(self: *Self, key: *K) ?*V #inline = self.raw&.get_ptr(key);
    // this lets you do weird stuff where key matching doesn't mean really exactly the same. 
    fn get_key(self: *Self, key: *K) ?K #inline = self.raw&.get_key(key);
    
    // Note: this doesn't adjust self.len_including_tombstones! because we use that for resizing so need to account for the tombstone.
    fn remove(self: *Self, key: *K) ?V #inline = self.raw&.remove(key);
    fn remove(self: *Self, key: K) ?V #inline = self.raw&.remove(key&);
    
    fn for_keys(self: *Self, $body: @Fn(key: K) void) void = self.raw&.for_keys(body);
    fn each(self: *Self, $body: @Fn(key: K, value: *V) void) void = self.raw&.each(body);
    fn drop(self: *Self) void #inline = self.raw&.drop(self);
    
    fn capacity(self: *Self) i64 = self.raw.capacity;
    fn clear(self: *Self) void = self.raw&.clear();
    
    // This is the same as:
    // self.get_ptr(key) || { self.insert(key, if_absent()); self.get_ptr(key) }
    // but faster because it only needs to hash the key once 
    fn get_or_insert(self: *Self, key: K, $if_absent: @Fn() V) void = 
        get_or_insert(self.raw&, key, self.alloc, if_absent);
    
    Self
}

fn zero_unused_slots();

fn RawHashMap($K: Type, $V: Type) Type = {
    Hasher :: TrivialHasher;
    // TODO: instead of complicated traits, just require(eq, Ty(K, K), bool);
    Self :: @struct(arr: Slice(Entry), len_including_tombstones: i64, capacity: i64, E :: Entry);
    Entry :: @struct(hash: i64, key: K, value: V);
    load_percent :: 70; // TODO: default arugments in functions so its not annoying to let you set this -- Jun 12
    
    fn init() Self #fold = (arr = @as(Slice(Entry)) empty(), len_including_tombstones = 0, capacity = 0);
    
    ::if(?*V);
    ::if(u64);
    
    // Note: this can't be an overload set becuase you want to instantiate the template multiple times with the same key (and different values).
    checked_hash :: fn(key: *K) i64 = {
        hasher: Hasher = init();
        hasher&.hash(key);
        h: u64 = hasher&.end();
        h: i64 = h.bitcast();
        h := h.abs();  // since i use mod to map to a slot, hash can't be negative. 
        
        // I use 0 to indicate an empty slot and 1 as a tombstone to mark removals. 
        if h == 0 || h == 1 {
            h = 2;
        };
        
        h
    };
    
    fn insert(self: *Self, key: K, value: V, gpa: Alloc) ?V = {
        valid, slot, h := expand_to_find_slot(self, key&, gpa);
        if valid {
            old := slot.value;
            slot.value = value;
            return(Some = old);
        };
        self.len_including_tombstones += int(slot.hash == 0);
        slot[] = (hash = h, key = key, value = value);
        .None
    }
    
    expand_to_find_slot :: fn(self: *Self, key: *K, gpa: Alloc) Ty(bool, *Entry, i64) = {
        full := self.len_including_tombstones >= self.capacity;
        if(full, => self.resize(gpa)); // does it sooner than necessary (if replacing a slot) but thats fine. 
        h    := key.checked_hash();
        slot := self.find_slot(key, h);
        (slot.hash != 0 && slot.hash != 1, slot, h)
    };
    
    fn get_or_insert(self: *Self, key: K, gpa: Alloc, $if_absent: @Fn() V) *V = {
        valid, slot, h := expand_to_find_slot(self, key&, gpa);
        if !valid {
            value := if_absent();
            // do this after `if_absent` so we don't increment if they early return.
            self.len_including_tombstones += int(slot.hash == 0);
            slot[] = (hash = h, key = key, value = value);
        };
        
        slot.value&
    }

    fn get(self: *Self, key: K) ?V #inline = self.get(key&);
    fn get_ptr(self: *Self, key: K) ?*V #inline = self.get_ptr(key&);
    
    fn get(self: *Self, key: *K) ?V = {
        @match(self.get_ptr(key)) {
            fn Some(t) => (Some = t[]);
            fn None(t) => .None;
        }
    }
    
    /// SAFETY: the returned pointer is invalid after a call to 'insert' (which may trigger a resize)
    fn get_ptr(self: *Self, key: *K) ?*V = {
        if(self.len_including_tombstones == 0, => return(.None));
        slot := self.find_slot(key, key.checked_hash());
        if(slot.hash == 0, => .None, => (Some = slot.value&))
    }
    
    // this lets you do weird stuff where key matching doesn't mean really exactly the same. 
    fn get_key(self: *Self, key: *K) ?K = {
        if(self.len_including_tombstones == 0, => return(.None));
        slot := self.find_slot(key, key.checked_hash());
        if(slot.hash == 0, => .None, => (Some = slot.key))
    }
    
    fn remove(self: *Self, key: K) ?V #inline = self.remove(key&);
    
    // Note: this doesn't adjust self.len_including_tombstones! because we use that for resizing so need to account for the tombstone.
    fn remove(self: *Self, key: *K) ?V = {
        if(self.len_including_tombstones == 0, => return(.None));
        slot := self.find_slot(key, key.checked_hash());
        if(slot.hash == 0, => return(.None));
        slot.hash = 1; // tombstone
        (Some = slot.value)
    }
    
    fn clear(self: *Self) void = {
        each self.arr { e | 
            e.hash = 0;
        };
    }
    
    fn find_slot(self: *Self, key: *K, h: i64) *Entry = {
        @debug_assert(self.arr.len != 0);
        i := h.mod(self.arr.len);
        
        range(0, self.arr.len) { _ |
            check := self.arr[i]&;
            // 0 means empty, not in map
            if check.hash == 0 || (check.hash == h && check.key& == key) {
                return(check);
            };
            // 1 => placeholder for a removed value
            // othewise, continue, might have been bumped
            i = mod(i + 1, self.arr.len);
        };
        
        panic("Unreachable unless you set the load factor to 100%")
    }
    
    fn resize(self: *Self, gpa: Alloc) void #inline = 
        hashmap_expand_erased(ptr_cast_unchecked(Self, RawHashMapErased, self), size_of(Entry), align_of(Entry), gpa, load_percent);
    
    // TODO: have a bake_relocatable_value that calls this to help reproducible builds. 
    fn zero_unused_slots(self: *Self) void = 
        hashmap_erased_zero_unused_slots(ptr_cast_unchecked(Self, RawHashMapErased, self), size_of(Entry));
    
    fn for_keys(self: *Self, $body: @Fn(key: K) void) void = {
        each self { (k, _) |
            body(k);
        };
    }
    
    fn each(self: *Self, $body: @Fn(key: K, value: *V) void) void = {
        each self.arr {slot|
            if slot.hash.ne(0).and(slot.hash.ne(1)) {
                body(slot.key, slot.value&);
            };
        };
    }
    
    // TODO: drop entries? 
    fn drop(self: *Self, alloc: Alloc) void = {
        alloc.dealloc(Entry, self.arr);
        self.arr = empty();
        self.len_including_tombstones = 0;
        self.capacity = 0;
    }
    
    Self
}

// :Polymorphise
RawHashMapErased :: @struct(entries: rawptr, entry_count: i64, len_including_tombstones: i64, capacity: i64);
hashmap_expand_erased :: fn(self: *RawHashMapErased, entry_size: i64, entry_align: i64, gpa: Alloc, load_percent: i64) void #noinline = {
    old_mem, old_count := (self.entries, self.entry_count);
    new_count := max(old_count * 2, 4);
    new_mem   := u8.raw_from_ptr(gpa.alloc_raw(new_count * entry_size, entry_align).as_ptr());
    
    index_hash :: fn(mem, i) => i64.ptr_from_raw(mem.offset(i * entry_size));
    slice_slot :: fn(mem, i) => u8.ptr_from_raw(mem.offset(i * entry_size)).slice(entry_size);
    
    range(0, new_count) { i |
        index_hash(new_mem, i)[] = 0;
    };
    self.capacity = new_count * load_percent / 100;
    real_len := 0;
    range(0, old_count) { old_i |
        h := index_hash(old_mem, old_i)[];
        if h != 0 && h != 1 {
            // Same as find_slot() but assumes the key is not already in the table so doesn't need to call the eq() overload. 
            // The new size is always bigger than the old one, so there will always be space for all the entries. 
            new_i := h.mod(new_count); 
            while => index_hash(new_mem, new_i)[] != 0 {
                new_i = mod(new_i + 1, new_count)
            };
            slice_slot(new_mem, new_i).copy_from(slice_slot(old_mem, old_i));
            real_len += 1;
        };
    };
    self.len_including_tombstones = real_len;
    
    gpa.dealloc_raw(self.entries, self.entry_count * entry_size, entry_align);
    self.entry_count = new_count;
    self.entries = new_mem;
};

// :Polymorphise
hashmap_erased_zero_unused_slots :: fn(self: *RawHashMapErased, entry_size: i64) void #noinline = {
    range(0, self.entry_count) { i |
        h := i64.ptr_from_raw(self.entries.offset(i * entry_size))[];
        if h == 0 || h == 1 {
            slot := u8.ptr_from_raw(self.entries.offset(i * entry_size + 8)).slice(entry_size - 8);
            slot.set_zeroed();
        }
    };
};

fn HashSet($V: Type) Type = {
    // TODO: fixed. change this to void -- Feb 8, 2025
    Self :: HashMap(V, bool); // TODO: emit_bc missing arg var when its void beacuse it thinks it doesnt matter -- Jun 27
    fn insert(self: *Self, v: V) bool = {
        self.insert(v, false).is_some() // TODO: invert? i guess makes more sense if return true if *did* add. 
    }
    
    Self
}
