fn insert() void = unit;
fn get() void = unit;
fn get_ptr() void = unit;
fn hash();
fn remove();
fn for_keys();
fn get_key();
fn resize();

// TODO: use a real hashing thing. wyhash is pretty small: https://github.com/wangyi-fudan/wyhash
// TODO: explicit wrapping_op functions. 
fn hash(i: *i64) i64 = i[].mul(345671).add(9765431);
fn hash(s: *Str) i64 = {
    h := 0;
    for(s[]) {c|
        c: i64 = c.zext();
        h += c&.hash().mul(4256465971);
    };
    h
}

fn capacity();

/// This is the kind where you get bumped to the next slot if you collide, not the kind where you spill out into a linked list.
/// TODO: reuse the first tombstone slot when inserting after a remove. currently im being dumb and just wasting search time until a resize. 
/// (K: eq + hash + move, V: move)
/// TODO: take hasher as param once i have default args? 
/// TODO: api for getting the real len (not including tombstones)
fn HashMap($K: Type, $V: Type) Type = {
    R :: RawHashMap(K, V);
    Self :: @struct(raw: R, alloc: Alloc, $Raw := R);
    fn init(gpa: Alloc) Self = (raw = init(), alloc = gpa);
    
    fn insert(self: *Self, key: K, value: V) ?V #inline = self.raw&.insert(key, value, self.alloc);
    fn get(self: *Self, key: K) ?V #inline = self.raw&.get(key&);
    fn get_ptr(self: *Self, key: K) ?*V #inline = self.raw&.get_ptr(key&);
    fn get(self: *Self, key: *K) ?V #inline = self.raw&.get(key);
    
    /// SAFETY: the returned pointer is invalid after a call to 'insert' (which may trigger a resize)
    fn get_ptr(self: *Self, key: *K) ?*V #inline = self.raw&.get_ptr(key);
    // this lets you do weird stuff where key matching doesn't mean really exactly the same. 
    fn get_key(self: *Self, key: *K) ?K #inline = self.raw&.get_key(key);
    
    // Note: this doesn't adjust self.len_including_tombstones! because we use that for resizing so need to account for the tombstone.
    fn remove(self: *Self, key: *K) ?V #inline = self.raw&.remove(key);
    fn remove(self: *Self, key: K) ?V #inline = self.raw&.remove(key&);
    
    fn for_keys(self: *Self, $body: @Fn(key: K) void) void = self.raw&.for_keys(body);
    fn each(self: *Self, $body: @Fn(key: K, value: *V) void) void = self.raw&.each(body);
    fn drop(self: *Self) void #inline = self.raw&.drop(self);
    
    fn capacity(self: *Self) i64 = self.raw.capacity;
    fn clear(self: *Self) void = self.raw&.clear();
    
    Self
}

fn zero_unused_slots();

fn RawHashMap($K: Type, $V: Type) Type = {
    Hasher :: TrivialHasher;
    // TODO: instead of complicated traits, just require(eq, Ty(K, K), bool);
    Self :: @struct(arr: Slice(Entry), len_including_tombstones: i64, capacity: i64, E :: Entry);
    Entry :: @struct(hash: i64, key: K, value: V);
    load_percent :: 70; // TODO: default arugments in functions so its not annoying to let you set this -- Jun 12
    
    fn init() Self = (arr = @as(Slice(Entry)) empty(), len_including_tombstones = 0, capacity = 0);
    
    ::?*V;
    ::if_opt(*V, ?V);
    ::if(?V);
    ::if(?*V);
    ::if(u64);
    
    // Note: this can't be an overload set becuase you want to instantiate the template multiple times with the same key (and different values).
    checked_hash :: fn(key: *K) i64 = {
        mask: u64 : (@as(u64) 1).bit_not();  // :sema_regression didn't need the type on the literal before. 
        hasher: Hasher = init();
        hasher&.hash(key);
        h := hasher&.end();
        // I use 0 to indicate an empty slot and 1 as a tombstone to mark removals. 
        // TODO: compiler bug! u64 auto casts to i64 here for some reason??
        h := if(h.bit_and(mask).eq(@as(u64) 0), => h.add(@as(u64) 4), => h);
        // TODO: stupid!!
        h: i64 = h.bitcast();
        h.abs()
    };
    
    fn insert(self: *Self, key: K, value: V, gpa: Alloc) ?V = {
        full := self.len_including_tombstones.add(1).gt(self.capacity);
        if(full, => self.resize(gpa)); // does it sooner than necessary (if replacing a slot) but thats fine. 
        h := key&.checked_hash();
        slot := self.find_slot(key&, h);
        
        ::if(?V);
        @switch(slot.hash) {
            @case(0) fn() ?V => {
                slot[] = (hash = h, key = key, value = value);
                self.len_including_tombstones += 1;
                .None
            };
            @case(1) fn() ?V => {
                slot[] = (hash = h, key = key, value = value);
                .None
            };
            @default fn(_: i64) ?V => {
                old := slot.value;
                slot.value = value;
                (Some = old)
            };
        }
    }


    fn get(self: *Self, key: K) ?V #inline = self.get(key&);
    fn get_ptr(self: *Self, key: K) ?*V #inline = self.get_ptr(key&);
    
    fn get(self: *Self, key: *K) ?V = {
        found: ?*V = self.get_ptr(key);
        @match(found) {
            (fn Some(t) ?V = some(t[]));
            (fn None(t) ?V = none());
        }
    }
    
    /// SAFETY: the returned pointer is invalid after a call to 'insert' (which may trigger a resize)
    fn get_ptr(self: *Self, key: *K) ?*V = {
        ::if(?*V);
        if(self.len_including_tombstones.eq(0), => (None = ())) {
            slot := self.find_slot(key, key.checked_hash());
            if(slot.hash.eq(0), => (None = ()), => (Some = slot.value&))
        } 
    }
    
    // this lets you do weird stuff where key matching doesn't mean really exactly the same. 
    fn get_key(self: *Self, key: *K) ?K = {
        ::if(?K); // :sema_regression just order tho? 
        if(self.len_including_tombstones.eq(0), => (None = ())) {
            slot := self.find_slot(key, key.checked_hash());
            if(slot.hash.eq(0), => (None = ()), => (Some = slot.key))
        } 
    }
    
    fn remove(self: *Self, key: K) ?V #inline = self.remove(key&);
    
    // Note: this doesn't adjust self.len_including_tombstones! because we use that for resizing so need to account for the tombstone.
    fn remove(self: *Self, key: *K) ?V = {
        if(self.len_including_tombstones.eq(0), => (None = ())) {
            slot := self.find_slot(key, key.checked_hash());
            if(slot.hash.eq(0), => (None = ())) {
                slot.hash = 1; // tombstone
                (Some = slot.value)
            }
        } 
    }
    
    fn clear(self: *Self) void = {
        each self.arr { e | 
            e.hash = 0;
        };
    }
    
    fn find_slot(self: *Self, key: *K, h: i64) *Entry = {
        @debug_assert(self.arr.len != 0);
        i := h.mod(self.arr.len);
        
        range(0, self.arr.len) { _ |
            check := self.arr[i]&;
            if check.hash == h && check.key& == key {
                return(check);
            };
            if check.hash == 0 {
                return(check);  // empty, not in map
            };
            // 1 => placeholder for a removed value
            // othewise, continue, might have been bumped
            i = mod(i + 1, self.arr.len);
        };
        
        panic("Unreachable unless you set the load factor to 100%")
    }
    
    fn resize(self: *Self, gpa: Alloc) void = {
        old_arr := self.arr;
        self.arr = gpa.alloc(Entry, old_arr.len.mul(2).max(4));
        each(self.arr) {slot|
            slot.hash = 0;
        };
        self.capacity = self.arr.len.mul(load_percent).div(100);
        real_len := 0;
        each(old_arr) {old_slot|
            if(old_slot.hash.ne(0).and(old_slot.hash.ne(1))) {
                new_slot := self.find_slot(old_slot.key&, old_slot.hash);
                new_slot[] = old_slot[];
                real_len += 1;
            };
        };
        self.len_including_tombstones = real_len;
        
        gpa.dealloc(Entry, old_arr);
    }
    
    // TODO: have a bake_relocatable_value that calls this to help reproducible builds. 
    fn zero_unused_slots(self: *Self) void = {
        each self.arr { slot |
            if slot.hash == 0 || slot.hash == 1 {
                slot.key = K.zeroed();
                slot.value = V.zeroed();
            };
        };
    }
    
    fn for_keys(self: *Self, $body: @Fn(key: K) void) void = {
        each self { (k, _) |
            body(k);
        };
    }
    
    fn each(self: *Self, $body: @Fn(key: K, value: *V) void) void = {
        each self.arr {slot|
            if slot.hash.ne(0).and(slot.hash.ne(1)) {
                body(slot.key, slot.value&);
            };
        };
    }
    
    // TODO: drop entries? 
    fn drop(self: *Self, alloc: Alloc) void = {
        alloc.dealloc(Entry, self.arr);
        self.arr = empty();
        self.len_including_tombstones = 0;
        self.capacity = 0;
    }
    
    Self
}

fn HashSet($V: Type) Type = {
    Self :: HashMap(V, bool); // TODO: emit_bc missing arg var when its void beacuse it thinks it doesnt matter -- Jun 27
    fn insert(self: *Self, v: V) bool = {
        self.insert(v, false).is_some() // TODO: invert? i guess makes more sense if return true if *did* add. 
    }
    
    Self
}

fn EnumMap($Key: Type, $Value: Type) Type = {
    info :: get_type_info_ref(Key);
    :: {
        assert(info.is(.Enum), "EnumMap key must be enum");  // TODO: fmt type 
        assert(info[].Enum.sequential, "for now, EnumMap requires sequential integer values");
    };
    :: enum(Key);
    :: from_raw(Key);
    
    Self :: @struct(data: Array(Value, info[].Enum.fields.len));
    
    fn len(self: *Self) i64 = 
        self.data&.len();
    
    fn init(copied: Value) Self = {
        (data = init(copied))
    }
    
    fn insert(self: *Self, key: Key, value: Value) void = {
        self.data&[key.raw()] = value;
    }
    
    fn index(self: *Self, key: Key) *Value = {
        self.data&.index(key.raw())
    }
    
    fn get(self: *Self, key: Key) Value = {
        self.data&.index(key.raw())[]
    }
    
    fn each(self: *Self, $body: @Fn(k: Key, v: *Value) void) void = {
        enumerate self.data& { k, v |
            body(k.from_raw().unwrap(), v);
        };
    }
    
    Self
}
