// TODO: fix scope resolution for generics 
// TODO: These are just cringe forward declarations to create the overload sets. 
fn list() void;
fn push() void;
fn items() void;
fn drop() void;
fn push_all() void;
fn clone();
fn pop_last() void;
fn reserve() void;
fn repeated() void;
fn clear() void;
fn as_ptr() void;
fn ordered_remove();
fn unordered_remove();
fn pop();
fn add_unique();
fn ordered_retain();
fn unordered_retain();
fn truncate();
fn deep_clone();
fn unordered_remove_discard();
fn next_uninit();

fn list($T: Type, a: Alloc) List(T) #generic = {
    list(a)
}

fn list($T: Type, cap: i64, a: Alloc) List(T) #generic = {
    list(cap, a)
}

fn fixed_list();

fn List($T: Type) Type = {
    Self :: @struct(maybe_uninit: Slice(T), len: i64, gpa: Alloc, E :: T);
    fn list(cap: i64, gpa: Alloc) Self #inline = 
        (maybe_uninit = gpa.alloc(T, cap), len = 0, gpa = gpa);  // todo: no alloc if len 0 since i dont free
    fn list(gpa: Alloc) Self #inline = list(0, gpa);
    
    fn fixed_list(buf: []T) Self #inline = 
        (maybe_uninit = buf, len = 0, gpa = panicking_allocator);
    
    fn assume_owned(self: []T, by: Alloc) Self = 
        (maybe_uninit = self, len = self.len, gpa = by);
   
    fn repeated(t: T, count: i64, gpa: Alloc) Self = {
        // TODO: better error message if you forget the type annotataion here -- Jun 4
        self: Self = list(count, gpa);
        range(0, count, fn(_) => self&.push(t));
        self
    }
    
    // TOOD: rename shallow_clone?
    // Can't reuse the allocation because drop() will free it. TODO: COW?
    fn clone(from: Slice(T), gpa: Alloc) Self = {  // TODO: better error message if you put the = on the wrong side of the type
        self: Self = list(from.len(), gpa);
        self&.push_all(from);
        self
    }
    
    fn clone(from: *Self) Self = {  // TODO: better error message if you put the = on the wrong side of the type
        self: Self = list(from.len(), from.gpa);
        self&.push_all(from.items());
        self
    }
    
    fn clone(from: Self) Self #inline = from&.clone();
    
    fn deep_clone(self: *Self, a: Alloc) Self = {
        // :InlineFoldHack
        // This works without the extra `::` on the latest compiler but I don't want to update `boot` yet so here we are. 
        // also theres a hack with @debug_assert anyway so it doesn't work ~~super~~ well. 
        @if(::!T.has_pointers(), { self.items().clone(a) }, {
            s: Self = list(self.len, a);
            s.len = self.len;
            enumerate self { i, e |
                s[i] = e.deep_clone(a);
            };
            s
        })
    }
    
    fn items(self: *Self) Slice(T) #inline = {
        self.maybe_uninit.slice(0, self.len)
    }
    fn items(self: Self) Slice(T) #inline = self&.items();
    
    // A pointer to the first element of the list. 
    // SAFETY: dont use the pointer if the list is empty. invalidated by resize. 
    fn as_ptr(self: *Self) *T = self.maybe_uninit.ptr;
    
    #inline
    fn index(self: *Self, i: i64) *T #inline = {
        @safety(.Bounds) i < self.len;
        self.maybe_uninit.index_unchecked(i)
    }
    
    // TODO: hack because auto deref doesn't work through function calls
    fn index(self: Self, i: i64) *T #inline = {
        @safety(.Bounds) i < self.len;
        self.maybe_uninit.index_unchecked(i)
    }
    
    fn len(self: Self) i64 #inline #fold = self.len;
    fn len(self: *Self) i64 #inline = self.len;

    ::DeriveIndexable(Self, T);
    ::DeriveIndexable(*Self, T);

    fn reserve(self: *Self, extra: i64) void #inline = {
        if self.maybe_uninit.len < self.len + extra {
            reserve_expand_erased(Self.raw_from_ptr(self), extra, T.size_of());
        };
    }

    fn push_assume_capacity(self: *Self, v: T) void #inline = {
        self.maybe_uninit.index_unchecked(self.len)[] = v;
        self.len += 1;
    }
    
    fn push(self: *Self, v: T) void = {
        reserve(self, 1);
        last := self.maybe_uninit.index_unchecked(self.len); 
        last[] = v;
        self.len += 1;
    }
    
    fn index_unchecked(self: Self, i: i64) *T = 
        self.maybe_uninit.index_unchecked(i);
    
    fn pop_last(self: *Self) ?T = {
        ::if(?T);
        if(eq(self.len, 0), => .None) {
            i := sub(self.len, 1);
            self.len = i;
            (Some = self.maybe_uninit.get(i))
        }
    }
    fn pop(self: *Self) ?T #inline = self.pop_last();
    
    fn ordered_remove(self: *Self, index: i64) ?T = {
        ::if(?T);
        @safety(.Bounds) index >= 0;
        if(index.ge(self.len), => .None) {
            current := self.len.sub(1);
            out := T.zeroed();
            while(=> current.ne(index.sub(1))) {
                if current >= 0 {
                    next_out := self[current];
                    self[current] = out;
                    out = next_out;
                };
                current -= 1;
            };
            
            self.len -= 1;
            (Some = out)
        }
    }
    
    // just swap in the last thing so you don't have to shift all the later elements.
    fn unordered_remove(self: *Self, index: i64) ?T #inline = {
        ::if(?T);
        if(index.ge(self.len), => .None) {
            out := self[index];
            if index == self.len - 1 {
                self.len -= 1;
            } else {
                if self.pop() { end |
                    self[index] = end;
                };
            };
            (Some = out)
        }
    }
    
    fn unordered_remove_discard(self: *Self, i: i64) void #inline = {
        @debug_assert(i < self.len);
        if i != self.len - 1 {
            self[i] = self[self.len - 1];
        };
        self.len -= 1;
    }
    
    fn insert(self: *Self, idx: i64, e: T) void = {
        self.reserve(1);
        i := self.len;
        self.len += 1;
        // TODO: call memcpy on a block instead of shuffling one by one.
        while => i > idx {
            self[i] = self[i - 1];
            i -= 1;
        };
        self[idx] = e;
    }
    
    fn push_all(self: *Self, new: Slice(T)) void = {
        reserve(self, len(new));
        dest := subslice(self.maybe_uninit, self.len, len(new));
        dest.copy_from(new);
        self.len += new.len;
    }
    
    // note: this doesn't drop the entries. 
    fn drop(self: *Self) void = {
        if self.maybe_uninit.len != 0 {
            self.gpa.dealloc(T, self.maybe_uninit);
            self.maybe_uninit.len = 0;
            self.len = 0;
        };
    }
    
    /// Retains capacity
    fn clear(self: *Self) void = {
        self.len = 0;
    }
    
    fn add_unique(self: *Self, t: T) bool = {
        each self { check |
            if(check == t&, => return(false));
        };
        self.push(t);
        true
    }
    
    // SAFETY: dont early return!
    // TODO: have a defer or something that fixes it if you try to early return
    /// This is better than calling ordered_remove a bunch of times.
    fn ordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        slot := 0;
        range(0, self.len) { i |
            if should_keep(self.index(i)) {
                // Only start doing moves once we've created a gap.
                if slot != i {
                    self[slot] = self[i];
                };
                slot += 1;
            } else {
                // Don't increment slot, we will put the next good one here.
            };
        };
        self.len = slot;
    }
    
    /// Does not visit in order and does not maintain the order of the list.
    fn unordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        i := 0;
        while => i < self.len {
            if should_keep(self.index(i)) {
                i += 1;
            } else {
                self.unordered_remove_discard(i);
                // don't increment i, check this slot again. 
            };
        };
    }
    
    Self
}

fn push_assume_capacity();
fn as_raw();
fn as_raw_list();

// The rust stdlib Vec does this thing where you manually outline the slow path, so it probably helps a bit. 
// this is more aggressive and not generating it for every single type.
fn reserve_expand_erased(list: rawptr, extra: i64, size_of: i64) void = {
    ListHeader :: @struct(ptr: rawptr, cap: i64, len: i64, gpa: Alloc);
    list := ListHeader.ptr_from_raw(list);
    cap := 4.max(list.cap.add(extra).max(list.cap.mul(2)));
    new_memory := {list.gpa.vptr}(list.gpa.data, .Allocate, rawptr_from_int(0), cap * size_of, 4); // TODO: alignment
    copy_no_alias(new_memory, list.ptr, list.len * size_of);
    if list.cap != 0 {
        {list.gpa.vptr}(list.gpa.data, .Deallocate, list.ptr, list.cap * size_of, 4); // TODO: alignment
    };
    list.ptr = new_memory;
    list.cap = cap;
}

// This is like zigs ArrayListUnmanaged, so you don't have to store a billion copies of the same allocator. 
// Originally it carefully matched rust's abi and its a pain to change now so that's why the fields are in a strange order. TODO: fix eventually. 
fn RawList($T: Type) Type = {
    Self :: @struct(cap: i64, ptr: *T, len: i64, E :: T);
   
    fn init(a: Alloc, cap: i64) Self = {
        m := a.alloc(T, cap);
        (ptr = m.ptr, len = 0, cap = cap)
    }
    
    fn items(self: Self) Slice(T) #inline = (ptr = self.ptr, len = self.len);
    fn items(self: *Self) Slice(T) #inline = (ptr = self.ptr, len = self.len);
    
    fn as_raw(self: List(T)) Self #inline = 
        (cap = self.maybe_uninit.len, ptr = self.maybe_uninit.ptr, len = self.len);
    
    // TODO: name this assume_owned too? 
    fn as_raw_list(self: Slice(T)) Self #inline = 
        (cap = self.len, ptr = self.ptr, len = self.len);
    
    fn empty() Self #inline = 
        Self.zeroed();
    
    fn assume_owned(self: Self, by: Alloc) List(T) #inline = 
        (maybe_uninit = (ptr = self.ptr, len = self.cap), len = self.len, gpa = by);
        
    fn clear(self: *Self) void = {
        self.len = 0;
    }
    
    fn len(self: Self) i64 #fold = self.len;
    fn len(self: *Self) i64 = self.len;
    
    fn index(self: Self, i: i64) *T #inline = self.items().index(i);
    fn index(self: *Self, i: i64) *T #inline = self[].items().index(i);
    
    ::DeriveIndexable(Self, T);
    ::DeriveIndexable(*Self, T);
    
    // This type doesn't store the allocator so you have to pass it in every time.
    fn push(self: *Self, t: T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.push(t);
        self[] = s.as_raw();
    }
    
    fn insert(self: *Self, i: i64, t: T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.insert(i, t);
        self[] = s.as_raw();
    }
    
    fn push_all(self: *Self, t: []T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.push_all(t);
        self[] = s.as_raw();
    }
    
    fn ordered_remove(self: *Self, i: i64) ?T = {
        s := self[].assume_owned(panicking_allocator); 
        t := s&.ordered_remove(i);
        self[] = s.as_raw();
        t
    }
    
    fn truncate(self: *Self, i: i64) void = {
        if(self.len < i, => return());
        self.len = i;
    }
    
    fn deep_clone(self: *Self, a: Alloc) Self = {
        // :InlineFoldHack
        @if(::!T.has_pointers(), { self.items().clone(a).as_raw() }, {
            s: List(T) = list(self.len, a);
            s.len = self.len;
            enumerate self { i, e |
                s[i] = e.deep_clone(a);
            };
            s.as_raw()
        })
    }
    fn add_unique(self: *Self, t: T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.add_unique(t);
        self[] = s.as_raw();
    }
    
    fn drop(self: *Self, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.drop();
        self[] = empty();
    }
    
    fn ordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        s := self[].assume_owned(panicking_allocator);
        s&.ordered_retain(should_keep);
        self[] = s.as_raw();
    }
    
    fn pop(self: *Self) ?T = {
        s := self[].assume_owned(panicking_allocator);
        res := s&.pop();
        self[] = s.as_raw();
        res
    }
    
    fn unordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        s := self[].assume_owned(panicking_allocator);
        s&.unordered_retain(should_keep);
        self[] = s.as_raw();
    }
    
    fn next_uninit(self: *Self) ?*T #inline = {
        if(self.len >= self.cap, => return(.None));
        self.len += 1;
        (Some = self.index(self.len - 1))
    }
    
    Self
}

// TODO: this is really inefficient if you already own 'a'
fn concat(a: Str, b: Str, gpa: Alloc) List(u8) = {
    c: List(u8) = list(a.len().add(b.len()), gpa);
    c&.push_all(a);
    c&.push_all(b);
    c
}

fn split(haystack: Str, needle: Str, gpa: Alloc) List(Str) = {
    @safety(.Bounds) needle.len > 0;
    lines: List(Str) = list(1, gpa);
    i := 0;
    last := haystack.len - needle.len;
    start := 0;
    while => i <= last {
        continue :: local_return;
        if haystack[i] == needle[0] {
            check := haystack.subslice(i + 1, needle.len - 1);
            if check == needle.slice(1, needle.len) {
                found := haystack.subslice(start, i - start);
                lines&.push(found);
                i += needle.len;
                start = i;
                continue();
            };
        };
        i += 1; 
    };
    found := haystack.subslice(start, max(0, haystack.len - start));
    lines&.push(found);
    lines
}

// TODO: better error message if you call this on Ptr(Str)
fn split_lines(s: Str, gpa: Alloc) List(Str) #inline = 
    split(s, "\n", gpa); 

// It's such a pain to google an ascii table every time.  
// Since this is #fold, the length check happens during compilation if the argument to a call is a constant.
// So this function can take the place of a char literal in other languages without new syntax. 
fn char(s: Str) i64 #fold = {
    // TODO: can't get field of const cause cant take pointer. 
    // TODO: @assert that takes a format string so its less painful to print the arg as well. 
    assert(s.len().eq(1), "char str expected length 1. TODO: utf8"); 
    s[0].zext()
}

// The intent is that you always call this with a constant argument and use it like C's '' literals.
fn ascii(s: Str) u8 #fold = {
    c: i64 = s[0].zext();
    // TODO: safety checked intcast instead of trunc?
    assert(s.len().eq(1), "ascii str expected length 1"); 
    assert(c.lt(128), "expected ascii"); // TODO: make an @assert fmt version. 
    c.trunc()
}

// TODO
fn push(self: *List(u8), v: i64) void = 
    self.push(@as(u8) v.trunc());

