fn list($T: Type, a: Alloc) List(T) #generic #inline = {
    list(a)
}

fn list($T: Type, cap: i64, a: Alloc) List(T) #generic #inline = {
    list(cap, a)
}

fn List($T: Type) Type = {
    Self :: @struct(maybe_uninit: Slice(T), len: i64, gpa: Alloc, Element :: T);
    fn list(cap: i64, gpa: Alloc) Self #inline = 
        (maybe_uninit = gpa.alloc_uninit(T, cap), len = 0, gpa = gpa);  // todo: no alloc if len 0 since i dont free
    fn list(gpa: Alloc) Self #inline = list(0, gpa);
    
    fn fixed_list(buf: []T) Self #inline = 
        (maybe_uninit = buf, len = 0, gpa = panicking_allocator);
    
    fn assume_owned(self: []T, by: Alloc) Self = 
        (maybe_uninit = self, len = self.len, gpa = by);
   
    fn repeated(t: T, count: i64, gpa: Alloc) Self = {
        // TODO: better error message if you forget the type annotataion here -- Jun 4
        self: Self = list(count, gpa);
        range(0, count, fn(_) => self&.push(t));
        self
    }
    
    // TOOD: rename shallow_clone?
    // Can't reuse the allocation because drop() will free it. TODO: COW?
    fn clone(from: Slice(T), gpa: Alloc) Self = {  // TODO: better error message if you put the = on the wrong side of the type
        self: Self = list(from.len(), gpa);
        self&.push_all(from);
        self
    }
    
    fn clone(from: *Self) Self = {  // TODO: better error message if you put the = on the wrong side of the type
        self: Self = list(from.len(), from.gpa);
        self&.push_all(from.items());
        self
    }
    
    fn clone(from: Self) Self #inline = from&.clone();
    
    fn items(self: *Self) Slice(T) #inline = {
        self.maybe_uninit.slice(0, self.len)
    }
    fn items(self: Self) Slice(T) #inline = self&.items();
    
    // A pointer to the first element of the list. 
    // SAFETY: dont use the pointer if the list is empty. invalidated by resize. 
    fn as_ptr(self: *Self) *T = self.maybe_uninit.ptr;
    
    #inline
    fn index(self: *Self, i: i64) *T #inline = {
        @safety(.Bounds) i.ult(self.len);
        self.maybe_uninit.index_unchecked(i)
    }
    
    // TODO: hack because auto deref doesn't work through function calls
    fn index(self: Self, i: i64) *T #inline = {
        @safety(.Bounds) i.ult(self.len);
        self.maybe_uninit.index_unchecked(i)
    }
    
    fn len(self: Self) i64 #inline #fold = self.len;
    fn len(self: *Self) i64 #inline = self.len;

    ::DeriveAsSliceIndexable(Self, T);
    ::DeriveAsSliceIndexable(*Self, T);

    fn reserve(self: *Self, extra: i64) void #inline = {
        if self.maybe_uninit.len < self.len + extra {
            reserve_expand_erased(Self.raw_from_ptr(self), extra, T.size_of());
        };
    }

    fn push_assume_capacity(self: *Self, v: T) void #inline = {
        @debug_assert(self.len < self.maybe_uninit.len);
        self.maybe_uninit.index_unchecked(self.len)[] = v;
        self.len += 1;
    }
    
    fn push(self: *Self, v: T) void #inline = {
        dest := self.push_uninit();
        dest[] = v;
    }
    
    fn push_uninit(self: *Self) *T = {
        reserve(self, 1);
        last := self.maybe_uninit.index_unchecked(self.len); 
        self.len += 1;
        last
    }
    
    fn index_unchecked(self: Self, i: i64) *T = 
        self.maybe_uninit.index_unchecked(i);
    
    fn pop_last(self: *Self) ?T #inline = {
        ::if(?T);
        if(eq(self.len, 0), => .None) {
            i := sub(self.len, 1);
            self.len = i;
            (Some = self.maybe_uninit.get(i))
        }
    }
    fn pop(self: *Self) ?T #inline = self.pop_last();
    
    fn ordered_remove(self: *Self, index: i64) ?T = {
        ::if(?T);
        @safety(.Bounds) index >= 0;
        if(index.ge(self.len), => .None) {
            current := self.len.sub(1);
            out := T.zeroed();
            while(=> current.ne(index.sub(1))) {
                if current >= 0 {
                    next_out := self[current];
                    self[current] = out;
                    out = next_out;
                };
                current -= 1;
            };
            
            self.len -= 1;
            (Some = out)
        }
    }
    
    // just swap in the last thing so you don't have to shift all the later elements.
    fn unordered_remove(self: *Self, index: i64) ?T #inline = {
        item := self.items().unordered_remove(index);
        self.len -= int(item&.is_some());
        item
    }
    
    fn unordered_remove_discard(self: *Self, i: i64) void #inline = {
        s := self.items();
        s&.unordered_remove_discard(i);
        self.len -= 1;
    }
    
    fn insert(self: *Self, idx: i64, e: T) void #inline = {
        self.reserve(1);
        self.maybe_uninit.copy_overlapping(idx + 1, idx, self.len - idx);
        self.len += 1;
        self[idx] = e;
    }
    
    fn push_all(self: *Self, new: Slice(T)) void = {
        reserve(self, len(new));
        dest := subslice(self.maybe_uninit, self.len, len(new));
        dest.copy_from(new);
        self.len += new.len;
    }
    
    fn push_repeated(self: *Self, count: i64, value: T) void = {
        reserve(self, count);
        range(self.len, self.len + count) { i |
            self.maybe_uninit.index_unchecked(i)[] = value;
        };
        self.len += count;
    }
    
    // note: this doesn't drop the entries. 
    fn drop(self: *Self) void = {
        if self.maybe_uninit.len != 0 {
            self.gpa.dealloc(T, self.maybe_uninit);
            self.maybe_uninit.len = 0;
            self.len = 0;
        };
    }
    
    /// Retains capacity
    fn clear(self: *Self) void = {
        self.len = 0;
    }
    
    fn add_unique(self: *Self, t: T) bool = {
        each self { check |
            if(check == t&, => return(false));
        };
        self.push(t);
        true
    }
    
    fn ordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        s := self.items();
        ordered_retain(s&, should_keep);
        self.len = s.len;
    }
    
    fn unordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        s := self.items();
        unordered_retain(s&, should_keep);
        self.len = s.len;
    }
    
    // :SLOW don't copy twice if had to resize for this
    fn slowly_prepend_all(self: *Self, s: []T) void = {
        self.reserve(s.len);
        self.maybe_uninit.copy_overlapping(s.len, 0, self.len);
        self.maybe_uninit.slice(0, s.len).copy_from(s);
        self.len += s.len;
    }
    
    fn try_push(self: *Self, t: T) bool = {
        if(self.len >= self.cap, => return(false));
        self.push_assume_capacity(t);
        true
    }
    
    Self
}

// :Polymorphise
// The rust stdlib Vec does this thing where you manually outline the slow path, so it probably helps a bit. 
// this is more aggressive and not generating it for every single type.
fn reserve_expand_erased(list: rawptr, extra: i64, size_of: i64) void = {
    ListHeader :: @struct(ptr: rawptr, cap: i64, len: i64, gpa: Alloc);
    list := ListHeader.ptr_from_raw(list);
    cap := 4.max(list.cap.add(extra).max(list.cap.mul(2)));
    new_memory := alloc_raw(list.gpa, cap * size_of, 8); // TODO: alignment
    // TODO: use the extra space
    new := u8.raw_from_ptr(new_memory.ptr);
    copy_no_alias(new, list.ptr, list.len * size_of);
    @if(SLOW_MEMORY_DEBUGGING && !IS_BOOTSTRAPPING) 
        new_memory.rest(list.len * size_of).set_bytes(SLOW_MEMORY_JUNK);
    dealloc_raw(list.gpa, list.ptr, list.cap * size_of, 8); // TODO: alignment
    list.ptr = new;
    
    // :UpdateBoot we can't just use new.len/size because the old one didn't return a slice
    list.cap = @if(has_feature("@franca/alloc_slice"), new_memory.len / size_of, cap);
}

fn alloc_extra(a: Alloc, $T: Type, cap: i64) []T #generic = {
    mem := a.alloc_raw(cap * size_of(T), align_of(T));
    len := @if(has_feature("@franca/alloc_slice"), mem.len / size_of(T), cap);
    (ptr = ptr_cast_unchecked(u8, T, mem.ptr), len = len)
}

// This is like zigs ArrayListUnmanaged, so you don't have to store a billion copies of the same allocator. 
// Originally it carefully matched rust's abi and its a pain to change now so that's why the fields are in a strange order. TODO: fix eventually. 
fn RawList($T: Type) Type = {
    Self :: @struct(cap: i64, ptr: *T, len: i64, Element :: T);
   
    fn init(a: Alloc, cap: i64) Self = {
        m := a.alloc_extra(T, cap);
        (ptr = m.ptr, len = 0, cap = m.len)
    }
    
    fn items(self: Self) Slice(T) #inline = (ptr = self.ptr, len = self.len);
    fn items(self: *Self) Slice(T) #inline = (ptr = self.ptr, len = self.len);
    
    fn as_raw(self: List(T)) Self #inline = self&.as_raw();
    
    fn as_raw(self: *List(T)) Self #inline = 
        (cap = self.maybe_uninit.len, ptr = self.maybe_uninit.ptr, len = self.len);
    
    // TODO: name this assume_owned too? 
    fn as_raw_list(self: Slice(T)) Self #inline = 
        (cap = self.len, ptr = self.ptr, len = self.len);
    
    fn empty() Self #fold = 
        Self.zeroed();
    
    fn assume_owned(self: Self, by: Alloc) List(T) #inline = 
        (maybe_uninit = (ptr = self.ptr, len = self.cap), len = self.len, gpa = by);
        
    fn clear(self: *Self) void = {
        self.len = 0;
    }
    
    fn len(self: Self) i64 #fold = self.len;
    fn len(self: *Self) i64 = self.len;
    
    fn index(self: Self, i: i64) *T #inline = self.items().index(i);
    fn index(self: *Self, i: i64) *T #inline = self[].items().index(i);
    
    fn index_unchecked(self: Self, i: i64) *T = 
        self.ptr.offset(i);
        
    ::DeriveAsSliceIndexable(Self, T);
    ::DeriveAsSliceIndexable(*Self, T);
    
    // This type doesn't store the allocator so you have to pass it in every time.
    fn push(self: *Self, t: T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.push(t);
        self[] = s.as_raw();
    }
    
    fn insert(self: *Self, i: i64, t: T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.insert(i, t);
        self[] = s.as_raw();
    }
    
    fn push_all(self: *Self, t: []T, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.push_all(t);
        self[] = s.as_raw();
    }
    
    fn push_all_assume_capacity(self: *Self, t: []T) void = {
        @debug_assert(self.len >= 0 && t.len >= 0 && self.len + t.len <= self.cap);
        self.len += t.len;
        self.items().slice(self.len-t.len, self.len).copy_from(t);
    }
    
    fn ordered_remove(self: *Self, i: i64) ?T = {
        s := self[].assume_owned(panicking_allocator); 
        t := s&.ordered_remove(i);
        self[] = s.as_raw();
        t
    }
    
    fn truncate(self: *Self, i: i64) void = {
        if(self.len < i, => return());
        self.len = i;
    }
    
    fn add_unique(self: *Self, t: T, in: Alloc) bool = {
        s := self[].assume_owned(in);
        added := s&.add_unique(t);
        self[] = s.as_raw();
        added
    }
    
    fn drop(self: *Self, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.drop();
        self[] = empty();
    }
    
    fn ordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        s := self.items();
        ordered_retain(s&, should_keep);
        self.len = s.len;
    }
    
    fn pop(self: *Self) ?T = {
        s := self[].assume_owned(panicking_allocator);
        res := s&.pop();
        self[] = s.as_raw();
        res
    }
    
    fn reserve(self: *Self, extra: i64, in: Alloc) void = {
        s := self[].assume_owned(in);
        s&.reserve(extra);
        self[] = s.as_raw();
    }
    
    fn unordered_retain(self: *Self, $should_keep: @Fn(t: *T) bool) void = {
        s := self.items();
        unordered_retain(s&, should_keep);
        self.len = s.len;
    }
    
    fn next_uninit(self: *Self) ?*T #inline = {
        if(self.len >= self.cap, => return(.None));
        self.len += 1;
        (Some = self.index(self.len - 1))
    }
    
    fn slice(self: Self, first: i64, past_last: i64) []T #inline = 
        self.items().slice(first, past_last);
    
    fn push_assume_capacity(self: *Self, v: T) void #inline = {
        @debug_assert(self.len < self.cap);
        self.ptr.slice(self.cap).index_unchecked(self.len)[] = v;
        self.len += 1;
    }
    
    fn try_push(self: *Self, t: T) bool = {
        if(self.len >= self.cap, => return(false));
        self.push_assume_capacity(t);
        true
    }
        
    Self
}

#where(PointerTo(IsCollection(List)))
fn deep_clone(out: ~S, self: S, a: Alloc) void = {
    s := self.items();
    tmp := @uninitialized(@type s);
    deep_clone(tmp&, s&, a);
    out[] = tmp.assume_owned(a);
}

#where(PointerTo(IsCollection(RawList)))
fn deep_clone(out: ~S, self: S, a: Alloc) void = {
    s := self.items();
    tmp := @uninitialized(@type s);
    deep_clone(tmp&, s&, a);
    out[] = tmp.as_raw_list();
}

fn split(haystack: Str, needle: Str, gpa: Alloc) List(Str) = {
    @safety(.Bounds) needle.len > 0;
    lines: List(Str) = list(1, gpa);
    i := 0;
    last := haystack.len - needle.len;
    start := 0;
    while => i <= last {
        continue :: local_return;
        if haystack[i] == needle[0] {
            check := haystack.subslice(i + 1, needle.len - 1);
            if check == needle.rest(1) {
                found := haystack.subslice(start, i - start);
                lines&.push(found);
                i += needle.len;
                start = i;
                continue();
            };
        };
        i += 1; 
    };
    found := haystack.subslice(start, max(0, haystack.len - start));
    lines&.push(found);
    lines
}

fn concat(bytes: ~SSu8, in: Alloc) []u8 #where = {
    size := 0;
    for(bytes, fn(it) => { size += it.len });
    out := in.alloc_uninit(u8, size);  // allocate exactly the right size so the caller can free it
    size := 0;
    for bytes { it |
        out.subslice(size, it.len).copy_from(it);
        size += it.len;
    };
    out
}

// It's such a pain to google an ascii table every time.  
// Since this is #fold, the length check happens during compilation if the argument to a call is a constant.
// So this function can take the place of a char literal in other languages without new syntax. 
fn char(s: Str) i64 #fold = {
    // TODO: can't get field of const cause cant take pointer. 
    // TODO: @assert that takes a format string so its less painful to print the arg as well. 
    assert(s.len().eq(1), "char str expected length 1. TODO: utf8"); 
    s[0].zext()
}

// The intent is that you always call this with a constant argument and use it like C's '' literals.
fn ascii(s: Str) u8 #fold = {
    c: i64 = s[0].zext();
    // TODO: safety checked intcast instead of trunc?
    assert(s.len().eq(1), "ascii str expected length 1"); 
    assert(c.lt(128), "expected ascii"); // TODO: make an @assert fmt version. 
    c.trunc()
}

fn reserve_type(bytes: *List(u8), $T: Type) *T #generic = {
    it := bytes.reserve_type(T, 1);
    it.ptr
}

fn reserve_type(bytes: *List(u8), $T: Type, n: i64) []T #generic = {
    bytes.reserve(T.size_of() * n);
    ptr := bytes.maybe_uninit.ptr.offset(bytes.len);
    // TODO: is alignment real?
    //@debug_assert(u8.int_from_ptr(ptr).mod(T.align_of()) == 0, "unaligned reserve");
    ptr := ptr_cast_unchecked(u8, T, ptr);
    bytes.len += T.size_of() * n;
    ptr.slice(n)
}

fn zero_pad_to_align(bytes: *List(u8), align: i64) void = {
    ptr := bytes.maybe_uninit.ptr.offset(bytes.len);
    extra := u8.int_from_ptr(ptr).mod(align);
    if extra != 0 {
        extra := align - extra;
        bytes.reserve(extra);
        range(0, extra) { _ |
            bytes.push(0);
        };
    };
    @debug_assert_eq(bytes.len.mod(align), 0, "alignment confusion. allocation aligned differently from our length.");
}

fn zero_pad_len_to_align(bytes: *List(u8), align: i64) void = {
    extra := bytes.len.mod(align);
    if extra != 0 {
        extra := align - extra;
        bytes.push_zeroes(extra);
    };
}

fn push_zeroes(self: *List(u8), count: i64) void = {
    self.reserve(count);
    range(0, count, fn(_) => self.push(0));  // TODO: be less silly 
}