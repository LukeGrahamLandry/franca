// TODO: These are just cringe forward declarations to create the overload sets. 
fn for();
fn each();
fn enumerate();
fn get();
fn set();
fn subslice();
fn index();
fn slice();
fn len();
fn copy_from();
fn rest();
fn find();
fn last();
fn is_empty();
fn slice_last();
fn empty() void;
fn each_rev();
fn contains();
fn index_unchecked();
fn interpret_as_bytes();
fn position();
fn enumerate_rev();
fn for_rev();
fn set_zeroed();
fn copy_overlapping();
fn slice_pop_first();

fn range(start: i64, end: i64, $yield: Fn(i64, void)) void = {
    i := start;
    while(=> i.lt(end)) {
        yield(i);
        i = i.add(1);
    };
}

fn inline_range($start: i64, $end: i64, $yield: @Fn(i: i64) void) void = {
    @inline_range(start, end) yield;
}

fn range_rev(start: i64, end: i64, $yield: Fn(i64, void)) void = {
    i := end - 1;
    while(=> i >= start) {
        yield(i);
        i -= 1;
    };
}

// TODO: PROBLEM: now that im not just taking the first overload, this not using comptiem cache because its not #generic so multiple versions of inner functions can get made 
//       I still want to replace @comptiem with just having const args but need to make @impl use the cache. 

fn single();

// :blessed: String literals and @slice create values of this type. 
fn Slice($T: Type) Type = {
    Self :: @struct(ptr: *T, len: i64, Element :: T);
    
    ::ptr_utils(T);
    
    fn slice(ptr: *T) Self = (ptr = ptr, len = 1);
    fn slice(ptr: *T, count: i64) Self = (ptr = ptr, len = count);
    // TODO: this is jail if you pass it to rust
    fn empty() Self #fold = Self.zeroed();
    
    fn as_ptr(self: Self) *T #inline = self.ptr;
    
    fn len(self: *Self) i64 #inline = self.len;
    fn index(self: *Self, i: i64) *T #inline =  {
        @safety(.Bounds) i < self.len;
        self.ptr.offset(i)
    }
    
    fn len(self: Self) i64 #inline #fold = self.len;
    fn index(self: Self, i: i64) *T #inline = {
        @safety(.Bounds) i < self.len;
        self.ptr.offset(i)
    }
    
    fn index_unchecked(self: Self, i: i64) *T = 
        self.ptr.offset(i);
    
    ::DeriveIndexable(Self, T);
    ::DeriveIndexable(*Self, T); 
    
    fn single(t: T, a: Alloc) Self = 
        slice(t&, 1).clone(a).items();
    
    fn slice(self: Self, first: i64, past_last: i64) Self = {
        // TODO: first should be lt
        // TODO: its sad that im doing double bounds check in common case where its just index()
        @safety(.Bounds) first.le(self.len()).and(past_last.le(self.len())); 
        (ptr = self.ptr.offset(first), len = past_last - first) 
    }
    
    fn subslice(self: Self, first: i64, count: i64) Self #inline = 
        self.slice(first, first.add(count));
    
    fn rest(self: Self, first: i64) Self #inline = 
        self.slice(first, self.len());
    
    fn slice_last(self: Self, count: i64) ?Self = {
        ::if(?Self);
        if(self.len < count, => .None) {
            (Some = self.slice(self.len.sub(count), self.len))
        }
    }
    
    fn interpret_as_bytes(self: Self) []u8 = {
        ptr := ptr_cast_unchecked(T, u8, self.ptr);
        (ptr = ptr, len = self.len * :: T.size_of())
    }
   
    fn copy_from(dest: Self, src: Self) void #inline = {
        @safety(.Bounds) dest.len == src.len;
        // TODO: make this less painful to look at
        //copy_aligned(T.raw_from_ptr(dest.ptr), T.raw_from_ptr(src.ptr), dest.len * T.size_of(), T.align_of());
        copy_overlapping(T.raw_from_ptr(dest.ptr), T.raw_from_ptr(src.ptr), dest.len * T.size_of());
    }
    
    fn copy_overlapping(self: Self, dest: i64, src: i64, len: i64) void = {
        dest := self.slice(dest, dest + len);
        src  := self.slice(src,  src  + len);
        dest.copy_from(src);
    }
    
    // note: you can't call T.zeroed() here because it wants to comptime call us. 
    fn set_zeroed(self: Self) void #inline = 
        self.interpret_as_bytes().set_bytes(0);
  
    // TODO: this api involves hella copying. caller should pass in a list of BakedEntry. :SLOW
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = 
        bake_relocatable_slice_erased(T.raw_from_ptr(self.ptr), self.len, T);
        
    fn shallow_copy(self: Self, a: Alloc) Self = {
        buf := a.alloc(T, self.len);
        buf.copy_from(self);
        buf
    }
    
    Self
}

IsSlice :: IsCollection(Slice);
PointerTo :: fn($Inner: @Fn(t: Type) bool) = (fn(T: Type) = is_ptr(T) && Inner(Deref(T)));
IsCollection :: fn($Inner: @Fn(e: Type) Type) = (fn(T: Type) bool = {
    check :: fn($T: Type) Type = Inner(T.Element);
    has_const_field(T, @symbol Element) && invoke_specialized(Type, Type, check, T) == T
});

fn slice_pop_first(self: ~T) T #where(IsSlice) = {
    self.ptr = self.ptr.offset(1);
    self.len -= 1;
    self
}

fn shallow_copy();

fn deep_clone(self: ~S, a: Alloc) Deref(S) #where(PointerTo(IsSlice)) = {
    T :: Deref(S).Element;
    s := self[].shallow_copy(a);
    @if(T.has_pointers()) {
        enumerate self { i, e |
            s[i] = e.deep_clone(a);
        };
    };
    s
}

// TODO: make compile error here show the right location. 
fn bake_relocatable_slice_erased(addr: rawptr, length: i64, T: Type) []BakedEntry = {
    ::List(BakedEntry); ::if(BakedEntry); ::if(BakedVarId);
    
    // TODO: StructName.(initilizer) so you're not forced to use @as or put the type on the other side? 
    len: BakedEntry = (Num = (value = length));
    ptr: BakedEntry = if length == 0 {
        // TODO: should we make this non-zero so you can pass them to rust? 
        (Num = (value = 0))
    } else {
        BIG :: 104857600;
        addr := int_from_rawptr(addr);
        if length > BIG && addr < BIG {
            @println("(ptr = %, len = %) is a really weird slice. are you sure you don't have the components flipped?", addr, length); 
        };
        :: ?BakedVarId;
        // TODO: this is wrong because what if someone previously baked a smaller new into the same memory.   :FUCKED -- Jul 16
        //       we need to still emit and compare to make sure we would have baked the same thing? 
        //       should that be a compile error or just have two? what if someone was relying on them being aliased? 
        id := or lookup_baked(addr) { () BakedVarId |
            info := get_meta(T);
            size_of: i64 = info.stride_bytes.zext();
            // TODO: assert alignment
            // TODO: assert len isnt big enough to be a pointer?
            // TODO: need to check against c.baked.lookup (the first ptr not each individual element) :BLOAT
            bytes: []u8 = (ptr = u8.ptr_from_int(addr), len = length * size_of); // TODO: decide if i need to clone. 
            v: BakedVarId = if info.contains_pointers {
                parts: List(BakedEntry) = list(ast_alloc());
                off := 0;
                range(0, length) { _ | 
                    bytes := bytes.slice(off, off + size_of);
                    // TODO: its a bit un-intuative that you can't just call bake_relocatable_value(v), the dyn version uses the compiler auto implementation if none exists. 
                    entries := dyn_bake_relocatable_value(bytes, T, false); // TODO: default arg. TTODO: explain somewhere
                    parts&.push_all(entries);
                    off += size_of;
                };
                // TODO: without cast
                bake_value(@as(BakedVar) (VoidPtrArray = parts.as_raw()))
            } else {
                bake_value(@as(BakedVar) (Bytes = bytes.as_raw_list())) // TODO: this should take jit_ptr param? even tho its sad if you had to do work just for me to throw away. and then would want to warn if conflicting handlers or it could be super confusing.  -- Jun 30
            };
            
            cache_baked(addr, v);
            v
        };

        (AddrOf = id)
    };
    
    @slice(ptr, len) ast_alloc()
}

// Create a slice containing `elements` in memory provided by `allocator`.  
// The first element is used for type inference. TODO: let macros access the expression's result location type. 
// 
// this is better than making an @slice on the stack and then copying. 
// but also @slice on the stack miscompiles if the length is > 50. 
//
#macro fn slice(elements: FatExpr, allocator: FatExpr) FatExpr = {
    e := elements&.items();
    e[0] = compile_ast(e[0]);
    @{
        T :: @[@literal e[0].ty];
        mem := @[allocator].alloc(T, @[@literal e.len]);
        indexer :: resolve_overload(index, Ty([]T, i64), *T, @[@literal elements.loc]);
        @[{
            stmts := FatStmt.list(e.len, ast_alloc());
            enumerate e { i, e |
                stmts&.push(loc = e.loc, stmt = (Set = (place = @{ indexer(mem, @[@literal i])[] }, value = e[])));
            };
            make_block(stmts, @{mem})
        }]
    }
}

#macro fn list(elements: FatExpr, allocator: FatExpr) FatExpr = @{
    a := @[allocator];
    s := @slice(@[elements]) a;
    s: List(@type s[0]) = (maybe_uninit = s, len = s.len, gpa = a);
    s
};

#macro fn const_slice(elements: FatExpr) FatExpr = @{
    :: @slice(@[elements]) ast_alloc()
};

// TODO: can't do this in fn Slice because it tries to deal with all the overloads too soon? 
//       oh no its cause it conflicts with Slice(u8)
fn display_slice($T: Type) void = {
    fn display(self: Slice(T), writer: *List(u8)) void = {
        "[".display(writer);
        prefix := "";
        each self { e | 
            display(prefix, writer);
            display(e, writer);
            prefix = ", ";
        };
        "]".display(writer);
    }
    
    fn display(self: *Slice(T), writer: *List(u8)) void = 
        self[].display(writer);
}

/// A null terminated sequence of bytes.
CStr :: @struct(ptr: *u8);
fn as_ptr(s: CStr) *u8 = s.ptr;
/// When emitting an aot constant, include all the way until a zero, not just a single byte like it would for a normal *u8. 
fn bake_relocatable_value(self: *CStr) Slice(BakedEntry) = {
    ::List(BakedEntry); // :sema_regression but might just be order. 
    if self.ptr.is_null() {
        ptr: BakedEntry = (Num = (value = 0));
        return(ptr.repeated(1, temp()).items());
    };
    addr := u8.int_from_ptr(self.ptr);
    // TODO: this is a bit scary because what if someone emitted the same string as a slice before so didn't include the zero terminator
    //if lookup_baked(addr) { v | 
    //    ptr: BakedEntry = (AddrOf = v);
    //    return(ptr.repeated(1, temp()).items());
    //};
    
    // TODO: need to check against  c.baked.lookup :BLOAT
    bytes: List(u8) = list(ast_alloc());
    for self[] { b|
        bytes&.push(b);
    };
    bytes&.push(@as(u8) 0.trunc());
    value := bake_value(@as(BakedVar) (Bytes = bytes.as_raw()));
    cache_baked(addr, value);
    ptr: BakedEntry = (AddrOf = value);
    ptr.repeated(1, temp()).items()
}

Str :: Slice(u8);

fn as_array();

// A value type fixed size array. Like [T; len] in Rust or [len] T in Zig.
fn Array($T: Type, $count: i64) Type = {
    Self :: {
        // TODO: HACK. I should be able to call intern_type directly but can't call deconstruct_values on a tagged thing. this mostly just works by luck. -- May 27
        info: TypeInfo = (Array = (inner = T, len = count.trunc()));
        intern_type_ref(info&)
    };
    
    // TODO: these inits where it returns by value are dumb. have an @array instead. 
    
    fn init(elements: Slice(T)) Self #inline = {
        @assert_eq(elements.len, count, "arrays have fixed length"); // TODO: better message 
        self := @uninitialized Self;
        enumerate(elements){ i, t |
            self&.index(i)[] = t[];
        };
        self
    }
    
    fn init(copied: T) Self = {
        self := @uninitialized Self;
        range(0, count){ i |
            self&.index(i)[] = copied;
        };
        self
    }
    
    /// If a slice is the right length, it can be cast to a pointer to an array. 
    fn as_array(elements: Slice(T)) ?*Self = {
        :: if(?*Self);
        if(elements.len.ne(count), => .None) {
            (Some = ptr_cast_unchecked(From = T, To = Self, ptr = elements.ptr))
        }
    }
    
    fn eq(lhs: Self, rhs: Self) bool = lhs&.items().eq(rhs&.items());
    
    ::ptr_utils(T);
    fn index(self: *Self, i: i64) *T #inline = {
        @safety(.Bounds) i < count;
        self.as_ptr().offset(i)
    }
    
    fn index_unchecked(self: *Self, i: i64) *T = {
        self.as_ptr().offset(i)
    }

    fn len(self: *Self) i64 #inline = count;
    
    fn items(self: *Self) Slice(T) #inline = (ptr = self.as_ptr(), len = count);
    
    fn as_ptr(self: *Self) *T #inline = ptr_cast_unchecked(From = Self, To = T, ptr = self);
    fn set_zeroed(self: *Self) void #inline = self.items().set_zeroed();
    
    // TODO: instead of generating whole new versions of all these functions,
    //       have a DeriveAsSliceIndexable that just aliases Self as a Slice(T) and reuses the slice ones. 
    //       can use that for List too. 
    ::DeriveIndexable(*Self, T);
    
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = {
        parts: List(BakedEntry) = list(ast_alloc());
        each self { v | 
            bytes := T.cast_to_bytes(v);
            entries := dyn_bake_relocatable_value(bytes, T, false);
            parts&.push_all(entries);
        };
        parts.items()
    }
    
    Self
}

fn HashEach($Collection: Type, $Hasher: Type) void = {
    fn hash(hasher: *Hasher, self: *Collection) void = {
        each self[] { t |
            hasher.hash(t);
        };
    }
}

/*
// TODO: this would be an easy start for traits. 
fn resolve(e: FatExpr) FatExpr #macro = {
    assert(e.expr&.is(.Closure), "@resolve expected function sig");
    func := e.expr.Closure;
    todo()
}
:: @resolve fn index(self: Self) *T => ();
:: @resolve fn len(self: Self) i64  => ();
*/

fn index_of();

// Note: Passing *Self and Self mean different things. Containers that are already logically a pointer may want both for convenience. 
// TODO: trait bounds. fn index(Self) *T; fn len(Self) i64;
// IMPORTANT: DO NOT impl this for a linked list! It expects fast random access to the collection!
fn DeriveIndexable($Self: Type, $T: Type) void = {
    fn set(self: Self, i: i64, v: T) void #inline = {
        self[i] = v; 
    }
    
    fn get(self: Self, i: i64) T #inline = self[i];
    
    // TODO: the other fn type syntax so you can name the arguments in the closure as documentation. 
    // TODO: be more consistant about how tuples are expanded into args to fix Fn((a, b), c) === Fn(a, b, c) when you want Fn(Ty(a, b), c)
    //       seems like a massive mistake that I create ambiguity and then just guess something stupid and hope for the best. 
    fn enumerate(arr: Self, $f: @Fn(i: i64, e: *T) void) void = {
        i := 0;
        start_length := arr.len();
        while => i < start_length {
            f(i, arr.index(i));
            i = i.add(1);
        };
        // TODO: new comptime jit hits `Poison expression InProgressMacro` here :SemaRegression
        // @debug_assert(arr.len() == start_length, "it's actually Ko to mutate an array while iterating over it");
    }
    
    // TODO: I don't love that I have to write the deref version seperatly
    //       but I dont have let destructuring (only function args) so its a pain to take tuples by pointer. 
    //       I should be able to destructure Ptr(A, B) to (Ptr(A), Ptr(B))
    fn for(arr: Self, $f: @Fn(e: T) void) void = 
        each(arr, fn(v) => f(v[]));
    
    fn each(arr: Self, $f: Fn(*T, void)) void = {   
        i := 0;
        start_length := arr.len();
        while => i < start_length {
            f(arr.index(i));
            i = i.add(1);
        };
        // TODO: new comptime jit hits `Poison expression InProgressMacro` here :SemaRegression
        // @debug_assert(arr.len() == start_length, "it's actually Ko to mutate an array while iterating over it");
    }
    
    fn enumerate_rev(arr: Self, $f: @Fn(i64, *T) void) void = {
        i := arr.len() - 1;
        while => i >= 0 {
            f(i, arr.index(i));
            i -= 1;
        };
    }
   
    fn for_rev(arr: Self, $f: @Fn(T) void) void = 
        enumerate_rev(arr, fn(_, t) => f(t[]));
     
    fn contains(self: Self, needle: *T) bool = 
        self.index_of(needle).is_some();
    
    fn index_of(self: Self, needle: T) ?i64 =
        index_of(self, needle&);
     
    fn index_of(self: Self, needle: *T) ?i64 = {
        enumerate self { i, check |
            if(check == needle, => return(Some = i));
        };
        .None
    }
    
    fn contains(self: Self, needle: T) bool = 
        self.contains(needle&);
    
    fn contains(self: Self, needle: []T) bool = {
        if(self.len < needle.len, => return(false));
        range(0, self.len - needle.len + 1) { i |
            continue :: local_return;
            enumerate needle { j, a | 
                if(!(a == self.index(i + j)), => continue()); 
            };
            return(true);
        };
        false
    }
    
    fn contains(self: Self, $where: @Fn(e: *T) bool) bool = {
        each self { check |
            if(where(check), => { return(true); });
        };
        false
    }
    
    // TODO: add addr_eq/identical if that's really what you want but like... it never is I feel. 
    // TODO: for slices, if !contains_ptrs && !has_padding && no eq special overloads, just call some vectorized memcmp and skip a bunch of pointless code gen work. 
    // TODO: if it has a trivial .items() (like lists, RawList), it would be better to convert and call eq on that 
    //       so you don't have to generate this function many times. cause those versions wouldn't be easy to deduplicate later. 
    fn eq(lhs: Self, rhs: Self) bool = {
        if(lhs.len != rhs.len, => return(false));
        // TODO: can only do this for slice. could for list too if we expose like fn identity_ptr maybe? 
        //l_ptr := T.int_from_ptr(lhs.ptr);
        //r_ptr := T.int_from_ptr(rhs.ptr);
        //if(l_ptr == r_ptr, => return(true)); // TODO: is this too creepy? it means we might skip calling your eq overload. 
        
        range(0, lhs.len) { i |
            // @safety(.NonCompliantOverload) lhs.len == rhs.len;  // TODO: decide if i want to do this. 
            if(!(lhs.index(i) == rhs.index(i)), => return(false));
        };
        true
    }
    
    fn ne(lhs: Self, rhs: Self) bool #inline = 
        !lhs.eq(rhs);
    
    fn last(self: Self) ?*T = {
        ::if(?*T);
        if(self.is_empty(), => .None) {
            (Some = self.index(self.len.sub(1)))
        }
    }
    
    fn is_empty(self: Self) bool = self.len().eq(0);
    
    // TODO: allow putting the $ on the arg in @Fn?
    /// This is special because it ensures the arg value is comptime known in the body.
    fn inline_for($self: Self, $body: @Fn(f: *T) void) void = {
        @inline_for(self) body;
    }
    
    fn position(self: Self, $cond: @Fn(v: *T) bool) ?i64 = {
        enumerate self { i, v |
            if(cond(v), => return(Some = i));
        };
        .None
    }
    
    fn reverse(self: Self) void = {
        // odd rounds down so middle element stays in place
        range(0, self.len() / 2) { i |
            self.swap(i, self.len() - i - 1);
        };
    }
    
    fn swap(self: Self, i: i64, j: i64) void #inline = {
        if(i == j, => return());
        temp := self[i];
        self[i] = self[j];
        self[j] = temp;
    }
}

// TODO: constraint
fn each_rev(arr: ~T, $f: @Fn(t: *(@if(is_ptr(T), Deref(T), T)).Element) void) void #where = 
   enumerate_rev(arr, fn(_, t) => f(t));

fn reverse();
fn swap();

fn ends_with(haystack: Str, needle: Str) bool = 
    if(haystack.len < needle.len, => false) {
        start := subslice(haystack, sub(len(haystack), len(needle)), len(needle)); 
        eq(start, needle)
    };

// TODO: these should just be on Slice(T)
fn starts_with(haystack: Str, needle: Str) bool = 
    haystack.len >= needle.len && haystack.slice(0, needle.len) == needle;

::display_slice(i64);

fn largest_below(options: []i64, cap: i64) i64 = {
    n := 0;
    for options { i | 
        if i < cap && i > n {   
            n = i
        };
    };
    n
}

fn pop_type(bytes: *[]u8, $T: Type) *T #generic = {
    @debug_assert(bytes.len >= T.size_of(), "pop_type: not enough bytes");
    //@debug_assert(u8.int_from_ptr(bytes.ptr).mod(T.align_of()) == 0, "pop_type: unaligned"); // TODO: is alignment real? 
    start := bytes.ptr;
    bytes.ptr = bytes.ptr.offset(T.size_of());
    bytes.len -= T.size_of();
    ptr_cast_unchecked(u8, T, start)
}

fn is_all_zeroes(buf_in: []u8) bool = {
    chunks, extra := div_mod(buf_in.len, 8);
    buf: []i64 = (ptr = ptr_cast_unchecked(u8, i64, buf_in.ptr), len = chunks);  // TODO: is alignment real?
    for buf { b |
        if(b != 0, => return(false));
    };
    buf := buf_in.rest(buf_in.len - extra);
    for buf { b |
        if(b != 0, => return(false));
    };
    true
}

fn starts_with(haystack: CStr, needle: Str) bool = {
    range(0, needle.len) { i |
        if(haystack[i] != needle[i] || haystack[i] == 0, => return(false));
    };
    true
};
