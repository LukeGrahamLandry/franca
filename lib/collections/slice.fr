// TODO: These are just cringe forward declarations to create the overload sets. 
fn for();
fn each();
fn enumerate();
fn get();
fn set();
fn fill();
fn subslice();
fn index();
fn slice();
fn len();
fn copy_from();
fn rest();
fn find();
fn last();
fn is_empty();
fn slice_last();
fn empty() void;
fn each_rev();
fn contains();
fn index_unchecked();
fn interpret_as_bytes();
fn position();
fn enumerate_rev();
fn for_rev();

fn range(start: i64, end: i64, $yield: Fn(i64, void)) void = {
    i := start;
    while(=> i.lt(end)) {
        yield(i);
        i = i.add(1);
    };
}

fn range_rev(start: i64, end: i64, $yield: Fn(i64, void)) void = {
    i := end - 1;
    while(=> i >= start) {
        yield(i);
        i -= 1;
    };
}

// TODO: PROBLEM: now that im not just taking the first overload, this not using comptiem cache because its not #generic so multiple versions of inner functions can get made 
//       I still want to replace @comptiem with just having const args but need to make @impl use the cache. 

fn single();

// :blessed: String literals and @slice create values of this type. 
fn Slice($T: Type) Type = {
    Self :: @struct(ptr: *T, len: i64);
    
    ::ptr_utils(T);
    
    fn slice(ptr: *T) Self = (ptr = ptr, len = 1);
    fn slice(ptr: *T, count: i64) Self = (ptr = ptr, len = count);
    // TODO: this is jail if you pass it to rust
    fn empty() Self = Self.zeroed();
    
    
    fn len(self: *Self) i64 #inline = self.len;
    fn index(self: *Self, i: i64) *T #inline =  {
        @safety(.Bounds) i < self.len;
        self.ptr.offset(i)
    }
    
    fn len(self: Self) i64 #inline #fold = self.len;
    fn index(self: Self, i: i64) *T #inline = {
        @safety(.Bounds) i < self.len;
        self.ptr.offset(i)
    }
    
    fn index_unchecked(self: Self, i: i64) *T = {
        self.ptr.offset(i)
    }
    
    ::DeriveIndexable(Self, T);
    ::DeriveIndexable(*Self, T); 
    
    fn single(t: T, a: Alloc) Self = 
        slice(t&, 1).clone(a).items();
    
    fn slice(self: Self, first: i64, past_last: i64) Self = {
        // TODO: first should be lt
        // TODO: its sad that im doing double bounds check in common case where its just index()
        @safety(.Bounds) first.le(self.len()).and(past_last.le(self.len())); 
        new_ptr := self.ptr.offset(first);
        // TODO: shouldn't need the extra binding. broke with scan ahead consts / renumber comptime. 
        //       only for tests/fmt.fr, that can't be the only place that calls items(), some scary ordering thing? -- Apr 22
        res: Self = (ptr = new_ptr, len = past_last.sub(first)); 
        res
    }
    
    fn subslice(self: Self, first: i64, count: i64) Self = {
        self.slice(first, first.add(count))
    }
    
    fn rest(self: Self, first: i64) Self = {
        self.slice(first, self.len())
    }
    
    fn slice_last(self: Self, count: i64) ?Self = {
        ::if(?Self);
        if(self.len < count, => .None) {
            (Some = self.slice(self.len.sub(count), self.len))
        }
    }
    
    fn interpret_as_bytes(self: Self) []u8 = {
        ptr := ptr_cast_unchecked(T, u8, self.ptr);
        (ptr = ptr, len = self.len * :: T.size_of())
    }
   
    fn copy_from(dest: Self, src: Self) void = {
        @safety(.Bounds) dest.len == src.len;
        // Last I checked these compile the compiler at the same speed. 
        // It was slower to write a dumb loop and also slower to pass stride as a const arg to copy_aligned instead of doing the multiply here,
        // I assume both were just becuase they caused more specializations so we compiled the same code more times. 
        // There's also a lot of personal joy in not needing to dlopen libc for this.    -- Sep 17 
        // memcpy(T.raw_from_ptr(dest.ptr), T.raw_from_ptr(src.ptr), dest.len * @run T.size_of());
        copy_aligned(T.raw_from_ptr(dest.ptr), T.raw_from_ptr(src.ptr), dest.len * T.size_of(), T.align_of());
    }
  
    // TODO: sad to JIT compile this for all types when emitting AOT, not just needed. :SLOW
    //       can outline this because it can't call bake_relocatable_value(T) directly anyway. but should fix the actual problem anyway.
    // TODO: this api involves hella copying. caller should pass in a list of BakedEntry. :SLOW
    // This looks kinda ugly but it used to be 35 lines of rust (instead of 31 here) and was a magic special case that assumed anything with ptr,len field names was a slice. 
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = {
        ::List(BakedEntry); ::if(BakedEntry); ::if(BakedVarId);
        
        // TODO: StructName.(initilizer) so you're not forced to use @as or put the type on the other side? 
        len: BakedEntry = (Num = (value = self.len, ty = .I64));
        ptr: BakedEntry = if self.len.eq(0) {
            // TODO: should we make this non-zero so you can pass them to rust? 
            (Num = (value = 0, ty = .P64))
        } {
            addr := T.int_from_ptr(self.ptr);
            BIG :: 104857600;
            if self.len > BIG && addr < BIG {
                @println("(ptr = %, len = %) is a really weird slice. are you sure you don't have the components flipped?", addr, self.len); 
            };
            :: ?BakedVarId;
            // TODO: this is wrong because what if someone previously baked a smaller new into the same memory.   :FUCKED -- Jul 16
            //       we need to still emit and compare to make sure we would have baked the same thing? 
            //       should that be a compile error or just have two? what if someone was relying on them being aliased? 
            id := or lookup_baked(addr) { () BakedVarId |
                info := ::get_meta(T);
                // TODO: assert alignment
                // TODO: assert len isnt big enough to be a pointer?
                // TODO: need to check against c.baked.lookup (the first ptr not each individual element) :BLOAT
                v: BakedVarId = if info.contains_pointers {
                    parts: List(BakedEntry) = list(ast_alloc());
                    each self { v | 
                        bytes := T.cast_to_bytes(v);
                        // TODO: its a bit un-intuative that you can't just call bake_relocatable_value(v), the dyn version uses the compiler auto implementation if none exists. 
                        entries := dyn_bake_relocatable_value(bytes, T, false); // TODO: default arg. TTODO: explain somewhere
                        parts&.push_all(entries);
                    };
                    // TODO: without cast
                    bake_value(@as(BakedVar) (VoidPtrArray = parts.rs()))
                }{
                    bytes := self[].interpret_as_bytes(); // TODO: decide if i need to clone. 
                    bake_value(@as(BakedVar) (Bytes = bytes.rs())) // TODO: this should take jit_ptr param? even tho its sad if you had to do work just for me to throw away. and then would want to warn if conflicting handlers or it could be super confusing.  -- Jun 30
                };
                
                cache_baked(addr, v);
                v
            };
    
            (AddrOf = id)
        };
        
        @slice(ptr, len) ast_alloc()
    }
    
    Self
}

// TODO: single element if arg isn't a tuple.
#macro fn slice(elements: FatExpr, allocator: FatExpr) FatExpr = @{
    s := @slice @[elements];
    s := clone(s, @[allocator]);
    s.items()
};

#macro fn list(elements: FatExpr, allocator: FatExpr) FatExpr = @{
    s := @slice @[elements];
    s := clone(s, @[allocator]);
    s
};

// TODO: can't do this in fn Slice because it tries to deal with all the overloads too soon? 
//       oh no its cause it conflicts with Slice(u8)
fn display_slice($T: Type) void = {
    fn display(self: Slice(T), writer: *List(u8)) void = {
        "[".display(writer);
        // TODO: use each? but then need better error if its defined for T but not *T. 
        prefix := "";
        each(self) { (e: *T) void | 
            prefix.display(writer);
            e.display(writer);
            prefix = ", ";
        };
        "]".display(writer);
    }
    
    fn display(self: *Slice(T), writer: *List(u8)) void = self[].display(writer);
}

/// A null terminated sequence of bytes.
CStr :: @struct(ptr: *u8);

/// When emitting an aot constant, include all the way until a zero, not just a single byte like it would for a normal *u8. 
fn bake_relocatable_value(self: *CStr) Slice(BakedEntry) = {
    addr := u8.int_from_ptr(self.ptr);
    // TODO: this is a bit scary because what if someone emitted the same string as a slice before so didn't include the zero terminator
    //if lookup_baked(addr) { v | 
    //    ptr: BakedEntry = (AddrOf = v);
    //    return(ptr.repeated(1, temp()).items());
    //};
    
    // TODO: need to check against  c.baked.lookup :BLOAT
    bytes: List(u8) = list(ast_alloc());
    for self[] { b|
        bytes&.push(b);
    };
    bytes&.push(@as(u8) 0.trunc());
    value := bake_value(@as(BakedVar) (Bytes = bytes.rs()));
    cache_baked(addr, value);
    ptr: BakedEntry = (AddrOf = value);
    ::List(BakedEntry); // :sema_regression but might just be order. 
    ptr.repeated(1, temp()).items()
}

:: {
   
};

Str :: Slice(u8);

fn as_array();

// A value type fixed size array. Like [T; len] in Rust or [len] T in Zig.
fn Array($T: Type, $count: i64) Type = {
    Self :: {
        // TODO: HACK. I should be able to call intern_type directly but can't call deconstruct_values on a tagged thing. this mostly just works by luck. -- May 27
        info: TypeInfo = (Array = (inner = T, len = count.trunc()));
        intern_type_ref(info&)
    };
    
    fn init(elements: Slice(T)) Self #no_tail = {
        @assert_eq(elements.len, count, "arrays have fixed length"); // TODO: better message 
        self := Self.uninitialized();
        enumerate(elements){ i, t |
            self&.index(i)[] = t[];
        };
        self
    }
    
    fn init(copied: T) Self #no_tail = {
        self := Self.uninitialized();
        range(0, count){ i |
            self&.index(i)[] = copied;
        };
        self
    }
    
    /// If a slice is the right length, it can be cast to a pointer to an array. 
    fn as_array(elements: Slice(T)) ?*Self = {
        :: if(?*Self);
        if(elements.len.ne(count), => .None) {
            (Some = ptr_cast_unchecked(From = T, To = Self, ptr = elements.ptr))
        }
    }
    
    fn eq(lhs: Self, rhs: Self) bool = lhs&.items().eq(rhs&.items());
    
    ::ptr_utils(T);
    fn index(self: *Self, i: i64) *T = {
        @safety(.Bounds) i < count;
        self.as_ptr().offset(i)
    }
    
    fn len(self: *Self) i64 #inline = count;
    
    fn items(self: *Self) Slice(T) = (ptr = self.as_ptr(), len = count);
    
    fn as_ptr(self: *Self) *T #inline = ptr_cast_unchecked(From = Self, To = T, ptr = self);
    
    // TODO: instead of generating whole new versions of all these functions,
    //       have a DeriveAsSliceIndexable that just aliases Self as a Slice(T) and reuses the slice ones. 
    //       can use that for List too. 
    ::DeriveIndexable(*Self, T);
    
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = {
        parts: List(BakedEntry) = list(ast_alloc());
        each self { v | 
            bytes := T.cast_to_bytes(v);
            entries := dyn_bake_relocatable_value(bytes, T, false);
            parts&.push_all(entries);
        };
        parts.items()
    }
    
    Self
}

fn HashEach($Collection: Type, $Hasher: Type) void = {
    fn hash(hasher: *Hasher, self: *Collection) void = {
        each self[] { t |
            hasher.hash(t);
        };
    }
}

/*
// TODO: this would be an easy start for traits. 
fn resolve(e: FatExpr) FatExpr #macro = {
    assert(e.expr&.is(.Closure), "@resolve expected function sig");
    func := e.expr.Closure;
    todo()
}
:: @resolve fn index(self: Self) *T => ();
:: @resolve fn len(self: Self) i64  => ();
*/

// Note: Passing *Self and Self mean different things. Containers that are already logically a pointer may want both for convenience. 
// TODO: trait bounds. fn index(Self) *T; fn len(Self) i64;
// IMPORTANT: DO NOT impl this for a linked list! It expects fast random access to the collection!
fn DeriveIndexable($Self: Type, $T: Type) void = {
    fn set(self: Self, i: i64, v: T) void #inline = {
        self[i] = v; 
    }
    
    fn get(self: Self, i: i64) T #inline = self[i];
    
    // TODO: the other fn type syntax so you can name the arguments in the closure as documentation. 
    // TODO: be more consistant about how tuples are expanded into args to fix Fn((a, b), c) === Fn(a, b, c) when you want Fn(Ty(a, b), c)
    //       seems like a massive mistake that I create ambiguity and then just guess something stupid and hope for the best. 
    fn enumerate(arr: Self, $f: Fn(Ty(i64, *T), void)) void = {
        i := 0;
        while(=> lt(i, arr.len())) {
            e: *T = arr.index(i);
            f(i, e);
            i = i.add(1);
        };
    }
    
    // TODO: I don't love that I have to write the deref version seperatly
    //       but I dont have let destructuring (only function args) so its a pain to take tuples by pointer. 
    //       I should be able to destructure Ptr(A, B) to (Ptr(A), Ptr(B))
    fn for(arr: Self, $f: @Fn(e: T) void) void = 
        each(arr, fn(v: *T) void => f(v[]));
    
    fn each(arr: Self, $f: Fn(*T, void)) void = {
        i := 0;
        while(=> lt(i, arr.len())) {
            f(arr.index(i));
            i = i.add(1);
        };
    }
    
    fn enumerate_rev(arr: Self, $f: @Fn(i64, *T) void) void = {
        i := arr.len() - 1;
        while(=> i >= 0) {
            f(i, arr.index(i));
            i -= 1;
        };
    }
   
    fn for_rev(arr: Self, $f: @Fn(T) void) void = {
        i := arr.len() - 1;
        while(=> i >= 0) {
            f(arr.index(i)[]);
            i -= 1;
        };
    }
     
    fn each_rev(arr: Self, $f: Fn(*T, void)) void = {
        i := arr.len() - 1;
        while(=> i >= 0) {
            f(arr.index(i));
            i -= 1;
        };
    }
    
    fn contains(self: Self, needle: *T) bool = {
        each self { check |
            if(check == needle, => { return(true); });
        };
        false
    }
    
    fn contains(self: Self, needle: []T) bool = {
        if(self.len < needle.len, => return(false));
        range(0, self.len - needle.len + 1) { i |
            continue :: local_return;
            enumerate needle { j, a | 
                if(!(a == self.index(i + j)), => continue()); 
            };
            return(true);
        };
        false
    }
    
    fn contains(self: Self, $where: @Fn(e: *T) bool) bool = {
        each self { check |
            if(where(check), => { return(true); });
        };
        false
    }
    
    
    // TODO: add addr_eq/identical if that's really what you want but like... it never is I feel. 
    // TODO: for slices, if !contains_ptrs && !has_padding && no eq special overloads, just call some vectorized memcmp and skip a bunch of pointless code gen work. 
    // TODO: if it has a trivial .items() (like lists, RsVec), it would be better to convert and call eq on that 
    //       so you don't have to generate this function many times. cause those versions wouldn't be easy to deduplicate later. 
    fn eq(lhs: Self, rhs: Self) bool = {
        if(lhs.len != rhs.len, => return(false));
        // TODO: can only do this for slice. could for list too if we expose like fn identity_ptr maybe? 
        //l_ptr := T.int_from_ptr(lhs.ptr);
        //r_ptr := T.int_from_ptr(rhs.ptr);
        //if(l_ptr == r_ptr, => return(true)); // TODO: is this too creepy? it means we might skip calling your eq overload. 
        
        range(0, lhs.len) { i |
            // @safety(.NonCompliantOverload) lhs.len == rhs.len;  // TODO: decide if i want to do this. 
            if(!(lhs.index(i) == rhs.index(i)), => return(false));
        };
        true
    }
    
    fn ne(lhs: Self, rhs: Self) bool #inline = 
        !lhs.eq(rhs);
    
    /// Set all elements of the slice to 't'. 
    fn fill(self: Self, t: T) void = {
        each(self){ ptr |
            ptr[] = t;
        };
    }
    
    fn last(self: Self) ?*T = {
        ::if(?*T);
        if(self.is_empty(), => .None) {
            (Some = self.index(self.len.sub(1)))
        }
    }
    
    fn is_empty(self: Self) bool = self.len().eq(0);
    
    // TODO: allow putting the $ on the arg in @Fn?
    /// This is special because it ensures the arg value is comptime known in the body.
    fn inline_for($self: Self, $body: @Fn(f: *T) void) void = {
        @inline_for(self) body;
    }
    
    fn position(self: Self, $cond: @Fn(v: *T) bool) ?i64 = {
        enumerate self { i, v |
            if(cond(v), => return(Some = i));
        };
        .None
    }
    
    fn reverse(self: Self) void = {
        // odd rounds down so middle element stays in place
        range(0, self.len() / 2) { i |
            self.swap(i, self.len() - i - 1);
        };
    }
    
    fn swap(self: Self, i: i64, j: i64) void #inline = {
        if(i == j, => return());
        temp := self[i];
        self[i] = self[j];
        self[j] = temp;
    }
}

fn reverse();
fn swap();

fn ends_with(haystack: Str, needle: Str) bool = 
    if(haystack.len < needle.len, => false) {
        start := subslice(haystack, sub(len(haystack), len(needle)), len(needle)); 
        eq(start, needle)
    };

// TODO: these should just be on Slice(T)
fn starts_with(haystack: Str, needle: Str) bool = 
    if(haystack.len < needle.len, => false) {
        start := subslice(haystack, 0, len(needle)); 
        eq(start, needle)
    };

fn copy_aligned(dest: rawptr, src: rawptr, count_bytes: i64, $align: i64) void = {
    @if(@run align == 0) return();
    T :: UInt(align * 8); ::ptr_utils(T);
    src  := T.ptr_from_raw(src); 
    dest := T.ptr_from_raw(dest); 
    range(0, count_bytes / T.size_of()) { _ |
        dest[] = src[];
        dest = dest.offset(1); // units are T not bytes!
        src = src.offset(1);
    };
}
