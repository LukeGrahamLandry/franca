fn range(start: i64, end: i64, $yield: Fn(i64, void)) void = {
    i := start;
    while(=> i.lt(end)) {
        yield(i);
        i = i.add(1);
    };
}

fn inline_range($start: i64, $end: i64, $yield: @Fn(i: i64) void #duplicated) void = {
    @inline_range(start, end) yield;
}

fn range_rev(start: i64, end: i64, $yield: Fn(i64, void)) void = {
    i := end - 1;
    while(=> i >= start) {
        yield(i);
        i -= 1;
    };
}

// :blessed: String literals and @slice create values of this type. 
fn Slice($T: Type) Type = {
    Self :: @struct(ptr: *T, len: i64, Element :: T);
    
    ::ptr_utils(T);
    
    fn slice(ptr: *T) Self = (ptr = ptr, len = 1);
    fn slice(ptr: *T, count: i64) Self = (ptr = ptr, len = count);
    // TODO: this is jail if you pass it to rust
    fn empty() Self #fold = Self.zeroed();
    
    fn as_ptr(self: Self) *T #inline = self.ptr;
    
    fn len(self: *Self) i64 #inline = self.len;
    fn index(self: *Self, i: i64) *T #inline =  {
        @safety(.Bounds) i.ult(self.len);
        self.ptr.offset(i)
    }
    
    fn len(self: Self) i64 #inline #fold = self.len;
    fn index(self: Self, i: i64) *T #inline = {
        @safety(.Bounds) i.ult(self.len);
        self.ptr.offset(i)
    }
    
    fn index_unchecked(self: Self, i: i64) *T = 
        self.ptr.offset(i);
    
    ::DeriveIndexable(Self, T);
    ::DeriveIndexable(*Self, T); 
    
    fn single(t: T, a: Alloc) Self = 
        slice(t&, 1).clone(a).items();
    
    fn slice(self: Self, first: i64, past_last: i64) Self = {
        // TODO: first should be lt
        // TODO: its sad that im doing double bounds check in common case where its just index()
        @safety(.Bounds) first.ule(self.len()).and(past_last.ge(first));
        (ptr = self.ptr.offset(first), len = past_last - first) 
    }
    
    fn subslice(self: Self, first: i64, count: i64) Self #inline = {
        @safety(.Bounds) (count >= 0);
        self.slice(first, first.add(count))
    }
    
    fn rest(self: Self, first: i64) Self #inline = 
        self.slice(first, self.len());
    
    fn slice_last(self: Self, count: i64) ?Self = {
        ::if(?Self);
        if(self.len < count, => .None) {
            (Some = self.slice(self.len.sub(count), self.len))
        }
    }
    
    fn interpret_as_bytes(self: Self) []u8 = {
        ptr := ptr_cast_unchecked(T, u8, self.ptr);
        (ptr = ptr, len = self.len * :: T.size_of())
    }
   
    fn copy_from(dest: Self, src: Self) void #inline = {
        @safety(.Bounds) dest.len == src.len;
        // TODO: make this less painful to look at
        //copy_aligned(T.raw_from_ptr(dest.ptr), T.raw_from_ptr(src.ptr), dest.len * T.size_of(), T.align_of());
        copy_overlapping(T.raw_from_ptr(dest.ptr), T.raw_from_ptr(src.ptr), dest.len * T.size_of());
    }
    
    fn copy_overlapping(self: Self, dest: i64, src: i64, len: i64) void = {
        dest := self.slice(dest, dest + len);
        src  := self.slice(src,  src  + len);
        dest.copy_from(src);
    }
    
    // note: you can't call T.zeroed() here because it wants to comptime call us. 
    fn set_zeroed(self: Self) void #inline = 
        self.interpret_as_bytes().set_bytes(0);
  
    // TODO: this api involves hella copying. caller should pass in a list of BakedEntry. :SLOW
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = 
        bake_relocatable_slice_erased(T.raw_from_ptr(self.ptr), self.len, T);
        
    fn shallow_copy(self: Self, a: Alloc) Self = {
        buf := a.alloc(T, self.len);
        buf.copy_from(self);
        buf
    }
    
    fn unordered_remove(self: Self, index: i64) ?T #inline = {
        if(index >= self.len, => return(.None));  // TODO: why not just debug assert?
        out := self[index];
        if index != self.len - 1 {
            self[index] = self[self.len - 1];
        };
        (Some = out)
    }
    
    fn contains_address(self: Self, address: *T) bool = {
        n := ptr_diff(self.ptr, address);
        n.ult(self.len)
    }
    
    Self
}

IsSlice :: IsCollection(Slice);
PointerTo :: fn($Inner: @Fn(t: Type) bool) = (fn(T: Type) = is_ptr(T) && Inner(Deref(T)));
IsCollection :: fn($Inner: @Fn(e: Type) Type) = (fn(T: Type) bool = {
    check :: fn($T: Type) Type = Inner(T.Element);
    has_const_field(T, @symbol Element) && invoke_specialized(Type, Type, check, T) == T
});

fn slice_pop_first(self: ~T) T #where(IsSlice) = {
    self.ptr = self.ptr.offset(1);
    self.len -= 1;
    self
}

fn deep_clone(out: ~S, self: S, a: Alloc) void #where(PointerTo(IsSlice)) = {
    T :: Deref(S).Element;
    @if(T.has_pointers(), {
        out[] = a.alloc(T, self.len);
        enumerate self { i, e |
            out := out.index(i);
            deep_clone(out, e, a);
        };
    }, { out[] = self[].shallow_copy(a) })
}

// TODO: make compile error here show the right location. 
fn bake_relocatable_slice_erased(addr: rawptr, length: i64, T: Type) []BakedEntry = {
    ::List(BakedEntry); ::if(BakedEntry); ::if(BakedVarId);
    
    // TODO: StructName.(initilizer) so you're not forced to use @as or put the type on the other side? 
    len: BakedEntry = (Num = (value = length));
    ptr: BakedEntry = if length == 0 {
        // TODO: should we make this non-zero so you can pass them to rust? 
        (Num = (value = 0))
    } else {
        BIG :: 104857600;
        addr := int_from_rawptr(addr);
        if length > BIG && addr < BIG {
            @println("(ptr = %, len = %) is a really weird slice. are you sure you don't have the components flipped?", addr, length); 
        };
        :: ?BakedVarId;
        info := get_meta(T);
        size_of: i64 = info.stride_bytes.zext();
        // TODO: assert alignment
        // TODO: assert len isnt big enough to be a pointer?
        // TODO: need to check against c.baked.lookup (the first ptr not each individual element) :BLOAT
        bytes: []u8 = (ptr = u8.ptr_from_int(addr), len = length * size_of); // TODO: decide if i need to clone. 
        
        // TODO: this is wrong because what if someone previously baked a smaller new into the same memory.   :FUCKED -- Jul 16
        //       we need to still emit and compare to make sure we would have baked the same thing? 
        //       should that be a compile error or just have two? what if someone was relying on them being aliased? 
        id, addend := emplace_bake(bytes) {
            //@println("baking a new slice %*%=%", length, size_of, bytes.len);
            ::if(BakedVar);
            if info.contains_pointers {
                parts: List(BakedEntry) = list(ast_alloc());
                off := 0;
                range(0, length) { _ | 
                    bytes := bytes.slice(off, off + size_of);
                    // TODO: its a bit un-intuative that you can't just call bake_relocatable_value(v), the dyn version uses the compiler auto implementation if none exists. 
                    entries := dyn_bake_relocatable_value(bytes, T, false); // TODO: default arg. TTODO: explain somewhere
                    parts&.push_all(entries);
                    off += size_of;
                };
                (VoidPtrArray = parts.as_raw())
            } else {
                (Bytes = bytes.as_raw_list())
            }
        };
        
        entry_with_addend(id, addend)
    };
    
    @slice(ptr, len) ast_alloc()
}

// Create a slice containing `elements` in memory provided by `allocator`.  
// The first element is used for type inference. TODO: let macros access the expression's result location type. 
// 
// this is better than making an @slice on the stack and then copying. 
// but also @slice on the stack miscompiles if the length is > 50. 
//
#macro fn slice(elements: FatExpr, allocator: FatExpr) FatExpr = {
    e := elements&.items();
    e[0] = compile_ast(e[0]);
    @{
        T :: @[@literal e[0].ty];
        mem := @[allocator].alloc(T, @[@literal e.len]);
        // SAFETY: allocated an array of the correct size so there's no need to bounds check for each element. 
        p := mem.ptr;
        step := size_of(T);
        cheat :: fn(p: *T, bytes: i64) *T #ir(.add, .Kl);
        @[{
            stmts := FatStmt.list(e.len, ast_alloc());
            enumerate e { i, e |
                stmts&.push(loc = e.loc, stmt = (Eval = @{
                    p[] = @[e[]];
                    p = p.cheat(step);
                }));
            };
            make_block(stmts, @{mem})
        }]
    }
}

#macro fn list(elements: FatExpr, allocator: FatExpr) FatExpr = @{
    a := @[allocator];
    s := @slice(@[elements]) a;
    s: List(@type s[0]) = (maybe_uninit = s, len = s.len, gpa = a);
    s
};

#macro fn const_slice(elements: FatExpr) FatExpr = @{
    :: @slice(@[elements]) ast_alloc()
};

// TODO: can't do this in fn Slice because it tries to deal with all the overloads too soon? 
//       oh no its cause it conflicts with Slice(u8)
fn display_slice($T: Type) void = {
    fn display(self: Slice(T), writer: *List(u8)) void = {
        "[".display(writer);
        prefix := "";
        each self { e | 
            display(prefix, writer);
            display(e, writer);
            prefix = ", ";
        };
        "]".display(writer);
    }
    
    fn display(self: *Slice(T), writer: *List(u8)) void = 
        self[].display(writer);
}

/// A null terminated sequence of bytes.
CStr :: @struct(ptr: *u8);

fn as_ptr(s: CStr) *u8 = s.ptr;

/// When emitting an aot constant, include all the way until a zero, not just a single byte like it would for a normal *u8. 
fn bake_relocatable_value(self: *CStr) Slice(BakedEntry) = {
    ::List(BakedEntry); ::if(BakedEntry);
    ptr: BakedEntry = if self.ptr.is_null() {
        (Num = (value = 0))
    } else {
        vbytes := self[].str();
        vbytes.len += 1;  // null terminator
        id, off := emplace_bake(vbytes, => (Bytes = vbytes.as_raw_list()));
        entry_with_addend(id, off)
    };
    ptr.repeated(1, temp()).items()
}

Str :: Slice(u8);

// A value type fixed size array. Like [T; len] in Rust or [len] T in Zig.
fn Array($T: Type, $count: i64) Type = {
    Self :: {
        // :UpdateBoot I should be able to call intern_type directly but can't call deconstruct_values on a tagged thing. this mostly just works by luck. -- May 27
        info: TypeInfo = (Array = (inner = T, len = count.trunc()));
        intern_type_ref(info&)
    };
    
    // You probably don't want to call this with @slice, because you can just use a tuple literal. 
    fn init(elements: Slice(T)) Self #inline = {
        @assert_eq(elements.len, count, "arrays have fixed length"); // TODO: better message 
        ptr_cast_unchecked(T, Self, elements.ptr)[]
    }
    
    fn init(copied: T) Self #inline = {
        self := @uninitialized Self;
        range(0, count){ i |
            self&.index(i)[] = copied;
        };
        self
    }
    
    /// If a slice is the right length, it can be cast to a pointer to an array. 
    fn as_array(elements: Slice(T)) ?*Self = {
        :: if(?*Self);
        if(elements.len.ne(count), => .None) {
            (Some = ptr_cast_unchecked(From = T, To = Self, ptr = elements.ptr))
        }
    }
    
    fn eq(lhs: Self, rhs: Self) bool = lhs&.items().eq(rhs&.items());
    
    ::ptr_utils(T);
    fn index(self: *Self, i: i64) *T #inline = {
        @safety(.Bounds) i.ult(count);
        self.as_ptr().offset(i)
    }
    
    fn index_unchecked(self: *Self, i: i64) *T = {
        self.as_ptr().offset(i)
    }

    fn len(self: *Self) i64 #inline = count;
    
    fn items(self: *Self) Slice(T) #inline = (ptr = self.as_ptr(), len = count);
    
    fn as_ptr(self: *Self) *T #inline = ptr_cast_unchecked(From = Self, To = T, ptr = self);
    fn set_zeroed(self: *Self) void #inline = self.items().set_zeroed();
    
    // TODO: instead of generating whole new versions of all these functions,
    //       have a DeriveAsSliceIndexable that just aliases Self as a Slice(T) and reuses the slice ones. 
    //       can use that for List too. 
    ::DeriveIndexable(*Self, T);
    
    //
    // :CompilerBug Jul 6, 2025
    // TODO: this should fold and not create the overload set 
    //       even without the '::' because has_pointers is #fold
    // TODO: Without this check, the use of Array(i64, 7) in @syscall makes 
    //       it think there's a conflicting overload. maybe there's an @check yielding 
    //       unexpectedly that lets this get added twice? in this case it's fine 
    //       because you don't actually need bake overload for ints but presumably 
    //       it could happen for other types too. 
    //
    ::@if(::has_pointers(T)) {
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = {
        parts: List(BakedEntry) = list(ast_alloc());
        each self { v | 
            bytes := T.cast_to_bytes(v);
            entries := dyn_bake_relocatable_value(bytes, T, false);
            parts&.push_all(entries);
        };
        parts.items()
    }
    };
    
    Self
}

fn HashEach($Collection: Type, $Hasher: Type) void = {
    fn hash(hasher: *Hasher, self: *Collection) void = {
        each self[] { t |
            hasher.hash(t);
        };
    }
}

/*
// TODO: this would be an easy start for traits. 
fn resolve(e: FatExpr) FatExpr #macro = {
    assert(e.expr&.is(.Closure), "@resolve expected function sig");
    func := e.expr.Closure;
    todo()
}
:: @resolve fn index(self: Self) *T => ();
:: @resolve fn len(self: Self) i64  => ();
*/

// Note: Passing *Self and Self mean different things. Containers that are already logically a pointer may want both for convenience. 
// TODO: trait bounds. fn index(Self) *T; fn len(Self) i64;
// IMPORTANT: DO NOT impl this for a linked list! It expects fast random access to the collection!
fn DeriveIndexable($Self: Type, $T: Type) void = {
    fn set(self: Self, i: i64, v: T) void #inline = {
        self[i] = v; 
    }
    
    fn get(self: Self, i: i64) T #inline = self[i];
    
    // TODO: the other fn type syntax so you can name the arguments in the closure as documentation. 
    // TODO: be more consistant about how tuples are expanded into args to fix Fn((a, b), c) === Fn(a, b, c) when you want Fn(Ty(a, b), c)
    //       seems like a massive mistake that I create ambiguity and then just guess something stupid and hope for the best. 
    fn enumerate(arr: Self, $f: @Fn(i: i64, e: *T) void) void = {
        i := 0;
        start_length := arr.len();
        while => i < start_length {
            f(i, arr.index(i));
            i = i.add(1);
        };
        // TODO: new comptime jit hits `Poison expression InProgressMacro` here :SemaRegression
        // @debug_assert(arr.len() == start_length, "it's actually Ko to mutate an array while iterating over it");
    }
    
    // TODO: I don't love that I have to write the deref version seperatly
    //       but I dont have let destructuring (only function args) so its a pain to take tuples by pointer. 
    //       I should be able to destructure Ptr(A, B) to (Ptr(A), Ptr(B))
    fn for(arr: Self, $f: @Fn(e: T) void) void = 
        each(arr, fn(v) => f(v[]));
    
    fn each(arr: Self, $f: Fn(*T, void)) void = {   
        i := 0;
        start_length := arr.len();
        while => i < start_length {
            f(arr.index(i));
            i = i.add(1);
        };
        // TODO: new comptime jit hits `Poison expression InProgressMacro` here :SemaRegression
        // @debug_assert(arr.len() == start_length, "it's actually Ko to mutate an array while iterating over it");
    }
    
    fn enumerate_rev(arr: Self, $f: @Fn(i64, *T) void) void = {
        i := arr.len() - 1;
        while => i >= 0 {
            f(i, arr.index(i));
            i -= 1;
        };
    }
   
    fn for_rev(arr: Self, $f: @Fn(T) void) void = 
        enumerate_rev(arr, fn(_, t) => f(t[]));
     
    fn contains(self: Self, needle: *T) bool = 
        self.index_of(needle).is_some();
    
    fn index_of(self: Self, needle: T) ?i64 =
        index_of(self, needle&);
     
    fn index_of(self: Self, needle: *T) ?i64 = {
        enumerate self { i, check |
            if(check == needle, => return(Some = i));
        };
        .None
    }
    
    fn contains(self: Self, needle: T) bool = 
        self.contains(needle&);
    
    fn contains(self: Self, contiguous: []T) bool = {
        if(self.len < contiguous.len, => return(false));
        range(0, self.len - contiguous.len + 1) { i |
            continue :: local_return;
            enumerate contiguous { j, a | 
                if(!(a == self.index(i + j)), => continue()); 
            };
            return(true);
        };
        false
    }
    
    fn contains(self: Self, $where: @Fn(e: *T) bool) bool =
        index_of(self, where).is_some();
    
    fn index_of(self: Self, $where: @Fn(e: *T) bool) ?i64 = {
        enumerate self { i, check |
            if(where(check), => return(Some = i));
        };
        .None
    }
    
    fn find(self: Self, $where: @Fn(e: *T) bool) ?*T = {
        if self.index_of(where) { i |
            return(Some = self.index(i));
        };
        .None
    }
    // TODO: add addr_eq/identical if that's really what you want but like... it never is I feel. 
    // TODO: for slices, if !contains_ptrs && !has_padding && no eq special overloads, just call some vectorized memcmp and skip a bunch of pointless code gen work. 
    // TODO: if it has a trivial .items() (like lists, RawList), it would be better to convert and call eq on that 
    //       so you don't have to generate this function many times. cause those versions wouldn't be easy to deduplicate later. 
    fn eq(lhs: Self, rhs: Self) bool = {
        if(lhs.len != rhs.len, => return(false));
        // TODO: can only do this for slice. could for list too if we expose like fn identity_ptr maybe? 
        //l_ptr := T.int_from_ptr(lhs.ptr);
        //r_ptr := T.int_from_ptr(rhs.ptr);
        //if(l_ptr == r_ptr, => return(true)); // TODO: is this too creepy? it means we might skip calling your eq overload. 
        
        range(0, lhs.len) { i |
            // @safety(.NonCompliantOverload) lhs.len == rhs.len;  // TODO: decide if i want to do this. 
            if(!(lhs.index(i) == rhs.index(i)), => return(false));
        };
        true
    }
    
    fn ne(lhs: Self, rhs: Self) bool #inline = 
        !lhs.eq(rhs);
    
    fn last(self: Self) ?*T = {
        ::if(?*T);
        if(self.is_empty(), => .None) {
            (Some = self.index(self.len.sub(1)))
        }
    }
    
    fn is_empty(self: Self) bool = self.len().eq(0);
    
    // TODO: allow putting the $ on the arg in @Fn?
    /// This is special because it ensures the arg value is comptime known in the body.
    fn inline_for($self: Self, $body: @Fn(f: *T) void #duplicated) void = {
        @inline_for(self) body;
    }
    
    fn position(self: Self, $cond: @Fn(v: *T) bool) ?i64 = {
        enumerate self { i, v |
            if(cond(v), => return(Some = i));
        };
        .None
    }
    
    fn reverse(self: Self) void = {
        // odd rounds down so middle element stays in place
        range(0, self.len() / 2) { i |
            self.swap(i, self.len() - i - 1);
        };
    }
    
    fn swap(self: Self, i: i64, j: i64) void #inline = {
        if(i == j, => return());
        temp := self[i];
        self[i] = self[j];
        self[j] = temp;
    }
}

// TODO: constraint
fn each_rev(arr: ~T, $f: @Fn(t: *(@if(is_ptr(T), Deref(T), T)).Element) void) void #where = 
   enumerate_rev(arr, fn(_, t) => f(t));

fn ends_with(haystack: Str, needle: Str) bool = 
    if(haystack.len < needle.len, => false) {
        start := subslice(haystack, sub(len(haystack), len(needle)), len(needle)); 
        eq(start, needle)
    };

// TODO: these should just be on Slice(T)
fn starts_with(haystack: Str, needle: Str) bool = 
    haystack.len >= needle.len && haystack.slice(0, needle.len) == needle;

::display_slice(i64);

fn largest_below(options: []i64, cap: i64) i64 = {
    n := 0;
    for options { i | 
        if i < cap && i > n {   
            n = i
        };
    };
    n
}

fn pop_type(bytes: *[]u8, $T: Type) *T #generic = {
    @debug_assert(bytes.len >= T.size_of(), "pop_type: not enough bytes");
    //@debug_assert(u8.int_from_ptr(bytes.ptr).mod(T.align_of()) == 0, "pop_type: unaligned"); // TODO: is alignment real? 
    start := bytes.ptr;
    bytes.ptr = bytes.ptr.offset(T.size_of());
    bytes.len -= T.size_of();
    ptr_cast_unchecked(u8, T, start)
}

// TODO: is alignment real?
fn peek_type(bytes: []u8, $T: Type) *T #generic = {
    @debug_assert_ge(bytes.len, T.size_of(), "not enough bytes");
    ptr_cast_unchecked(u8, T, bytes.ptr)
}

fn cast_front_unaligned($T: Type, bytes: []u8) T #generic = 
    bytes.peek_type(T)[];

fn is_all_zeroes(buf_in: []u8) bool = {
    chunks, extra := div_mod(buf_in.len, 8);
    buf: []i64 = (ptr = ptr_cast_unchecked(u8, i64, buf_in.ptr), len = chunks);  // TODO: is alignment real?
    for buf { b |
        if(b != 0, => return(false));
    };
    buf := buf_in.rest(buf_in.len - extra);
    for buf { b |
        if(b != 0, => return(false));
    };
    true
}

fn starts_with(haystack: CStr, needle: Str) bool = {
    range(0, needle.len) { i |
        if(haystack[i] != needle[i] || haystack[i] == 0, => return(false));
    };
    true
};
