ArenaAlloc :: @struct(
    parent: Alloc,
    chunk: ?*ArenaHeader,
    start: i64,
    end: i64,
    base_size: i64,
    making_new_chunk: bool,
    canary: bool = SLOW_ARENA_CANARY,
);

ArenaHeader :: @rec @struct(
    size: i64,
    used := 0, // for debugging but should use in arena_owns as well for more precision. probably worth the 8 bytes, especially since we're wasting 16 on tags anyway. 
    prev: ?*ArenaHeader,
    next: ?*ArenaHeader,
);

fn init(parent: Alloc, base_size: i64) ArenaAlloc #inline = 
    (parent = parent, chunk = .None, start = 0, end = 0, base_size = base_size, making_new_chunk = false);

fn new_arena_chunk(allocator: Alloc, size: i64, prev: ?*ArenaHeader, making_new_chunk: *bool) *ArenaHeader = {
    if making_new_chunk[] {
        scary_log(size);
        scary_log("recursing in new_arena_chunk");
        Syscall'exit(1);
    };
    making_new_chunk[] = true;
    buf := allocator.alloc(u8, size.max(ArenaHeader.size_of())); // TODO: align
    making_new_chunk[] = false;
    chunk := ptr_cast_unchecked(From = u8, To = ArenaHeader, ptr = buf.ptr);
    chunk[] = (size = buf.len, prev = prev, next = .None);
    chunk
}

arena_allocator_fn :: fn(self: rawptr, action: Alloc.Action, ptr: rawptr, count: i64, align: i64) []u8 = {
    // note: we can't use @match yet... (because of printing i guess?) // TODO: check again now that ast_alloc is in the compiler
    if action == Alloc.Action.Allocate {
        return(ptr = u8.ptr_from_raw(arena_alloc(self, count, align)), len = count);
    };
    self := ArenaAlloc.ptr_from_raw(self);
    if action == .FreeAll {
        self.reset_retaining_capacity();
        return Alloc.void_result;
    };
    if action == .Deinit {
        self.deinit();
        return Alloc.void_result;
    };
    if action == .ArenaMark {
        it := self.mark().erase();
        return(it.opaque)
    };
    if action == .ArenaReset {
        self.reset_retaining_capacity(mark_from_raw(ptr, count).unerase());
        return Alloc.void_result;
    };
    
    // Free is Noop
    empty()
};

ArenaCanary :: @struct {
    secret: i64 = GOOD;
    start: *u8;
    size: i64;
    GOOD :: 12345678;
    BAD :: 87654321;
};

// Don't try to printf debug in here...
// TODO: make this less derranged. 
arena_alloc :: fn(raw_self: rawptr, size: i64, align: i64) rawptr = {
    self := ArenaAlloc.ptr_from_raw(raw_self);
    
    chunk := self.chunk.or() {
        next := new_arena_chunk(self.parent, self.base_size, .None, self.making_new_chunk&);
        self.start = ArenaHeader.size_of();
        self.end = next.size;
        self.chunk = (Some = next);
        next
    };
    if SLOW_ARENA_CANARY {
        // TODO: support largert alignments
        align = 8;
        size = (size + size_of(ArenaCanary)).align_to(8);
    };
    loop {
        self.start = align_to(self.start, align);
        end  := self.start + size;
        
        if end <= self.end {
            ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            mem := ptr.offset(self.start);
            self.start = end;
            if SLOW_ARENA_CANARY {
                // TODO: doesn't work if some was skipped in the align_to at the start but that won't happen because align is fixed now
                dest := bit_cast_unchecked(*u8, *ArenaCanary, ptr.offset(self.start - size_of(ArenaCanary)));
                dest[] = (start = mem, size = size);
            };
            return(u8.raw_from_ptr(mem));
        };
        
        if chunk.next { next |
            chunk.used = self.start;
            self.chunk = (Some = next);
            self.start = ArenaHeader.size_of();
            self.end = next.size;
            chunk = next;
        } else {
            chunk.used = self.start;
            if size >= MAX_ARENA_BUCKET {
                panic("allocating so many bytes bro");
            };
            MAX_ARENA_BUCKET :: 1.shift_left(34);
            next_size := size.add(8).max(chunk.size.mul(2)).min(MAX_ARENA_BUCKET);
            new := new_arena_chunk(self.parent, next_size, (Some = chunk), self.making_new_chunk&);
            chunk.next = (Some = new);
            self.chunk = (Some = new);
            self.start = ArenaHeader.size_of();
            self.end = new.size;
            chunk = new;
        };
        chunk.used = ArenaHeader.size_of();
    }
};

// note: Alloc.Mark.zeroed() means unsupported so that's why the +1 for start
// :ugly
fn erase(self: ArenaMarker) Alloc.Mark = 
    (opaque = (ptr = u8.ptr_from_int(@if(self.chunk.is_some(), ArenaHeader.int_from_ptr(self.chunk.Some), 0)), len = self.start + 1));
// :ugly
fn unerase(self: Alloc.Mark) ArenaMarker = 
    (chunk = @if(self.opaque.ptr.is_null(), .None, (Some = bit_cast_unchecked(*u8, *ArenaHeader, self.opaque.ptr))), start = self.opaque.len - 1);

ArenaMarker :: @struct(chunk: ?*ArenaHeader, start: i64);
fn mark(self: *ArenaAlloc) ArenaMarker #inline = (chunk = self.chunk, start = self.start);

// remember, you can't do this if you acceppted an output allocator as a parameter because they might have passed you the temp allocator. 
// # Safety: This invalidates any allocations in self before the corresponding call to mark. 
// Memory is NOT returned to the backing allocator. 
fn reset_retaining_capacity(self: *ArenaAlloc, mark: ArenaMarker) void = {
    if mark.chunk.is_none() {
        self.reset_retaining_capacity();
        return();  
    };
    chunk := mark.chunk.unwrap();
    
    @if(SLOW_ARENA_CANARY) if self.canary {
        c := self.chunk.unwrap();
        c.used = self.start;
        ::ptr_utils(ArenaHeader);
        while => !identical(c, chunk) {
            consume_canaries(c, c.used);
            if !SLOW_MEMORY_DEBUGGING {
                c.used = ArenaHeader.size_of();
            };
            c = c.prev.unwrap();
        };
        // TODO: the debug stuff isn't done to the last chunk
        //scary_log("---");
    };
    
    @if(SLOW_PROTECT_ARENA) {
        c := chunk.next;
        chunk.next = .None;
        // TODO: and this doesn't work with leak detection because you can't free this now (and if you keep a list, 
        // keep chunk.size outsize because it can't be read once protected, and also will change if i do something for partially reset chunks)
        while => c { it |
            c = it.next;
            size := it.size.align_back(page_size());
            start := ArenaHeader.raw_from_ptr(it);
            next_page := start.align_to(page_size());
            if size > 0 && ptr_diff(start, next_page) < size {
                Syscall'mprotect(next_page, size, 0).unwrap();
            };
        };
        
        // TODO: do something for the partially reset chunk
    };
    // do this after SLOW_PROTECT_ARENA so you don't waste time on the stuff that gets mprotected away 
    @if(SLOW_MEMORY_DEBUGGING) self.debug_stomp_after(mark);
    
    self.start = mark.start;
    self.end = chunk.size;
    self.chunk = (Some = chunk);
    chunk.used = self.start;
}

fn consume_canaries(c: *ArenaHeader, used: i64) void = {
    base := ArenaHeader.raw_from_ptr(c);
    it := base.offset(used);
    @assert(used.ule(c.size), "chunk.used oob");
    //scary_log(used); scary_log(" "); scary_log(base); scary_log(" XXX ");
    
    while => !(identical(it, base) || identical(it, base.offset(ArenaHeader.size_of()))){
        @assert(base.int_from_rawptr().ult(it.int_from_rawptr()));
        canary := ArenaCanary.ptr_from_raw(it.offset(-size_of(ArenaCanary)));
        start := u8.raw_from_ptr(canary.start);
        //scary_log("size="); scary_log(canary.size); scary_log(" start="); scary_log(start); scary_log(" "); scary_log(" secret="); scary_log(canary.secret); scary_log(" ");
        saved := it.offset(-canary.size);
        //scary_log(start); scary_log("_"); scary_log(saved); scary_log(" | ");
        if canary.secret != ArenaCanary.GOOD {
            scary_log(" secret="); scary_log(canary.secret);
        };
        @assert(canary.secret == ArenaCanary.GOOD, "bad canary.secret");
        @assert(identical(start, saved));
        canary.secret = ArenaCanary.BAD;
        it = start;
    };
}

// set all the memory after a mark to 0xABABABA in the hopes of crashing you if you try to use it accidentally.
// you can call this at the top of reset_retaining_capacity to reveal use-after-reset bugs more quickly,
// at the cost of some performance. this should not change the behaviour of correct programs. 
fn debug_stomp_after(self: *ArenaAlloc, mark: ArenaMarker) void = {
    stomp_all :: fn(chunk) => stomp_some(chunk, ArenaHeader.size_of());
    stomp_some :: fn(chunk, clear_after) => {
        clear_after = max(clear_after, ArenaHeader.size_of());
        if chunk.used > clear_after {
            mem: []u8 = (ptr = ptr_cast_unchecked(ArenaHeader, u8, chunk).offset(clear_after), len = chunk.used - clear_after);
            mem.set_bytes(SLOW_MEMORY_JUNK);
            chunk.used = clear_after;
        };
    };
    if mark.chunk { chunk | 
        ::ptr_utils(ArenaHeader);
        if chunk.identical(self.chunk.unwrap()) {
            chunk.used = self.start;
        };
        stomp_some(chunk, mark.start);
        while => chunk.next { c | 
            chunk = c;
            stomp_all(chunk);
        };
    } else {
        if self.chunk { chunk | 
            chunk.used = self.start;
            stomp_all(chunk);
        };
    }
};

// # Safety: This invalidates all allocations in self!
// Memory is NOT returned to the backing allocator. 
fn reset_retaining_capacity(self: *ArenaAlloc) void = {
    if self.chunk { chunk |
        chunk.used = self.start;
        while => chunk.prev { c | 
            @if(SLOW_ARENA_CANARY) if self.canary {
                consume_canaries(chunk, chunk.used);
            };
            @if(SLOW_MEMORY_DEBUGGING) {
                // TODO: this doesn't happen to the last chunk
                S :: ArenaHeader.size_of();
                mem: []u8 = (ptr = ptr_cast_unchecked(ArenaHeader, u8, chunk).offset(S), len = max(chunk.used - S, 0));
                mem.set_bytes(SLOW_MEMORY_JUNK);
            };
            chunk.used = ArenaHeader.size_of();
            chunk = c;
        };
        @if(SLOW_ARENA_CANARY) if self.canary {
            consume_canaries(chunk, chunk.used);
        };
        self.start = ArenaHeader.size_of();
        self.end = chunk.size;
        self.chunk = (Some = chunk);
        chunk.used = self.start;
    }
}

// Memory is returned to the backing allocator. 
// # Safety
//     - This invalidates all allocations in self!
fn deinit(self: *ArenaAlloc) void = {
    ::if_opt(*ArenaHeader, bool);
    if self.chunk { chunk |
        chunk.used = self.start;
        while => chunk.next { it |
            chunk = it;
        };
        loop {
            @if(SLOW_ARENA_CANARY) if self.canary {
                consume_canaries(chunk, chunk.used);
            };
            
            prev := chunk.prev;
            
            ptr := ptr_cast_unchecked(From = ArenaHeader, To = u8, ptr = chunk);
            buf := slice(ptr, chunk.size);
            addr := u8.int_from_ptr(ptr);
            self.parent.dealloc(u8, buf);
            
            if prev { prev |
                chunk = prev;
            } else {
                self.chunk = .None;
                self.start = 0;
                self.end = 0;
                return();
            };
        };
    };
    
}

fn current_allocated_size(self: *ArenaAlloc) i64 = {
    size := 0;
    if self.chunk { chunk |
        // go all the way back then all the way forward because we might not be at the latest chunk. 
        while(=> chunk.prev.is_some()) {
            chunk = chunk.prev.unwrap();
        };
        
        while(=> chunk.next.is_some()) {
            size += chunk.size;
            chunk = chunk.next.unwrap();
        };
        size += chunk.size;
    };
    size
}

// doesn't count those after the current chunk (that were allocated before the last reset).
fn current_used_size(self: *ArenaAlloc) i64 = {
    size := 0;
    if self.chunk { chunk |
        size += self.start;
        while => chunk.prev.is_some() {
            chunk = chunk.prev.unwrap();
            size += chunk.used;
        };
        size += chunk.used;
    };
    size
}

fn borrow(self: *ArenaAlloc) Alloc = {
    self := ArenaAlloc.raw_from_ptr(self);
    (data = self, vptr = arena_allocator_fn)
}

fn bake_relocatable_value(self: *ArenaAlloc) Slice(BakedEntry) = {
    empty: ArenaAlloc = init(self.parent, self.base_size);
    bytes := ArenaAlloc.cast_to_bytes(empty&);
    entries := dyn_bake_relocatable_value(bytes, ArenaAlloc, true);
    entries
}

todo_allocator: Alloc : (data = 0.rawptr_from_int(), vptr = todo_allocator_fn);

// TODO: respect alignment. but im pretty sure malloc always gives you 16 bits and mostly good enough. 
todo_allocator_fn :: fn(_: rawptr, action: Alloc.Action, ptr: rawptr, count: i64, align: i64) []u8 = { 
    act := @as(i64) action;
    // Note: we can't use @match yet! because this is comptime ast_alloc rn
    if(act != 0, => return(empty()));
    a := general_allocator();
    f := a.vptr;
    f(a.data, action, ptr, count, align)
};
