
const usize = u64;
const isize = i64;

fn assert_eq(_: Any, __: Any) Unit;  // TODO: shadowing args breaks my debug check for not losing things


// Math
// fn add(a: i64, b: i64) i64; // bootstrapped!
// fn sub(a: i64, b: i64) i64;
fn mul(a: i64, b: i64) i64;
fn div(_: i64, __: i64) i64;
fn eq(_: i64, __: i64) bool;
fn ne(_: i64, __: i64) bool;
fn lt(_: i64, __: i64) bool;
fn gt(_: i64, __: i64) bool;
fn le(a: i64, b: i64) bool;
fn ge(_: i64, __: i64) bool;
fn shift_left(value: i64, shift_amount: i64) i64;
fn bit_or(a: i64, b: i64) i64;
fn bit_not(a: i64) i64;
fn bit_and(a: i64, b: i64) i64;
fn neg(a: i64) i64 = sub(0, a);

// TODO
// A value-type like a tuple but all elements are the same type. 
//  fn StaticArray(inner: Type, count: i64) Type;
// TODO: need to support using comptime args in other args. 
// @comptime fn static_array(Inner: Type, count: i64, element: Inner) StaticArray(Inner);
// @comptime fn static_array_uninit(Inner: Type, count: i64) StaticArray(Inner);
//  fn Comptime(T: Type) Type;
//  fn Runtime(T: Type) Type;
// const NullTerminated = Ptr(u8);


/// Get the discriment used at runtime. 
fn tag_value(E: Type, case_name: Symbol) i64;

// Control flow
// fn if(cond: bool, false_branch: Fn(Any, Any), true_branch: Fn(Any, Any)) Any = {
//     (cond, false_branch, true_branch)!if  // TODO: better generics so you dont have to manually instantiate something per return type
// }

/*  // TODO: support baking when fully constant arg. 
fn while(cond: Fn(Unit, bool), body: Fn(Unit, Unit)) Unit = {
    (cond, body)!while;
}
*/

fn not(b: bool) bool = (b, fn() bool = false, fn() bool = true)!if;

// Note: these are not short circuiting because arguments are always evaluated. TODO: varients that accept closures
fn and(a: bool, b: bool) bool = (a, fn() bool = b, fn() bool = false)!if;
fn or(a: bool, b: bool) bool = (a, fn() bool = true, fn() bool = b)!if;

// Measured in interpreter stack slots. 
fn size_of(T: Type) i64;
// These don't care about the size of the type, they just measure in interp stack slots. 

// @cap(raw_ptr)
fn raw_slice(ptr: VoidPtr, first: i64, one_past_last: i64) VoidPtr;
/// The result may have more room than you asked for (which is useful for growable collections in allocators that use fixed size blocks). 
// @cap(raw_ptr)
@impure fn alloc(Element: Type, count: i64) VoidPtr; 
/// `ptr` must be exactly a slice returned by `alloc` (not a subslice of it).
// @cap(raw_ptr)
@impure fn dealloc(Element: Type, ptr: VoidPtr, len: i64) Unit;

// Inspecting the interpreter
fn is_comptime() bool;
fn is_uninit(_: VoidPtr) bool;
fn is_oob_stack(_: VoidPtr) bool;
// @cap(io)
@env fn print_callstack() Unit;

fn mod(big: i64, divisor: i64) i64 = {
    sub(big, mul(div(big, divisor), divisor))
}

fn max(a: i64, b: i64) i64 = {
    (lt(a, b), fn()=b, fn()=a)!if
}

fn min(a: i64, b: i64) i64 = {
    (gt(a, b), fn()=b, fn()=a)!if
}

// @cap(io)
@env fn puts(msg: (Ptr(i64), i64)) Unit;
// Halts the program and reports an error. 
// The return value can typecheck as anything because it never returns. 
// @cap(throw)
fn panic(msg: (Ptr(i64), i64)) Never;
// @cap(throw)
fn assert(cond: bool, msg: (Ptr(i64), i64)) Unit = (cond, fn() Unit=(), fn() Unit=panic(msg))!if;


// There must be a not insane way to do this but i gave up and read the two's complement wikipedia page.
/// Convert an i64 to an i<bit_count> with the (64-<bit_count>) leading bits 0.
fn signed_truncate(x: i64, bit_count: i64) i64 = {
    let mask = sub(shift_left(1, bit_count), 1);
    (le(x, 0), fn() i64 = {
        bit_and(add(bit_not(mul(x, neg(1))), 1), mask)
    }, fn() i64 = x)!if
}

/// The parser emits calls to this function for 0b____ and 0x____ tokens. 
@comptime
fn from_bit_literal(bit_count: i64, value: i64) IntType(bit_count, false) = @as(IntType(bit_count, false)) value;  // TODO: actual types
