//! About #ir:
//! These used to be just normal functions defined in the language that used inline assembly. 
//! Now they're implemented in the compiler (see ../backend). 
//! They behave like #inline functions: type checking, overloading, and function pointers work as normal. 
//! Some can be used by thier conventional infix operator syntax which the parser desugars to a function call to the intrinsic. 

// TODO: float to int rounding

fn add(a: i64, b: i64) i64 #ir(.add, .Kl)   #fold; // +
fn sub(a: i64, b: i64) i64 #ir(.sub, .Kl)   #fold; // -
fn mul(a: i64, b: i64) i64 #ir(.mul, .Kl)   #fold; // *
fn div(a: i64, b: i64) i64 #ir(.div, .Kl)   #fold; // /
fn eq(a: i64, b: i64) bool  #ir(.ceql, .Kl)  #fold; // ==
fn ne(a: i64, b: i64) bool  #ir(.cnel, .Kl)  #fold; // !=
fn le(a: i64, b: i64) bool  #ir(.cslel, .Kl) #fold; // <=
fn ge(a: i64, b: i64) bool  #ir(.csgel, .Kl) #fold; // >=
fn lt(a: i64, b: i64) bool  #ir(.csltl, .Kl) #fold; // <
fn gt(a: i64, b: i64) bool  #ir(.csgtl, .Kl) #fold; // >

// TODO: define behaviour when shift > log size. make it a SafetyCheck
fn shift_left(value: i64, shift_amount: i64) i64             #ir(.shl, .Kl) #fold;
fn shift_right_logical(value: i64, shift_amount: i64) i64    #ir(.shr, .Kl) #fold;
fn shift_right_arithmetic(value: i64, shift_amount: i64) i64 #ir(.sar, .Kl) #fold;
fn bit_or(a: i64, b: i64) i64   #ir(.or, .Kl)  #fold;
fn bit_and(a: i64, b: i64) i64 #ir(.and, .Kl) #fold;
fn bit_xor(a: i64, b: i64) i64 #ir(.xor, .Kl) #fold;
fn bit_xor(a: u64, b: u64) u64 #ir(.xor, .Kl) #fold;
fn bit_not(a: i64) i64 #fold = bit_xor(a, -1);

fn int_from_rawptr(ptr: rawptr) i64 #ir(.copy, .Kl);
fn rawptr_from_int(ptr: i64) rawptr #ir(.copy, .Kl);

fn add(a: f64, b: f64) f64 #ir(.add, .Kd); // +
fn sub(a: f64, b: f64) f64 #ir(.sub, .Kd); // -
fn mul(a: f64, b: f64) f64 #ir(.mul, .Kd); // *
fn div(a: f64, b: f64) f64 #ir(.div, .Kd); // /

fn eq(a: f64, b: f64) bool #ir(.ceqd, .Kl); // ==
fn ne(a: f64, b: f64) bool #ir(.cned, .Kl); // !=
fn le(a: f64, b: f64) bool #ir(.cled, .Kl); // <=
fn ge(a: f64, b: f64) bool #ir(.cged, .Kl); // >=
fn lt(a: f64, b: f64) bool #ir(.cltd, .Kl); // <
fn gt(a: f64, b: f64) bool #ir(.cgtd, .Kl); // >

// Note: remember i32->i8 is probably a noop if you wanted a value preserving intcast!
fn trunc(v: i64) u8   #ir(.copy, .Kw);
fn trunc(v: i64) u16 #ir(.copy, .Kw);
fn trunc(v: u64) u16 #ir(.copy, .Kw);
fn trunc(v: u64) u32 #ir(.copy, .Kw);
fn trunc(v: u32) u16 #ir(.copy, .Kw);
fn trunc(v: i32) u16 #ir(.copy, .Kw);
fn trunc(v: i64) u32 #ir(.copy, .Kw); 
fn trunc(v: u16) u8   #ir(.copy, .Kw);
fn trunc(v: u64) u8   #ir(.copy, .Kw);  // Nobody uses this it seems? 
fn trunc(v: u32) u8   #ir(.copy, .Kw);  // Nobody uses this it seems? 
fn trunc(v: i64) i32 #ir(.copy, .Kw);

// TODO: if you typo copy paste this conflicting overload, you try to call an uncompiled function! -- Jul 25 :FUCKED
// TODO: bring back system for testing that things fail with the right error message. 
// fn zext(v: u32) u64 #intrinsic(.ZeroExtend32To64);

fn zext(v: u32) u64 #ir(.extuw, .Kl) #fold;
fn zext(v: u8)  u64  #ir(.extub, .Kl) #fold;
fn zext(v: u8)  u32  #ir(.extub, .Kw) #fold;
fn zext(v: u16) u32 #ir(.extuh, .Kw) #fold;
fn zext(v: u32) i64 #ir(.extuw, .Kl) #fold;
fn zext(v: u16) i64 #ir(.extuh, .Kl) #fold;
fn zext(v: u16) u64 #ir(.extuh, .Kl) #fold;
fn zext(v: i32) i64 #ir(.extuw, .Kl) #fold;  // its a bad habit to get into using this. i should make unsigned numbers less painful. 
fn zext(v: u8)  i64  #ir(.extub, .Kl) #fold;
fn zext(v: u8)  u16  #ir(.extub, .Kw) #fold;

fn bit_or(a: i16, b: i16) i16 #redirect(Ty(i32, i32), i32);
fn bit_or(a: i32, b: i32) i32 #ir(.or, .Kw);
fn shift_left(a: i32, b: i64) i32 #ir(.shl, .Kw);
fn zext(v: u8) i32 #ir(.extub, .Kw) #fold;

fn int(a: f64) i64     #ir(.dtosi, .Kl); // preserves the value (not the bit pattern)
fn float(a: i64) f64   #ir(.sltof, .Kd); // preserves the value (not the bit pattern)
fn bitcast(a: f64) i64 #ir(.cast, .Kl);  // preserves the bit pattern (not the value)
fn bitcast(a: i64) f64 #ir(.cast, .Kd);  // preserves the bit pattern (not the value)
fn cast(v: f64) f32 #ir(.truncd, .Ks);
fn cast(v: f32) f64   #ir(.exts, .Kd);

fn intcast(v: i64) i32 #ir(.copy, .Kw);
fn intcast(v: i32) i64 #ir(.extsw, .Kl);
fn zext(v: i16) i32    #ir(.extsh, .Kw);
fn zext(v: i16) i64    #ir(.extsh, .Kl);

fn div_mod(big: i64, divisor: i64) Ty(i64, i64) #inline = {
    d := big / divisor;
    (d, big - d * divisor)
}

fn mod(big: i64, divisor: i64) i64 #ir(.rem, .Kl);
fn mod(big: u64, divisor: u64) u64 #ir(.urem, .Kl);
fn max(a: i64, b: i64) i64 #inline = if(lt(a, b), =>b, =>a);
fn min(a: i64, b: i64) i64 #inline = if(gt(a, b), =>b, =>a);
::if(f64);
fn max(a: f64, b: f64) f64 #inline = if(lt(a, b), =>b, =>a);
fn min(a: f64, b: f64) f64 #inline = if(gt(a, b), =>b, =>a);
fn max(a: u64, b: u64) u64 = {
    @debug_assert(a >= 0 && b >= 0, "TODO: remove this with new backend that does proper unsigned compares");
    @if(a < b, b, a)
}
fn neg(a: i64) i64 #fold #ir(.neg, .Kl);
fn neg(a: i16) i16 #fold #redirect(i64, i64); // TODO: sign extension?
fn neg(a: i32) i32 #fold #ir(.neg, .Kw);

fn abs(val: i64) i64 = {
    all_sign_bit := val.shift_right_arithmetic(63);
    // +) nop 
    // -) subtract one and flip all the bits. https://en.wikipedia.org/wiki/Two%27s_complement
    bit_xor(val + all_sign_bit, all_sign_bit)
}
fn abs(val: i32) i32 = if(val.ge(0), => val, => val.neg()); // TODO: wait for new backend where im consistant about int repr
fn eq(lhs: Symbol, rhs: Symbol) bool #redirect(Ty(u32, u32), bool);
fn to_bool(b: i64) bool = 
    b != 0;

// TODO: this gets a bit silly if I have to define this stuff all again but with different types. 
//       it would be cool if it could be done with traits. like anything with an ordering could have a fn max (bad example for floats i guess)
fn neg(a: f64) f64 #fold #ir(.neg, .Kd);

/// Yes, really...
/// TODO: this could be fewer instructions but this is more fun. 
fn is_nan(a: f64) bool = a != a;  
fn is_nan(a: f32) bool = a != a;

// Convert an i64 to an i<bit_count> with the leading bits 0.
// so the mask takes care of negative numbers. 
fn signed_truncate(x: i64, bit_count: i64) u32 = {
    mask := 1.shift_left(bit_count).sub(1);
    mask.bit_and(x).trunc()
}

fn s_trunc(x: i64, $bit_count: i64) SInt(bit_count) #generic = {
    mask :: 1.shift_left(bit_count).sub(1);
    mask.bit_and(x) // todo: this wont work for real sizes i8/i16, it will want a trunc. really need to store the other small sizes properly and have real bit fields.
}

////////////////
/// Booleans ///
////////////////
// u1 is a number so this is kinda like math... idk.

// Note: its a fun game to make this call the other fn if instead of the macro and measure how much slower it compiles itself. 
fn if(cond: bool, $then: @Fn() void) void =
    @if(cond, then(), ());

fn if($T: Type) void = {
    fn if(cond: bool, $then: @Fn() T, $else: @Fn() T) T = @if(cond, then(), else());
}

// TODO: auto impl generics
::if(i64); ::if(void); ::if(bool); ::if(u8); 

// don't #inline this or not() gets confusing. alternatively revert not to `b.int() == 0`
fn eq(a: bool, b: bool) bool #ir(.ceqw, .Kw) #fold = 
    a.int().eq(b.int());

fn not(b: bool) bool #inline #fold =
    b == false;

fn int(val: bool) i64 #fold #ir(.extuw, .Kl); // bool is always 0 or 1

// Note: these are not short circuiting because arguments are always evaluated. 
fn and(a: bool, b: bool) bool #ir(.and, .Kw);
fn or(a: bool, b: bool)  bool #ir(.or, .Kw);
fn xor(a: bool, b: bool) bool #ir(.xor, .Kw);

fn nand(a: bool, b: bool) bool = or(!a, !b);

// Note: these are short circuiting. The closure will only be called if necessary. 
fn and(a: bool, $b: @Fn() bool) bool = 
    @if(a, b(), false);  // operator &&
fn or(a: bool, $b: @Fn() bool) bool = 
    @if(a, true, b());   // operator ||
// TODO: those should go in sugar.fr

/////////////////
/// Fake Ints ///
/////////////////
// TODO: this sucks. dynamically make these for uxx & ixx.
// TODO: store these in the next sane size up, not always i64. 

fn UInt(bits: i64) Type #fold = IntType(bits, false);
fn SInt(bits: i64) Type #fold = IntType(bits, true);

u1  :: UInt(1);
u2  :: UInt(2);
u4  :: UInt(4);
u5  :: UInt(5);
u6  :: UInt(6);
u8  :: UInt(8);
u12 :: UInt(12);
u16 :: UInt(16);
u19 :: UInt(19);
u32 :: UInt(32);
u64 :: UInt(64);
i7  :: SInt(7);
i8  :: SInt(8);
i16 :: SInt(16);
i19 :: SInt(19);
i26 :: SInt(26);
i32 :: SInt(32);
i33 :: SInt(33);

/////////////////
/// Redirects ///
/////////////////

// TODO: new backend has real unsigned cmp, use them here.

fn Ord($T: Type) void = {
    fn eq(lhs: T, rhs: T) bool = lhs.int().eq(rhs.int());
    fn ne(lhs: T, rhs: T) bool = lhs.int().ne(rhs.int());
    fn lt(lhs: T, rhs: T) bool = lhs.int().lt(rhs.int());
    fn le(lhs: T, rhs: T) bool = lhs.int().le(rhs.int());
    fn gt(lhs: T, rhs: T) bool = lhs.int().gt(rhs.int());
    fn ge(lhs: T, rhs: T) bool = lhs.int().ge(rhs.int());
}

fn int(i: i64) i64 #unsafe_noop_cast;
fn int(i: u8)  i64 #inline = i.zext();    ::Ord(u8);
fn int(i: u32) i64 #inline = i.zext();    ::Ord(u32);
fn int(i: u64) i64 #inline = i.bitcast(); 
fn int(i: i32) i64 #inline = i.intcast(); ::Ord(i32);
fn int(i: u16) i64 #inline = i.zext();    ::Ord(u16);
fn int(i: i16) i64 #inline = i.zext();    ::Ord(i16);// TODO: wrong!!! signed

fn eq(lhs: u64, rhs: u64) bool #ir(.ceql, .Kl);
fn ne(lhs: u64, rhs: u64) bool #ir(.cnel, .Kl);
fn lt(lhs: u64, rhs: u64) bool #ir(.cultl, .Kl);
fn le(lhs: u64, rhs: u64) bool #ir(.culel, .Kl);
fn gt(lhs: u64, rhs: u64) bool #ir(.cugtl, .Kl);
fn ge(lhs: u64, rhs: u64) bool #ir(.cugel, .Kl);

// TODO: new backend has single instruction 32 bit math. use them here.

::{
    UNum :: fn($T: Type) void = {
        fn add(lhs: T, rhs: T) T = lhs.int().add(rhs.int()).trunc();
        fn sub(lhs: T, rhs: T) T = lhs.int().sub(rhs.int()).trunc();
        fn mul(lhs: T, rhs: T) T = lhs.int().mul(rhs.int()).trunc();
        fn div(lhs: T, rhs: T) T = lhs.int().div(rhs.int()).trunc(); 
        fn min(lhs: T, rhs: T) T = lhs.int().min(rhs.int()).trunc(); 
        fn max(lhs: T, rhs: T) T = lhs.int().max(rhs.int()).trunc(); 
        fn mod(lhs: T, rhs: T) T = lhs.int().mod(rhs.int()).trunc();
    };
    UNum(u8); 
    UNum(u16);
    UNum(u32);
    UNum(i32);
    // div/mod/min/max would be wrong for u64 because signed. 
};

#redirect(Ty(i64, i64), i64) #fold fn shift_left(value: u64, shift_amount: i64) u64;
#redirect(Ty(i64, i64), i64)  fn bit_or(lhs: u64, rhs: u64) u64;
#redirect(Ty(i64, i64), i64)  fn bit_and(lhs: u64, rhs: u64) u64;
#redirect(i64, i64)           fn bit_not(value: u64) u64;
#redirect(Ty(i64, i64), i64)  fn mul(lhs: u64, rhs: u64) u64;
#redirect(Ty(i64, i64), i64)  fn add(lhs: u64, rhs: u64) u64;
#redirect(Ty(i64, i64), i64)  fn sub(lhs: u64, rhs: u64) u64;  // TODO: does llvm make signed overflow UB?

fn div(lhs: u64, rhs: u64) u64 #ir(.udiv, .Kl);

fn bit_or(lhs: u32, rhs: u32) u32 #ir(.or, .Kw);
    
fn bit_and(lhs: u32, rhs: u32) u32 #ir(.and, .Kw);

fn shift_left(value: u32, shift_amount: i64) u32 #ir(.shl, .Kw);

fn bit_or(lhs: u8, rhs: u8) u8 #ir(.or, .Kw) = 
    lhs.int().bit_or(rhs.int()).trunc();
// TODO: bit_not can't just redirect because that would flip the leading zeros too. 

fn shift_right_logical(value: u32, shift_amount: i64) u32 #ir(.shr, .Kw);
fn shift_right_logical(value: u64, shift_amount: i64) u64 #ir(.shr, .Kl);
fn shift_right_logical(value: i32, shift_amount: i64) i32 #ir(.shr, .Kw);

fn bit_and(a: i32, b: i32) i32 #redirect(Ty(u32, u32), u32);

// TODO: :SLOW?
fn pow(base: i64, exp: i64) i64 = {
    n := 1;
    range(0, exp) {_|
        n *= base;
    };
    n
}

MAX_u16 :: 0xFFFF;
MAX_i32 :: 2147483647;
MIN_i32 :: -2147483648;
MAX_u32 :: 0xFFFFFFFF;
MAX_i64 :: 9223372036854775807;
MIN_i64 :: -MAX_i64 - 1;  // -9223372036854775808 but it feels fragile to parse that (tho it does overflow into the right thing). 
                          // there are more negative numbers than positive numbers!

MANTISSA_DIGITS_f64 :: 53;
MANTISSA_DIGITS_f32 :: 24;

fn eq(a: f32, b: f32) bool #ir(.ceqs, .Kl);
fn ne(a: f32, b: f32) bool #ir(.cnes, .Kl);
fn le(a: f32, b: f32) bool #ir(.cles, .Kl);
fn ge(a: f32, b: f32) bool #ir(.cges, .Kl);
fn lt(a: f32, b: f32) bool #ir(.clts, .Kl);
fn gt(a: f32, b: f32) bool #ir(.cgts, .Kl);

fn add(a: f32, b: f32) f32 #ir(.add, .Ks);
fn sub(a: f32, b: f32) f32 #ir(.sub, .Ks);
fn mul(a: f32, b: f32) f32 #ir(.mul, .Ks);
fn div(a: f32, b: f32) f32 #ir(.div, .Ks);

fn trailing_zeros(b: i64) i64 #ir(.ctz, .Kl);
fn count_ones(b: u64) u32 #ir(.ones, .Kl);
fn count_ones(b: *u64) u32 = 
    count_ones(b[]);

NS_PER_MS :: 1000 * 1000;
NS_PER_S  :: NS_PER_MS * 1000;
NS_PER_US :: 1000;
US_PER_S  :: 1000 * 1000;

// TODO: this is dumb. there's a cpu instruction for this. 
fn sqrt(x: f64) f64 = 
    newton_root(x, 2);

fn cbrt(x: f64) f64 = 
    newton_root(x, 3);

// TODO: im sure there's some trick because we know how much precision a float has. 
fn newton_root(x: f64, $p: i64) f64 = {
    guess := x;
    range(0, 15 /* chosen by fair die roll */) { _ |
        x1 := 1.0;
        x2 := guess;
        inline_range(0, p - 1) { $_ |
            x1 *= guess;
            x2 *= guess;
        };
        guess -= (x2 - x) / (x1 * p.float());
    };
    guess
}

fn floor(x: f64) f64 =
    x.int().float();

fn fmodf(a: f32, b: f32) f32 #libc;
fn roundf(a: f32) f32 #libc;

fn align_to(offset: i64, align: i64) i64 = 
    ((offset + align - 1) / align) * align;

fn ms(t: TimeSpec) i64 =
    t.seconds * 1000 + t.nanoseconds / NS_PER_MS;

PI :: 3.1415926535897932;

fn clamp(v: f64, lo: f64, hi: f64) f64 #inline =
    if(v < lo, => lo, => if(v > hi, => hi, => v));

fn deg_to_rad(deg: f32) f32 = 
    deg * PI / 180.0;

fn rad_to_deg(rad: f32) f32 = 
    rad * 180.0 / PI;

fn bit_and(a: u8, b: u8) u8 #ir(.and, .Kw);
fn shift_right_logical(a: u8, b: u8) u8 #ir(.shr, .Kw);
fn shift_right_logical(a: u16, b: u16) u16 #ir(.shr, .Kw);
fn shift_left(a: u8, b: i64) u8 = 
    a.zext().shift_left(b).trunc();
fn bit_and(a: u16, b: u16) u16 #ir(.and, .Kw);