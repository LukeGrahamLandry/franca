
fn div_mod(big: i64, divisor: i64) Ty(i64, i64) #inline = {
    d := big / divisor;
    (d, big - d * divisor)
}

fn mod(big: i64, divisor: i64) i64 #ir(.rem, .Kl) = 
    sub(big, mul(div(big, divisor), divisor));
fn mod(big: u64, divisor: u64) u64 #ir(.urem, .Kl) =
    sub(big, mul(div(big, divisor), divisor));
fn max(a: i64, b: i64) i64 #inline = if(lt(a, b), =>b, =>a);
fn min(a: i64, b: i64) i64 #inline = if(gt(a, b), =>b, =>a);
::if(f64);
fn max(a: f64, b: f64) f64 #inline = if(lt(a, b), =>b, =>a);
fn min(a: f64, b: f64) f64 #inline = if(gt(a, b), =>b, =>a);
fn max(a: u64, b: u64) u64 = {
    @debug_assert(a >= 0 && b >= 0, "TODO: remove this with new backend that does proper unsigned compares");
    @if(a < b, b, a)
}
fn neg(a: i64) i64 #fold #ir(.neg, .Kl) = 
    sub(0, a);
fn neg(a: i16) i16 #fold #redirect(i64, i64); // TODO: sign extension?
fn neg(a: i32) i32 #fold #ir(.neg, .Kw) = 
    sub(@as(i32)0, a);

fn abs(val: i64) i64 = {
    all_sign_bit := val.shift_right_arithmetic(63);
    // +) nop 
    // -) subtract one and flip all the bits. https://en.wikipedia.org/wiki/Two%27s_complement
    bit_xor(val + all_sign_bit, all_sign_bit)
}
fn abs(val: i32) i32 = if(val.ge(0), => val, => val.neg()); // TODO: wait for new backend where im consistant about int repr
fn eq(lhs: Symbol, rhs: Symbol) bool #redirect(Ty(u32, u32), bool);
fn to_bool(b: i64) bool = 
    b != 0;

// TODO: this gets a bit silly if I have to define this stuff all again but with different types. 
//       it would be cool if it could be done with traits. like anything with an ordering could have a fn max (bad example for floats i guess)
fn neg(a: f64) f64 #fold #ir(.neg, .Kd) = 
    sub(0.0, a);

/// Yes, really...
fn is_nan(a: f64) bool = 
    a != a;  // TODO: this could be fewer instructions but this is more fun. 

fn is_nan(a: f32) bool = a != a;
// Convert an i64 to an i<bit_count> with the leading bits 0.
// so the mask takes care of negative numbers. 
fn signed_truncate(x: i64, bit_count: i64) u32 = {
    mask := 1.shift_left(bit_count).sub(1);
    mask.bit_and(x).trunc()
}

fn s_trunc(x: i64, $bit_count: i64) SInt(bit_count) #generic = {
    mask :: 1.shift_left(bit_count).sub(1);
    mask.bit_and(x) // todo: this wont work for real sizes i8/i16, it will want a trunc. really need to store the other small sizes properly and have real bit fields.
}

////////////////
/// Booleans ///
////////////////
// u1 is a number so this is kinda like math... idk.

// Note: its a fun game to make this call the other fn if instead of the macro and measure how much slower it compiles itself. 
fn if(cond: bool, $then: @Fn() void) void =
    @if(cond, then(), ());

fn if($T: Type) void = {
    fn if(cond: bool, $then: @Fn() T, $else: @Fn() T) T = @if(cond, then(), else());
}

// TODO: auto impl generics
::if(i64); ::if(void); ::if(bool); ::if(u8); 

// don't #inline this or not() gets confusing. alternatively revert not to `b.int() == 0`
fn eq(a: bool, b: bool) bool #ir(.ceqw, .Kw) #fold = 
    a.int().eq(b.int());

fn not(b: bool) bool #inline #fold =
    b == false;

fn int(val: bool) i64 #intrinsic(.ZeroExtend8To64) #fold #ir(.extuw, .Kl); // bool is always 0 or 1

// Note: these are not short circuiting because arguments are always evaluated. 
fn and(a: bool, b: bool) bool #ir(.and, .Kw) = 
    a.if(=> b, => false);
fn or(a: bool, b: bool)  bool #ir(.or, .Kw) = 
    a.if(=> true, => b);
fn xor(a: bool, b: bool) bool #ir(.xor, .Kw) = 
    a.if(=> !b, => b);

fn nand(a: bool, b: bool) bool = or(!a, !b);

// Note: these are short circuiting. The closure will only be called if necessary. 
fn and(a: bool, $b: @Fn() bool) bool = 
    @if(a, b(), false);  // operator &&
fn or(a: bool, $b: @Fn() bool) bool = 
    @if(a, true, b());   // operator ||
// TODO: those should go in sugar.fr

/////////////////
/// Fake Ints ///
/////////////////
// TODO: this sucks. dynamically make these for uxx & ixx.
// TODO: store these in the next sane size up, not always i64. 

fn UInt(bits: i64) Type #fold = IntType(bits, false);
fn SInt(bits: i64) Type #fold = IntType(bits, true);

u1  :: UInt(1);
u2  :: UInt(2);
u4  :: UInt(4);
u5  :: UInt(5);
u6  :: UInt(6);
u8  :: UInt(8);
u12 :: UInt(12);
u16 :: UInt(16);
u19 :: UInt(19);
u32 :: UInt(32);
u64 :: UInt(64);
i7  :: SInt(7);
i8  :: SInt(8);
i16 :: SInt(16);
i19 :: SInt(19);
i26 :: SInt(26);
i32 :: SInt(32);
i33 :: SInt(33);

/////////////////
/// Redirects ///
/////////////////

// TODO: new backend has real unsigned cmp, use them here.

fn Ord($T: Type) void = {
    fn eq(lhs: T, rhs: T) bool = lhs.int().eq(rhs.int());
    fn ne(lhs: T, rhs: T) bool = lhs.int().ne(rhs.int());
    fn lt(lhs: T, rhs: T) bool = lhs.int().lt(rhs.int());
    fn le(lhs: T, rhs: T) bool = lhs.int().le(rhs.int());
    fn gt(lhs: T, rhs: T) bool = lhs.int().gt(rhs.int());
    fn ge(lhs: T, rhs: T) bool = lhs.int().ge(rhs.int());
}

fn int(i: i64) i64 #unsafe_noop_cast;
fn int(i: u8)  i64 #inline = i.zext();    ::Ord(u8);
fn int(i: u32) i64 #inline = i.zext();    ::Ord(u32);
fn int(i: u64) i64 #inline = i.bitcast(); 
fn int(i: i32) i64 #inline = i.intcast(); ::Ord(i32);
fn int(i: u16) i64 #inline = i.zext();    ::Ord(u16);
fn int(i: i16) i64 #inline = i.zext();    ::Ord(i16);// TODO: wrong!!! signed

// note: these are wrong on the legacy backend because it only does signed numbers. 
//       but they're right on the new one.  TODO: clean this up. 
fn eq(lhs: u64, rhs: u64) bool #ir(.ceql, .Kl)  = lhs.int().eq(rhs.int());
fn ne(lhs: u64, rhs: u64) bool #ir(.cnel, .Kl)  = lhs.int().ne(rhs.int());
fn lt(lhs: u64, rhs: u64) bool #ir(.cultl, .Kl) = lhs.int().lt(rhs.int());
fn le(lhs: u64, rhs: u64) bool #ir(.culel, .Kl) = lhs.int().le(rhs.int());
fn gt(lhs: u64, rhs: u64) bool #ir(.cugtl, .Kl) = lhs.int().gt(rhs.int());
fn ge(lhs: u64, rhs: u64) bool #ir(.cugel, .Kl) = lhs.int().ge(rhs.int());

// TODO: new backend has single instruction 32 bit math. use them here.

::{
    UNum :: fn($T: Type) void = {
        fn add(lhs: T, rhs: T) T = lhs.int().add(rhs.int()).trunc();
        fn sub(lhs: T, rhs: T) T = lhs.int().sub(rhs.int()).trunc();
        fn mul(lhs: T, rhs: T) T = lhs.int().mul(rhs.int()).trunc();
        fn div(lhs: T, rhs: T) T = lhs.int().div(rhs.int()).trunc(); 
        fn min(lhs: T, rhs: T) T = lhs.int().min(rhs.int()).trunc(); 
        fn max(lhs: T, rhs: T) T = lhs.int().max(rhs.int()).trunc(); 
        fn mod(lhs: T, rhs: T) T = lhs.int().mod(rhs.int()).trunc();
    };
    UNum(u8); 
    UNum(u16);
    UNum(u32);
    UNum(i32);
    // div/mod/min/max would be wrong for u64 because signed. 
};

#redirect(Ty(i64, i64), i64) #fold fn shift_left(value: u64, shift_amount: i64) u64;
#redirect(Ty(i64, i64), i64)  fn bit_or(lhs: u64, rhs: u64) u64;
#redirect(Ty(i64, i64), i64)  fn bit_and(lhs: u64, rhs: u64) u64;
#redirect(i64, i64)           fn bit_not(value: u64) u64;
#redirect(Ty(i64, i64), i64)  fn mul(lhs: u64, rhs: u64) u64;
#redirect(Ty(i64, i64), i64)  fn add(lhs: u64, rhs: u64) u64;
#redirect(Ty(i64, i64), i64)  fn sub(lhs: u64, rhs: u64) u64;  // TODO: does llvm make signed overflow UB?

fn div(lhs: u64, rhs: u64) u64 #ir(.udiv, .Kl) = 
    bitcast(lhs.int() / rhs.int());  // wrong on old backend

fn bit_or(lhs: u32, rhs: u32) u32 #ir(.or, .Kw) = 
    bit_or(@as(i64) lhs.zext(), @as(i64) rhs.zext()).trunc();
    
fn bit_and(lhs: u32, rhs: u32) u32 #ir(.and, .Kw) = 
    bit_and(@as(i64) lhs.zext(), @as(i64) rhs.zext()).trunc();

fn shift_left(value: u32, shift_amount: i64) u32 #ir(.shl, .Kw) #fold = 
    shift_left(@as(i64) value.zext(), shift_amount).trunc();

fn bit_or(lhs: u8, rhs: u8) u8 #ir(.or, .Kw) = 
    lhs.int().bit_or(rhs.int()).trunc();
// TODO: bit_not can't just redirect because that would flip the leading zeros too. 

fn shift_right_logical(value: u32, shift_amount: i64) u32 #ir(.shr, .Kw) = 
    shift_right_logical(@as(i64) value.zext(), shift_amount).trunc();
fn shift_right_logical(value: u64, shift_amount: i64) u64 #ir(.shr, .Kl) = 
    shift_right_logical(@as(i64) value.bitcast(), shift_amount).bitcast();
fn shift_right_logical(value: i32, shift_amount: i64) i32 #ir(.shr, .Kw) = 
    shift_right_logical(value.intcast(), shift_amount).intcast();

fn bit_and(a: i32, b: i32) i32 #redirect(Ty(u32, u32), u32);

// TODO: :SLOW?
fn pow(base: i64, exp: i64) i64 = {
    n := 1;
    range(0, exp) {_|
        n *= base;
    };
    n
}

MAX_u16 :: 0xFFFF;
MAX_i32 :: 2147483647;
MIN_i32 :: -2147483648;
MAX_u32 :: 0xFFFFFFFF;
MAX_i64 :: 9223372036854775807;
MIN_i64 :: -MAX_i64 - 1;  // -9223372036854775808 but it feels fragile to parse that (tho it does overflow into the right thing). 
                          // there are more negative numbers than positive numbers!

MANTISSA_DIGITS_f64 :: 53;
MANTISSA_DIGITS_f32 :: 24;

// note: subtly wrong on old backends!
fn eq(a: f32, b: f32) bool #ir(.ceqs, .Kl) = (eq(@as(f64) a.cast(), b.cast()));
fn ne(a: f32, b: f32) bool #ir(.cnes, .Kl) = (ne(@as(f64) a.cast(), b.cast()));
fn le(a: f32, b: f32) bool #ir(.cles, .Kl) = (le(@as(f64) a.cast(), b.cast()));
fn ge(a: f32, b: f32) bool #ir(.cges, .Kl) = (ge(@as(f64) a.cast(), b.cast()));
fn lt(a: f32, b: f32) bool #ir(.clts, .Kl) = (lt(@as(f64) a.cast(), b.cast()));
fn gt(a: f32, b: f32) bool #ir(.cgts, .Kl) = (gt(@as(f64) a.cast(), b.cast()));

fn add(a: f32, b: f32) f32 #ir(.add, .Ks) = cast(add(@as(f64) a.cast(), b.cast()));
fn sub(a: f32, b: f32) f32 #ir(.sub, .Ks) = cast(sub(@as(f64) a.cast(), b.cast()));
fn mul(a: f32, b: f32) f32 #ir(.mul, .Ks) = cast(mul(@as(f64) a.cast(), b.cast()));
fn div(a: f32, b: f32) f32 #ir(.div, .Ks) = cast(div(@as(f64) a.cast(), b.cast()));

// TODO: do this properly. i cheated with `int count_ones(long *x) { return __builtin_popcountll(*x); }`
// the software one below was 30/560 samples, this one is 6. 
// too bad it has to spill everything for it. would be better if i had real inline asm. 
fn count_ones(b: *u64) u32 #asm #aarch64 = (
    0xfd400000, // ldr d0, [x0]
    0x0e205800, // cnt.8b v0, v0
    0x2e303800, // uaddlv.8b h0, v0
    0x1e260000, // fmov w0, s0
    ret()
);

fn count_ones(b: *u64) u32 #asm #x86_bytes = (fn(out) = {
    out.push_all(@slice(0xf3, 0x48, 0x0f, 0xb8, 0x07)); // popcntq	(%rdi), %rax
    out.push(@as(u8) PrimaryOp.Ret);
});

fn count_ones(b: u64) u32 = 
    count_ones(b&);

fn trailing_zeros(b: i64) i64 #asm #aarch64 = (
    // 0b101101011000000000110 is ctz... but it doesn't exist? I guess i don't have FEAT_CSSC?
    @bits(Bits.X64, 0b101101011000000000000, @as(u5) x0, @as(u5) x0),  // rbit
    @bits(Bits.X64, 0b101101011000000000100, @as(u5) x0, @as(u5) x0),  // clz
    ret()
);

// TODO: this returns 0 if b is 0 which is unfortunate but i never call it that way yet. should be fixed anyway tho. 
// I cheated with `int trailing_zeroes(long x) { return __builtin_ctzl(x); }`, needs -msse4.2
fn trailing_zeros(b: i64) i64 #asm #x86_bytes = (fn(out) = {
    out.push_all(@slice(0x48, 0x0f, 0xbc, 0xc7)); // bsfq	%rdi, %rax
    out.push(@as(u8) PrimaryOp.Ret);
});

NS_PER_MS :: 1000 * 1000;
NS_PER_S  :: NS_PER_MS * 1000;
NS_PER_US :: 1000;
US_PER_S  :: 1000 * 1000;

// TODO: do these myself
fn sqrt(x: f64) f64 #libc;
fn cbrt(x: f64) f64 #libc;
fn floor(x: f64) f64 #libc;
fn fmodf(a: f32, b: f32) f32 #libc;
fn roundf(a: f32) f32 #libc;

fn align_to(offset: i64, align: i64) i64 = 
    ((offset + align - 1) / align) * align;

fn ms(t: TimeSpec) i64 =
    t.seconds * 1000 + t.nanoseconds / NS_PER_MS;

PI :: 3.1415926535897932;

fn clamp(v: f64, lo: f64, hi: f64) f64 #inline =
    if(v < lo, => lo, => if(v > hi, => hi, => v));

fn deg_to_rad(deg: f32) f32 = 
    deg * PI / 180.0;

fn rad_to_deg(rad: f32) f32 = 
    rad * 180.0 / PI;

fn bit_and(a: u8, b: u8) u8 #ir(.and, .Kw);
fn shift_right_logical(a: u8, b: u8) u8 #ir(.shr, .Kw);
fn shift_right_logical(a: u16, b: u16) u16 #ir(.shr, .Kw);
fn shift_left(a: u8, b: i64) u8 = 
    a.zext().shift_left(b).trunc();
fn bit_and(a: u16, b: u16) u16 #ir(.and, .Kw);