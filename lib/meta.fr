
fn with(vtable: *ImportVTable, comp: Compiler) CompCtx = 
    (data = comp, vtable = vtable);

fn get_tagged(comp: CompCtx, tag: Str) Slice(FuncId) = {
    tag := {comp.vtable.intern_string}(comp.data, tag);
    {comp.vtable.get_fns_with_tag}(comp.data, tag)
}

// TODO: overloading
fn get_unique_named(c: CompCtx, name: Str) ?FuncId = {
    f := {c.vtable.intern_string}(c.data, name);
    {c.vtable.find_unique_func}(c.data, f)
}

// TODO: no unwrap! return error.
fn get_jitted(c: CompCtx, f: FuncId) rawptr = {
    res := {c.vtable.compile_func}(c.data, f, .Jit);
    or res { err |
        c.report_error(err)
    };
    ptr := {c.vtable.get_jitted_ptr}(c.data, f);
    ptr := or ptr { err |
        c.report_error(err)
    };
    ptr
}

fn default_build_options(vtable: *ImportVTable) *BuildOptions = 
    temp().boxed(BuildOptions, ());

fn add_comptime_library(c: CompCtx, lib_name: Symbol, handle: import("@/lib/dynamic_lib.fr").Handle) void = {
    {c.vtable.add_comptime_library}(c.data, lib_name, bit_cast_unchecked(import("@/lib/dynamic_lib.fr").Handle, rawptr, handle));
}

fn report_error(c: CompCtx, err: *CompileError) Never = {
    out: List(u8) = list(temp());
    {c.vtable.fmt_error}(c.data, err, out&);
    panic(out.items())
}

fn new_with_src(vtable: *ImportVTable, src: Str, options: *BuildOptions) Compiler = {
    new_with_src(vtable, src, "-", options)
}

// This copies src,filename,options 
fn new_with_src(vtable: *ImportVTable, src: Str, filename: Str, options: *BuildOptions) Compiler = {
    comp := {vtable.init_compiler}(options);
    src := src.shallow_copy(vtable'get_alloc(comp));
    filename := filename.shallow_copy(vtable'get_alloc(comp));
    file := {vtable.add_file}(comp, filename, src).shrink();
    stmts := {vtable.parse_stmts}(comp, file&);
    stmts := or stmts { err |
        vtable.with(comp).report_error(err)
    };
    //@debug_assert(stmts.len != 0, "parse_stmts returned 0");
    res := {vtable.make_and_resolve_and_compile_top_level}(comp, stmts);
    or res { err |
        vtable.with(comp).report_error(err)
    };
    comp
}

fn intern_string(comp: CompCtx, s: Str) Symbol = {
    {comp.vtable.intern_string}(comp.data, s)
}

fn get_string(comp: CompCtx, s: Symbol) Str = {
    {comp.vtable.get_string}(comp.data, s)
}

fn get_function(comp: CompCtx, f: FuncId) *Func = {
    {comp.vtable.get_function}(comp.data, f)
}

fn get_function_name(comp: CompCtx, f: FuncId) Symbol = {
    comp.get_function(f)[].name
}

UnknownType :: @builtin UnknownType;

fn intern_type(ty: TypeInfo) Type = intern_type_ref(ty&);

// TODO: really you want the compiler ffi things to be defined here so they're easy to see and hang comments off. 
//       but for now its convient to have thier signetures in the same place as i assign thier comptime_addr.  -- Apr 20

fn const_eval($T: Type, value: FatExpr) T #generic = 
    const_eval(T)(value);

fn const_eval($T: Type) (@Fn(value: FatExpr) T) #generic = (fn(value: FatExpr) T = {
    out := @uninitialized T;
    const_eval(value, T, T.raw_from_ptr(out&)); // crashes on type error
    out
});

fn ne(a: Type, b: Type) bool #redirect(Ty(u32, u32), bool);

// This checks if types are identical. 
// You might want to use can_assign_types instead since that has more knowledge about assignable but not identical types.
fn eq(a: Type, b: Type) bool #redirect(Ty(u32, u32), bool);

#fold fn int(s: Symbol) i64 = {
    i := ptr_cast_unchecked(From = Symbol, To = u32, ptr = s&)[];
    i.zext()
}

fn get_flag(self: *Func, flag: FnFlag) bool #inline = {
    mask := 1.shift_left(@as(i64) flag);
    self.flags.bit_and(mask) != 0
}

fn set_flag(self: *Func, flag: FnFlag) void = {
    mask := 1.shift_left(@as(i64) flag);
    new := self.flags.bit_or(mask);
    self.flags = new;
}

fn unset_flag(self: *Func, flag: FnFlag) void = {
    mask := 1.shift_left(@as(i64) flag);
    new := self.flags.bit_and(mask.bit_not());
    self.flags = new;
}

fn ident(self: *Binding) ?Symbol = {
    @match(self.name) {
        fn Var(v) => (Some = v.name);
        fn Ident(v) => (Some = v);
        @default => .None;
    }
}

fn ne(a: FuncId, b: FuncId) bool #redirect(Ty(u32, u32), bool);

fn TypedIndex($T: Type) void = {
    fn as_index(f: T) i64 = f.to_index().zext();
    
    fn to_index(f: T) u32 #ir(.copy, .Kw);
    
    fn from_index(idx: i64) T = {
        val: u32 = idx.trunc();
        ptr_cast_unchecked(From = u32, To = T, ptr = val&)[]
    }
}

:: TypedIndex(FuncId);
:: TypedIndex(OverloadSet);
:: TypedIndex(LabelId);
:: TypedIndex(Type);

fn Ty(a: Type, b: Type, c: Type, d: Type, e: Type) Type #fold = 
    Ty(@slice(a, b, c, d, e));

fn has_pointers(T: Type) bool #fold = {
    i := T.get_meta();
    i.contains_pointers
}

fn align_of(T: Type) i64 #fold = {
    i := T.get_meta();
    i.align_bytes.zext()
}

fn get_fields(S: Type) Slice(Field) #fold = {
    info := get_type_info_ref(S); 
    assert(info.is(.Struct), "Expected struct type");
    info.Struct.fields.items() 
}

get_field_type :: fn(S: Type, name: Symbol) Type #fold = {
    fields := S.get_fields();
    for fields { f | 
        if f.name == name {
            return(f.ty);
        };
    };
    debug_log_type(S);
    panic("Could not find field")
};

fn get_variants(S: Type) Slice(Ty(Symbol, Type)) #fold = {
    info := get_type_info_ref(S); 
    assert(info.is(.Tagged), "Expected @tagged type");
    fuck := info.Tagged.cases;
    ::List(Ty(Symbol, Type));
    fuck.items() 
}

// SAFETY: S must be a struct with field f. 
get_field_ptr :: fn($S: Type, self: *S, $f: *Field) *f[].ty #generic #inline = {
    inner := S.int_from_ptr(self);
    inner := inner + (:: f[].byte_offset);
    inner := f[].ty.ptr_from_int(inner);
    inner
};

get_variant_ptr :: fn($E: Type, self: *E, $t: E.Tag()) *E.get_variant_type(t) #generic #inline = {
    inner := E.int_from_ptr(self);
    inner := inner + 8;
    inner := E.get_variant_type(t).ptr_from_int(inner);
    inner
};

fn get_default_value($field: *Field) ?field.ty #generic #fold = {
    #use("@/compiler/ast_external.fr");
    var := field.get_default() || return(.None);
    expr: FatExpr = (expr = (GetVar = var), ty = field.ty, done = false, loc = @source_location());
    (Some = const_eval(field.ty, expr))
};

// TODO: if you get garbage memory / safety cvheck failed here its becasue it doesnt typecheck maybe. -- Jul 16 :FUCKED
get_variant_type :: fn($E: Type, t: E.Tag()) Type #generic #fold = {
    cases := E.get_variants();
    ::tagged(E); // :sema_regression. but might just be order?
    // TODO: it doesn't like this being debug_assert when i moved some of the Hash impls on ast types into emit_bc. 
    if !(t.ordinal() < cases.len && t.ordinal() >= 0) {
        panic("ICE: invalid tag");
    };
    data := cases.index(t.ordinal());
    data._1
};

get_variant_ptr_for :: fn($E: Type, self: *E, $case: *Ty(Symbol, Type)) *case[]._1 #generic #inline = {
    inner := E.int_from_ptr(self);
    inner := inner + 8;
    inner := case[]._1.ptr_from_int(inner);
    inner
};

EnumBacking :: fn(E: Type) Type = {
    info := get_type_info_ref(T); 
    assert(info.is(.Enum), "Expected @enum type");
    info.Enum.raw 
};

get_cases :: fn($E: Type) []E #generic #fold = {
    info := get_type_info_ref(E); 
    assert(info.is(.Enum), "Expected @enum type");
    fields := info.Enum.fields&;
    out: List(E) = list(fields.len, ast_alloc());
    each fields { f |
        ptr := f._1&.rawptr_from_value();
        ptr := E.ptr_from_raw(ptr);
        out&.push(ptr[]);
    };
    out.items()
};

fn enum_count(E: Type) i64 #fold = {
    info := get_type_info_ref(E); 
    assert(info.is(.Enum), "Expected @enum type");
    info.Enum.fields.len
}

for_enum :: fn($E: Type, $f: @Fn(e: E) void) void #generic = {
    values := @run E.get_cases();
    for(values, f);
};

inline_for_enum :: fn($E: Type, $f: @Fn(e: *E) void) void #generic = {
    values :: E.get_cases();
    @inline_for(values) f;
};

// TODO: footgun: they might not be in order by oridinal or the values might not even be ints. 
// TODO: make it clear that the slice should be in a constant somehow. 
fn get_enum_names(E: Type) Slice(Str) #fold = {
    info := get_type_info_ref(E);
    assert(info.is(.Enum), "expected enum type");
    names: List(Str) = list(ast_alloc());
    
    for (info.Enum.fields.items()) { i|
        names&.push(i._0.str());
    };
    names.items()
}

fn get_enum_names_symbols(E: Type) Slice(Symbol) = {
    info := get_type_info_ref(E);
    assert(info.is(.Enum), "expected enum type");
    names: List(Symbol) = list(ast_alloc());
    
    for (info.Enum.fields.items()) { i|
        names&.push(i._0);
    };
    names.items()
}

#use("@/lib/collections/map.fr") 
fn create_enum_name_table_outlined(E: Type, names: *PackedStrings) HashMap(Str, *u8).Raw = {
    // These are passed in so the binary doesn't include two copies of the name bytes if you also call E.name_str(). 
    packed, offs := names[];

    t: HashMap(Str, *u8) = init(ast_alloc());
    info := get_type_info_ref(E); 
    assert(info.is(.Enum), "Expected @enum type for from_name");
    enumerate info.Enum.fields.items() { i, f |
        ptr := f[]._1&.rawptr_from_value();
        // TODO: reboxing the copy is a dumb hack :SLOW -- Nov 14
        bytes1: []u8 = (ptr = u8.ptr_from_raw(ptr), len = size_of(E));
        bytes2 := bytes1.shallow_copy(ast_alloc());   // TODO: is alignment real
        
        name := packed.subslice(offs[i*2].zext(), offs[i*2+1].zext());
        t&.insert(name, bytes2.ptr);
    };
    t.raw&.zero_unused_slots();
    t.raw
}

// TODO: could do some fancy perfect hashing thing since we know the keys at comptime. 
#use("@/lib/collections/map.fr") 
fn from_name($E: Type, name: Str) ?E #generic = {
    @if(E.enum_count() < 5) {
        inline_for_enum E { $e |
            ee :: e[];
            if name == (:: ee.name_str()) {
                return(Some = ee);
            };
        }; 
        return(.None);
    };

    // TODO: we can't always store a hash table with E by value in a constant because we don't support field sizes that are not a multiple of 8 bytes -- Nov 14
    M :: HashMap(Str, *E);
    table :: @static(M.Raw) {
        t := create_enum_name_table_outlined(E, packed_enum_names(E));
        t2 := ptr_cast_unchecked(@type t, M.Raw, t&)[];  // IMPORTANT TO CAST BACK so the compiler knows to include the constants as the right type
        t2
    };
    
    ::M;
    if table.get(name) { p |
        return(Some = p[])
    };
    .None
}

// get an enum representing the fields of struct.
fn Fields($S: Type) Type = :: {
    T :: Ty(Symbol, Values);
    info := get_type_info_ref(S);
    assert(info.is(.Struct), "Fields expected struct type");
    ff := info.Struct.fields&;
    fields := T.list(ff.len, ast_alloc());
    enumerate ff { i, f |
        fields&.push(@as(T) (f.name, (Small = (i, 8))));
    };
    type: TypeInfo = (Enum = (raw = i64, fields = fields.as_raw(), sequential = true));
    intern_type_ref(type&)
}

fn is_sequential_enum(E: Type) bool #fold = {
    info := get_type_info_ref(E);
    info.is(.Enum) && info.Enum.sequential
}

fn is_enum(E: Type) bool #fold = {
    info := get_type_info_ref(E);
    info.is(.Enum)
}

fn offset_of($S: Type, f: Fields(S)) i64 #generic #fold = {
    ::enum(Fields(S));
    require_layout_ready(S); 
    info := get_type_info_ref(S);
    off := info.Struct.fields[f.raw()].byte_offset;
    off
}

fn is_ptr(T: Type) bool #fold = 
    get_type_info_ref(T).is(.Ptr);

fn Deref(T: Type) Type #fold = {
    info := get_type_info_ref(T);
    @assert(info.is(.Ptr), "used Deref(Type) on non-pointer");
    info.Ptr
}

fn has_field(S: Type, name: Symbol) bool #fold = {
    info := get_type_info_ref(S);
    if(!info.is(.Struct), => return(false));
    each info.Struct.fields& { f |
        if(f.name == name, => return(true));
    };
    false
}

fn has_const_field(S: Type, name: Symbol) bool #fold = {
    info := get_type_info_ref(S);
    if(!info.is(.Struct), => return(false));
    s := scope_of(Type, S);
    if(s == NOSCOPE, => return(false));  // :UpdateBoot get_constant used to crash on this
    get_constant(s, name).is_some()
}

fn has_feature($s: Str) bool #fold = 
    __builtin_compiler_has_feature(s);

fn scope_of($T: Type, t: T) ScopeId #generic = {
    it := scope_from_value(T, raw_from_ptr(T, t&));
    it.expect("scope_of to succeed")
}

fn get_constant($T: Type, s: ScopeId, name: Symbol) ?T #generic = {
    out := zeroed(T);
    get_constant(s, name, T, T.raw_from_ptr(out&)) || return(.None);
    (Some = out)
}

fn FieldType($S: Type, name: Fields(S)) Type #generic #fold = {
    ::enum(Fields(S));
    S.get_fields()[name.raw()].ty
}

fn get_field($S: Type, self: *S, $name: Fields(S)) *FieldType(S, name) #generic = {
    p := S.raw_from_ptr(self);
    p := p.offset(offset_of(S, name));
    p := FieldType(S, name).ptr_from_raw(p);
    p
}

fn scope_to_new_type($s: ScopeId) Type #fold = {
    t: TypeInfo = (Struct = (
        fields = empty(),
        layout_done = true,
        is_tuple = false,
        is_union = false,
        scope = s,
    ));
    intern_type_ref(t&)
}

// Holds a value that is zeroed when baking an AOT executable. 
// If a constant holds transient state, you can use this to avoid bloating your executable. 
// (even more important for something that changes depending on the build environment like lib/context.fr/OS). 
// The clearing happens both for real exes and .frc cache files. 
fn ClearOnAotBake($T: Type) Type = {
    Self :: @struct {
        it: T;
        // this forces TypeMeta.contains_pointers so the bake overload get called even if T is a value type
        _hack := @static(u8);  

    };
    
    fn bake_relocatable_value(self: *Self) Slice(BakedEntry) = {
        empty := zeroed Self;
        bytes := Self.cast_to_bytes(empty&);
        entries := dyn_bake_relocatable_value(bytes, Self, true);
        entries
    }
    
    Self
}

fn AsmFunction(template: FuncId, arm64: []u32, amd64: @FnPtr(out: *List(u8)) void) FuncId = {
    crash :: fn(out: *List(u8)) void = 
        out.push_all(@const_slice(0x00, 0x00, 0x0B));  // 0 locals. unreachable. end. 
    AsmFunction(template, arm64, amd64, crash)
}

fn AsmFunction(template: FuncId, arm64: []u32, amd64: @FnPtr(out: *List(u8)) void, wasm32: @FnPtr(out: *List(u8)) void, rv64: []u32) FuncId = {
    a := ast_alloc();
    amd64_bytes := u8.list(a);
    amd64(amd64_bytes&);
    wasm32_bytes := u8.list(a);
    ::import("@/lib/encoding/leb128.fr");
    uleb_patch_delta wasm32_bytes& {
        wasm32(wasm32_bytes&);
    };
    
    create_asm_func(template, arm64.shallow_copy(a).as_raw_list(), amd64_bytes.as_raw(), wasm32_bytes.as_raw(), rv64.shallow_copy(a).as_raw_list())
}

fn AsmFunction(template: FuncId, arm64: []u32, amd64: @FnPtr(out: *List(u8)) void, wasm32: @FnPtr(out: *List(u8)) void) FuncId = {
    #use("@/backend/rv64/bits.fr");
    crash :: @const_slice(ebreak, ebreak);
    AsmFunction(template, arm64, amd64, wasm32, crash)
}

fn create_asm_func(template: FuncId, arm64: RawList(u32), amd64: RawList(u8), wasm32: RawList(u8), rv64: RawList(u32)) FuncId = {
    func := get_function_ast(template, true, true, false, false);
    
    if has_feature("@franca/no_merged_funcimpl") {
        func.body = (Asm = (
            arm64 = arm64.items().interpret_as_bytes(), 
            amd64 = amd64.items(), 
            rv64 = rv64.items().interpret_as_bytes(), 
            wasm32 = wasm32.items(),
        ));
    } else {
        // :UpdateBoot get rid of this case
        impl := FuncImpl.list(2, ast_alloc());
        impl&.push(JittedAarch64_OLD = arm64);
        impl&.push(X86AsmBytes_OLD = amd64);
        func.body = (Merged_OLD = impl.as_raw());
    };
    
    func.set_flag(.BodyIsSpecial);
    func.set_flag(.EnsuredCompiled);
    
    template
}

fn AsmFunctionWasmOnly(template: FuncId, wasm32: @FnPtr(out: *List(u8)) void) FuncId #fold = {
    ::import("@/backend/arm64/bits.fr");
    ::import("@/backend/amd64/bits.fr");
    arm :: @const_slice(brk(0));
    amd :: fn(out: *List(u8)) = @asm_x64(0xCC, 0xCC) out;
    AsmFunction(template, arm, amd, wasm32)
}

// 
// `path` is a string like you would pass to import(), the scope it resolves to must have a variable called `exports` 
// of type ScopeId containing FuncId constants. The scope you get back from `import_module("@/foo.fr")` is equivalent to 
// what you'd get from `import("@/foo.fr").exports`, except that you're explictly declaring that you don't care about 
// any side effects from running the comptime code required to compile the imported functions. So instead of the new 
// code loading in the same CompCtx as calls this import_module(), it can be done in a new CompCtx and saved in a .frc 
// file to be reused without recompiling (if none of the contributing files have changed). 
//
// TODO: reuse if the same path is used from multiple places in the same compiler instance
fn import_module(path: Str) ScopeId = {
    fr := current_compiler_context();
    opts := get_build_options();
    no_cache := opts.no_cache || IS_BOOTSTRAPPING;
    exports0 := const_eval(ScopeId)(@{ @[@literal path].import().exports });
    if no_cache || true {
        // The reference implementation just loads the new module in the same CompCtx. 
        return exports0;
    };
    // else, start a new CompCtx, compile the imported module there, export it as frc_inlinable and import that back into the original CompCtx. 
    
    // TODO: not everything works with this enabled
    // - examples/import_wuffs/test.fr
    // - IS_BOOSTRAPPING
    // - wasm
    
    exports1 := {
        s := fr'vtable'cached_compile_module(path, opts, ast_alloc());
        s := s.or(fn(err) => fr.report_error(err));
        s := fr'vtable'import_frc(fr.data, s);
        s := s.or(fn(err) => fr.report_error(err));
        s
    };
    
    // The FrcModule includes type information but i don't have a way to link them to the corresponding types in our CompCtx. 
    // So the hacky solution for now is to load only the signetures from our version of the exports scope and just stomp 
    // it over the signetures of the imported functions. This avoids any complicated nominal-ness of types and still lets 
    // you call the imported functions without casting through a rawptr every time. 
    for get_constants(exports0) { name |
        fid0  := get_constant(FuncId, exports0, name).unwrap();
        fid1  := get_constant(FuncId, exports1, name).unwrap();
        func0 := get_function_ast(fid0, true, true, true, false);
        func1 := get_function_ast(fid1, true, true, true, false);
        
        func1.finished_arg = func0.finished_arg;
        func1.finished_ret = func0.finished_ret;
        func1.ret = func0.ret;
        @debug_assert_eq(func0.arg.bindings.len, func1.arg.bindings.len);
        range(0, func0.arg.bindings.len) { i |
            func1.arg.bindings[i].ty = func0.arg.bindings[i].ty;
        };
        
        // for good luck...
        func0.body = (Redirect = fid1);
        func0.set_flag(.BodyIsSpecial);;
        func0.set_flag(.EnsuredCompiled);
    };
    
    exports1
}

fn cache_baked(vbytes: []u8, v: BakedVarId) void = {
    @if(has_feature("@franca/internal_pointers"), {
        cache_baked_vbytes(vbytes, v);
    }, {
        cache_baked(u8.int_from_ptr(vbytes.ptr), v);
    })
}

fn lookup_baked(vbytes: []u8) ?Ty(BakedVarId, i32) = {
    @if(has_feature("@franca/internal_pointers"), {
        return(lookup_baked_vbytes(vbytes));
    }, {
        id := lookup_baked(u8.int_from_ptr(vbytes.ptr)) || return(.None);
        return(Some = (id, 0));
    })
}

fn emplace_bake(vbytes: []u8, $body: @Fn() BakedVar) Ty(BakedVarId, i32) = {
    xx := lookup_baked(vbytes);
    or xx {
        id := @if(has_feature("@franca/internal_pointers"), {
            fr := current_compiler_context();
            id := fr'vtable'reserve_baked(fr.data, (Some = vbytes));
            var := body();
            fr'vtable'put_baked_var(fr.data, id, var);
            id
        }, {
            var := body();
            bake_value(var)
        });
        (id, @as(i32) 0)
    }
}

fn entry_with_addend(id: BakedVarId, addend: i32) BakedEntry = {
    if has_feature("@franca/internal_pointers") {
        return(AddrOfA = (base = id, addend = addend))
    };
    @assert_eq(addend, 0, ":UpdateBoot");
    (AddrOf = id)
}

fn import() ScopeId #fold = import("@");

// TODO: this needs to interact with the caching :FUCKED
// TODO: unify this with import so it doesn't depend on working directory. 
fn include_bytes(path: Str) []u8 #fold = {
    a := ast_alloc();  // assert comptime
    import("@/lib/sys/fs.fr")'read_entire_file_or_crash(a, path)
}

fn include_bytes(path: Str, expect_len: i64) []u8 #fold = {
    b := include_bytes(path);
    @assert_eq(b.len, expect_len, "include_bytes(%) wrong size", path);
    b
}

fn include_bytes($T: Type, path: Str) *T #fold #generic = {
    b := include_bytes(path, size_of(T));
    ptr_cast_unchecked(u8, T, b.ptr)  // TODO: is alignment real?
}
