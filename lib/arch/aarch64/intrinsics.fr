//! #one_ret_pic SAFETY: one return at the end (or HACK), c abi, no relative jumps outside the function, no stack frame. 
//! - means it can be done inline by just chopping off the ret at the end. makes it a bit less insane to use fns for add, etc. 
//! - always inlines, trusts you that its small enough to be worth it. 
//! - still really dumb rn cause i do redundant stack loads/stores. 
// TODO: some consistent way of expressing debug mode bounds checks for div, shift, etc.

#one_ret_pic #aarch64 #asm
fn add(a: i64, b: i64) i64 = (
    add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn sub(a: i64, b: i64) i64 = (
    sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn mul(a: i64, b: i64) i64 = (
    madd(Bits.X64, x0, x0, x1, 0b11111),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn div(a: i64, b: i64) i64 = (
    sdiv(Bits.X64, x0, x0, x1),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn shift_left(value: i64, shift_amount: i64) i64 = (
    lslv(Bits.X64, x0, x0, x1),
    ret(),
);

// zero fill
#one_ret_pic #aarch64 #asm
fn shift_right_logical(value: i64, shift_amount: i64) i64 = (
    lsrv(Bits.X64, x0, x0, x1),
    ret(),
);

// sign bit fill
#one_ret_pic #aarch64 #asm
fn shift_right_arithmetic(value: i64, shift_amount: i64) i64 = (
    asrv(Bits.X64, x0, x0, x1),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn bit_or(a: i64, b: i64) i64 = (
    orr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
    ret(),
);

#one_ret_pic #aarch64 #asm // TODO: actually test
fn bit_not(a: i64) i64 = (
    orn(Bits.X64, x0, x0, 0b11111, Shift.LSL, @as(u6) 0),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn bit_and(a: i64, b: i64) i64 = (
    and_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn bit_xor(a: i64, b: i64) i64 = (
    eor(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn bit_xor(a: u64, b: u64) u64 = (
    eor(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

//! IMPORTANT: cond is inverted is backwards because CSINC
fn cmp_with_cond(inv_cond: Cond) Ty(u32, u32, u32) = (
    cmp(Bits.X64, x0, x1),
    cset(Bits.X64, x0, inv_cond),
    ret(),
);

#one_ret_pic #aarch64 #asm
fn eq(a: i64, b: i64) bool = cmp_with_cond(Cond.NE);

#one_ret_pic #aarch64 #asm
fn ne(a: i64, b: i64) bool = cmp_with_cond(Cond.EQ);

#one_ret_pic #aarch64 #asm
fn le(a: i64, b: i64) bool = cmp_with_cond(Cond.GT);

#one_ret_pic #aarch64 #asm
fn ge(a: i64, b: i64) bool = cmp_with_cond(Cond.LT);

#one_ret_pic #aarch64 #asm
fn lt(a: i64, b: i64) bool = cmp_with_cond(Cond.GE);

#one_ret_pic #aarch64 #asm
fn gt(a: i64, b: i64) bool = cmp_with_cond(Cond.LE);

// TODO: this is just the same as add.
// offsets measured in bytes.  
#one_ret_pic #aarch64 #asm
fn offset(ptr: rawptr, bytes: i64) rawptr = (
    add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

//////
/// Floats
// TODO: my macro system should be able to make all these binary operators much neater
//       something like @bin_math_op(add, f64, fadd, FType.D64)
//       There's wierdness around bootstrapping (for ints, prosumably the compiler doesn't need floats),
//       but I do that by precompiling to bit literals anyway so its fine. 
//       Other problem is name resolution, but they can probably just add themselves to the .gen int versions so its fine. 

// TODO: maybe its weird that #bs implies @pub
#aarch64 #one_ret_pic #asm
fn add(a: f64, b: f64) f64 = (
    fadd(FType.D64, x0, x0, x1),
    ret(),
);

#aarch64 #one_ret_pic #asm
fn sub(a: f64, b: f64) f64 = (
    fsub(FType.D64, x0, x0, x1),
    ret(),
);

#aarch64 #one_ret_pic #asm
fn mul(a: f64, b: f64) f64 = (
    fmul(FType.D64, x0, x0, x1),
    ret(),
);

#aarch64 #one_ret_pic #asm
fn div(a: f64, b: f64) f64 = (
    fdiv(FType.D64, x0, x0, x1),
    ret(),
);

//! IMPORTANT: cond is inverted is backwards because CSINC
fn fcmp_with_cond(inv_cond: Cond) Ty(u32, u32, u32) = (
    fcmp(FType.D64, x0, x1),
    cset(Bits.X64, x0, inv_cond),
    ret(),
);

#aarch64 #one_ret_pic #asm
fn eq(a: f64, b: f64) bool = fcmp_with_cond(Cond.NE);
#aarch64 #one_ret_pic #asm
fn ne(a: f64, b: f64) bool = fcmp_with_cond(Cond.EQ);
#aarch64 #one_ret_pic #asm
fn le(a: f64, b: f64) bool = fcmp_with_cond(Cond.GT);
#aarch64 #one_ret_pic #asm
fn ge(a: f64, b: f64) bool = fcmp_with_cond(Cond.LT);
#aarch64 #one_ret_pic #asm
fn lt(a: f64, b: f64) bool = fcmp_with_cond(Cond.GE);
#aarch64 #one_ret_pic #asm
fn gt(a: f64, b: f64) bool = fcmp_with_cond(Cond.LE);

// TODO: #c_call fn malicious_c_call() void;  // stomps all the registers its allowed to. 

// preserves the value (not the bit pattern). rounds towards zero. 
#aarch64 #one_ret_pic #asm
fn int(a: f64) i64 = (
    fcvtzs(Bits.X64, FType.D64, x0, x0),
    ret(),
);

// preserves the value (not the bit pattern). 
#aarch64 #one_ret_pic #asm
fn float(a: i64) f64 = (
    scvtf(Bits.X64, FType.D64, x0, x0),
    ret(),
);

// preserves the bit pattern (not the value)
#aarch64 #one_ret_pic #asm
fn bitcast(a: f64) i64 = (
    fmov_from(x0, x0),
    ret(),
);

// preserves the bit pattern (not the value)
#aarch64 #one_ret_pic #asm
fn bitcast(a: i64) f64 = (
    fmov_to(x0, x0),
    ret(),
);

#aarch64 #one_ret_pic #asm
fn cast(v: f64) f32 = (
    fcnv(.D64, .S32, x0, x0),
    ret(),
);

#aarch64 #one_ret_pic #asm
fn cast(v: f32) f64 = (
    fcnv(.S32, .D64, x0, x0),
    ret(),
);

// I always use the whole register so its fine? 
// TODO: its hard to think about why this works, because it seems using #unsafe_noop_cast directly doesn't work.
#aarch64 #one_ret_pic #asm fn zext(v: u32) u64 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: u8)  u64 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: u8)  u32 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: u16) u32 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: u32) i64 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: u16) i64 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: i32) i64 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn zext(v: u16) u64 = (ret(), ret());

trunc8 :: fn() Ty(u32, u32, u32) = (
    movz(.X64, x1, 0x00FF, .Left0),
    and_sr(.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

trunc16 :: fn() Ty(u32, u32, u32) = (
    movz(.X64, x1, 0xFFFF, .Left0),
    and_sr(.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

trunc32 :: fn() Ty(u32, u32, u32, u32) = (
    movz(.X64, x1, 0xFFFF, .Left0),
    movk(.X64, x1, 0xFFFF, .Left16),
    and_sr(.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
    ret(),
);

#aarch64 #one_ret_pic #asm fn trunc(v: u64)  u8 = trunc8();
#aarch64 #one_ret_pic #asm fn trunc(v: u32)  u8 = trunc8();
#aarch64 #one_ret_pic #asm fn trunc(v: u16)  u8 = trunc8();
#aarch64 #one_ret_pic #asm fn trunc(v: i64)  u8 = trunc8();
#aarch64 #one_ret_pic #asm fn trunc(v: i64) u16 = trunc16();
#aarch64 #one_ret_pic #asm fn trunc(v: u64) u16 = trunc16();
#aarch64 #one_ret_pic #asm fn trunc(v: u32) u16 = trunc16();
#aarch64 #one_ret_pic #asm fn trunc(v: u64) u32 = trunc32();
#aarch64 #one_ret_pic #asm fn trunc(v: i64) u32 = trunc32();

// F579 trunc: [2618477313, 2620574465, 2315255840, 3596551104]
// F579 trunc: [2752662785, 2754759937, 2315255840, 3596551104]

// Always store things sign extended. 
#aarch64 #one_ret_pic #asm fn intcast(v: i64) i32 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn intcast(v: i32) i64 = (ret(), ret())

#aarch64 #one_ret_pic #asm fn int_from_rawptr(ptr: rawptr) i64 = (ret(), ret());
#aarch64 #one_ret_pic #asm fn rawptr_from_int(ptr: i64) rawptr = (ret(), ret());
