//! Instruction encoding for aarch64. 
//! Magic numbers from https://developer.arm.com/documentation/ddi0487/latest/ (section 6.2 is the good bit)
//! Helpful for debugging: https://armconverter.com/ (toggle the endianness)
//!
//! Note: @bits and @BITS do the same thing but the latter is provided by the compiler instead of the standard library,
//!       so it can be used to bootstrap the instructions you need to compile the former. 
// TODO: you can't mark these functions #inline, it confuses imm_const_eval (which #asm is)? -- Jul 24

// TODO: fix uniques so i can still use int literals
RegI :: u5; // Unique(u5);
RegO :: u5; // Unique(u5);
Bits :: @enum(u1) (W32 = 0b0, X64 = 0b1);
Shift :: @enum(u2) (LSL = 0b00, LSR = 0b01, ASR = 0b10);

/// These are used by instructions that check the flags after a cmp/fcmp. 
/// Int: signed vs unsigned
/// Float: if either is NaN, ordered=false and unordered=true
Cond :: @enum(u4) (
    EQ = 0b0000, // equal                               | eq,   ordered
    NE = 0b0001, // not equal                           | ne, unordered
    HS = 0b0010, // unsigned greater than or equal      | ge, unordered
    LO = 0b0011, // unsigned less than                  | lt,   ordered
    MI = 0b0100,
    PL = 0b0101,
    VS = 0b0110,
    VC = 0b0111,
    HI = 0b1000, // unsigned greater than               | gt, unordered
    LS = 0b1001, // unsigned less than or equal         | le,   ordered
    GE = 0b1010, // signed greater than or equal        | ge,   ordered
    LT = 0b1011, // signed less than                    | lt, unordered
    GT = 0b1100, // signed greater than                 | gt,   ordered
    LE = 0b1101, // signed less than or equal           | le, unordered
   AL2 = 0b1110, // Always, (yes it has two encodings).
   AL1 = 0b1111, // Always
);

// TODO: using unique is wrong becuase you want JumpRel(i19) === JumpRel(i19). 
//       Really Unique shouldn't be a function because you don't want it to act like other comptime generics. 
// Note: Branch instructions replace the normal ip increment. So b(1) is a no-op and b(0) is an infinite loop on that instruction.
fn JumpRel(Int: Type) = Int;  // measured in instructions, not bytes

fn add_sr(sf: Bits, dest: RegO, a: RegI, b: RegI, shift: Shift, amount: u6) u32 =
    @bits(sf, 0b0001011, shift, 0b0, a, amount, b, dest);

fn add_im(sf: Bits, dest: RegO, src: RegI, imm: u12, lsl_12: u1) u32 =
    @bits(sf, 0b00100010, lsl_12, imm, src, dest);

fn orr(sf: Bits, dest: RegO, a: RegI, b: RegI, shift: Shift, amount: u6) u32 =
    @bits(sf, 0b0101010, shift, 0b0, a, amount, b, dest);

/// dest = a | ~(b <shift> amount)  // TODO: make sure that order is right.
fn orn(sf: Bits, dest: RegO, a: RegI, b: RegI, shift: Shift, amount: u6) u32 =
    @bits(sf, 0b0101010, shift, 0b1, a, amount, b, dest);

fn and_sr(sf: Bits, dest: RegO, a: RegI, b: RegI, shift: Shift, amount: u6) u32 =
    @bits(sf, 0b0001010, shift, 0b0, a, amount, b, dest);

fn sub_sr(sf: Bits, dest: RegO, a: RegI, b: RegI, shift: Shift, amount: u6) u32 =
    @bits(sf, 0b1001011, shift, 0b0, b, amount, a, dest);

fn sub_im(sf: Bits, dest: RegO, src: RegI, imm: u12, lsl_12: u1) u32 =
    @bits(sf, 0b10100010, lsl_12, imm, src, dest);

// TODO: really the sub instructions should have a set_flags: u1 = 0 and this could just call that.
//       in general default arguments could make a lot of this neater I think, but then it gets harder for the rust code to use.
fn cmp(sf: Bits, a: RegI, b: RegI) u32 =  // should be cmp_sr with more args but like who would ever want that?
    @bits(sf, 0b1101011, Shift.LSL, 0b0, b, 0b000000, a, 0b11111);

fn cmp_im(sf: Bits, src: RegI, imm: u12, lsl_12: u1) u32 =
    @bits(sf, 0b11100010, lsl_12, imm, src, 0b11111);

// Note: this one can't use sp (r31 is ZXR), use add_im instead. 
fn mov(sf: Bits, dest: RegO, src: RegI) u32 = 
    orr(sf, dest, src, 0b11111, Shift.LSL, 0b000000);

/// dest = (x * y) + add
/// This works with signed numbers, even though there's another instruction that says signed mul? i guess the difference matters for the ones that have multiple output registers.
fn madd(sf: Bits, dest: RegO, x: RegI, y: RegI, add: RegI) u32 =
    @bits(sf, 0b0011011000, x, 0b0, add, y, dest);

/// dest = dividend / divisor
fn sdiv(sf: Bits, dest: RegO, dividend: RegI, divisor: RegI) u32 =
    @bits(sf, 0b0011010110, divisor, 0b000011, dividend, dest);

fn cbz(sf: Bits, offset: JumpRel(i19), val: RegI) u32 =
    @bits(sf, 0b0110100, offset, val);

fn cbnz(sf: Bits, offset: JumpRel(i19), val: RegI) u32 =
    @bits(sf, 0b0110101, offset, val);

fn b(offset: JumpRel(i26), set_link: u1) u32 =
    @bits(set_link, 0b00101, offset);

fn b_cond(offset: JumpRel(i19), cond: Cond) u32 =
    @bits(0b01010100, offset, 0b0, cond);

Hw :: @enum(u2) (Left0 = 0b00, Left16 = 0b01, Left32 = 0b10, Left48 = 0b11);

/// Zero the other bits.
fn movz(sf: Bits, dest: RegO, imm: u16, hw: Hw) u32 =
    @bits(sf, 0b10100101, hw, imm, dest); // TOOD: this broke on @bits when u16 became a real type. 

/// Keep the other bits.
fn movk(sf: Bits, dest: RegO, imm: u16, hw: Hw) u32 =
    @bits(sf, 0b11100101, hw, imm, dest);

// TODO: this is an alias for the one where you specify a register to jump to. 
fn ret() u32 = 0xD65F03C0;

// Note: be very careful! The scale of offset_words depends on sf. They save bits by making unaligned access un-encodable.

fn str_uo(sf: Bits, src: RegI, addr: RegI, offset_words: u12) u32 = 
    @bits(0b1, sf, 0b11100100, offset_words, addr, src);

fn ldr_uo(sf: Bits, dest: RegO, addr: RegI, offset_words: u12) u32 = 
    @bits(0b1, sf, 0b11100101, offset_words, addr, dest);

/// Load two sequential words (addr, addr+word_size) into two registers. idk why offset can be negative in this one but not in ldr.
fn ldp_so(sf: Bits, dest1: RegO, dest2: RegO, addr: RegI, offset_words: i7) u32 =
    @bits(sf, 0b010100101, offset_words, dest2, addr, dest1);

fn stp_so(sf: Bits, src1: RegO, src2: RegO, addr: RegI, offset_words: i7) u32 =
    @bits(sf, 0b010100100, offset_words, src2, addr, src1);

/// Branch to an address in a register. Maybe set link register to next pc (return address).
fn br(addr: RegI, set_link: u1) u32 = 
    @bits(0b1101011000, set_link, 0b11111000000, addr, 0b00000);

/// Raise a hardware exception
fn brk(context: u16) u32 =
    @bits(0b11010100001, context, 0b00000);

/// IMPORTANT: this is an alias of CSINC so the cond is inverted from what you'd expect from a real assembler.
/// cond=TRUE === dest=0      cond=FALSE === dest=1
/// https://stackoverflow.com/questions/56519227/arm-cset-condition-encoding-as-vs-docu
fn cset(sf: Bits, dest: RegI, zero_when: Cond) u32 =
    @bits(sf, 0b0011010100, 0b11111, zero_when, 0b01, 0b11111, dest);

/// Logical Shift Left. <shift> is truncated to the bottom 5/6 bits. (1 << 64 === 1)
fn lslv(sf: Bits, dest: RegI, val: RegI, shift: RegI) u32 =
    @bits(sf, 0b0011010110, shift, 0b001000, val, dest);

/// Logical shift Right
fn lsrv(sf: Bits, dest: RegI, val: RegI, shift: RegI) u32 =
    @bits(sf, 0b0011010110, shift, 0b001001, val, dest);

/// Arithmetic shift Right
fn asrv(sf: Bits, dest: RegI, val: RegI, shift: RegI) u32 =
    @bits(sf, 0b0011010110, shift, 0b001010, val, dest);

// eXclusive or 
fn eor(sf: Bits, dest: RegI, a: RegI, b: RegI, shift: Shift, amount: u6) u32 =
    @bits(sf, 0b1001010, shift, 0b0, a, amount, b, dest);
 
// Load a byte and zero extend it. 
fn ldrb_uo(dest: RegO, src_addr: RegI, offset_bytes: u12) u32 = 
    @bits(0b00, 0b11100101, offset_bytes, src_addr, dest);

// Store the least significant byte of <src>.
fn strb_uo(src: RegI, dest_addr: RegI, offset_bytes: u12) u32 = 
    @bits(0b00, 0b11100100, offset_bytes, dest_addr, src);
    
// Load two bytes and zero extend. 
// Note: offset_pairs is offset_bytes/2
fn ldrh_uo(dest: RegO, src_addr: RegI, offset_pairs: u12) u32 = 
    @bits(0b01, 0b11100101, offset_pairs, src_addr, dest);

// Store the least significant two bytes of <src>.
// Note: offset_pairs is offset_bytes/2
fn strh_uo(src: RegI, dest_addr: RegI, offset_pairs: u12) u32 = 
    @bits(0b01, 0b11100100, offset_pairs, dest_addr, src);

// TODO: there's a bit of a confusing dependency where this needs to import things but everything expands to calls to these...
//       but I think it works out because you just need to bootstrap with a different backend. 
//       so need a way of delegating comptime calls to precompiled libs. 
//       think about the boundary between magic generic comptime calls and normal calls that just happen to be at comptime. 
//       and the implementation of the magic generics will include normal calls that work on TypeInfos or whatever. 
//       should document `Type <: TypeId === Ptr(TypeInfo)` 

FType :: @enum(u2) (H16 = 0b11, S32 = 0b00, D64 = 0b01);
FRegI :: u5; // Unique(u5);
FRegO :: u5; // Unique(u5);

/// 'offset_scaled' == 'offset_bytes / size_of(F)'
/// X64===D64 and W32===S32
fn f_ldr_uo(sf: Bits, dest: FRegO, addr: RegI, offset_scaled: u12) u32 = 
    @bits(0b1, sf, 0b11110101, offset_scaled, addr, dest);

/// 'offset_scaled' == 'offset_bytes / size_of(F)'
/// X64===D64 and W32===S32
fn f_str_uo(sf: Bits, src: FRegI, addr: RegI, offset_scaled: u12) u32 = 
    @bits(0b1, sf, 0b11110100, offset_scaled, addr, src);

// Note: int mul doesn't exist (?) so it used madd with the zero register but I guess there's no zero reguister for floats
// TODO: test that this is the right encoding, might as well provide it just in case
//  fn fmadd(sf: FType, dest: FRegO, x: FRegI, y: FRegI, add: FRegI) u32 =
//    @bits(0b00011111, size, 0b0, x, 0b0, add, y, dest);

// TODO: all the bin ops have the same encoding with a different magic number. make it a table. 

fn fadd(size: FType, dest: FRegO, a: FRegI, b: FRegI) u32 =
    @bits(0b00011110, size, 0b1, a, 0b001010, b, dest);

fn fsub(size: FType, dest: FRegO, a: FRegI, b: FRegI) u32 =
    @bits(0b00011110, size, 0b1, b, 0b001110, a, dest);
    
fn fmul(size: FType, dest: FRegO, a: FRegI, b: FRegI) u32 =
    @bits(0b00011110, size, 0b1, a, 0b000010, b, dest);
    
fn fdiv(size: FType, dest: FRegO, dividend: FRegI, divisor: FRegI) u32 =
    @bits(0b00011110, size, 0b1, divisor, 0b000110, dividend, dest);

// TODO: this has a different encoding for comparing to zero without using an extra register. 
fn fcmp(size: FType, a: FRegI, b: FRegI) u32 =
    @bits(0b00011110, size, 0b1, b, 0b001000, a, 0b00000);

/// Floating-point Convert to Signed Integer, rounding toward Zero. 
fn fcvtzs(sf: Bits, size: FType, dest: RegO, src: FRegI) u32 =
    @bits(sf, 0b0011110, size, 0b111000000000, src, dest);

/// Signed integer Convert to Floating-point 
fn scvtf(sf: Bits, size: FType, dest: FRegO, src: RegI) u32 =
    @bits(sf, 0b0011110, size, 0b100010000000, src, dest);

fn fmov(size: FType, dest: FRegO, src: FRegI) u32 =
    @bits(0b00011110, size, 0b100000010000, src, dest);

/// 64bit int to 64bit float
fn fmov_to(dest: FRegO, src: RegI) u32 =
    @bits(Bits.X64, 0b0011110, FType.D64, 0b10, 0b0, 0b11, 0b1, 0b000000, src, dest);

/// 64bit float to 64bit int
fn fmov_from(dest: RegO, src: FRegI) u32 =
    @bits(Bits.X64, 0b0011110, FType.D64, 0b10, 0b0, 0b11, 0b0, 0b000000, src, dest);

/// change floating point prcision while trying to preserve value. 
fn fcnv(from: FType, to: FType, dest: RegO, src: FRegI) u32 =
    @bits(0b00011110, from, 0b10001, to, 0b10000, src, dest);

fn sbmf(immr: u6, imms: u6, dest: RegO, src: RegI) u32 =
    @bits(Bits.X64, 0b001001101, immr, imms, src, dest);

//////
/// Notes about calling conventions. 
/// - https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms

x0: u5 : 0;
x1: u5 : 1;
x2: u5 : 2;
x3: u5 : 3;
x4: u5 : 4;
x5: u5 : 5;
x6: u5 : 6;
x7: u5 : 7;
x8: u5 : 8;
x9: u5 : 9;
x16: u5 : 16;
x17: u5 : 17;
/// Platform Register. Idk what that means but the calling convention says don't use it. 
x18: u5 : 18;
x19: u5 : 19;
x20: u5 : 20;
x21: u5 : 21;
x22: u5 : 22;
x23: u5 : 23;
x24: u5 : 24;
x25: u5 : 25;
x26: u5 : 26;
x27: u5 : 27;
x28: u5 : 28;
/// Frame Pointer. 
fp: u5 : 29;
/// Link Register. 
lr: u5 : 30;
/// Stack Pointer. 
/// It's actually special in the ISA, only some instructions can refer to it (sometimes 31 means Zero Register). 
sp: u5 : 31;
xzr: u5 : 31; // TODO: different types for gpr/sp vs gpr/xzr? -- Apr 28

// TODO: implicit range assertions because we get the bit count from the type of the ints. 
fn bits(arg: FatExpr) FatExpr #macro = {
    @ct_assert(arg.expr&.is(.Tuple), arg.loc, "Expected @Bits(Tuple...)");
    parts := arg.expr.Tuple;

    shift := 32;
    out := @{ 0 }; 
    each parts { int |
        ty := get_type_int(int[]); // TODO: this shallow copy is sketchy.

        shift -= ty.bit_count;
        @ct_assert(shift >= 0, arg.loc, "expected 32 bits not >%. TODO: other sizes.", shift.abs() + 32);
        
        ::if(FatExpr);
        // TODO: HACK. this will get more sane when weird int sizes are represented as the next one up instead of always i64.
        real_int: FatExpr = if ty.bit_count.eq(16).or(ty.bit_count == 32).or(ty.bit_count == 8) {
            @{ @as(i64) @[int[]].zext() }
        } else {
            // note: we're using the pointer into the array as the box because we know it leaks so its fine. 
            (expr = (Cast = int), loc = arg.loc, ty = i64, done = false)
        };
        
        if ty.signed {
            real_int = @{ @as(i64) signed_truncate(@[real_int], @[@literal(i64) ty.bit_count]).zext() }
        };
        
        out = @{ @[out].bit_or(@[real_int].shift_left(@[@literal shift])) };
    };

    if shift != 0 {
        compile_error("shift != 0; expected 32 bits. TODO: other sizes.", arg.loc);
    };
    
    @{ @as(u32) @[out].trunc() }
}
