// this one does something os specific that doesn't rely on the program starting through franca_runtime_init
fn query_cli_args() []CStr = {
    @match(query_current_os()) {
        fn macos() => {
            argc := MacosLibc'_NSGetArgc()[];
            argv := MacosLibc'_NSGetArgv()[];
            // Safety: the os wouldn't lie to us... hopefully.
            slice(argv, argc)
        }
        // TODO: uhg, its not ok to allocate here! :slow
        //       if we're gonna be slow we might as well be super slow
        //       but if im committed to doing it this way caller should pass in allocator or i should cache it, or something, idk. 
        fn linux() => slowly_get_cli_args_from_proc_self(page_allocator)  // :Leak
        @default => unreachable();
    }
}

fn timestamp() i64 = // :TodoLinux 
    clock_ms(MacosLibc.CLOCK_REALTIME);

fn clock_ms(clock_id: i64) i64 = {
    time_spec: TimeSpec = (seconds = 0, nanoseconds = 0);
    Syscall'clock_gettime(clock_id, time_spec&);
    time_spec.ms()
}

fn timestamp_s() f64 = { // :TodoLinux 
    clock_id := MacosLibc.CLOCK_REALTIME;
    t: TimeSpec = (seconds = 0, nanoseconds = 0);
    Syscall'clock_gettime(clock_id, t&);
    t.seconds.float() + t.nanoseconds.float() / NS_PER_S
}

// Calling this is probably not super useful because you don't know if you're running JIT or AOT. 
fn get_executable_path(a: Alloc) Str = {
    @match(query_current_os()) {
        fn macos() => MacosLibc'get_executable_path(a);
        fn linux() => LinuxLibc'get_executable_path(a);
        @default => unreachable();
    }
}

fn read_link(path: CStr, out: *List(u8)) bool = {
    start := out.len;
    if out.maybe_uninit.len == out.len {
        out.reserve(32);
    };
    loop {
        cap := out.maybe_uninit.len - start;
        len := Syscall'readlink(path, out.maybe_uninit.ptr.offset(start), cap) 
            || return(false);
        
        if len < cap {
            out.len += len;
            return(true);
        };
        @debug_assert_eq(cap, len);
        
        // else, we probably didn't have enough room to read the whole file path, try again. 
        out.reserve(2 * out.maybe_uninit.len);
    }
}

fn get_working_directory(a: Alloc) List(u8) = {
    out: List(u8) = list(512, a);
    s := or Syscall'getcwd(out.maybe_uninit.ptr, out.maybe_uninit.len) {
        panic("failed to get_working_directory") // TODO: return error?
    };
    out.len = s.slow_len();
    out
}

// Calling `query_current_{arch, os}` at comptime is probably wrong because 
// you'll observe the host environment not the target. 

current_os :: query_current_os;

current_arch :: AsmFunction(fn() Arch = (), {
    #use("@/backend/arm64/bits.fr");
    @const_slice(
        movz(.X64, x0, 0x0000, .Left0), 
        ret(), 
    )
}) { (out: *List(u8)) |
    #use("@/backend/amd64/bits.fr");
    @asm_x64(
        encode_imm(PrimaryOp.MovImm32, X86Reg.rax, 1), 
        PrimaryOp.Ret
    ) out;
} wasm { (out: *List(u8)) |
    out.push_all(@slice(0x00, 0x42, 0x03, 0x0B)); // 0 locals. i64.const(3). end.
};

fn query_current_arch() Arch = current_arch();

//
// You must call this function when writting machine code into memory and then trying to execute it (simply mprotect-ing it to Exec is not enough)!
// Some ISAs don't guarentee coherency between instruction and data caches.  
//
// x86 doesn't this. TODO: what about riscv?
// This fixes 'illegal hardware instruction'.
// sleep(Duration::from_millis(1)) also works (in debug mode). That's really cool, it gives it time to update the other cache because instructions and data are seperate?!
// Especially fun becuase if you run it in lldb so you break on the error and disassemble... you get perfectly valid instructions because it reads the data cache!
//
// - https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/caches-and-self-modifying-code
// - https://stackoverflow.com/questions/35741814/how-does-builtin-clear-cache-work
// - https://stackoverflow.com/questions/10522043/arm-clear-cache-equivalent-for-ios-devices
// - https://github.com/llvm/llvm-project/blob/main/compiler-rt/lib/builtins/clear_cache.c
// - https://github.com/apple/darwin-libplatform/blob/main/src/cachecontrol/arm64/cache.s
// - https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/sys_icache_invalidate.3.htmls
//
fn clear_instruction_cache(beg: rawptr, end: rawptr) void = {
    if query_current_arch() == .aarch64 {
        @debug_assert(ptr_diff(beg, end) >= 0);
        @if(identical(beg, end)) return();
        
        __clear_cache :: fn(beg: rawptr, end: rawptr) void #weak #libc;
        if query_current_os() == .macos {
            __clear_cache(beg, end);
        } else {
            // TODO: do this in asm and use largest of the two cache line sizes instead of just picking a number
            // it seems to work without this but i think its the right thing to do
            beg := beg.align_back(128);
            end := end.align_to(128);
            
            // glibc won't give me a dynamically linked __clear_cache() (but you get one if you make it 
            // not #weak and link statically), so do it myself (these instructions don't work on apple). June 2025. 
            impl :: AsmFunction(fn(beg: rawptr, end: rawptr) void = (), {
                aarch64_clear_instruction_cache
            }) { (out: *List(u8)) | 
                out.push(0xc3); // unreachable
            };
            impl(beg, end);
        };
    };
    @assert(query_current_arch() != .rv64);
};

aarch64_clear_instruction_cache :: { 
    #use("@/backend/arm64/bits.fr"); 
    start :: x0; end :: x1; p :: x2; step :: x3; info :: x4;
@const_slice(
    // get size of a cache line (can be different for data and instructions)
    SYS(0b1, 0b11, 0b011, 0b0000, 0b0000, 0b001, info),  // msr CTR_EL0
    // DminLine = info[19..=16]; IminLine = info[3..=0]
    
    // loop over the range and flush one d-cache line at a time
    add_sr(.X64, step, xzr, info, .LSL, @as(u6) @as(i64) 63-19),
    add_sr(.X64, step, xzr, step, .LSR, @as(u6) @as(i64) 63-(19-16+1)),
    mov(.X64, p, start),
    // a:
    SYS(0b0, 0b01, 0b011, 0b0111, 0b1011, 0b001, p), // dc cvau
    add_sr(.X64, p, p, step, .LSL, 0), 
    cmp(.X64, p, end),
    b_cond(-3, .LO), // :a
    SYS(0b0, 0b00, 0b011, 0b0011, 0b1011, 0b100, 0b11111), // dsb ish
    
    // loop over the range and flush one i-cache line at a time
    add_sr(.X64, step, xzr, info, .LSL, @as(u6) @as(i64) 63-3),
    add_sr(.X64, step, xzr, step, .LSR, @as(u6) @as(i64) 63-(3-0+1)),
    mov(.X64, p, start),
    // b:
    SYS(0b0, 0b01, 0b011, 0b0111, 0b0101, 0b001, p), // ic ivau
    add_sr(.X64, p, p, step, .LSL, 0), 
    cmp(.X64, p, end),
    b_cond(-3, .LO), // :b
    SYS(0b0, 0b00, 0b011, 0b0011, 0b1111, 0b110, 0b11111), // isb
    
    ret(),
)
};

fn clear_instruction_cache(beg: rawptr, len: i64) void = 
    clear_instruction_cache(beg, beg.offset(len));

// :JitMemProtect
fn apple_thread_jit_write_protect(executable_on_current_thread: bool) void = {
    @if(query_current_os() != .macos) return();
    if !prefer_syscalls() {
        pthread_jit_write_protect_np(executable_on_current_thread);
        return();
    };
    @if(query_current_arch() != .aarch64) return();
    
    // See /devlog.md/Mar 13, 2025/
    // they seem to always be the same?
    // A, B, C := (3472275312702652419, 2310346747086372864, 2310346747088470016);
    A, B, C := (magic(0xc10c), magic(0xc118), magic(0xc110));
    if(A.bit_and(0xFF) == 0, => return());
    // MSR(if(executable_on_current_thread, => B, => C), 0xd51cf2e0); // S3_4_C15_C2_7  // clearly this one is bad...
    MSR(if(executable_on_current_thread, => B, => C), 0xd51ef1a0);    // S3_6_C15_C1_5
    fence();
    
    magic :: fn(i: i64) => i64.ptr_from_int(0xFFFFF0000.bit_or(i))[];
    MSR :: fn(x: i64, $msr: u32) void = {
        xxx :: AsmFunction(fn(x: i64) void = (), {
            @const_slice(msr, 0xd5033fdf, 0xD65F03C0) // msr, isb, ret
        }) { (out: *List(u8)) | 
            out.push_all(@slice(0xc3, 0xc3)); // unreachable
        };
        xxx(x)
    };
};
