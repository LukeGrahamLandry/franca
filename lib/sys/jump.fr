// 
// See tests/exceptional.fr for a basic usage example 
// and lib/sys/sync/green.fr for something more complex.
// 

JumpBuf :: @struct {
    fp: rawptr;
    // lr is the address that will be returned to when you throw(). 
    // On amd64 it's the value from the top of the stack, not a register. 
    lr: rawptr;
    sp: rawptr;
    // userdata is passed as the first **argument** 
    // if you manually poke a function pointer in as lr
    userdata: rawptr;
    // [x19..=x28] or [rbx, r12..=r15]
    registers: Array(i64, 12);
    // arm64: low 64 bits of [v8..=v15] (amd64 has no callee saved floats)
    floats: Array(f64, 12);
};

// The asm hardcodes these values 
WhichJump :: @enum(i64) (Try = 0, Catch = 1);

// TODO: version of these that let you pass userdata to throw and retrive it from try(which=Catch)
fn try(buf: *JumpBuf) WhichJump = {  // aka setjmp
    @run assert_eq(@as(i64) WhichJump.Try, 0);
    // note: you can't do anything in this function before this call 
    //       because you can't fuck with the return address before it gets saved in the jumpbuf
    _userdata, which := try_impl(buf);
    which
}

// TODO: this won't get inlined by the backend because it ends in hlt. doesn't matter but makes be sad.
fn throw(buf: *JumpBuf) Never = {  // aka longjmp
    @if(TODOWASM) @if(query_current_arch() == .wasm32) panic("TODO: longjmp throw");
    throw_impl(buf)
}

try_impl :: AsmFunction(fn(_: *JumpBuf) Ty(rawptr, WhichJump) = (), @const_slice(
    stp_so(.X64, fp, lr, x0, @as(i7) 0),
    // <skip>
    stp_so(.X64, x19, x20, x0, @as(i7) 4),
    stp_so(.X64, x21, x22, x0, @as(i7) 6),
    stp_so(.X64, x23, x24, x0, @as(i7) 8),
    stp_so(.X64, x25, x26, x0, @as(i7) 10),
    stp_so(.X64, x27, x28, x0, @as(i7) 12),
    
    add_im(.X64, x1, sp, @as(u12) 0, 0b0), 
    str_uo(.X64, x1, x0, @as(u12) 2), // idk if st___ can encode sp
    
    f_stp_so(.D64, x8,  x9,  x0, @as(i7) 16),
    f_stp_so(.D64, x10, x11, x0, @as(i7) 18),
    f_stp_so(.D64, x12, x13, x0, @as(i7) 20),
    f_stp_so(.D64, x14, x15, x0, @as(i7) 22),

    // return (_, WhichJump.Try)
    movz(.X64, x1, 0x0000, .Left0), 
    ret(), 
), fn(out: *List(u8)) => @asm_x64(
    // Save integer registers
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.rbp, X86Reg.rdi, 0*8),
    // <skip>
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.rsp, X86Reg.rdi, 2*8),
    // <skip>
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.rbx, X86Reg.rdi, 4*8),
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.r12, X86Reg.rdi, 5*8),
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.r13, X86Reg.rdi, 6*8),
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.r14, X86Reg.rdi, 7*8),
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.r15, X86Reg.rdi, 8*8),
    
    // Save the return address 
    encode_op_reg(PrimaryOp.PopBase, X86Reg.rax),
    encode_op_reg(PrimaryOp.PushBase, X86Reg.rax),
    encode_base_plus_offset(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi, 1*8),
    
    // return (_, WhichJump.Try)
    encode_imm(PrimaryOp.MovImm32, X86Reg.rdx, 0), 
    PrimaryOp.Ret
) out, fn(out: *List(u8)) => {
    // not implemented. throw will assert. 

    out.push(0x00);  // 0 locals
    
    range(0, 2) { i |
        off: u8 = trunc(i*8);
        out.push(0x20);  // local.get(0)
        out.push(0x00);  // ^
        out.push(0xA7);  // trunc
        out.push(0x42);  // i64.const(0)
        out.push(0x00);  // ^
        out.push(0x37);  // i64.store
        out.push(0x03);  // ^ align
        out.push(off);   // ^
    };
    
    out.push(0x0B);  // end
}, @run {
    o := @ref u32.list(32, ast_alloc());
    iterate_riscv_saved { r, off, float |
        o.push(store(r, .A0, off, 8, float));
    };
    o.push(I(0, RvReg.Zero.bits(), @as(u3) OpI.add, RvReg.A1.bits(), .op_imm));
    o.push(jalr(.RA, 0, .Zero));
    o.items()
});

throw_impl :: AsmFunction(fn(_: *JumpBuf) Never = (), @const_slice(
    ldp_so(.X64, fp, lr, x0, @as(i7) 0),
    ldr_uo(.X64, x1, x0, @as(u12) 2), // idk if l___ can encode sp
    add_im(.X64, sp, x1, @as(u12) 0, 0b0), 

    ldp_so(.X64, x19, x20, x0, @as(i7) 4),
    ldp_so(.X64, x21, x22, x0, @as(i7) 6),
    ldp_so(.X64, x23, x24, x0, @as(i7) 8),
    ldp_so(.X64, x25, x26, x0, @as(i7) 10),
    ldp_so(.X64, x27, x28, x0, @as(i7) 12),
    
    f_ldp_so(.D64, x8,  x9,  x0, @as(i7) 16),
    f_ldp_so(.D64, x10, x11, x0, @as(i7) 18),
    f_ldp_so(.D64, x12, x13, x0, @as(i7) 20),
    f_ldp_so(.D64, x14, x15, x0, @as(i7) 22),
    
    // return (userdata, WhichJump.Catch)
    ldr_uo(.X64, x0, x0, @as(u12) 3),
    movz(.X64, x1, 0x0001, .Left0),
    ret(), // we set lr from the jumpbuf so we'll go back to the caller of try. 
), fn(out: *List(u8)) => @asm_x64(
    // Restore integer registers
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.rbp, X86Reg.rdi, 0*8),
    // <skip>
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.rsp, X86Reg.rdi, 2*8),
    // <skip>
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.rbx, X86Reg.rdi, 4*8),
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.r12, X86Reg.rdi, 5*8),
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.r13, X86Reg.rdi, 6*8),
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.r14, X86Reg.rdi, 7*8),
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.r15, X86Reg.rdi, 8*8),
    
    // Restore the return address
    encode_op_reg(PrimaryOp.PopBase, X86Reg.rax),  // discard the current return address
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.rax, X86Reg.rdi, 1*8),
    encode_op_reg(PrimaryOp.PushBase, X86Reg.rax),  // replace with the saved one 
    
    // return (userdata, WhichJump.Catch) 
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.rax, X86Reg.rdi, 3*8),  // first return value
    encode_base_plus_offset(PrimaryOp.MovRegLoad, X86Reg.rdi, X86Reg.rdi, 3*8),  // first **argument**
    encode_imm(PrimaryOp.MovImm32, X86Reg.rdx, 1),  // second return value
    PrimaryOp.Ret,
) out, (fn(out: *List(u8)) void = out.push_all(@const_slice(0x00, 0x00, 0x0B))), @run {
    o := @ref u32.list(32, ast_alloc());
    iterate_riscv_saved { r, off, float |
        o.push(load(r, .A0, off, 8, false, float));
    };
    // return (userdata, WhichJump.Catch)
    o.push(load(.A0, .A0, 24, 8, false, false));
    o.push(I(1, RvReg.Zero.bits(), @as(u3) OpI.add, RvReg.A1.bits(), .op_imm));
    o.push(jalr(.RA, 0, .Zero));
    o.items()
});

#use("@/backend/amd64/bits.fr");
#use("@/backend/arm64/bits.fr");
#use("@/backend/rv64/bits.fr");
iterate_riscv_saved :: fn($body: @Fn(r: RvReg, off: i64, float: bool) void #duplicated) = {
rv64_rclob :: (@const_slice(
      RvReg.S1,  .S2,   .S3,   .S4,  .S5,  .S6,  .S7,
     .S8,  .S9,  .S10,  .S11,
    .FS0, .FS1, .FS2,  .FS3,  .FS4, .FS5, .FS6, .FS7,
    .FS8, .FS9, .FS10, .FS11,
));
    body(.FP, 0, false);
    body(.RA, 8, false);
    body(.SP, 16, false);
    off := 32;
    enumerate rv64_rclob { i, it |
        body(it[], off, i > 10);
        off += 8;
        if i == 10 {
            off += 8;
        };
    };
}
