
// size_of(Thread) is the same on macos and glibc.
// the others are always done by reference so just have to be the larger of the two sizes. 
Thread :: @struct(opaque: rawptr);
ThreadAttr :: @struct(sig: i64, opaque: Array(u8, 56));
PMutex :: @struct(sig: i64, opaque: Array(u8, 56));
PCond :: @struct(sig: i64, opaque: Array(u8, 40));

fn start_thread($T: Type, callee: @FnPtr(userdata: *T) void, userdata: *T) Thread #generic = {
    stack := page_allocator.alloc(u8, 1.shift_left(25));
    callee := ptr_cast_unchecked(@type callee, @FnPtr(rawptr) void, callee&)[]; // TODO: make this less painful
    xx := start_thread(stack, callee, T.raw_from_ptr(userdata));
    or xx {
        panic("failed to start thread")
    }
}

// this uses the first page of `stack` for the dynamic context. 
fn start_thread(stack: []u8, callee: @FnPtr(userdata: rawptr) void, userdata: rawptr) Result(Thread, void) = {
    @assert(query_context_is_implicit(), "tried to start thread but compiled using static context which is not thread safe.");
    out := Thread.zeroed();
    attr := @uninitialized ThreadAttr;
    macos_page_size :: 16384;
    // TODO: put it at the end and offset forward to give the thread less memory so the stack grows left away from the context. instead of left towards the context. 
    // TODO: do we trust it to not stomp in my stuff in between? 
    context_src: []u8 = (ptr = u8.ptr_from_raw(get_dynamic_context()), len = size_of_dynamic_context);
    context_dest := stack.slice(16, 16 + size_of_dynamic_context);
    context_dest.copy_from(context_src);
    ptr_cast_unchecked(u8, rawptr, stack.ptr)[] = userdata;
    ptr_cast_unchecked(u8, @type callee, stack.ptr.offset(8))[] = callee;
    reserved := 16 + size_of_dynamic_context;
    stack_low := stack.ptr.offset(macos_page_size);
    @debug_assert_lt(reserved, macos_page_size);
    @assert_eq(u8.int_from_ptr(stack_low).mod(16), 0, "start_thread unaligned stack");  // TODO: return error
    
    new_context := u8.raw_from_ptr(stack.ptr.offset(16));
    set_new_thread_id(DefaultContext.ptr_from_raw(new_context));
    
    // TODO: check PTHREAD_STACK_MIN
    @try(pthread_attr_init(attr&)) return;
    @try(pthread_attr_setstack(attr&, stack_low, stack.len - macos_page_size)) return; 
    ok := pthread_create(out&, attr&, franca_runtime_init_thread, new_context);
    if ok.is_ok() {
        return(Ok = out);
    };
    .Err
}

fn set_new_thread_id(ctx: *DefaultContext) void #inline = {
    NEXT_THREAD_ID :: @static(i64) 1;  // :ThreadIdStartsAtOne
    ctx.thread_index = i64.atomic_inc(NEXT_THREAD_ID);
}

franca_runtime_init_thread :: fn(franca_userdata: rawptr) void = {
    if(!query_context_is_implicit(), => abort());
    userdata := ptr_cast_unchecked(u8, rawptr, franca_userdata.offset(-16))[];
    callee := ptr_cast_unchecked(u8, @FnPtr(userdata: rawptr) void, franca_userdata.offset(-8))[];
    set_dynamic_context(franca_userdata);
    t: ArenaAlloc = init(page_allocator, 1.shift_left(20));
    c := context(DefaultContext);
    c.temporary_allocator = t&;
    callee(userdata);
    t&.deinit();
    // TODO: should we have them pass us the allocator so we can free our stack? 
};

// This is kinda sad cause there's one cpu instruction to do it but 
// it'd rather just implement the super atomic you can do everything 
// with than all the special cases. 
fn atomic_inc($T: Type, p: *T) T #generic #inline = {
    cas(T, p, fn(old) => old + 1)
}

fn atomic_set($T: Type, p: *T, new: T) T #generic #inline = {
    cas(T, p, fn(_) => new)
}
    
// returns the old value. 
fn cas($T: Type, p: *T, $update: @Fn(old: T) T) T #generic = {
    prev := p[];
    dowhile {
        old := prev;
        new := update(old);
        prev = T.cas(p, old, new);
        !T.same_bits(prev, old)
    };
    prev
}

fn type_is_wide(T: Type) bool #fold = @switch(size_of(T)) {
    @case(4) => false;
    @case(8) => true;
    @default => panic("invalid size for type_is_wide");
};

fn cas($T: Type, p: *T, old: T, new: T) T #generic #inline = {
    // TODO: sad that i don't want to use Qbe.Cls here
    @if(type_is_wide(T), {
        part0 :: fn(p: *T) void #ir(.cas0, .Kl);
        part1 :: fn(old: T, new: T) T #ir(.cas1, .Kl);
        part0(p);
        part1(old, new)
    }, {
        part0 :: fn(p: *T) void #ir(.cas0, .Kw);
        part1 :: fn(old: T, new: T) T #ir(.cas1, .Kw);
        part0(p);
        part1(old, new)
    })
}

fn same_bits($T: Type, a: T, b: T) bool #generic = {
    @if(type_is_wide(T), {
        eq :: fn(a: T, b: T) bool #ir(.ceql, .Kw) #intrinsic(.Eq);
        eq(a, b)
    }, {
        eq :: fn(a: T, b: T) bool #ir(.ceqw, .Kw) #intrinsic(.Eq);
        eq(a, b)
    })
}

fn pthread_attr_init(out: *ThreadAttr) voidResult #libc;
fn pthread_create(out: *Thread, attr: *ThreadAttr, callee: @FnPtr(userdata: rawptr) void, userdata: rawptr) voidResult #libc #threads;
fn pthread_attr_setstack(attr: *ThreadAttr, ptr: *u8, len: i64) voidResult #libc;
fn pthread_mutex_init(mutex: *PMutex, todo_attr_pass_zero_for_now: i64) voidResult #libc;
fn pthread_cond_signal(c: *PCond) voidResult #libc;
fn pthread_cond_wait(c: *PCond, mutex: *PMutex) voidResult #libc;
fn pthread_cond_timedwait(c: *PCond, mutex: *PMutex, abstime: *TimeSpec) voidResult #libc;
fn pthread_cond_init(c: *PCond, todo_attr_pass_zero_for_now: i64) voidResult #libc;
fn pthread_mutex_unlock(m: *PMutex) voidResult #libc;
fn pthread_mutex_lock(m: *PMutex) voidResult #libc;
fn pthread_mutex_destroy(m: *PMutex) voidResult #libc;
fn pthread_getcpuclockid(thread: Thread, clock_id_out: *i32) voidResult #libc;
fn pthread_join(thread: Thread, value_ptr: *rawptr) voidResult #libc;

fn lock(self: *PMutex) void = {
    pthread_mutex_lock(self).unwrap();
}

fn unlock(self: *PMutex) void = {
    pthread_mutex_unlock(self).unwrap();
}

fn with(self: *PMutex, $body: @Fn() void) void = {
    self.lock();
    body();
    self.unlock();  // TODO: defer
}
