//! I know this is remarkably bad codegen. 
//! Its only job is to be fast to compile because its used for throw away comptime functions that are often only run once.
//! #c_call means https://en.wikipedia.org/wiki/Calling_convention#ARM_(A64)
//
// TODO: it would be nice to use walk_bc.fr for this to avoid some boring stack stuff, 
//       but that's geared towards ssa-like irs where you're allowed to freely copy around ssa vars. 
//       im not sure if its worth reworking that to give you hooks to shuffle between registers and stack slots as needed.  
//          -- Jul 15
//
// TODO: be able to load directly into a float register

// TODO: put all these config options somewhere consistant.
ZERO_DROPPED_REG :: false;
ZERO_DROPPED_SLOTS :: false;
TRACE_ASM :: false;
TRACE_CALLS :: false; 
USE_LLVM_DIS :: true;

// This should always be true. Otherwise you're doing a call for every add, mul, etc. 
DO_BASIC_ASM_INLINE :: true;

SpOffset :: @struct(id: i64);

IncReg :: @struct(reg: i64, offset_bytes: i64);
AsmVal :: @tagged(
    Increment: IncReg, // not a pointer derefernce, just adding a static number to the value in the register.
    Literal: i64,
    Spill: SpOffset,
    FloatReg: i64,
);
::tagged(AsmVal);
::DeriveFmt(AsmVal);

MEM_64: i64 : 0b11;
MEM_32: i64 : 0b10;
MEM_16: i64 : 0b01;
MEM_08: i64 : 0b00;

fn ne(a: FuncId, b: FuncId) bool #redirect(Ty(u32, u32), bool);

fn emit_bc_and_aarch64(program: CompilerRs, f: FuncId, when: ExecStyle) PRes #compiler = {
    mark := __temp_alloc.mark();
    body := @try(program.emit_bc(f, when)) return;
    
    // TODO: this should be true but breaks for aot with new baked constant trait. maybe its counting random callees when just done for aot but not actually asm?  -- Jun 19
    // debug_assert!(!program[f].get_flag(FnFlag::AsmDone), "ICE: tried to double compile?");
    program.aarch64&.extend_blanks(f);
    
    // We might have noticed that this function was a duplicate while generating bytecode.
    // So it was Normal before but now its a Redirect. 
    if body.func != f {|
        if program.aarch64&.get_fn(body.func) { old |
            program.aarch64.dispatch[f.as_index()] = old;
        };
        
        return(.Ok);
    };
    
    zone := zone_begin(.EmitAarch64); // TODO: defer
    @if(ENABLE_TRACY) {
        func := program.index(f);
        real_name := get_string(program.cast(), func.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    
    a: EmitAsm = new(program, when, body, f);
    a&.compile(f);
    zone_end(zone);
    __temp_alloc.reset_retaining_capacity(mark);
    .Ok
}

fn get_jitted_function(self: CompilerRs, f: FuncId) ?rawptr #compiler = {
    self.aarch64&.get_fn(f)
}

fn put_jitted_function(self: CompilerRs, f: FuncId, addr: rawptr) void #compiler = {
    self.aarch64&.extend_blanks(f);
    self.aarch64.dispatch[f.as_index()] = addr;
}

fn bump_dirty(self: CompilerRs) void #compiler = {
    self.aarch64&.bump_dirty();
}


fn copy_inline_asm(self: CompilerRs, f: FuncId, insts: []u32) void #compiler = {
    self.aarch64&.copy_inline_asm(f, insts);
}

EmitAsm :: @struct(
    program: CompilerRs,
    asm: *Jitted,
    vars: List(?SpOffset),
    next_slot: SpOffset,
    f: FuncId,
    when: ExecStyle,
    release_stack: List(*u32),
    state: BlockState,
    block_ips: List(?*u32),
    clock: u16,
    markers: List(Ty(List(u8), usize)),
    log_asm_bc: bool,
    body: *FnBody,
);

SpSlot :: @struct(at: SpOffset, size: u16, clock: u16);
BlockState :: @struct(
    stack: List(AsmVal),
    free_reg: List(i64),
    free_reg_float: List(i64),
    open_slots: List(SpSlot),
);

fn clone(self: *BlockState) BlockState = {
    (stack = self.stack&.clone(), free_reg = self.free_reg&.clone(), free_reg_float = self.free_reg_float&.clone(), open_slots = self.open_slots&.clone())
}

fn new(compile: CompilerRs, when: ExecStyle, body: *FnBody, f: FuncId) EmitAsm #once = {
    (
        program = compile,
        asm = compile.aarch64&,
        vars = list(temp()),
        next_slot = (id = 0),
        f = f,
        when = when,
        release_stack = list(temp()),
        state = (stack = list(temp()), free_reg = list(temp()), free_reg_float = list(temp()), open_slots = list(temp())),
        block_ips = list(temp()),
        clock = 0,
        markers = list(temp()),
        log_asm_bc = false,
        body = body,
    )
}

// Note: you can only use Self once because this doesn't reset fields.
fn compile(self: *EmitAsm, f: FuncId) void #once = {
    if(self.asm.get_fn(f).is_some(), => return());

    func := self.program[f]&;
    @assert(!func.body&.is(.JittedAarch64) && !func.body&.is(.ComptimeAddr));

    self.log_asm_bc = func.has_tag(.log_asm_bc);
    self.bc_to_asm(f);
    asm := self.asm.save_current(f);
    
    // TODO: assert that we don't generate code for a bunch of empty functions (just the size of prolouge + epiloge)
    //       when doing instantiations like `::tagged(T);`. should get deduplicated in bytecode or earlier. 
    // for now just make sure not to spam when disassembling. 
    if(asm.len <= 8, => return());

    // TODO: this is kinda dumb, I could just be a bit more abstract about how im emitting instructions and then also emit text assembler.
    if TRACE_ASM.or(self.log_asm_bc).or(=> func.has_tag(.log_asm)) {|
        self.program[][].codemap.show_error_line(func.loc);
        addr := u32.int_from_ptr(asm.ptr);
        @println("% is at addr=%", self.program.pool.get(func.name), addr);
        @if(USE_LLVM_DIS) print_llvm_mc_dis(asm);
    };
}

fn print_llvm_mc_dis(asm: []u32) void #once = {
    @println("% instrucitons.", asm.len());
    hex: List(u8) = list(asm.len * 17, temp());
    for asm { op |
        range(0, 4) { _ |
            byte := op.bit_and(0x000000FF);
            op = op.shift_right_logical(8);
            hex&.push_all("0x");
            // TODO: clean this up. it's an old copy paste from fmt_hex for llvm ir hex floats.
            range(14, 16) {i|
                shift := 15.sub(i).mul(4);
                mask := 15.shift_left(shift);
                aaaaa: i64 = byte.zext().bit_and(mask);
                digit := aaaaa.shift_right_logical(shift);
                ::if(u8);
                d := if(digit.lt(10), => "0".ascii().add(digit.trunc()), => "A".ascii().add(digit.sub(10).trunc())); 
                hex&.push(d);
            };
            hex&.push_all(" ");
        };
    };
    
    out := open_temp_file();
    out.fd&.write(hex.items());
    // TODO: figure out how to call a version that looks it up in env/PATH like a shell does
    //       even better have the caller pass in anything like this in general 
    //       but this is just for debugging the compiler so its not a big deal.
    LLVM_MC_PATH :: "/opt/homebrew/opt/llvm/bin/llvm-mc"; // "llvm-mc"
    success := run_cmd_blocking(LLVM_MC_PATH, @slice("--disassemble", out&.s_name()));
    if(!success, => eprintln("ICE(debugging): couldn't disassemble"));
    out.remove();

    //println!("=== Asm for {f:?}: {} ===", self.program.pool.get(func.name));
    //it := dis.split('\n');
    //it.nth(1);
    //for (i, line) in it.enumerate() {
    //    for (s, offset) in &self.markers {
    //        if *offset == i {
    //            println!("{s}");
    //        }
    //    }
    //    println!("{line}");
    //}
    //println!("===")
}

fn bc_to_asm(self: *EmitAsm, f: FuncId) void #once = {
    ff := self.program[f]&;
    @if(TRACE_ASM) @println("=== F% % ===", f.as_index(), self.program.pool.get(ff.name));
    //debugln!("{}", self.program[f].body.log(self.program.pool));

    func := self.body;
    self.f = f;
    self.next_slot = (id = 0);
    self.vars = .None.repeated(func.vars.len, temp());
    self.block_ips = .None.repeated(self.body.blocks.len, temp());

    self.asm.mark_start(f);
    self.asm.push(sub_im(Bits.X64, sp, sp, 16, 0));
    self.asm.push(stp_so(Bits.X64, fp, lr, sp, 0)); // save our return address
    self.asm.push(add_im(Bits.X64, fp, sp, 0, 0)); // Note: normal mov encoding can't use sp
    self.asm.push(brk(0));
    reserve_stack := self.asm.prev();

    // The code expects arguments on the virtual stack (the first thing it does might be save them to variables but that's not my problem).

    sig := self.body.signeture;
    if sig.first_arg_is_indirect_return {|
        self.state.stack&.push(Increment = (reg = 8, offset_bytes = 0));
    };

    // TODO: assert int_count <= 8 && float_count <= 8 or impl pass on stack.
    (int_count, float_count) := self.ccall_reg_to_stack(sig.args);

    // Any registers not containing args can be used.
    range(int_count, 8) { i |
        self.state.free_reg&.push(i);
    };
    range(float_count, 8) { i |
        self.state.free_reg_float&.push(i);
    };
    // debug_assert_eq!(self.state.stack.len() as u16, sig.arg_slots);

    @if(TRACE_ASM) @println("entry: (%)", self.state.stack.items());
    self.emit_block(0, false);

    slots := self.next_slot.id;
    @assert(
        slots < 4096,
        "not enough bits to refer to all slots (%/4096 bytes) in %",
        slots,
        self.program.pool.get(self.program[f].name)
    );
    extra := slots.mod(16);
    if extra != 0 {|
        slots += 16 - extra; // play by the rules
    };
    self.asm.patch(reserve_stack, sub_im(Bits.X64, sp, sp, @as(u12) slots, 0));
    
    for self.release_stack { it |
        self.asm.patch(it, add_im(Bits.X64, sp, fp, 0, 0)); // Note: normal mov encoding can't use sp
        self.asm.patch(it.offset(1), ldp_so(Bits.X64, fp, lr, sp, 0)); // get our return address
        self.asm.patch(it.offset(2), add_im(Bits.X64, sp, sp, 16, 0));
    };
}

fn emit_block(self: *EmitAsm, b: i64, args_not_vstacked: bool) void = {
    if(self.block_ips[b].is_some(), => return());
    self.block_ips[b] = (Some = self.asm.next);
    f := self.f;
    block := self.body.blocks[b]&;

    self.clock = block.clock;
    // TODO: handle normal args here as well.
    if args_not_vstacked {|
        self.state.free_reg&.clear();
        self.state.free_reg_float&.clear();
        (int_count, float_count) := self.ccall_reg_to_stack(block.arg_prims);
        range(int_count, 8) { i |
            self.drop_reg(i);
        };
        range(float_count, 8) { i |
            self.drop_reg_float(i);
        };
    };
    @debug_assert(self.state.stack.len >= block.arg_prims.len);
    func := self.body;
    is_done := false;
    block := self.body.blocks[b]&;
    range(0, func.blocks[b].insts.len) { i |
        if(is_done, => return());
        inst := block.insts[i];
        is_done = self.emit_inst(b, inst, i);
        @if(TRACE_ASM) @println("%:% % => % | free: %", b, i, inst&.tag(), self.state.stack.items(), self.state.free_reg.items());
    };
    @debug_assert(is_done);
}

fn emit_inst(self: *EmitAsm, b_idx: usize, inst: Bc, i: usize) bool #once = {
    // TODO: bring this back 
    //if TRACE_ASM.or(self.log_asm_bc) {|
    //    ins := self.asm.offset_words(self.asm.current_start, self.asm.next) - 1;
    //    self.markers.push((format!("[{b_idx}:{i}] {inst:?}: {:?}", self.state.stack), ins as usize));
    //};
    @if(TRACE_ASM) @println("[%:%] %", b_idx, i, self.state.stack.items());
    @match(inst) {
        fn Nop() => ();
        fn SaveSsa(f) => {
            @assert(self.vars[f.id.zext()].is_none());
            slot := self.create_slots(8);
            reg := self.pop_to_reg();
            self.store_u64(reg, sp, slot.id.trunc());
            self.drop_reg(reg);
            self.vars[f.id.zext()] = (Some = slot);
        }
        fn LoadSsa(f) => {
            // debug_assert!(self.body.is_ssa_var.get(id as usize));
            slot := self.vars[f.id.zext()].unwrap();
            self.state.stack&.push(Spill = slot);
        }
        fn AddrVar(f) => {
            idx: i64 = f.id.zext();
            if self.vars[idx] { (slot: SpOffset) |
                self.state.stack&.push(Increment = (
                    reg = sp,
                    offset_bytes = slot.id,
                ));
            } else {|
                ty := self.body.vars[idx];
                slot := self.create_slots(ty.size);
                self.vars[idx] = (Some = slot);
                self.state.stack&.push(Increment = (
                    reg = sp,
                    offset_bytes = slot.id,
                ));
            };
        }
        fn GetCompCtx() => {
            addr := (**SelfHosted).int_from_ptr(self.program);
            self.state.stack&.push(Literal = addr);
        }
        fn LastUse(f) => {
            slot := self.vars[f.id.zext()]; 
            if slot { slot |
                ty := self.body.vars[f.id.zext()];
                self.drop_slot(slot, ty.size);
            };
        }
        fn NoCompile() => {
            @panic("tried to compile unreachable block in %", self.program[self.f]&.log(self.program.pool));
        }
        fn PushConstant(f) => self.state.stack&.push(Literal = f.value);
        fn PushGlobalAddr(id) => {
            (ptr, _) := self.program[][].get_baked(id)[];
            self.state.stack&.push(Literal = ptr.int_from_rawptr());
        }
        fn GetNativeFnPtr(fid) => {
            // TODO: use adr+adrp instead of an integer. but should do that in load_imm so it always happens.

            if self.asm.get_fn(fid) { (ptr: rawptr) |
                self.state.stack&.push(Literal = ptr.int_from_rawptr());
            } else {|
                @assert(fid.as_index() < 4096);
                reg := self.get_free_reg();
                self.asm.extend_blanks(fid);
                self.load_imm(reg, self.asm.get_dispatch()); // NOTE: this means you can't ever resize
                self.asm.push(ldr_uo(Bits.X64, reg, reg, @as(u12) @as(i64) fid.as_index()));
                self.state.stack&.push(Increment = (reg = reg, offset_bytes = 0));
            };
        }
        fn JumpIf(f) => {
            cond := self.pop_to_reg();
            @debug_assert_eq(f.slots, 0); // emit_bc doesn't do this
            self.spill_abi_stompable();
            self.asm.push(brk(0));
            inst := self.asm.prev();
            
            // branch if zero so true before false
            // we only do one branch so true block must be directly after the check.
            // this is the only shape of flow graph that its possible to generate with my ifs/whiles.
            @debug_assert(self.block_ips[f.true_ip.id.zext()].is_none());

            self.drop_reg(cond);
            state := self.state&.clone();
            self.emit_block(f.true_ip.id.zext(), true);
            self.state = state;
            self.emit_block(f.false_ip.id.zext(), true);
            
            false_ip := self.block_ips[f.false_ip.id.zext()].unwrap();
            offset := self.asm.offset_words(inst, false_ip);
            @debug_assert(cond < 32);
            @debug_assert_ne(offset, 0, "!if ice: while(1);");
            @debug_assert(offset > 0, "always jumping forward, over the false block");
            self.asm.patch(inst, cbz(Bits.X64, offset, cond));

            return(true);
        }
        fn Goto(f) => {
            idx: i64 = f.ip.id.zext();
            block := self.body.blocks[idx]&;
            
            // TODO: fix ending with unreachable() :block_arg_count_wrong_unreachable
            //debug_assert_eq!(slots, block.arg_slots, "goto {ip:?} {}", self.body.log(self.program));
            if block.incoming_jumps == 1 {|
                @debug_assert(self.block_ips[idx].is_none());
                self.emit_block(idx, false);
            } else {|
                self.stack_to_ccall_reg(block.arg_prims);
                self.spill_abi_stompable();
                if self.block_ips[idx] { (to_ip: *u32) |
                    // we've already emitted it, so we know where it is, so just go there. 
                    dist := self.asm.offset_words(self.asm.next, to_ip);
                    @debug_assert_ne(dist, 0, "while(1);");
                    @debug_assert(dist < 0, "always jumping backward, to a previously emitted block"); 
                    self.asm.push(b(dist, 0));
                } else {|
                    // If we haven't emitted it yet, it will be right after us, so just fall through.
                    self.emit_block(idx, true);
                };
            };
            return(true);
        }
        fn CallDirect(f) => {
            // TODO: use with_link for tail calls. need to "Leave holes for stack fixup code." like below
            // TODO: if you already know the stack height of the callee, you could just fixup to that and jump in past the setup code. but lets start simple.
            self.dyn_c_call(f.sig) {|
                if(f.tail, => self.emit_stack_fixup());
                self.branch_func(f.f, !f.tail)
            };
            return(f.tail);
        }
        fn Intrinsic(op) => self.inst_intrinsic(op);
        fn Ret0()     => return(self.emit_return(empty()));
        fn Ret1(prim) => return(self.emit_return(@slice(prim)));
        fn Ret2(f)    => return(self.emit_return(@slice(f._0, f._1)));
        // Note: we do this statically, it wont get actually applied to a register until its needed because loads/stores have an offset immediate.
        fn IncPtrBytes(bytes) => {
            val: *AsmVal = self.state.stack&.last().unwrap();
            @match(val) {
                fn Increment(f) => {
                    f.offset_bytes += bytes.zext();
                }
                fn Literal(v) => {
                    v[] += bytes.zext();
                }
                fn Spill(_) => {
                    // TODO: should it hold the add statically? rn it does the load but doesn't need to restore because it just unspills it.
                    (reg, offset_bytes) := self.pop_to_reg_with_offset();
                    self.state.stack&.push(Increment = (
                        reg = reg,
                        offset_bytes = offset_bytes.zext() + bytes.zext(),
                    ));
                }
                fn FloatReg(_) => panic("ICE: don't try to gep a float");
            };
        }
        fn Load(ty) => {
            (addr, offset_bytes) := self.pop_to_reg_with_offset(); // get the ptr

            dest := self.get_free_reg();
            @match(ty) {
                // TODO: track open float registers so this can just switch over MEM type.
                fn I32() => self.load_one(MEM_32, dest, addr, offset_bytes);
                fn F32() => self.load_one(MEM_32, dest, addr, offset_bytes);
                fn I16() => self.load_one(MEM_16, dest, addr, offset_bytes);
                fn I8()  => self.load_one(MEM_08, dest, addr, offset_bytes);
                @default => self.load_u64(dest, addr, offset_bytes);
            };
            self.state.stack&.push(Increment = (reg = dest, offset_bytes = 0));
            self.drop_reg(addr);
        }
        fn StorePost(ty) => {
            addr := self.state.stack&.pop().expect("enough stack for store");
            value := self.state.stack&.pop().expect("enough stack for store");
            self.emit_store(addr, value, ty);
        }
        fn StorePre(ty) => {
            value := self.state.stack&.pop().expect("enough stack for store");
            addr := self.state.stack&.pop().expect("enough stack for store");
            self.emit_store(addr, value, ty);
        }
        fn CallFnPtr(f) => {
            // TODO: tail call
            self.dyn_c_call(f.sig) {|
                // dyn_c_call will have popped the args, so now stack is just the pointer to call
                callee := self.state.stack&.pop().expect("enough stack for call");
                // TODO: this does redundant spilling every time!
                @match(callee) {
                    fn Literal(p) => self.load_imm(x16, p);
                    fn Spill(offset) => self.load_u64(x16, sp, offset.id.trunc());
                    @default => {
                        panic("ICE: unspilled register");
                    };
                };
                self.asm.push(br(x16, 1));
            };
        }
        fn Unreachable() => {
            self.asm.push(brk(0xbabe));
            return(true);
        }
        fn Snipe(skip) => {
            index := self.state.stack.len - skip.zext() - 1;
            @match(self.state.stack&.ordered_remove(index).unwrap()) {
                fn Increment(f) => self.drop_reg(f.reg);
                fn Literal(_)   => ();
                fn Spill(_)     => ();
                fn FloatReg(_)  => {
                    panic("unexpected float reg");
                }
            };
        }
        fn PeekDup(skip) => {
            val := self.state.stack[self.state.stack.len - skip.zext() - 1];
            @match(val) {
                fn Increment(f) => {
                    if f.reg == sp {|
                        self.state.stack&.push(val);
                    } else {|
                        new := self.get_free_reg();
                        self.asm.push(mov(Bits.X64, new, f.reg));
                        self.state.stack&.push(Increment = (reg = new, offset_bytes = f.offset_bytes));
                    };
                }
                fn Literal(_)   => self.state.stack&.push(val);
                fn Spill(_)     => self.state.stack&.push(val);
                fn FloatReg(reg)  => {
                    new := self.get_free_reg_float();
                    self.asm.push(fmov(FType.D64, new, reg));  // I assume this works if the thing is an f32 also? just the top is zeros. 
                    self.state.stack&.push(FloatReg = new);
                }
            };
        }
        fn CopyBytesToFrom(bytes) => {
            self.state.stack&.push(Literal = bytes.zext());
            self.stack_to_ccall_reg(@slice(Prim.P64, Prim.P64, Prim.I64));
            self.spill_abi_stompable();
            // TODO: fancy version of this?
            addr: rawptr : fn(dest: *u8, src: *u8, len: i64) void = {
                range(0, len) { i |
                    dest.offset(i)[] = src.offset(i)[];
                };
            };
            // offset := self.asm.offset_words(self.asm.next, addr);
            self.load_imm(x17, addr.int_from_rawptr());
            self.asm.push(br(x17, 1));
            self.restore_all_registers();
        }
    };

    false
}

fn restore_all_registers(self: *EmitAsm) void = {
    range(0, 8) { i |
        self.state.free_reg&.add_unique(i);
    };
    range(0, 8) { i |
        self.state.free_reg_float&.add_unique(i);
    };
}

::display_slice(AsmVal);
::DeriveFmt(AsmVal);
::DeriveFmt(IncReg);
::DeriveFmt(SpOffset);

// TODO: refactor this. its a problem that im afraid of it! -- May 8
// TODO: improve float handling now that i track free registers -- Jul 24
fn stack_to_ccall_reg(self: *EmitAsm, types: [] Prim) void = {
    ::display_slice(Prim);
    @debug_assert(
        self.state.stack.len >= types.len,
        "found % things but wanted %",
        self.state.stack.len,
        types.len
    );
    next_int := 0;
    next_float := 0;

    enumerate types { slot_index, ty |
        continue :: local_return;
        @debug_assert_eq(slot_index, next_int + next_float);
        f := ty[].is_float();
        stack_index := self.state.stack.len - (types.len) + slot_index;

        if f {|
            @if_let(self.state.stack[stack_index]) fn FloatReg(have) => {
                if have == next_float {|
                    // its already where we want it.
                    self.state.stack[stack_index] = (Literal = 0); // just make sure we dont see it again later.
                    next_float += 1;
                    continue();
                };
            };

            @if(TRACE_ASM) @print("|(v%) <- (%)| ", next_float, self.state.stack[stack_index]&);

            range(0, self.state.stack.len) { i |
                @if_let(self.state.stack[i]) fn FloatReg(x) => {
                    if x == next_float {|
                        // Someone else already has the one we want, so spill that to the stack since i don't have fmov encoding. TODO
                        worked := self.try_spill(i, false, true);
                        @assert(worked);
                    };
                };
            };
            @match(self.state.stack[stack_index]) {
                fn Increment(f) => {
                    @debug_assert_eq(f.offset_bytes, 0, "dont GEP a float");
                    @debug_assert_ne(f.reg, sp, "dont fmov the stack pointer");
                    // TODO: fmov encoding.
                    worked := self.try_spill(stack_index, true, false);
                    @assert(worked);
                }
                fn Literal(x) => {
                    // TODO: fmov encoding.
                    reg := self.get_free_reg();
                    self.load_imm(reg, x);
                    self.state.stack[stack_index] = (Increment = (reg = reg, offset_bytes = 0));
                    worked := self.try_spill(stack_index, true, false);
                    @assert(worked);
                }
                fn Spill(_)    => ();
                fn FloatReg(_) => {
                    // TODO: fmov encoding.
                    // TODO: update now that we're tracking free float registers -- Jul 24
                    worked := self.try_spill(stack_index, true, true);
                    @assert(worked);
                }
            };

            slot := self.state.stack[stack_index];
            @assert(slot&.is(.Spill));
            slot := slot.Spill;
            self.asm.push(f_ldr_uo(Bits.X64, next_float, sp, @as(u12) @as(i64) slot.id / 8));
            // self.drop_slot(slot, 8); // TODO!

            self.state.free_reg_float&.ordered_retain(fn(r: *i64) => next_float != r[]); // TODO: unordered retain
            self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
            next_float += 1;
            continue();
        };
        
        @if_let(self.state.stack[stack_index]) fn Increment(f) => {
            if f.reg == next_int {|
                if f.offset_bytes != 0 {|
                    self.asm.push(add_im(Bits.X64, f.reg, f.reg, f.offset_bytes, 0));
                };

                // if we happen to already be in the right register, cool, don't worry about it.
                @if(TRACE_ASM) @print("|x% already| ", f.reg);
                self.state.free_reg&.ordered_retain(fn(r: *i64) => f.reg != r[]); // TODO: does unordered_retain work?
                self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
                next_int += 1;
                continue();
            };
        };

        found := false;
        if self.state.free_reg&.position(fn(r) => next_int == r[]) { want |
            found = true;
            self.state.free_reg&.ordered_remove(want); // TODO: does unordered_remove work here?
        };
        
        if !found {|
            break :: local_return;
            each self.state.stack { r | 
                @if_let(r) fn Increment(f) => {
                    if f.reg == next_int {|
                        found = true;
                        // The one we want is already used by something on the v-stack; swap it with a random free one. TODO: put in the right place if you can.
                        @debug_assert_ne(f.reg, sp); // encoding but unreachable
                        reg := self.get_free_reg();
                        self.asm.push(mov(Bits.X64, reg, f.reg));
                        r[] = (Increment = (reg = reg, offset_bytes = f.offset_bytes));
                        // Now x{i} is free and we'll use it below.
                        break();
                    };
                };
            };
        };
        
        // TODO: lexer error if you forget a quote and hit a new line. 
        @assert(found, "TODO: x% is not free. stack is %. free is %",
                next_int, self.state.stack.items(), self.state.free_reg.items());

        @if(TRACE_ASM) @print("|(x%) <- (%)| ", next_int, self.state.stack[stack_index]&);
        @match(self.state.stack[stack_index]) {
            fn Increment(f) => {
                @debug_assert_ne(next_int, f.reg);
                // even if offset_bytes is 0, we need to move it to the right register. and add can encode that mov even if reg is sp.
                self.asm.push(add_im(Bits.X64, next_int, f.reg, f.offset_bytes, 0));
                self.drop_reg(f.reg);
            }
            fn Literal(x) => self.load_imm(next_int, x);
            fn Spill(slot) => self.load_u64(next_int, sp, slot.id.trunc());
            fn FloatReg(f) => {
                fmov_from(next_int, f);
                self.drop_reg_float(f);
            }
        };
        self.state.free_reg&.ordered_retain(fn(r) => next_int != r[]); // TODO: does unordered_retain work here? 
        self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
        next_int += 1;
    };
    @if(TRACE_ASM) println("");
    self.state.stack.len -= types.len;
}

::if(AsmVal);
fn ccall_reg_to_stack(self: *EmitAsm, types: [] Prim) Ty(i64, i64) = {
    next_float := 0;
    next_int := 0;
    enumerate types { i, ty |
        @debug_assert_eq(i, next_int + next_float);
        f := ty[].is_float();
        v: AsmVal = if f {|
            self.state.free_reg_float&.ordered_retain(fn(r) => r[] != next_float);
            (FloatReg = next_float)
        } else {|
            self.state.free_reg&.ordered_retain(fn(r) => r[] != next_int); // TODO: does unordered_retain work?
            (Increment = (
                reg = next_int,
                offset_bytes = 0,
            ))
        };
        if f {|
            next_float += 1;
        } else {|
            next_int += 1;
        };
        self.state.stack&.push(v);
    };
    (next_int, next_float)
}

fn emit_store(self: *EmitAsm, addr: AsmVal, value: AsmVal, ty: Prim) void = {
    (reg, offset_bytes) := self.in_reg_with_offset(addr);
    p64 :: fn() => {
        if value&.is(.FloatReg) {|
            r := value.FloatReg;
            @assert_eq(offset_bytes.mod(8), 0, "TODO: align");
            self.asm.push(f_str_uo(Bits.X64, r, reg, @as(u12) @as(i64) (offset_bytes.zext() / 8)));
            self.drop_reg_float(r);
        } else {|
            val := self.in_reg(value);
            self.store_u64(val, reg, offset_bytes);
            self.drop_reg(val);
        };
    };
    @match(ty) {
        fn P64() => p64();
        fn F64() => p64();
        fn I64() => p64();
        fn F32() => {
            if value&.is(.FloatReg) {|
                r := value.FloatReg;
                @assert_eq(offset_bytes.mod(4), 0, "TODO: align");
                self.asm.push(f_str_uo(Bits.W32, r, reg, @as(u12) @as(i64) (offset_bytes.zext() / 4)));
                self.drop_reg_float(r);
            } else {|
                val := self.in_reg(value);
                self.store_one(MEM_32, val, reg, offset_bytes);
                self.drop_reg(val);
            };
        }
        fn I32() => {
            val := self.in_reg(value);
            self.store_one(MEM_32, val, reg, offset_bytes);
            self.drop_reg(val);
        }
        fn I16() => {
            val := self.in_reg(value);
            self.store_one(MEM_16, val, reg, offset_bytes);
            self.drop_reg(val);
        }
        fn I8() => {
            val := self.in_reg(value);
            self.store_one(MEM_08, val, reg, offset_bytes);
            self.drop_reg(val);
        }
    };
    self.drop_reg(reg);
}

fn load_u64(self: *EmitAsm, dest_reg: i64, src_addr_reg: i64, offset_bytes: u16) void = {
    self.load_one(MEM_64, dest_reg, src_addr_reg, offset_bytes)
}

fn load_one(self: *EmitAsm, register_type: i64, dest_reg: i64, src_addr_reg: i64, offset_bytes: u16) void = {
    @debug_assert_ne(dest_reg, sp, "ICE: tried to load into sp");
    scale := 1.shift_left(register_type);
    extra := (@as(i64) offset_bytes.zext()).mod(scale);
    if extra == 0 {|
        self.asm.push(ldr_uo_any(register_type, dest_reg, src_addr_reg, @as(u12) @as(i64) (offset_bytes.zext() / scale)));
        @debug_assert((@as(i64) offset_bytes.zext()) / scale < 1.shift_left(12), "ICE: not enough bits for load");
    } else {|
        // Note: this relies on the access actually being aligned once you combine the value in the register without our non %8 offset.
        reg := self.get_free_reg();
        @debug_assert((@as(i64) offset_bytes.zext()) < 1.shift_left(12), "ICE: not enough bits for load");
        self.asm.push(add_im(Bits.X64, reg, src_addr_reg, @as(u12) @as(i64) offset_bytes.zext(), 0));
        self.asm.push(ldr_uo_any(register_type, dest_reg, reg, 0));
        self.drop_reg(reg);
    }
}

fn store_u64(self: *EmitAsm, src_reg: i64, dest_addr_reg: i64, offset_bytes: u16) void = {
    self.store_one(MEM_64, src_reg, dest_addr_reg, offset_bytes)
}

fn store_one(self: *EmitAsm, register_type: i64, src_reg: i64, dest_addr_reg: i64, offset_bytes: u16) void = {
    scale := 1.shift_left(register_type);
    if src_reg == sp {|
        // TODO: this is weird. can only happen for exactly the sp, not an offset from it. so i guess somewhere else is saving an add, maybe its fine. -- May 2
        reg := self.get_free_reg();
        self.asm.push(add_im(Bits.X64, reg, sp, 0, 0)); // not mov!
        self.store_one(register_type, reg, dest_addr_reg, offset_bytes);
        self.drop_reg(reg);
        return();
    };
    if offset_bytes.mod(8) == 0 {|
        self.asm.push(str_uo_any(register_type, src_reg, dest_addr_reg, @as(u12) @as(i64) (offset_bytes.zext() / scale)));
        @debug_assert((@as(i64) offset_bytes.zext()) / scale < 1.shift_left(12), "ICE: not enough bits for store");
    } else {|
        reg := self.get_free_reg();
        self.asm.push(add_im(Bits.X64, reg, dest_addr_reg, @as(u12) @as(i64) offset_bytes.zext(), 0));
        self.asm.push(str_uo_any(register_type, src_reg, reg, 0));
        self.drop_reg(reg);
    };
}

fn spill_abi_stompable(self: *EmitAsm) void = {
    range(0, self.state.stack.len) { i |
        self.try_spill(i, true, true);
    };
}

// do_floats:false if you're just trying to free up a gpr, not saving for a call.
fn try_spill(self: *EmitAsm, i: usize, do_ints: bool, do_floats: bool) bool = {
    v := self.state.stack[i];
    if do_ints {|
        @if_let(v) fn Increment(f) => {
            if(f.reg == sp, => return(false));
    
            // Note: this assumes we don't need to preserve the value in the reg other than for this one v-stack slot.
            slot := self.create_slots(8);
            @if(TRACE_ASM) @print("(spill (x% + %) -> [sp, %]) ", f.reg, f.offset_bytes, slot.id);
            if f.offset_bytes != 0 {|
                self.asm.push(add_im(Bits.X64, f.reg, f.reg, f.offset_bytes, 0));
            };
    
            self.store_u64(f.reg, sp, slot.id.trunc());
            self.drop_reg(f.reg);
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    if do_floats {|
        @if_let(v) fn FloatReg(freg) => {
            slot := self.create_slots(8);
            self.asm.push(f_str_uo(Bits.X64, freg, sp, @as(u12) @as(i64) slot.id / 8));
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    false
}

fn get_free_reg(self: *EmitAsm) i64 = {
    or self.state.free_reg&.pop() {|
        i := 0;
        while => i < self.state.stack.len && !self.try_spill(i, true, false) {|
            i += 1;
        };
        if i == self.state.stack.len {|
            panic("ICE: spill to stack failed");
        };
        self.state.free_reg&.pop().unwrap()
    }
}

fn get_free_reg_float(self: *EmitAsm) i64 = {
    or self.state.free_reg_float&.pop() {|
        i := 0;
        while => i < self.state.stack.len && !self.try_spill(i, false, true) {|
            i += 1;
        };
        if i == self.state.stack.len {|
            panic("ICE: spill to stack failed");
        };
        self.state.free_reg_float&.pop().unwrap()
    }
}

::display_slice(i64);

fn drop_reg(self: *EmitAsm, reg: i64) void = {
    if reg != sp && reg != x8 {|
        @debug_assert(!self.state.free_reg&.contains(reg&), "drop r% -> %", reg, self.state.free_reg.items());
        self.state.free_reg&.push(reg);
        if ZERO_DROPPED_REG {|
            self.load_imm(reg, 0);
        };
    };
}

fn drop_reg_float(self: *EmitAsm, reg: i64) void = {
    @debug_assert(!self.state.free_reg_float&.contains(reg&), "drop F% -> %", reg, self.state.free_reg_float.items());
    self.state.free_reg_float&.push(reg);
}

fn pop_to_reg(self: *EmitAsm) i64 = {
    val := self.state.stack&.pop().unwrap();
    self.in_reg(val)
}

fn pop_to_reg_float(self: *EmitAsm) i64 = {
    val := self.state.stack&.pop().unwrap();
    self.in_reg_float(val)
}

fn in_reg(self: *EmitAsm, val: AsmVal) i64 = {
    @match(val) {
        fn Increment(f) => {
            if f.offset_bytes > 0 {|
                out := if(f.reg == sp, => self.get_free_reg(), => f.reg);
                if f.offset_bytes < 1.shift_left(12) {|
                    self.asm.push(add_im(Bits.X64, out, f.reg, f.offset_bytes, 0));
                } else {|
                    self.asm.push(add_im(Bits.X64, out, f.reg, @as(u12) @as(i64) f.offset_bytes.shift_right_logical(12), 1));
                    mask := 1.shift_left(12) - 1;
                    self.asm.push(add_im(Bits.X64, out, f.reg, @as(u12) @as(i64) f.offset_bytes.bit_and(mask), 0));
                };
                f.reg = out;
            };
            f.reg
        }
        fn Literal(x) => {
            r := self.get_free_reg();
            self.load_imm(r, x);
            r
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, sp, slot.id.trunc());
            // self.drop_slot(slot, 8); TODO!
            r
        }
        fn FloatReg(float) => {
            int := self.get_free_reg();
            fmov_from(int, float);
            self.drop_reg_float(float);
            int
        }
    }
}


fn in_reg_float(self: *EmitAsm, val: AsmVal) i64 = {
    @match(val) {
        fn FloatReg(r) => r;
        @default => {
            // TODO: load spill and immediates directly into float register
            int := self.in_reg(val);
            float := self.get_free_reg_float();
            self.asm.push(fmov_to(float, int));
            self.drop_reg(int);
            float
        };
    }
}

fn peek_to_reg_with_offset(self: *EmitAsm) Ty(i64, u16) = {
    val := self.state.stack&.last().unwrap()[];
    self.in_reg_with_offset(val)
}

fn in_reg_with_offset(self: *EmitAsm, val: AsmVal) Ty(i64, u16) = {
    @match(val) {
        fn Increment(f) => (f.reg, f.offset_bytes.trunc());
        fn Literal(x) => {
            r := self.get_free_reg();
            self.load_imm(r, x);
            (r, 0)
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, sp, slot.id.trunc());
            (r, 0)
        }
        fn FloatReg(_) => panic("ICE: in_reg_with_offset on a float register");
    }
}

fn pop_to_reg_with_offset(self: *EmitAsm) Ty(i64, u16) = {
    res := self.peek_to_reg_with_offset();
    self.state.stack&.pop().unwrap();
    res
}

fn create_slots(self: *EmitAsm, bytes: u16) SpOffset = {
    bytes = (bytes + 7) / 8 * 8; // TODO: use alignment instead of just wasting space :aligned_stack_slots
    enumerate self.state.open_slots { i, slot |
        if self.clock >= slot.clock && slot.size == bytes {|
            slot := self.state.open_slots&.ordered_remove(i).unwrap();  // TODO: does unordered_remove work? 
            return(slot.at); 
        };
    };

    made := self.next_slot;
    self.next_slot.id += bytes.zext();
    made
}

MAX_u16 :: 0xFFFF;
// TODO: this should check if adr is close enough since it statically knows the ip cause we're jitting.
fn load_imm(self: *EmitAsm, reg: i64, value: i64) void = {
    value: u64 = value.bitcast();
    bottom: u64 = MAX_u16.zext();
    self.asm.push(movz_rt(Bits.X64, reg, value.bit_and(bottom).trunc(), .Left0));
    value = value.shift_right_logical(16);
    if value != 0 {|
        range(1, 4) { shift |
            part := value.bit_and(bottom);
            if part != 0 {|
                // TODO: have intcast vs trunc like zig has. one safety checked that it fits in the target type and one that allows chopping stuff off. 
                self.asm.push(movk_rt(Bits.X64, reg, part.trunc(), shift));
            };
            value = value.shift_right_logical(16);
            if(value == 0, => return());
        };
    };
}

// Note: comp_ctx doesn't push the ctx here, bc needs to do that. this it just does the call with an extra argument.
/// <arg:n> -> <ret:m>
fn dyn_c_call(self: *EmitAsm, sig: PrimSig, $do_call: @Fn() void) void = {
    self.stack_to_ccall_reg(sig.args);
    self.spill_abi_stompable();
    // TODO: don't spill!
    if sig.first_arg_is_indirect_return {|
        @match(self.state.stack&.pop().unwrap()) {
            fn Increment(f) => {
                // we know emit_bc often makes the return location a stack variable. 
                // and for now, since we just spilled, any other register would have been wastefully stored so we'd be in the Spill case.
                @assert(f.reg == sp);
                self.asm.push(add_im(Bits.X64, x8, f.reg, f.offset_bytes, 0))
            }
            fn Literal(v) => self.load_imm(x8, v);
            fn Spill(slot) => self.load_u64(x8, sp, slot.id.trunc());
            fn FloatReg(_) => panic("ICE: tried to use float register as indirect return");
        };
    };
    do_call();

    ::if_opt(Prim, i64);
    ::if_opt(Prim, Ty(i64, i64));
    (int_countt, float_count) := if sig.ret1 { (fst: Prim) |
        if sig.ret2 { (snd: Prim) |
            self.ccall_reg_to_stack(@slice(fst, snd));
            ints := fst.int_count() + snd.int_count();
            @as(Ty(i64, i64)) (ints, 2 - ints) // TODO: annoying that you need the type here. 
        } else {|
            self.ccall_reg_to_stack(@slice(fst));
            ints := fst.int_count();
            (ints, 1 - ints)
        }
    } else {|
        (0, 0)
    };

    // now the extras are usable again.
    range(int_countt, 8) { i |
        self.state.free_reg&.add_unique(i); 
    };
    range(float_count, 8) { i |
        self.state.free_reg_float&.add_unique(i); 
    };
}

::enum(CallConv);
::if(u1);

// TODO: use with_link for tail calls.
// !with_link === tail
fn branch_func(self: *EmitAsm, f: FuncId, with_link: bool) void #once = {
    if DO_BASIC_ASM_INLINE {|
        func := self.program[f]&;
        // TODO: save result on the function so dont have to recheck every time?
        if func.body&.jitted_aarch64() { code |
            if func.cc.unwrap() == .OneRetPic {|
                @if(TRACE_ASM) println("OneRetPic call");
                // TODO: HACK: for no-op casts, i have two rets because I can't have single element tuples.
                if code.len == 2 && code[0] == ret() && code[1] == ret() {|
                    if !with_link {|
                        // If you're trying to do a tail call to fn add, that means emit the add instruction and then return.
                        self.asm.push(ret());
                    };
                    return();
                };
                for code.slice(0, code.len - 1) { op |
                    @debug_assert_ne(op, ret());
                    self.asm.push(op);
                };
                @debug_assert_eq(code.last().unwrap()[], ret());
                // If you're trying to do a tail call to fn add, that means emit the add instruction and then return.s
                if(!with_link, => self.asm.push(ret()));
                return();
            };
        };
    };

    // If we already emitted the target function, can just branch there directly.
    // This covers the majority of cases because I try to handle callees first.
    if self.asm.get_fn(f) { bytes |
        n := u32.int_from_ptr(self.asm.next);
        offset := bytes.int_from_rawptr() - n;
        offset = offset / 4;
        // TODO: use adr/adrp
        if offset.abs() < 1.shift_left(25) {|
            @if(TRACE_ASM) println("backwards call");
            self.asm.push(b(offset, if(with_link, => 1, => 0)));
            return(); // TODO: footgun. its too easy to `return;` which returns in other languages but here just evaluates and descards.
        };
    };

    // It's a function we haven't emitted yet, so we don't know where to jump to. (or we do know but it's far away)
    // The normal solution would be punt and let the linker deal with it or go back later and patch it ourselves.
    // But for now, I just spend a register on having a dispatch table and do an indirect call through that.
    // TODO: have a mapping. funcs take up slots even if never indirect called.
    
    self.load_imm(x16, self.asm.get_dispatch()); // NOTE: this means you can't ever resize
    if f.as_index() < 4096 {|
        @if(TRACE_ASM) println("short table call");
        self.asm.push(ldr_uo(Bits.X64, x16, x16, @as(u12) @as(i64) f.as_index()));
    } else {|
        @if(TRACE_ASM) println("long table call");
        // TODO: insert in range even tho this one is quite big. 
        full := f.as_index() * 8;
        bottom := full.bit_and(1.shift_left(12) - 1);
        top := (full - bottom).shift_right_logical(12); // TODO: don't need to minus? we're shifting it off anyway. 
        self.asm.push(add_im(Bits.X64, x16, x16, top, 1));
        self.asm.push(ldr_uo(Bits.X64, x16, x16, @as(u12) @as(i64) bottom / 8));
    };
    self.asm.push(br(x16, if(with_link, => 1, => 0)));
}

// Now have to be careful about trying to use x17
fn drop_slot(self: *EmitAsm, slot: SpOffset, bytes: u16) void = {
    bytes = (bytes + 7) / 8 * 8; // TODO: use alignment instead of just wasting space :aligned_stack_slots
    self.state.open_slots&.push((at = slot, size = bytes, clock = self.clock)); // TODO: keep this sorted by count?
    if ZERO_DROPPED_SLOTS {|
        self.load_imm(x17, 0);
        range(0, bytes.zext() / 8) { i |
            self.store_u64(x17, sp, (slot.id + (i * 8)).trunc());
        };
    };
}

fn emit_stack_fixup(self: *EmitAsm) void = {
    self.asm.push(brk(0));
    fst := self.asm.prev();
    self.asm.push(brk(0));
    self.asm.push(brk(0));
    self.release_stack&.push(fst);
}

// TODO: do i need to be using this instead? -- Jul 22
fn drop_if_unused(self: *EmitAsm, register: i64) void = {
    for self.state.stack { r |
        @if_let(r) fn Increment(f) => {
            if(f.reg == register, => return());
        };
    };
    self.drop_reg(register);
}

fn emit_return(self: *EmitAsm, sig: [] Prim) bool = {
    // We have the values on virtual stack and want them in r0-r7, that's the same as making a call.
    if !sig.is_empty() {|
        self.stack_to_ccall_reg(sig);
    };
    self.emit_stack_fixup();
    self.asm.push(ret(()));
    true
}

fn inst_intrinsic(self: *EmitAsm, op: Intrinsic) void #once = {
    // TODO: for add, include offset_bytes in the immediate if they fit or combine them and defer instead of doing 3 seperate adds.
    bin :: fn($do: @Fn(a: u5, b: u5) void) void => {
        snd: u5 = self.pop_to_reg();
        fst: u5= self.pop_to_reg();
        do(fst, snd);
        self.drop_reg(snd);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    // Note: could use better error message when you try to factor this into a '=' but keep 'bin' a '=>',
    //       "missing value self" because 'bin' closes over the outer one, not the one you'd add as an argument to this function.
    // TODO: allow mutiple callsites that are all tail of a '=>' function to only make one duplicate of it so there's less code bloat. 
    //! IMPORTANT: cond is inverted because CSINC
    bin_cmp :: fn(inv_cond: Cond) => bin(fn(a, b) => {
        self.asm.push(cmp(Bits.X64, a, b));
        self.asm.push(cset(Bits.X64, a, inv_cond));
    });
    bin_float :: fn($do: @Fn(a: u5, b: u5) void) void => {
        snd: u5 = self.pop_to_reg_float();
        fst: u5 = self.pop_to_reg_float();
        do(fst, snd);
        self.drop_reg_float(snd);
        self.state.stack&.push(FloatReg = fst);
    };
    bin_cmp_float :: fn(self: *EmitAsm, inv_cond: Cond) void = {
        snd: u5 = self.pop_to_reg_float();
        fst: u5 = self.pop_to_reg_float();
        out := self.get_free_reg();
        self.asm.push(fcmp(FType.D64, fst, snd));
        self.asm.push(cset(Bits.X64, @as(u5) out, inv_cond));
        self.drop_reg_float(fst);
        self.drop_reg_float(snd);
        self.state.stack&.push(Increment = (reg = out, offset_bytes = 0));
    };
        
    trunc8 :: fn(self: *EmitAsm) void = {
        fst: u5 = self.pop_to_reg();
        extra: u5 = self.get_free_reg();
        self.asm.push(movz(.X64, extra, 0x00FF, .Left0));
        self.asm.push(and_sr(.X64, fst, fst, extra, Shift.LSL, @as(u6) 0));
        self.drop_reg(extra);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    
    trunc16 :: fn(self: *EmitAsm) void = {
        fst: u5 = self.pop_to_reg();
        extra: u5 = self.get_free_reg();
        self.asm.push(movz(.X64, extra, 0xFFFF, .Left0));
        self.asm.push(and_sr(.X64, fst, fst, extra, Shift.LSL, @as(u6) 0));
        self.drop_reg(extra);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    
    trunc32 :: fn(self: *EmitAsm) void = {
        fst: u5 = self.pop_to_reg();
        extra: u5 = self.get_free_reg();
        self.asm.push(movz(.X64, extra, 0xFFFF, .Left0));
        self.asm.push(movk(.X64, extra, 0xFFFF, .Left16));
        self.asm.push(and_sr(.X64, fst, fst, extra, Shift.LSL, @as(u6) 0));
        self.drop_reg(extra);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    
    float_to_int :: fn($do: @Fn(in: u5, out: u5) void) void => {
        in: u5 = self.pop_to_reg_float();
        out: u5 = self.get_free_reg();
        do(in, out);
        self.drop_reg_float(in);
        self.state.stack&.push(Increment = (reg = out, offset_bytes = 0));
    };
    int_to_float :: fn($do: @Fn(in: u5, out: u5) void) void => {
        in: u5 = self.pop_to_reg();
        out: u5 = self.get_free_reg_float();
        do(in, out);
        self.drop_reg(in);
        self.state.stack&.push(FloatReg = out);
    };
    float_to_float :: fn($do: @Fn(in: u5) void) void => {
        in: u5 = self.pop_to_reg_float();
        do(in);
        self.state.stack&.push(FloatReg = in);
    };
    int_to_int :: fn($do: @Fn(in: u5) void) void => {
        in: u5 = self.pop_to_reg();
        do(in);
        self.state.stack&.push(Increment = (reg = in, offset_bytes = 0));
    };
    
    @match(op) {
        fn Add() => bin(fn(a, b) => self.asm.push(add_sr(Bits.X64, a, a, b, Shift.LSL, 0b000000)));
        fn Sub() => bin(fn(a, b) => self.asm.push(sub_sr(Bits.X64, a, a, b, Shift.LSL, 0b000000)));
        fn Mul() => bin(fn(a, b) => self.asm.push(madd(Bits.X64, a, a, b, 0b11111)));
        fn Div() => bin(fn(a, b) => self.asm.push(sdiv(Bits.X64, a, a, b)));
        fn Eq()  => bin_cmp(.NE);
        fn Ne()  => bin_cmp(.EQ);
        fn Le()  => bin_cmp(.GT);
        fn Ge()  => bin_cmp(.LT);
        fn Lt()  => bin_cmp(.GE);
        fn Gt()  => bin_cmp(.LE);
        fn IntToPtr() => (); // no-op
        fn PtrToInt() => (); // no-op
        fn ShiftLeft()            => bin(fn(a, b) => self.asm.push(lslv(Bits.X64, a, a, b)));
        fn ShiftRightLogical()    => bin(fn(a, b) => self.asm.push(lsrv(Bits.X64, a, a, b)));
        fn ShiftRightArithmetic() => bin(fn(a, b) => self.asm.push(asrv(Bits.X64, a, a, b)));
        fn BitOr()  => bin(fn(a, b) => self.asm.push(orr(Bits.X64, a, a, b, Shift.LSL, 0b000000)));
        fn BitAnd() => bin(fn(a, b) => self.asm.push(and_sr(Bits.X64, a, a, b, Shift.LSL, @as(u6) 0)));
        fn BitXor() => bin(fn(a, b) => self.asm.push(eor(Bits.X64, a, a, b, Shift.LSL, @as(u6) 0)));
        fn FAdd() => bin_float(fn(a, b) => self.asm.push(fadd(FType.D64, a, a, b)));
        fn FSub() => bin_float(fn(a, b) => self.asm.push(fsub(FType.D64, a, a, b)));
        fn FMul() => bin_float(fn(a, b) => self.asm.push(fmul(FType.D64, a, a, b)));
        fn FDiv() => bin_float(fn(a, b) => self.asm.push(fdiv(FType.D64, a, a, b)));
        fn FEq()  => self.bin_cmp_float(.NE);
        fn FNe()  => self.bin_cmp_float(.EQ);
        fn FLe()  => self.bin_cmp_float(.GT);
        fn FGe()  => self.bin_cmp_float(.LT);
        fn FLt()  => self.bin_cmp_float(.GE);
        fn FGt()  => self.bin_cmp_float(.LE);
        fn Trunc64To32() => self.trunc32();
        fn Trunc64To16() => self.trunc16();
        fn Trunc64To8()  => self.trunc8();
        fn Trunc32To16() => self.trunc16();
        fn Trunc32To8()  => self.trunc8();
        fn Trunc16To8()  => self.trunc8();
        // TODO: this about how signed numbers are represented. 
        fn SignExtend32To64() => (); // TODO: WRITE A TEST THAT FAILS BECAUSE OF THIS
        fn ZeroExtend32To64() => (); // no-op
        fn ZeroExtend16To64() => (); // no-op
        fn ZeroExtend8To64()  => (); // no-op
        fn ZeroExtend16To32() => (); // no-op
        fn ZeroExtend8To32()  => (); // no-op
        fn ZeroExtend8To16()  => (); // no-op
        fn IntToFloatValue()  => int_to_float(fn(in, out) => self.asm.push(scvtf(Bits.X64, FType.D64, out, in)));
        fn FloatToIntValue()  => float_to_int(fn(in, out) => self.asm.push(fcvtzs(Bits.X64, FType.D64, out, in)));
        fn IntToFloatBits()   => int_to_float(fn(in, out) => self.asm.push(fmov_to(out, in)));
        fn FloatToIntBits()   => float_to_int(fn(in, out) => self.asm.push(fmov_from(out, in)));
        fn ShrinkFloat() => float_to_float(fn(in) => self.asm.push(fcnv(.D64, .S32, in, in)));
        fn GrowFloat()   => float_to_float(fn(in) => self.asm.push(fcnv(.S32, .D64, in, in)));
        fn BitNot()      => int_to_int(fn(in) => self.asm.push(orn(Bits.X64, in, in, 0b11111, Shift.LSL, @as(u6) 0)));
        @default => {
            @panic("ICE: unimplemented aarch64 intrinsic %", op);
        };
    };
}


PAGE_SIZE :: 16384; // TODO

Jitted :: @struct(
    mmapped: []u32,
    dispatch: List(rawptr),
    // aarch64 instructions are always 4 bytes. 
    current_start: *u32,
    next: *u32,
    old: *u32,
);

fn new(bytes: i64) Jitted #once = {
    mmapped := page_allocator.alloc(u32, bytes / 4);
    (
        mmapped = mmapped,
        dispatch = list(99999, page_allocator), // Dont ever resize!
        current_start = mmapped.ptr,
        next = mmapped.ptr,
        old = mmapped.ptr,
    )
}

fn copy_inline_asm(self: *Jitted, f: FuncId, insts: []u32) void = {
    self.mark_start(f);
    for insts { op |
        self.push(op);
    };
    self.save_current(f);
}

// This is a better marker for not compiled yet.
// Depending on your mood this could be 0x1337 or whatever so you can recognise when you hit it in a debugger.
// TODO: generate shims so we know the function id
fn uncompiled_function_placeholder() rawptr = {
    addr: rawptr = fn(a: i64, b: i64, c: i64) void = {
        @panic("ICE: Tried to call un-compiled function. (x0=%, x1=%, x2=%)", a, b, c);
    };
    addr
}

fn get_dispatch(self: *Jitted) i64 = {
    rawptr.int_from_ptr(self.dispatch.maybe_uninit.ptr)
}

fn get_fn(self: *Jitted, f: FuncId) ?rawptr = {
    if(f.as_index() >= self.dispatch.len, => return(.None));
    addr := self.dispatch[f.as_index()];
    if(addr == uncompiled_function_placeholder(), => return(.None));
    (Some = addr)
}

fn offset_words(self: *Jitted, from_ip: *u32, to_ip: *u32) i64 = {
    from_ip.ptr_diff(to_ip)
}

fn prev(self: *Jitted) *u32 = {
    self.next.offset(-1)
}

fn patch(self: *Jitted, ip: *u32, inst_value: u32) void = {
    @debug_assert_eq(ip[], brk(0), "unexpected patch");
    ip[] = inst_value;
}

::ptr_utils(u32);

fn push(self: *Jitted, inst: u32) void = {
    //println(inst);
    //@debug_assert((self.next as usize) < self.high, "OOB {} {}", self.next as usize, self.high);
    self.next[] = inst;
    self.next = self.next.offset(1);
}

// Recursion shouldn't have to slowly lookup the start address.
fn mark_start(self: *Jitted, f: FuncId) void = {
    addr := u32.int_from_ptr(self.current_start);
    self.extend_blanks(f);
    self.dispatch[f.as_index()] = addr.rawptr_from_int();
}

fn extend_blanks(self: *Jitted, f: FuncId) void = {
    // TODO: call reserve :SLOW
    while => self.dispatch.len() < f.to_index().zext() + 1 {|
        self.dispatch&.push(uncompiled_function_placeholder());
    };
}

fn as_index(f: FuncId) i64 = f.to_index().zext();

// TODO: if you don't have this it just calls the FuncId for you because it decides same raw type is good enough :FUCKED -- Jul 20
fn as_index(f: Type) i64 = f.to_index().zext();

fn save_current(self: *Jitted, f: FuncId) []u32 = {
    //@debug_assert_ne!(self.next as usize, self.old as usize);
    //@debug_assert_ne!(self.next as usize, self.current_start as usize);
    self.extend_blanks(f);
    func_start := self.current_start;
    addr := u32.raw_from_ptr(self.current_start);
    self.dispatch[f.as_index()] = addr;  // TODO: i do this in mark_start too so this seems redundant
    start := u32.int_from_ptr(self.current_start);
    end := u32.int_from_ptr(self.next);
    //@debug_assert_eq(self.current_start as usize % 4, 0);
    // just a marker to make sure someone doesn't fall off the end of the function by forgetting to return.
    // also you can see it in the debugger disassembly so you can tell if you emitted a trash instruction inside a function or you're off the end in uninit memory. -- Apr 25
    self.push(brk(0xDAED));
    self.current_start = self.next;
    // TODO: have a slice_from(start, to = end);
    (ptr = func_start, len = (end - start) / 4)
}

fn bump_dirty(self: *Jitted) void = {
    beg := u32.raw_from_ptr(self.old);
    end := u32.raw_from_ptr(self.next);
    if beg != end {|
        //@debug_assert_eq(self.next as usize, self.current_start as usize);
        self.push(brk(0x3141));
        len := end.int_from_rawptr() - beg.int_from_rawptr();
        
        prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
        res := mprotect(beg, len, prot);
        assert(res.value.eq(0), "mprotect failed");
        
        // TODO: is this wrong now? if you were at the very end of a page you would have written the brk to the next one,
        //      but i guess thats find cause we rewrite anyway when we put instructions there. 
        page_start := end.int_from_rawptr() / PAGE_SIZE * PAGE_SIZE;
        self.next = u32.ptr_from_int(page_start + PAGE_SIZE);
        self.old = self.next;
        self.current_start = self.next;
        
        //
        // x86 doesn't this. TODO: what about riscv?
        // TODO: #[cfg(target_arch = "aarch64")]
        //
        // This fixes 'illegal hardware instruction'.
        // sleep(Duration::from_millis(1)) also works (in debug mode). That's really cool, it gives it time to update the other cache because instructions and data are seperate?!
        // Especially fun becuase if you run it in lldb so you break on the error and disassemble... you get perfectly valid instructions because it reads the data cache!
        // https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/caches-and-self-modifying-code
        // https://stackoverflow.com/questions/35741814/how-does-builtin-clear-cache-work
        // https://stackoverflow.com/questions/35741814/how-does-builtin-clear-cache-work
        // https://stackoverflow.com/questions/10522043/arm-clear-cache-equivalent-for-ios-devices
        // https://github.com/llvm/llvm-project/blob/main/compiler-rt/lib/builtins/clear_cache.c
        // https://github.com/apple/darwin-libplatform/blob/main/src/cachecontrol/arm64/cache.s
        // https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/sys_icache_invalidate.3.htmls
        //
        __clear_cache(beg, end);
    };
}

//
// TODO: this is very sad.
//       since core functions like `trunc` are written with inline assembly and not precompiled, 
//       the normal versions of encoding some instructions need to use the compiler's @BITS since
//       they can't compile the library @bits yet. so the functions below are the same as the versions in arch/aarch64instructions.fr,
//       except for which macros they call.   -- Jul 16
//

/// Zero the other bits.
fn movz_rt(sf: Bits, dest: RegO, imm: u16, hw: Hw) u32 =
    @bits(sf, 0b10100101, hw, imm, dest); // TOOD: this broke on @bits when u16 became a real type. 

/// Keep the other bits.
fn movk_rt(sf: Bits, dest: RegO, imm: u16, hw: Hw) u32 =
    @bits(sf, 0b11100101, hw, imm, dest);

fn str_uo_any(size: u2, src: RegI, addr: RegI, offset_words: u12) u32 = 
    @bits(size, 0b11100100, offset_words, addr, src);

fn ldr_uo_any(size: u2, dest: RegO, addr: RegI, offset_words: u12) u32 = 
    @bits(size, 0b11100101, offset_words, addr, dest);

// It's not symetrical or perfect but its beautiful and its mine
