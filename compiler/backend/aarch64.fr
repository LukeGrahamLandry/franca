//! I know this is remarkably bad codegen. 
//! Its only job is to be fast to compile because its used for throw away comptime functions that are often only run once.
//! #c_call means https://en.wikipedia.org/wiki/Calling_convention#ARM_(A64)
//
// TODO: it would be nice to use walk_bc.fr for this to avoid some boring stack stuff, 
//       but that's geared towards ssa-like irs where you're allowed to freely copy around ssa vars. 
//       im not sure if its worth reworking that to give you hooks to shuffle between registers and stack slots as needed.  
//          -- Jul 15

ZERO_DROPPED_REG :: false;
ZERO_DROPPED_SLOTS :: false;
TRACE_ASM :: false;

// I'm using u16 everywhere cause why not, extra debug mode check might catch putting a stupid big number in there. that's 65k bytes, 8k words, the uo instructions can only do 4k words.
SpOffset :: @struct(id: i64);

IncReg :: @struct(reg: i64, offset_bytes: i64);
AsmVal :: @tagged(
    Increment: IncReg, // not a pointer derefernce, just adding a static number to the value in the register.
    Literal: i64,
    Spill: SpOffset,
    FloatReg: i64,
);
::tagged(AsmVal);

/*
impl Debug for AsmVal {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            AsmVal::Increment { reg, offset_bytes } => write!(f, "x{reg} + {offset_bytes}"),
            AsmVal::Literal(x) => write!(f, "{x}"),
            AsmVal::Spill(slot) => write!(f, "[sp, {}]", slot.id),
            AsmVal::FloatReg(x) => write!(f, "v{x}"),
        }
    }
}
*/

MEM_64: i64 : 0b11;
MEM_32: i64 : 0b10;
MEM_16: i64 : 0b01;
MEM_08: i64 : 0b00;

fn emit_aarch64(program: CompilerRs, f: FuncId, when: ExecStyle, body: *FnBody) void #compiler = {
    // TODO: this should be true but breaks for aot with new baked constant trait. maybe its counting random callees when just done for aot but not actually asm?  -- Jun 19
    // debug_assert!(!program[f].get_flag(FnFlag::AsmDone), "ICE: tried to double compile?");
    program.aarch64&.extend_blanks(f);
    mark := __temp_alloc.mark();
    a: EmitAsm = new(program, when, body, f);
    a&.compile(f);
    __temp_alloc.reset_retaining_capacity(mark);
}

EmitAsm :: @struct(
    program: CompilerRs,
    asm: *Jitted,
    vars: List(?SpOffset),
    next_slot: SpOffset,
    f: FuncId,
    when: ExecStyle,
    patch_cbz: List(Ty(*u32, BbId, i64)),
    patch_b: List(Ty(*u32, BbId)),
    release_stack: List(Ty(*u32, *u32, *u32)),
    state: BlockState,
    block_ips: List(?*u32),
    clock: u16,
    markers: List(Ty(List(u8), usize)),
    log_asm_bc: bool,
    body: *FnBody,
);

SpSlot :: @struct(at: SpOffset, size: u16, clock: u16);
BlockState :: @struct(
    stack: List(AsmVal),
    free_reg: List(i64),
    open_slots: List(SpSlot),
);

fn clone(self: *BlockState) BlockState = {
    (stack = self.stack&.clone(), free_reg = self.free_reg&.clone(), open_slots = self.open_slots&.clone())
}


fn new(compile: CompilerRs, when: ExecStyle, body: *FnBody, f: FuncId) EmitAsm = {
    (
        program = compile,
        asm = compile.aarch64&,
        vars = list(temp()),
        next_slot = (id = 0),
        f = f,
        when = when,
        patch_cbz = list(temp()), // (patch_idx, jump_idx, register)
        patch_b = list(temp()),
        release_stack = list(temp()),
        state = (stack = list(temp()), free_reg = list(temp()), open_slots = list(temp())),
        block_ips = list(temp()),
        clock = 0,
        markers = list(temp()),
        log_asm_bc = false,
        body = body,
    )
}

// Note: you can only use Self once because this doesn't reset fields.
fn compile(self: *EmitAsm, f: FuncId) void = {
    if(self.asm.get_fn(f).is_some(), => return());

    func := self.program[f]&;
    @assert(!func.body&.is(.JittedAarch64) && !func.body&.is(.ComptimeAddr));

    self.log_asm_bc = func.has_tag(.log_asm_bc);
    self.bc_to_asm(f);
    self.asm.save_current(f);
    asm := self.asm.get_fn(f).unwrap();

    /* // TODO: bring this back
    if TRACE_ASM.or(self.log_asm_bc).or(=> func.has_tag(.log_asm)) {
        asm := unsafe { &*self.asm.ranges[f.as_index()] };
        let hex: String = asm
            .iter()
            .copied()
            .array_chunks::<4>()
            .map(|b| format!("{:#02x} {:#02x} {:#02x} {:#02x} ", b[0], b[1], b[2], b[3]))
            .collect();
        path := "target/temp.asm".to_string();
        fs::write(&path, hex).unwrap();
        dis := String::from_utf8(Command::new("llvm-mc").arg("--disassemble").arg(&path).output().unwrap().stdout).unwrap();
        println!();
        println!("=== Asm for {f:?}: {} ===", self.program.pool.get(func.name));

        it := dis.split('\n');
        it.nth(1);
        for (i, line) in it.enumerate() {
            for (s, offset) in &self.markers {
                if *offset == i {
                    println!("{s}");
                }
            }
            println!("{line}");
        }
        println!("===")
    }
    */
}

fn bc_to_asm(self: *EmitAsm, f: FuncId) void = {
    ff := self.program[f]&;
    //debugln!("=== {f:?} {} ===", self.program.pool.get(self.program[f].name));
    //debugln!("{}", self.program[f].body.log(self.program.pool));

    func := self.body;
    self.f = f;
    self.next_slot = (id = 0);
    self.vars = .None.repeated(func.vars.len, temp());
    self.block_ips = .None.repeated(self.body.blocks.len, temp());

    self.asm.mark_start(f);
    self.asm.push(sub_im(Bits.X64, sp, sp, 16, 0));
    self.asm.push(stp_so(Bits.X64, fp, lr, sp, 0)); // save our return address
    self.asm.push(add_im(Bits.X64, fp, sp, 0, 0)); // Note: normal mov encoding can't use sp
    self.asm.push(brk(0));
    reserve_stack := self.asm.prev();

    // The code expects arguments on the virtual stack (the first thing it does might be save them to variables but that's not my problem).

    sig := self.body.signeture;
    int_count: i64 = sig.arg_int_count.zext();

    if sig.first_arg_is_indirect_return {|
        self.state.stack&.push(Increment = (reg = 8, offset_bytes = 0));
    };

    // TODO: assert int_count <= 8 && float_count <= 8 or impl pass on stack.
    self.ccall_reg_to_stack(sig.args);

    // Any registers not containing args can be used.
    range(int_count, 8) { i |
        self.state.free_reg&.push(i);
    };
    // debug_assert_eq!(self.state.stack.len() as u16, sig.arg_slots);

    //debugln!("entry: ({:?})", self.state.stack);
    self.emit_block(0, false);

    // TODO: be able to destructure in the lambda args. -- Jul 15
    for self.patch_cbz { it |
        (inst, false_ip, reg) := it;
        false_ip := self.block_ips[false_ip.id.zext()].unwrap();
        offset := self.asm.offset_words(inst, false_ip);
        @debug_assert(reg < 32);
        @debug_assert_ne(offset, 0, "!if ice: while(1);");
        self.asm.patch(inst, cbz(Bits.X64, offset, reg));
    };
    for self.patch_b { it |
        (from_inst, to_ip) := it;
        to_ip := self.block_ips[to_ip.id.zext()].unwrap();
        dist := self.asm.offset_words(from_inst, to_ip);
        @debug_assert_ne(dist, 0, "while(1);");
        self.asm.patch(from_inst, b(dist, 0));
    };

    slots := self.next_slot.id;
    @assert(
        slots < 4096,
        "not enough bits to refer to all slots (%/4096 bytes) in %",
        slots,
        self.program.pool.get(self.program[f].name)
    );
    extra := slots.mod(16);
    if extra != 0 {|
        slots += 16 - extra; // play by the rules
    };
    self.asm.patch(reserve_stack, sub_im(Bits.X64, sp, sp, @as(u12) slots, 0));
    for self.release_stack { it |
        (fst, snd, thd) := it; // TODO: these are sequential so don't bother storing them all. 
        self.asm.patch(fst, add_im(Bits.X64, sp, fp, 0, 0)); // Note: normal mov encoding can't use sp
        self.asm.patch(snd, ldp_so(Bits.X64, fp, lr, sp, 0)); // get our return address
        self.asm.patch(thd, add_im(Bits.X64, sp, sp, 16, 0));
    };
}

fn emit_block(self: *EmitAsm, b: i64, args_not_vstacked: bool) void = {
    if(self.block_ips[b].is_some(), => return());
    self.block_ips[b] = (Some = self.asm.next);
    f := self.f;
    block := self.body.blocks[b]&;

    self.clock = block.clock;
    // TODO: handle normal args here as well.
    if args_not_vstacked {|
        self.state.free_reg&.clear();
        int_count := self.ccall_reg_to_stack(block.arg_prims);
        range(int_count, 8) { i |
            self.drop_reg(i);
        };
    };
    @debug_assert(self.state.stack.len >= block.arg_prims.len);
    func := self.body;
    is_done := false;
    range(0, func.blocks[b].insts.len) { i |
        if(is_done, => return());
        block := self.body.blocks[b]&;
        inst := block.insts[i];
        is_done = self.emit_inst(b, inst, i);
        //debugln!("{b}:{i} {inst:?} => {:?} | free: {:?}", self.state.stack, self.state.free_reg);
    };
    @debug_assert(is_done);
}

fn emit_inst(self: *EmitAsm, b: usize, inst: Bc, i: usize) bool = {
    // TODO: bring this back 
    //if TRACE_ASM.or(self.log_asm_bc) {|
    //    ins := self.asm.offset_words(self.asm.current_start, self.asm.next) - 1;
    //    self.markers.push((format!("[{b}:{i}] {inst:?}: {:?}", self.state.stack), ins as usize));
    //};
    @match(inst) {
        fn Nop() => ();
        fn SaveSsa(f) => {
            @assert(self.vars[f.id.zext()].is_none());
            slot := self.create_slots(1);
            reg := self.pop_to_reg();
            self.store_u64(reg, sp, slot.id.trunc());
            self.drop_reg(reg);
            self.vars[f.id.zext()] = (Some = slot);
        }
        fn LoadSsa(f) => {
            // debug_assert!(self.body.is_ssa_var.get(id as usize));
            slot := self.vars[f.id.zext()].unwrap();
            self.state.stack&.push(Spill = slot);
        }
        fn AddrVar(f) => {
            idx: i64 = f.id.zext();
            if self.vars[idx] { (slot: SpOffset) |
                self.state.stack&.push(Increment = (
                    reg = sp,
                    offset_bytes = slot.id,
                ));
            } else {|
                ty := self.body.vars[idx];
                count := self.program.slot_count(ty);
                slot := self.create_slots(count);
                self.vars[idx] = (Some = slot);
                self.state.stack&.push(Increment = (
                    reg = sp,
                    offset_bytes = slot.id,
                ));
            };
        }
        fn GetCompCtx() => {
            addr := (**SelfHosted).int_from_ptr(self.program);
            self.state.stack&.push(Literal = addr);
        }
        fn LastUse(f) => {
            slot := self.vars[f.id.zext()]; 
            if slot { slot |
                ty := self.body.vars[f.id.zext()];
                count := self.program.slot_count(ty);
                self.drop_slot(slot, count * 8);
            };
        }
        fn NoCompile() => {
            @panic("tried to compile unreachable block in %", self.program[self.f]&.log(self.program.pool));
        }

        fn PushConstant(f) => self.state.stack&.push(Literal = f.value);
        fn PushGlobalAddr(id) => {
            (ptr, _) := self.program[][].get_baked(id)[];
            self.state.stack&.push(Literal = ptr.int_from_rawptr());
        }
        fn GetNativeFnPtr(fid) => {
            // TODO: use adr+adrp instead of an integer. but should do that in load_imm so it always happens.

            if self.asm.get_fn(fid) { (ptr: rawptr) |
                self.state.stack&.push(Literal = ptr.int_from_rawptr());
            } else {|
                @assert(fid.as_index() < 4096);
                reg := self.get_free_reg();
                self.asm.extend_blanks(fid);
                self.load_imm(reg, self.asm.get_dispatch()); // NOTE: this means you can't ever resize
                self.asm.push(ldr_uo(Bits.X64, reg, reg, @as(u12) fid.as_index()));
                self.state.stack&.push(Increment = (reg = reg, offset_bytes = 0));
            };
        }
        fn JumpIf(f) => {
            cond := self.pop_to_reg();
            @debug_assert_eq(f.slots, 0); // emit_bc doesn't do this
            self.spill_abi_stompable();
            self.asm.push(brk(0));

            // branch if zero so true before false
            self.patch_cbz&.push((_0 = self.asm.prev(), _1 = f.false_ip, _2 = cond)); // TODO: it sees an inline tuple as multiple args.
            // we only do one branch so true block must be directly after the check.
            // this is the only shape of flow graph that its possible to generate with my ifs/whiles.
            @debug_assert(self.block_ips[f.true_ip.id.zext()].is_none());

            self.drop_reg(cond);
            state := self.state&.clone();
            self.emit_block(f.true_ip.id.zext(), true);
            self.state = state;
            self.emit_block(f.false_ip.id.zext(), true);

            return(true);
        }
        fn Goto(f) => {
            idx: i64 = f.ip.id.zext();
            block := self.body.blocks[idx]&;
            
            // TODO: fix ending with unreachable() :block_arg_count_wrong_unreachable
            //debug_assert_eq!(slots, block.arg_slots, "goto {ip:?} {}", self.body.log(self.program));
            if block.incoming_jumps == 1 {|
                @debug_assert(self.block_ips[idx].is_none());
                self.emit_block(idx, false);
            } else {|
                self.stack_to_ccall_reg(block.arg_prims);
                self.spill_abi_stompable();
                if self.block_ips[idx] { (_: *u32) |
                    // TODO: we already emitted it, why not just jump there now?
                    self.asm.push(brk(0));
                    t := (self.asm.prev(), f.ip); // TODO: seen as two args if done inline.
                    self.patch_b&.push(t);
                } else {|
                    // If we haven't emitted it yet, it will be right after us, so just fall through.
                    self.emit_block(idx, true);
                };
            };
            return(true);
        }
        fn CallDirect(f) => {
            // TODO: use with_link for tail calls. need to "Leave holes for stack fixup code." like below
            // TODO: if you already know the stack height of the callee, you could just fixup to that and jump in past the setup code. but lets start simple.
            self.dyn_c_call(f.sig) {|
                if(f.tail, => self.emit_stack_fixup());
                self.branch_func(f.f, !f.tail)
            };
            return(f.tail);
        }
        fn Ret0() => {
            return(self.emit_return(empty()));
        }
        fn Ret1(prim) => {
            return(self.emit_return(@slice(prim)));
        }
        fn Ret2(f) => {
            return(self.emit_return(@slice(f._0, f._1)));
        }
        // Note: we do this statically, it wont get actually applied to a register until its needed because loads/stores have an offset immediate.
        fn IncPtrBytes(bytes) => {
            @match(self.state.stack&.last().unwrap()) {
                fn Increment(f) => {
                    f.offset_bytes += bytes.zext();
                }
                fn Literal(v) => {
                    v[] += bytes.zext();
                }
                fn Spill(_) => {
                    // TODO: should it hold the add statically? rn it does the load but doesn't need to restore because it just unspills it.
                    (reg, offset_bytes) := self.pop_to_reg_with_offset();
                    self.state.stack&.push(Increment = (
                        reg = reg,
                        offset_bytes = offset_bytes.zext() + bytes.zext(),
                    ));
                }
                fn FloatReg(_) => {
                    panic("ICE: don't try to gep a float");
                }
            };
        }
        fn Load(ty) => {
            (addr, offset_bytes) := self.pop_to_reg_with_offset(); // get the ptr

            dest := self.get_free_reg();
            @match(ty) {
                // TODO: track open float registers so this can just switch over MEM type.
                fn I32() => self.load_one(MEM_32, dest, addr, offset_bytes);
                fn F32() => self.load_one(MEM_32, dest, addr, offset_bytes);
                fn I16() => self.load_one(MEM_16, dest, addr, offset_bytes);
                fn I8()  => self.load_one(MEM_08, dest, addr, offset_bytes);
                @default => self.load_u64(dest, addr, offset_bytes);
            };
            self.state.stack&.push(Increment = (reg = dest, offset_bytes = 0));
            self.drop_reg(addr);
        }
        fn StorePost(ty) => {
            addr := self.state.stack&.pop().expect("enough stack for store");
            value := self.state.stack&.pop().expect("enough stack for store");
            self.emit_store(addr, value, ty);
        }
        fn StorePre(ty) => {
            value := self.state.stack&.pop().expect("enough stack for store");
            addr := self.state.stack&.pop().expect("enough stack for store");
            self.emit_store(addr, value, ty);
        }
        fn CallFnPtr(f) => {
            // TODO: tail call
            self.dyn_c_call(f.sig) {|
                // dyn_c_call will have popped the args, so now stack is just the pointer to call
                callee := self.state.stack&.pop().expect("enough stack for call");
                // TODO: this does redundant spilling every time!
                @match(callee) {
                    fn Literal(p) => self.load_imm(x16, p);
                    fn Spill(offset) => self.load_u64(x16, sp, offset.id.trunc());
                    @default => {
                        panic("ICE: unspilled register");
                    };
                };
                self.asm.push(br(x16, 1));
            };
        }
        fn Unreachable() => {
            self.asm.push(brk(0xbabe));
            return(true);
        }
        fn Snipe(skip) => {
            index := self.state.stack.len - skip.zext() - 1;
            @match(self.state.stack&.ordered_remove(index).unwrap()) {
                fn Increment(f) => self.drop_reg(f.reg);
                fn Literal(_)   => ();
                fn Spill(_)     => ();
                fn FloatReg(_)  => {
                    panic("unexpected float reg");
                }
            };
        }
        fn PeekDup(skip) => {
            val := self.state.stack[self.state.stack.len - skip.zext() - 1];
            @match(val) {
                fn Increment(f) => {
                    if f.reg == sp {|
                        self.state.stack&.push(val);
                    } else {|
                        new := self.get_free_reg();
                        self.asm.push(mov(Bits.X64, new, f.reg));
                        self.state.stack&.push(Increment = (reg = new, offset_bytes = f.offset_bytes));
                    };
                }
                fn Literal(_)   => self.state.stack&.push(val);
                fn Spill(_)     => self.state.stack&.push(val);
                fn FloatReg(_)  => {
                    panic("unexpected float reg");
                }
            };
        }
        fn CopyBytesToFrom(bytes) => {
            self.state.stack&.push(Literal = bytes.zext());
            self.stack_to_ccall_reg(@slice(Prim.P64, Prim.P64, Prim.I64));
            self.spill_abi_stompable();
            // TODO: fancy version of this?
            addr: rawptr : fn(dest: *u8, src: *u8, len: i64) void = {
                range(0, len) { i |
                    dest.offset(i)[] = src.offset(i)[];
                };
            };
            // offset := self.asm.offset_words(self.asm.next, addr);
            self.load_imm(x17, addr.int_from_rawptr());
            self.asm.push(br(x17, 1));
            range(0, 8) { i |
                self.state.free_reg&.add_unique(i);
            };
        }
    };

    false
}

::display_slice(AsmVal);
::DeriveFmt(AsmVal);
::DeriveFmt(IncReg);
::DeriveFmt(SpOffset);

// TODO: refactor this. its a problem that im afraid of it! -- May 8
fn stack_to_ccall_reg(self: *EmitAsm, types: [] Prim) void = {
    ::display_slice(Prim);
    @debug_assert(
        self.state.stack.len >= types.len,
        "found % things but wanted %",
        self.state.stack.len,
        types
    );
    next_int := 0;
    next_float := 0;

    enumerate types { slot_index, ty |
        continue :: local_return;
        @debug_assert_eq(slot_index, next_int + next_float);
        f := ty[].is_float();
        stack_index := self.state.stack.len - (types.len) + slot_index;

        if f {|
            @if_let(self.state.stack[stack_index]) fn FloatReg(have) => {
                if have == next_float {|
                    // its already where we want it.
                    self.state.stack[stack_index] = (Literal = 0); // just make sure we dont see it again later.
                    next_float += 1;
                    continue();
                };
            };

            // debug!("|(v{}) <- ({:?})| ", next_float, self.state.stack[stack_index]);

            range(0, self.state.stack.len) { i |
                @if_let(self.state.stack[i]) fn FloatReg(x) => {
                    if x == next_float {|
                        // Someone else already has the one we want, so spill that to the stack since i don't have fmov encoding. TODO
                        worked := self.try_spill(i, true);
                        @assert(worked);
                    };
                };
            };

            @match(self.state.stack[stack_index]) {
                fn Increment(f) => {
                    @debug_assert_eq(f.offset_bytes, 0, "dont GEP a float");
                    @debug_assert_ne(f.reg, sp, "dont fmov the stack pointer");
                    // TODO: fmov encoding.
                    worked := self.try_spill(stack_index, false);
                    @assert(worked);
                }
                fn Literal(x) => {
                    // TODO: fmov encoding.
                    reg := self.get_free_reg();
                    self.load_imm(reg, x);
                    self.state.stack[stack_index] = (Increment = (reg = reg, offset_bytes = 0));
                    worked := self.try_spill(stack_index, false);
                    @assert(worked);
                }
                fn Spill(_)    => ();
                fn FloatReg(_) => {
                    // TODO: fmov encoding.
                    worked := self.try_spill(stack_index, true);
                    @assert(worked);
                }
            };

            slot := self.state.stack[stack_index];
            @assert(slot&.is(.Spill));
            slot := slot.Spill;
            self.asm.push(f_ldr_uo(Bits.X64, next_float, sp, @as(u12) @as(i64) slot.id / 8));
            // self.drop_slot(slot, 8); // TODO!

            next_float += 1;
            continue();
        };
        
        @if_let(self.state.stack[stack_index]) fn Increment(f) => {
            if f.reg == next_int {|
                if f.offset_bytes != 0 {|
                    self.asm.push(add_im(Bits.X64, f.reg, f.reg, f.offset_bytes, 0));
                };

                // if we happen to already be in the right register, cool, don't worry about it.
                //debug!("|x{} already| ", f.reg);
                self.state.free_reg&.ordered_retain(fn(r: *i64) => f.reg != r[]); // TODO: does unordered_retain work?
                self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
                next_int += 1;
                continue();
            };
        };

        found := false;
        if self.state.free_reg&.position(fn(r) => next_int == r[]) { want |
            found := true;
            self.state.free_reg&.ordered_remove(want); // TODO: does unordered_remove work here?
        };
        
        if !found {|
            break :: local_return;
            each self.state.stack { r | 
                @if_let(r) fn Increment(f) => {
                    if f.reg == next_int {|
                        found = true;
                        // The one we want is already used by something on the v-stack; swap it with a random free one. TODO: put in the right place if you can.
                        @debug_assert_ne(f.reg, sp); // encoding but unreachable
                        reg := self.get_free_reg();
                        self.asm.push(mov(Bits.X64, reg, f.reg));
                        r[] = (Increment = (reg = reg, offset_bytes = f.offset_bytes));
                        // Now x{i} is free and we'll use it below.
                        break();
                    };
                };
            };
        };
        @assert(found, "TODO: x% is not free. stack is %. free is %",
                next_int, self.state.stack.items(), self.state.free_reg.items());

        //debug!("|(x{}) <- ({:?})| ", next_int, self.state.stack[stack_index]);
        @match(self.state.stack[stack_index]) {
            fn Increment(f) => {
                @debug_assert_ne(next_int, f.reg);
                // even if offset_bytes is 0, we need to move it to the right register. and add can encode that mov even if reg is sp.
                self.asm.push(add_im(Bits.X64, next_int, f.reg, f.offset_bytes, 0));
                self.drop_reg(f.reg);
            }
            fn Literal(x) => self.load_imm(next_int, x);
            fn Spill(slot) => self.load_u64(next_int, sp, slot.id.trunc());
            fn FloatReg(_) => todo();
        };
        self.state.free_reg&.ordered_retain(fn(r) => next_int != r[]); // TODO: does unordered_retain work here? 
        self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
        next_int += 1;
    };
    self.state.stack.len -= types.len;
}

::if(AsmVal);
fn ccall_reg_to_stack(self: *EmitAsm, types: [] Prim) i64 = {
    next_float := 0;
    next_int := 0;
    enumerate types { i, ty |
        @debug_assert_eq(i, next_int + next_float);
        f := ty[].is_float();
        v: AsmVal = if(f, => (FloatReg = next_float)) {|
            self.state.free_reg&.ordered_retain(fn(r) => r[] != next_int); // TODO: does unordered_retain work?
            (Increment = (
                reg = next_int,
                offset_bytes = 0,
            ))
        };
        if f {|
            next_float += 1;
        } else {|
            next_int += 1;
        };
        self.state.stack&.push(v);
    };
    next_int
}

fn emit_store(self: *EmitAsm, addr: AsmVal, value: AsmVal, ty: Prim) void = {
    (reg, offset_bytes) := self.in_reg_with_offset(addr);
    p64 :: fn() => {
        if value&.is(.FloatReg) {|
            r := value.FloatReg;
            @assert_eq(offset_bytes.zext().mod(8), 0, "TODO: align");
            self.asm.push(f_str_uo(Bits.X64, r, reg, @as(u12) @as(i64) (offset_bytes.zext() / 8)))
        } else {|
            val := self.in_reg(value);
            self.store_u64(val, reg, offset_bytes);
            self.drop_reg(val);
        };
    };
    @match(ty) {
        // TODO: track open float registers
        fn P64() => p64();
        fn F64() => p64();
        fn I64() => p64();
        fn F32() => {
            if value&.is(.FloatReg) {|
                r := value.FloatReg;
                @assert_eq(offset_bytes.zext().mod(4), 0, "TODO: align");
                self.asm.push(f_str_uo(Bits.W32, r, reg, @as(u12) @as(i64) (offset_bytes.zext() / 4)))
            } else {|
                val := self.in_reg(value);
                self.store_one(MEM_32, val, reg, offset_bytes);
                self.drop_reg(val);
            };
        }
        fn I32() => {
            val := self.in_reg(value);
            self.store_one(MEM_32, val, reg, offset_bytes);
            self.drop_reg(val);
        }
        fn I16() => {
            val := self.in_reg(value);
            self.store_one(MEM_16, val, reg, offset_bytes);
            self.drop_reg(val);
        }
        fn I8() => {
            val := self.in_reg(value);
            self.store_one(MEM_08, val, reg, offset_bytes);
            self.drop_reg(val);
        }
    };
    self.drop_reg(reg);
}

fn load_u64(self: *EmitAsm, dest_reg: i64, src_addr_reg: i64, offset_bytes: u16) void = {
    self.load_one(MEM_64, dest_reg, src_addr_reg, offset_bytes)
}

fn load_one(self: *EmitAsm, register_type: i64, dest_reg: i64, src_addr_reg: i64, offset_bytes: u16) void = {
    @debug_assert_ne(dest_reg, sp);
    scale := 1.shift_left(register_type);
    if offset_bytes.zext().mod(scale) == 0 {|
        self.asm.push(ldr_uo(register_type, dest_reg, src_addr_reg, @as(u12) @as(i64) (offset_bytes.zext() / scale)));
        @debug_assert((@as(i64) offset_bytes.zext()) / scale < 1.shift_left(12));
    } else {|
        // Note: this relies on the access actually being aligned once you combine the value in the register without our non %8 offset.
        reg := self.get_free_reg();
        @debug_assert((@as(i64) offset_bytes.zext()) < 1.shift_left(12));
        self.asm.push(add_im(Bits.X64, reg, src_addr_reg, @as(u12) @as(i64) offset_bytes.zext(), 0));
        self.asm.push(ldr_uo(register_type, dest_reg, reg, 0));
        self.drop_reg(reg);
    }
}

fn store_u64(self: *EmitAsm, src_reg: i64, dest_addr_reg: i64, offset_bytes: u16) void = {
    self.store_one(MEM_64, src_reg, dest_addr_reg, offset_bytes)
}

fn store_one(self: *EmitAsm, register_type: i64, src_reg: i64, dest_addr_reg: i64, offset_bytes: u16) void = {
    scale := 1.shift_left(register_type);
    if src_reg == sp {|
        // TODO: this is weird. can only happen for exactly the sp, not an offset from it. so i guess somewhere else is saving an add, maybe its fine. -- May 2
        reg := self.get_free_reg();
        self.asm.push(add_im(register_type, reg, sp, 0, 0)); // not mov!
        self.store_one(register_type, reg, dest_addr_reg, offset_bytes);
        self.drop_reg(reg);
        return();
    };
    if offset_bytes.zext().mod(8) == 0 {|
        self.asm.push(str_uo(register_type, src_reg, dest_addr_reg, @as(u12) @as(i64) (offset_bytes.zext() / scale)));
        @debug_assert((@as(i64) offset_bytes.zext()) / scale < 1.shift_left(12));
    } else {|
        reg := self.get_free_reg();
        self.asm.push(add_im(Bits.X64, reg, dest_addr_reg, @as(u12) @as(i64) offset_bytes.zext(), 0));
        self.asm.push(str_uo(register_type, src_reg, reg, 0));
        self.drop_reg(reg);
    };
}

fn reset_free_reg(self: *EmitAsm) void = {
    self.state.free_reg&.clear();
    self.state.free_reg&.push_all(@slice(0, 1, 2, 3, 4, 5, 6, 7));
}

fn spill_abi_stompable(self: *EmitAsm) void = {
    range(0, self.state.stack.len) { i |
        self.try_spill(i, true);
    };
    //debugln!();
}

// do_floats:false if you're just trying to free up a gpr, not saving for a call.
fn try_spill(self: *EmitAsm, i: usize, do_floats: bool) bool = {
    v := self.state.stack[i];
    @if_let(v) fn Increment(f) => {
        if(f.reg == sp, => return(false));

        // Note: this assumes we don't need to preserve the value in the reg other than for this one v-stack slot.
        slot := self.create_slots(1);
        //debug!("(spill (x% + %) -> [sp, %]) ", f.reg, f.offset_bytes, slot.id);
        if f.offset_bytes != 0 {|
            self.asm.push(add_im(Bits.X64, f.reg, f.reg, f.offset_bytes, 0));
        };

        self.store_u64(f.reg, sp, slot.id.trunc());
        self.drop_reg(f.reg);
        self.state.stack[i] = (Spill = slot);
        return(true);
    };

    if do_floats {|
        @if_let(v) fn FloatReg(freg) => {
            slot := self.create_slots(1);
            self.asm.push(f_str_uo(Bits.X64, freg, sp, @as(u12) @as(i64) slot.id / 8));
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    false
}

fn get_free_reg(self: *EmitAsm) i64 = {
    if self.state.free_reg&.pop() { r |
        @debug_assert_ne(r, sp);
        r
    } else {|
        i := 0;
        while => i < self.state.stack.len && !self.try_spill(i, false) {|
            i += 1;
        };
        if i == self.state.stack.len {|
            panic("ICE: spill to stack failed");
        };
        self.state.free_reg&.pop().unwrap()
    }
}

::display_slice(i64);

fn drop_reg(self: *EmitAsm, reg: i64) void = {
    if reg != sp && reg != x8 {|
        @debug_assert(!self.state.free_reg&.contains(reg&), "drop r% -> %", reg, self.state.free_reg.items());
        self.state.free_reg&.push(reg);
        if ZERO_DROPPED_REG {|
            self.load_imm(reg, 0);
        };
    };
}

fn pop_to_reg(self: *EmitAsm) i64 = {
    val := self.state.stack&.pop().unwrap();
    self.in_reg(val)
}

fn in_reg(self: *EmitAsm, val: AsmVal) i64 = {
    @match(val) {
        fn Increment(f) => {
            if f.offset_bytes > 0 {|
                out := if(f.reg == sp, => self.get_free_reg(), => f.reg);
                if f.offset_bytes < 1.shift_left(12) {|
                    self.asm.push(add_im(Bits.X64, out, f.reg, f.offset_bytes, 0));
                } else {|
                    self.asm.push(add_im(Bits.X64, out, f.reg, f.offset_bytes.shift_right_logical(12), 1));
                    mask := 1.shift_left(12) - 1;
                    self.asm.push(add_im(Bits.X64, out, f.reg, @as(u12) @as(i64) f.offset_bytes.bit_and(mask), 0));
                };
                f.reg = out;
            };
            f.reg
        }
        fn Literal(x) => {
            r := self.get_free_reg();
            self.load_imm(r, x);
            r
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, sp, slot.id.trunc());
            // self.drop_slot(slot, 8); TODO!
            r
        }
        fn FloatReg(_) => panic("ICE: shouldn't need to move from float register.");
    }
}

fn peek_to_reg_with_offset(self: *EmitAsm) Ty(i64, u16) = {
    val := self.state.stack&.last().unwrap()[];
    self.in_reg_with_offset(val)
}

fn in_reg_with_offset(self: *EmitAsm, val: AsmVal) Ty(i64, u16) = {
    @match(val) {
        fn Increment(f) => (f.reg, f.offset_bytes.trunc());
        fn Literal(x) => {
            r := self.get_free_reg();
            self.load_imm(r, x);
            (r, 0)
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, sp, slot.id.trunc());
            (r, 0)
        }
        fn FloatReg(_) => panic("ICE: in_reg_with_offset on a float register");
    }
}

fn pop_to_reg_with_offset(self: *EmitAsm) Ty(i64, u16) = {
    res := self.peek_to_reg_with_offset();
    self.state.stack&.pop().unwrap();
    res
}

fn create_slots(self: *EmitAsm, count: u16) SpOffset = {
    enumerate self.state.open_slots { i, slot |
        if self.clock >= slot.clock && slot.size == count * 8 {|
            slot := self.state.open_slots&.ordered_remove(i).unwrap();  // TODO: does unordered_remove work? 
            return(slot.at); 
        };
    };

    made := self.next_slot;
    self.next_slot.id += count.zext() * 8;
    made
}

MAX_u16 :: 0xFFFF;
// TODO: this should check if adr is close enough since it statically knows the ip cause we're jitting.
fn load_imm(self: *EmitAsm, reg: i64, value: i64) void = {
    value: u64 = value.bitcast();
    bottom: u64 = MAX_u16.zext();
    self.asm.push(movz_rt(Bits.X64, reg, value.bit_and(bottom).trunc(), .Left0));
    value = value.shift_right_logical(16);
    if value != 0 {|
        range(1, 4) { shift |
            part := value.bit_and(bottom);
            if part != 0 {|
                // TODO: have intcast vs trunc like zig has. one safety checked that it fits in the target type and one that allows chopping stuff off. 
                self.asm.push(movk_rt(Bits.X64, reg, part.trunc(), shift));
            };
            value = value.shift_right_logical(16);
            if value == 0 {|
                return();
            };
        }
    }
}

// Note: comp_ctx doesn't push the ctx here, bc needs to do that. this it just does the call with an extra argument.
/// <arg:n> -> <ret:m>
fn dyn_c_call(self: *EmitAsm, sig: PrimSig, $do_call: @Fn() void) void = {
    self.stack_to_ccall_reg(sig.args);
    self.spill_abi_stompable();
    // TODO: don't spill!
    if sig.first_arg_is_indirect_return {|
        @match(self.state.stack&.pop().unwrap()) {
            fn Increment(f) => {
                // we know emit_bc often makes the return location a stack variable. 
                // and for now, since we just spilled, any other register would have been wastefully stored so we'd be in the Spill case.
                @assert(f.reg == sp);
                self.asm.push(add_im(Bits.X64, x8, f.reg, f.offset_bytes, 0))
            }
            fn Literal(v) => self.load_imm(x8, v);
            fn Spill(slot) => self.load_u64(x8, sp, slot.id.trunc());
            fn FloatReg(_) => panic("ICE: tried to use float register as indirect return");
        };
    };
    do_call();

    ::if_opt(Prim, i64);
    int_count := if sig.ret1 { (fst: Prim) |
        if sig.ret2 { (snd: Prim) |
            self.ccall_reg_to_stack(@slice(fst, snd));
            fst.int_count() + snd.int_count()
        } else {|
            self.ccall_reg_to_stack(@slice(fst));
            fst.int_count()
        }
    } else {|
        0
    };

    range(int_count, 8) { i |
        self.state.free_reg&.add_unique(i); // now the extras are usable again.
    };
}

DO_BASIC_ASM_INLINE :: true;


::enum(CallConv);
::if(u1);

// TODO: use with_link for tail calls.
// !with_link === tail
fn branch_func(self: *EmitAsm, f: FuncId, with_link: bool) void = {
    if DO_BASIC_ASM_INLINE {|
        func := self.program[f]&;
        // TODO: save result on the function so dont have to recheck every time?
        if func.body&.jitted_aarch64() { code |
            if func.cc.unwrap() == .OneRetPic {|
                // TODO: HACK: for no-op casts, i have two rets because I can't have single element tuples.
                if code.len == 2 && code[0] == ret() && code[1] == ret() {|
                    if !with_link {|
                        // If you're trying to do a tail call to fn add, that means emit the add instruction and then return.
                        self.asm.push(ret());
                    };
                    return();
                };
                for code.slice(0, code.len - 1) { op |
                    @debug_assert_ne(op, ret());
                    self.asm.push(op);
                };
                @debug_assert_eq(code.last().unwrap()[], ret());
                if !with_link {|
                    // If you're trying to do a tail call to fn add, that means emit the add instruction and then return.
                    self.asm.push(ret());
                };
                return();
            };
        };
    };

    // If we already emitted the target function, can just branch there directly.
    // This covers the majority of cases because I try to handle callees first.
    if self.asm.get_fn(f) { bytes |
        n := u32.int_from_ptr(self.asm.next);
        offset := bytes.int_from_rawptr() - n;
        @debug_assert(offset.mod(4) == 0, "instructions are u32 but % not mod 4", offset);
        offset /= 4;
        // TODO: use adr/adrp
        if offset.abs() < 1.shift_left(25) {|
            self.asm.push(b(offset, if(with_link, => 1, => 0)));
            return;
        };
    };

    // It's a function we haven't emitted yet, so we don't know where to jump to. (or we do know but it's far away)
    // The normal solution would be punt and let the linker deal with it or go back later and patch it ourselves.
    // But for now, I just spend a register on having a dispatch table and do an indirect call through that.
    // TODO: have a mapping. funcs take up slots even if never indirect called.
    
    // you don't really need to do this but i dont trust it cause im not following the calling convention
    self.load_imm(x16, self.asm.get_dispatch()); // NOTE: this means you can't ever resize
    if f.as_index() < 4096 {|
        self.asm.push(ldr_uo(Bits.X64, x16, x16, f.as_index()));
    } else {|
        full := f.as_index() * 8;
        bottom := full.bit_and(1.shift_left(12) - 1);
        top := (full - bottom).shift_right_logical(12);
        self.asm.push(add_im(Bits.X64, x16, x16, top, 1));
        self.asm.push(ldr_uo(Bits.X64, x16, x16, @as(u12) @as(i64) bottom / 8));
    };

    self.asm.push(br(x16, if(with_link, => 1, => 0)));
}

// Now have to be careful about trying to use x17
fn drop_slot(self: *EmitAsm, slot: SpOffset, bytes: u16) void = {
    self.state.open_slots&.push((at = slot, size = bytes, clock = self.clock)); // TODO: keep this sorted by count?
    if ZERO_DROPPED_SLOTS {|
        self.load_imm(x17, 0);
        range(0, bytes.zext() / 8) { i |
            self.store_u64(x17, sp, (slot.id + (i * 8)).trunc());
        };
    };
}

fn emit_stack_fixup(self: *EmitAsm) void = {
    self.asm.push(brk(0));
    a := self.asm.prev();
    self.asm.push(brk(0));
    b := self.asm.prev();
    self.asm.push(brk(0));
    // TODO: can't do this inline or its seen as multiple args.
    // TODO: but also we should just save one ptr and offset it because they're always sequential. 
    t := (a, b, self.asm.prev()); 
    self.release_stack&.push(t);
}

fn drop_if_unused(self: *EmitAsm, register: i64) void = {
    for self.state.stack { r |
        @if_let(r) fn Increment(f) => {
            if(f.reg == register, => return());
        };
    };
    self.drop_reg(register);
}

fn emit_return(self: *EmitAsm, sig: [] Prim) bool = {
    // We have the values on virtual stack and want them in r0-r7, that's the same as making a call.
    if !sig.is_empty() {|
        self.stack_to_ccall_reg(sig);
    };
    self.emit_stack_fixup();
    self.asm.push(ret(()));
    true
}


fn reg(self: AsmVal) ?Ty(i64, u16) = {
    @match(self) {
        fn Increment(f) => (Some = (f.reg, f.offset_bytes));
        fn Literal(_)   => .None;
        fn Spill(_)     => .None;
        fn FloatReg(_)  => todo();
    }
}

PAGE_SIZE :: 16384; // TODO

Jitted :: @struct(
    mmapped: []u32,
    dispatch: List(rawptr),
    // aarch64 instructions are always 4 bytes. 
    current_start: *u32,
    next: *u32,
    old: *u32,
);

fn new(bytes: i64) Jitted = {
    mmapped := page_allocator.alloc(u32, bytes / 4);
    (
        mmapped = mmapped,
        dispatch = list(99999, page_allocator), // Dont ever resize!
        current_start = mmapped.ptr,
        next = mmapped.ptr,
        old = mmapped.ptr,
    )
}

fn copy_inline_asm(self: *Jitted, f: FuncId, insts: []u32) void = {
    self.mark_start(f);
    for insts { op |
        self.push(op);
    };
    self.save_current(f);
}

// This is a better marker for not compiled yet.
// Depending on your mood this could be 0x1337 or whatever so you can recognise when you hit it in a debugger.
// TODO: generate shims so we know the function id
fn uncompiled_function_placeholder() rawptr = {
    addr: rawptr = fn(a: i64, b: i64, c: i64) void = {
        @panic("ICE: Tried to call un-compiled function. (x0=%, x1=%, x2=%)", a, b, c);
    };
    addr
}

fn get_dispatch(self: *Jitted) i64 = {
    rawptr.int_from_ptr(self.dispatch.maybe_uninit.ptr)
}

fn get_fn(self: *Jitted, f: FuncId) ?rawptr = {
    if(f.as_index() >= self.dispatch.len, => return(.None));
    addr := self.dispatch[f.as_index()];
    if(addr == uncompiled_function_placeholder(), => return(.None));
    (Some = addr)
}

fn offset_words(self: *Jitted, from_ip: *u32, to_ip: *u32) i64 = {
    from_ip.ptr_diff(to_ip)
}

fn prev(self: *Jitted) *u32 = {
    self.next.offset(-4)
}

fn patch(self: *Jitted, ip: *u32, inst_value: u32) void = {
    @debug_assert_eq(ip[], brk(0), "unexpected patch");
    ip[] = inst_value;
}

::ptr_utils(u32);

fn push(self: *Jitted, inst: u32) void = {
    //@debug_assert((self.next as usize) < self.high, "OOB {} {}", self.next as usize, self.high);
    self.next[] = inst;
    self.next = self.next.offset(1);
}

// Recursion shouldn't have to slowly lookup the start address.
fn mark_start(self: *Jitted, f: FuncId) void = {
    addr := u32.int_from_ptr(self.current_start);
    self.extend_blanks(f);
    self.dispatch[f.as_index()] = addr.rawptr_from_int();
}

fn extend_blanks(self: *Jitted, f: FuncId) void = {
    // TODO: call reserve :SLOW
    while => self.dispatch.len() < f.to_index().zext() + 1 {|
        self.dispatch&.push(uncompiled_function_placeholder());
    };
}

fn as_index(f: FuncId) i64 = f.to_index().zext();

fn save_current(self: *Jitted, f: FuncId) void = {
    //@debug_assert_ne!(self.next as usize, self.old as usize);
    //@debug_assert_ne!(self.next as usize, self.current_start as usize);
    self.extend_blanks(f);
    addr := u32.raw_from_ptr(self.current_start);
    self.dispatch[f.as_index()] = addr;
    //@debug_assert_eq(self.current_start as usize % 4, 0);
    // just a marker to make sure someone doesn't fall off the end of the function by forgetting to return.
    // also you can see it in the debugger disassembly so you can tell if you emitted a trash instruction inside a function or you're off the end in uninit memory. -- Apr 25
    self.push(brk(0xDEAD));
    self.current_start = self.next;
}

fn bump_dirty(self: *Jitted) void = {
    beg := u32.raw_from_ptr(self.old);
    end := u32.raw_from_ptr(self.next);
    if beg != end {|
        //@debug_assert_eq(self.next as usize, self.current_start as usize);
        //self.push(brk(0x9801));
        len := end.int_from_rawptr() - beg.int_from_rawptr();
        
        prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
        res := mprotect(beg, len, prot);
        assert(res.value.eq(0), "mprotect failed");
        
        page_start := end.int_from_rawptr() / PAGE_SIZE * PAGE_SIZE;
        self.next = u32.ptr_from_int(page_start + PAGE_SIZE);
        self.old = self.next;
        self.current_start = self.next;
        
        //
        // x86 doesn't this. TODO: what about riscv?
        // TODO: #[cfg(target_arch = "aarch64")]
        //
        // This fixes 'illegal hardware instruction'.
        // sleep(Duration::from_millis(1)) also works (in debug mode). That's really cool, it gives it time to update the other cache because instructions and data are seperate?!
        // Especially fun becuase if you run it in lldb so you break on the error and disassemble... you get perfectly valid instructions because it reads the data cache!
        // https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/caches-and-self-modifying-code
        // https://stackoverflow.com/questions/35741814/how-does-builtin-clear-cache-work
        // https://stackoverflow.com/questions/35741814/how-does-builtin-clear-cache-work
        // https://stackoverflow.com/questions/10522043/arm-clear-cache-equivalent-for-ios-devices
        // https://github.com/llvm/llvm-project/blob/main/compiler-rt/lib/builtins/clear_cache.c
        // https://github.com/apple/darwin-libplatform/blob/main/src/cachecontrol/arm64/cache.s
        // https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/sys_icache_invalidate.3.htmls
        __clear_cache(beg, end);
    };
}

//
// TODO: this is very sad.
//       since core functions like `trunc` are written with inline assembly and not precompiled, 
//       the normal versions of encoding some instructions need to use the compiler's @BITS since
//       they can't compile the library @bits yet. so the functions below are the same as the versions in arch/aarch64instructions.fr,
//       except for which macros they call.   -- Jul 16
//

/// Zero the other bits.
fn movz_rt(sf: Bits, dest: RegO, imm: u16, hw: Hw) u32 =
    @bits(sf, 0b10100101, hw, imm, dest); // TOOD: this broke on @bits when u16 became a real type. 

/// Keep the other bits.
fn movk_rt(sf: Bits, dest: RegO, imm: u16, hw: Hw) u32 =
    @bits(sf, 0b11100101, hw, imm, dest);

// It's not symetrical or perfect but its beautiful and its mine
