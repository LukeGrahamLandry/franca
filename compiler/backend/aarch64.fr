//! I know this is remarkably bad codegen. 
//! Its only job is to be fast to compile because its used for throw away comptime functions that are often only run once.
//! #c_call means https://en.wikipedia.org/wiki/Calling_convention#ARM_(A64)
//
// TODO: be able to load directly into a float register
// TODO: better memcpy. can ldr/str 16 bytes into a q float register and can ldp/stp on them for 32 at a time. 

MEM_64: i64 : 0b11;
MEM_32: i64 : 0b10;
MEM_16: i64 : 0b01;
MEM_08: i64 : 0b00;

OhWereDoingTypeSystemNow :: @struct(valid := true, interesting := false); // TODO: remove valid+interesting when x64 works.
EmitArm64 :: CodeGenJit(OhWereDoingTypeSystemNow, u32, sp, x9, false);

fn new(_: CompilerRs, __: *BcBackend) OhWereDoingTypeSystemNow = ();

fn add_free_rets(self: *EmitArm64, int_count: i64, float_count: i64) void = 
    self.add_free_args(int_count, float_count);

fn add_free_args(self: *EmitArm64, int_count: i64, float_count: i64) void = {
    range(int_count, 8) { i |
        self.state.free_reg&.set(i);
    };
    range(float_count, 8) { i |
        self.state.free_reg_float&.set(i);
    };
}

fn emit_header(self: *EmitArm64) *u32 = {
    self.asm.push(sub_im(Bits.X64, sp, sp, 16, 0));
    self.asm.push(stp_so(Bits.X64, fp, lr, sp, 0)); // save our return address
    self.asm.push(add_im(Bits.X64, fp, sp, 0, 0)); // Note: normal mov encoding can't use sp
    self.asm.push(brk(0));
    self.asm.prev()
}

fn patch_reserve_stack(self: *EmitArm64, reserve_stack: *u32) void = {
    slots := self.next_slot.id;
    @assert(
        slots < 4096,
        "not enough bits to refer to all slots (%/4096 bytes) in %",
        slots,
        self.comp.get_string(self.comp.get_function(self.body.func)[].name),
    );
    extra := slots.mod(16);
    if extra != 0 {
        slots += 16 - extra; // play by the rules
    };
    self.asm.patch(reserve_stack, sub_im(Bits.X64, sp, sp, @as(u12) slots, 0));
    
    for self.release_stack { it |
        self.asm.patch(it, add_im(Bits.X64, sp, fp, 0, 0)); // Note: normal mov encoding can't use sp
        self.asm.patch(it.offset(1), ldp_so(Bits.X64, fp, lr, sp, 0)); // get our return address
        self.asm.patch(it.offset(2), add_im(Bits.X64, sp, sp, 16, 0));
    };
}
// TODO: don't crash if you forget a closing squiggle

fn next_inst(self: *EmitArm64) *u32 = self.asm.next;

fn jump_to(self: *EmitArm64, to_ip: *u32) void = {
    dist := self.asm.offset_words(self.asm.next, to_ip);
    @debug_assert_ne(dist, 0, "while(1);");
    @debug_assert(dist < 0, "always jumping backward, to a previously emitted block"); 
    self.asm.push(b(dist, 0));
}

fn emit_other_inst(self: *EmitArm64, inst: Bc) bool #once = {
    @match(inst) {
        fn JumpIf(f) => {
            cond := self.pop_to_reg();
            @debug_assert_eq(f.slots, 0); // emit_bc doesn't do this
            self.spill_abi_stompable();
            self.asm.push(brk(0));
            inst := self.asm.prev();
            
            // branch if zero so true before false
            // we only do one branch so true block must be directly after the check.
            // this is the only shape of flow graph that its possible to generate with my ifs/whiles.
            @debug_assert(self.block_ips[f.true_ip.id.zext()].is_none());

            self.drop_reg(cond);
            state := self.state&.clone();
            self.emit_block(f.true_ip.id.zext(), true);
            self.state = state;
            self.emit_block(f.false_ip.id.zext(), true);
            
            false_ip := self.block_ips[f.false_ip.id.zext()].unwrap();
            offset := self.asm.offset_words(inst, false_ip);
            @debug_assert(cond < 32);
            @debug_assert_ne(offset, 0, "!if ice: while(1);");
            @debug_assert(offset > 0, "always jumping forward, over the false block");
            self.asm.patch(inst, cbz(Bits.X64, offset, cond));

            return(true);
        }
        fn Load(ty) => {
            addr, offset_bytes := self.pop_to_reg_with_offset(); // get the ptr

            dest := self.get_free_reg();
            @match(ty) {
                // TODO: track open float registers so this can just switch over MEM type.
                fn I32() => self.load_one(MEM_32, dest, addr, offset_bytes);
                fn F32() => self.load_one(MEM_32, dest, addr, offset_bytes);
                fn I16() => self.load_one(MEM_16, dest, addr, offset_bytes);
                fn I8()  => self.load_one(MEM_08, dest, addr, offset_bytes);
                @default => self.load_u64(dest, addr, offset_bytes);
            };
            self.state.stack&.push(Increment = (reg = dest, offset_bytes = 0));
            self.drop_reg(addr);
        }
        fn CallFnPtr(f) => {
            // TODO: tail call
            sig := self.body.decode_sig(f.sig);
            self.dyn_c_call(sig, f.context) {
                // dyn_c_call will have popped the args, so now stack is just the pointer to call
                callee := self.state.stack&.pop().expect("enough stack for call");
                // TODO: this does redundant spilling every time!
                @match(callee) {
                    fn Literal(p) => self.load_imm(x16, p);
                    fn Spill(offset) => self.load_u64(x16, sp, offset.id);
                    @default => {
                        panic("ICE: unspilled register");
                    };
                };
                self.asm.push(br(x16, 1));
            };
        }
        fn Unreachable() => {
            self.asm.push(brk(0xbabe));
            return(true);
        }
        fn PeekDup(skip) => {
            val := self.state.stack[self.state.stack.len - skip.zext() - 1];
            @match(val) {
                fn Increment(f) => {
                    if f.reg == sp {
                        self.state.stack&.push(val);
                    } else {
                        new := self.get_free_reg();
                        self.asm.push(mov(Bits.X64, new, f.reg));
                        self.state.stack&.push(Increment = (reg = new, offset_bytes = f.offset_bytes));
                    };
                }
                fn Literal(_)   => self.state.stack&.push(val);
                fn Spill(_)     => self.state.stack&.push(val);
                fn FloatReg(reg)  => {
                    new := self.get_free_reg_float();
                    self.asm.push(fmov(FType.D64, new, reg));  // I assume this works if the thing is an f32 also? just the top is zeros. 
                    self.state.stack&.push(FloatReg = new);
                }
            };
        }
        fn CopyBytesToFrom(bytes) => {
            // It takes me 7 instructions to call memcpy (not counting spilling other registers), so inline the easy case. 
            // TODO: do smaller ones if not multiple of 8. 
            if bytes.mod(8) == 0 && bytes <= 40 {
                // TODO: pop with offset here and combine the immediate constants instead of actually forming the values. 
                src := self.pop_to_reg();
                dest := self.pop_to_reg();
                temp := self.get_free_reg();
                range(0, bytes.zext() / 8) { i |
                    self.load_u64(temp, src, i * 8);
                    self.store_u64(temp, dest, i * 8);
                };
                self.drop_reg(temp);
                self.drop_reg(src);
                self.drop_reg(dest);
            } else {
                self.state.stack&.push(Literal = bytes.zext());
                self.stack_to_ccall_reg(@slice(Prim.P64, Prim.P64, Prim.I64), false);
                self.spill_abi_stompable();
                // TODO: fancy version of this?
                addr: rawptr : fn(dest: *u8, src: *u8, len: i64) void = {
                    range(0, len) { i |
                        dest.offset(i)[] = src.offset(i)[];
                    };
                };
                self.load_imm(x17, addr.int_from_rawptr());
                self.asm.push(br(x17, 1));
                self.restore_all_registers();
            };
        }
        @default => panic("unreachable: unhandled bc op");
    };

    false
}

fn emit_switch(self: *EmitArm64, cases: *RsVec(SwitchPayload)) void = {
    @debug_assert(cases.len >= 2, "switch op should have default + >=2 branches");
    inspect := self.pop_to_reg();
    // TODO: jump table? binary search? 
    // TODO: is it better to have all the jumps at the top? 
    normal_branches, default := cases.decode_switch();
    each normal_branches { option |
        self.spill_abi_stompable(); // TODO: this is dumb but you need to make sure the different branches don't spill things in different places. 
       
        assert(option.value < 4096 && option.value >= 0, "value must fit in u12"); // TODO: use the right number types sooner?
        self.asm.push(cmp_im(Bits.X64, @as(u5) inspect, @as(u12) option.value, 0));
        self.asm.push(brk(0));
        jump_inst := self.asm.prev();
        
        @debug_assert(self.block_ips[option.block.id.zext()].is_none(), "I only generate simple control flow");

        state := self.state&.clone();
        self.drop_reg(inspect); // Inside the new block
        self.emit_block(option.block.id.zext(), true);
        self.state = state;
        
        offset := self.asm.offset_words(jump_inst, self.asm.next);
        @debug_assert(offset > 0, "always jumping forward");
        self.asm.patch(jump_inst, b_cond(@as(i19) offset, .NE));
    };
    self.drop_reg(inspect);
    assert(default.is_some(), "i always have a default branch currently");
    default_block := default.unwrap();
    @debug_assert(self.block_ips[default_block.id.zext()].is_none(), "I only generate simple control flow!");
    self.emit_block(default_block.id.zext(), true);
}

// TODO: refactor this. its a problem that im afraid of it! -- May 8
// TODO: improve float handling now that i track free registers -- Jul 24
fn stack_to_ccall_reg(self: *EmitArm64, types: [] Prim, special: bool) i64 = {
    ::display_slice(Prim);
    @debug_assert(
        self.state.stack.len >= types.len,
        "found % things but wanted %",
        self.state.stack.len,
        types.len
    );
    next_int := 0;
    next_float := 0;

    enumerate types { slot_index, ty |
        continue :: local_return;
        @debug_assert_eq(slot_index, next_int + next_float);
        f := ty[].is_float();
        stack_index := self.state.stack.len - (types.len) + slot_index;

        if f {
            @if_let(self.state.stack[stack_index]) fn FloatReg(have) => {
                if have == next_float {
                    // its already where we want it.
                    self.state.stack[stack_index] = (Literal = 0); // just make sure we dont see it again later.
                    next_float += 1;
                    continue();
                };
            };

            @if(TRACE_ASM) @print("|(v%) <- (%)| ", next_float, self.state.stack[stack_index]&);

            range(0, self.state.stack.len) { i |
                @if_let(self.state.stack[i]) fn FloatReg(x) => {
                    if x == next_float {
                        // Someone else already has the one we want, so spill that to the stack since i don't have fmov encoding. TODO
                        worked := self.try_spill(i, false, true);
                        @assert(worked, "failed to spill");
                    };
                };
            };
            @match(self.state.stack[stack_index]) {
                fn Increment(f) => {
                    @debug_assert_eq(f.offset_bytes, 0, "dont GEP a float");
                    @debug_assert_ne(f.reg, sp, "dont fmov the stack pointer");
                    // TODO: fmov encoding.
                    worked := self.try_spill(stack_index, true, false);
                    @assert(worked, "failed to spill");
                }
                fn Literal(x) => {
                    // TODO: fmov encoding.
                    reg := self.get_free_reg();
                    self.load_imm(reg, x);
                    self.state.stack[stack_index] = (Increment = (reg = reg, offset_bytes = 0));
                    worked := self.try_spill(stack_index, true, false);
                    @assert(worked, "failed to spill");
                }
                fn Spill(_)    => ();
                fn FloatReg(_) => {
                    // TODO: fmov encoding.
                    // TODO: update now that we're tracking free float registers -- Jul 24
                    worked := self.try_spill(stack_index, true, true);
                    @assert(worked, "failed to spill");
                }
            };

            slot := self.state.stack[stack_index];
            @assert(slot&.is(.Spill), "expected spill");
            slot := slot.Spill;
            self.asm.push(f_ldr_uo(Bits.X64, next_float, sp, @as(u12) @as(i64) slot.id / 8));
            // self.drop_slot(slot, 8); // TODO!

            self.state.free_reg_float&.unset(next_float); 
            self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
            next_float += 1;
            continue();
        };
        
        @if_let(self.state.stack[stack_index]) fn Increment(f) => {
            if f.reg == next_int {
                self.apply_offset(f.reg, f.reg, f.offset_bytes);

                // if we happen to already be in the right register, cool, don't worry about it.
                @if(TRACE_ASM) @print("|x% already| ", f.reg);
                self.state.free_reg&.unset(f.reg); 
                self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
                next_int += 1;
                continue();
            };
        };

        found := false;
        if self.state.free_reg&.get(next_int) {
            found = true;
            self.state.free_reg&.unset(next_int);
        };
        
        if !found {
            break :: local_return;
            each self.state.stack { r | 
                @if_let(r) fn Increment(f) => {
                    if f.reg == next_int {
                        found = true;
                        // The one we want is already used by something on the v-stack; swap it with a random free one. TODO: put in the right place if you can.
                        @debug_assert_ne(f.reg, sp); // encoding but unreachable
                        reg := self.get_free_reg();
                        self.asm.push(mov(Bits.X64, reg, f.reg));
                        r[] = (Increment = (reg = reg, offset_bytes = f.offset_bytes));
                        // Now x{i} is free and we'll use it below.
                        break();
                    };
                };
            };
        };
        
        // TODO: lexer error if you forget a quote and hit a new line. 
        @assert(found, "TODO: x% is not free. stack is %.",
                next_int, self.state.stack.items());

        @if(TRACE_ASM) @print("|(x%) <- (%)| ", next_int, self.state.stack[stack_index]&);
        @match(self.state.stack[stack_index]) {
            fn Increment(f) => {
                @debug_assert_ne(next_int, f.reg);
                // even if offset_bytes is 0, we need to move it to the right register. and add can encode that mov even if reg is sp.
                self.apply_offset(next_int, f.reg, f.offset_bytes);
                self.drop_reg(f.reg);
            }
            fn Literal(x) => self.load_imm(next_int, x);
            fn Spill(slot) => self.load_u64(next_int, sp, slot.id);
            fn FloatReg(f) => {
                fmov_from(next_int, f);
                self.drop_reg_float(f);
            }
        };
        self.state.free_reg&.unset(next_int);
        self.state.stack[stack_index] = (Literal = 0); // make sure we dont try to spill it later
        next_int += 1;
    };
    @if(TRACE_ASM) println("");
    self.state.stack.len -= types.len;
    
    if special {
        // TODO: dont do this. but be careful! you have to spill x8 if its already on the stack
        //       because we need it now but it could be our own incoming return value if this is the first call in the function. 
        self.spill_abi_stompable();
        @match(self.state.stack&.pop().unwrap()) {
            fn Increment(f) => {
                // we know emit_bc often makes the return location a stack variable. 
                // and for now, since we just spilled, any other register would have been wastefully stored so we'd be in the Spill case.
                @assert(f.reg == sp, "spilled so must be in sp");
                self.apply_offset(x8, f.reg, f.offset_bytes);
            }
            fn Literal(v) => self.load_imm(x8, v);
            fn Spill(slot) => self.load_u64(x8, sp, slot.id);
            fn FloatReg(_) => panic("ICE: tried to use float register as indirect return");
        };
    };
    
    0
}

fn pop_stacked_args(self: *EmitArm64, amount_stacked: i64) void = {
    @debug_assert_eq(amount_stacked, 0, "TODO: implement stacked args on aarch64");
}

::if(AsmVal);

fn ccall_ret_reg_to_stack(self: *EmitArm64, types: [] Prim) void = { 
    self.ccall_reg_to_stack(types, false);
}

fn ccall_reg_to_stack(self: *EmitArm64, types: [] Prim, special: bool) Ty(i64, i64) = {
    if special {
        self.state.stack&.push(Increment = (reg = 8, offset_bytes = 0));
    };
    next_float := 0;
    next_int := 0;
    enumerate types { i, ty |
        @debug_assert_eq(i, next_int + next_float);
        f := ty[].is_float();
        v: AsmVal = if f {
            self.state.free_reg_float&.unset(next_float);
            (FloatReg = next_float)
        } else {
            self.state.free_reg&.unset(next_int); 
            (Increment = (
                reg = next_int,
                offset_bytes = 0,
            ))
        };
        if f {
            next_float += 1;
        } else {
            next_int += 1;
        };
        self.state.stack&.push(v);
    };
    (next_int, next_float)
}

fn emit_store(self: *EmitArm64, addr: AsmVal, value: AsmVal, ty: Prim) void = {
    reg, offset_bytes := self.in_reg_with_offset(addr);
    p64 :: fn() => {
        if value&.is(.FloatReg) {
            r := value.FloatReg;
            @assert_eq(offset_bytes.mod(8), 0, "TODO: align");
            self.asm.push(f_str_uo(Bits.X64, r, reg, @as(u12) @as(i64) (offset_bytes / 8)));
            self.drop_reg_float(r);
        } else {
            val := self.in_reg(value);
            self.store_u64(val, reg, offset_bytes);
            self.drop_reg(val);
        };
    };
    @match(ty) {
        fn P64() => p64();
        fn F64() => p64();
        fn I64() => p64();
        fn F32() => {
            if value&.is(.FloatReg) {
                r := value.FloatReg;
                @assert_eq(offset_bytes.mod(4), 0, "TODO: align");
                self.asm.push(f_str_uo(Bits.W32, r, reg, @as(u12) @as(i64) (offset_bytes / 4)));
                self.drop_reg_float(r);
            } else {
                val := self.in_reg(value);
                self.store_one(MEM_32, val, reg, offset_bytes);
                self.drop_reg(val);
            };
        }
        fn I32() => {
            val := self.in_reg(value);
            self.store_one(MEM_32, val, reg, offset_bytes);
            self.drop_reg(val);
        }
        fn I16() => {
            val := self.in_reg(value);
            self.store_one(MEM_16, val, reg, offset_bytes);
            self.drop_reg(val);
        }
        fn I8() => {
            val := self.in_reg(value);
            self.store_one(MEM_08, val, reg, offset_bytes);
            self.drop_reg(val);
        }
    };
    self.drop_reg(reg);
}

fn load_u64(self: *EmitArm64, dest_reg: i64, src_addr_reg: i64, offset_bytes: i64) void = {
    self.load_one(MEM_64, dest_reg, src_addr_reg, offset_bytes)
}

fn load_one(self: *EmitArm64, register_type: i64, dest_reg: i64, src_addr_reg: i64, offset_bytes: i64) void = {
    @debug_assert_ne(dest_reg, sp, "ICE: tried to load into sp");
    scale := 1.shift_left(register_type);
    extra := offset_bytes.mod(scale);
    if extra == 0 && offset_bytes >= 0 {
        self.asm.push(ldr_uo_any(register_type, dest_reg, src_addr_reg, @as(u12) @as(i64) (offset_bytes / scale)));
        @debug_assert((@as(i64) offset_bytes) / scale < 1.shift_left(12), "ICE: not enough bits for load");
    } else {
        // Note: this relies on the access actually being aligned once you combine the value in the register without our non %8 offset.
        reg := self.get_free_reg();
        self.apply_offset(reg, src_addr_reg, offset_bytes);
        self.asm.push(ldr_uo_any(register_type, dest_reg, reg, 0));
        self.drop_reg(reg);
    }
}

fn store_u64(self: *EmitArm64, src_reg: i64, dest_addr_reg: i64, offset_bytes: i64) void = {
    self.store_one(MEM_64, src_reg, dest_addr_reg, offset_bytes)
}

fn store_one(self: *EmitArm64, register_type: i64, src_reg: i64, dest_addr_reg: i64, offset_bytes: i64) void = {
    scale := 1.shift_left(register_type);
    if src_reg == sp {
        reg := self.get_free_reg();
        self.asm.push(add_im(Bits.X64, reg, sp, 0, 0)); // not mov!
        self.store_one(register_type, reg, dest_addr_reg, offset_bytes);
        self.drop_reg(reg);
        return();
    };
    if offset_bytes.mod(8) == 0 && offset_bytes >= 0 {
        self.asm.push(str_uo_any(register_type, src_reg, dest_addr_reg, @as(u12) @as(i64) (offset_bytes / scale)));
        @debug_assert((@as(i64) offset_bytes) / scale < 1.shift_left(12), "ICE: not enough bits for store");
    } else {
        reg := self.get_free_reg();
        self.apply_offset(reg, dest_addr_reg, offset_bytes);
        self.asm.push(str_uo_any(register_type, src_reg, reg, 0));
        self.drop_reg(reg);
    };
}

fn apply_offset(self: *EmitArm64, dest: i64, src: i64, offset_bytes: i64) void = {
    if dest == src && offset_bytes == 0 {
        return();
    };
    if offset_bytes >= 0 {
        if offset_bytes >= 1.shift_left(12) { 
            // TODO: probably never get here? there used to be only one codepath (try_spill) that considered this case. 
            self.asm.push(add_im(Bits.X64, dest, src, @as(u12) offset_bytes.shift_right_logical(12), 1));
            mask := 1.shift_left(12) - 1;
            self.asm.push(add_im(Bits.X64, dest, dest, @as(u12) offset_bytes.bit_and(mask), 0));
        } else {
            self.asm.push(add_im(Bits.X64, dest, src, @as(u12) offset_bytes, 0));
        };
    } else { 
        // emit_bc never uses negative IncPtrBytes but examples/bf2bc does.
        @debug_assert(offset_bytes.abs() < 1.shift_left(12), "not enough bits to encode offset %", offset_bytes);
        self.asm.push(sub_im(Bits.X64, dest, src, @as(u12) -offset_bytes, 0));
    };
}

// do_floats:false if you're just trying to free up a gpr, not saving for a call.
fn try_spill(self: *EmitArm64, i: usize, do_ints: bool, do_floats: bool) bool = {
    v := self.state.stack[i];
    if do_ints {
        @if_let(v) fn Increment(f) => {
            if(f.reg == sp, => return(false));
    
            // Note: this assumes we don't need to preserve the value in the reg other than for this one v-stack slot.
            slot := self.create_slots(8);
            @if(TRACE_ASM) @print("(spill (x% + %) -> [sp, %]) ", f.reg, f.offset_bytes, slot.id);
            self.apply_offset(f.reg, f.reg, f.offset_bytes);
    
            self.store_u64(f.reg, sp, slot.id);
            self.drop_reg(f.reg);
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    if do_floats {
        @if_let(v) fn FloatReg(freg) => {
            slot := self.create_slots(8);
            self.asm.push(f_str_uo(Bits.X64, freg, sp, @as(u12) @as(i64) slot.id / 8));
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    false
}

fn is_special_reg(self: *EmitArm64, reg: i64) bool = reg.eq(sp).or(reg == x8).or(reg == fp);

fn in_reg(self: *EmitArm64, val: AsmVal) i64 = {
    @match(val) {
        fn Increment(f) => {
            if f.offset_bytes > 0 {
                out := if(f.reg == sp, => self.get_free_reg(), => f.reg);
                self.apply_offset(out, f.reg, f.offset_bytes);
                f.reg = out;
            };
            f.reg
        }
        fn Literal(x) => {
            r := self.get_free_reg();
            self.load_imm(r, x);
            r
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, sp, slot.id);
            // self.drop_slot(slot, 8); TODO!
            r
        }
        fn FloatReg(float) => {
            int := self.get_free_reg();
            fmov_from(int, float);
            self.drop_reg_float(float);
            int
        }
    }
}

fn in_reg_float(self: *EmitArm64, val: AsmVal) i64 = {
    @match(val) {
        fn FloatReg(r) => r;
        @default => {
            // TODO: load spill and immediates directly into float register
            int := self.in_reg(val);
            float := self.get_free_reg_float();
            self.asm.push(fmov_to(float, int));
            self.drop_reg(int);
            float
        };
    }
}

// TODO: this should check if adr is close enough since it statically knows the ip cause we're jitting.
fn load_imm(self: *EmitArm64, reg: i64, value: i64) void = {
    value: u64 = value.bitcast();
    bottom: u64 = MAX_u16.zext();
    self.asm.push(movz(Bits.X64, reg, value.bit_and(bottom).trunc(), .Left0));
    value = value.shift_right_logical(16);
    if value != 0 {
        range(1, 4) { shift |
            part := value.bit_and(bottom);
            if part != 0 {
                // TODO: have intcast vs trunc like zig has. one safety checked that it fits in the target type and one that allows chopping stuff off. 
                self.asm.push(movk(Bits.X64, reg, part.trunc(), @as(Hw) shift));
            };
            value = value.shift_right_logical(16);
            if(value == 0, => return());
        };
    };
}


// TODO: use with_link for tail calls.
// !with_link === tail
fn branch_func(self: *EmitArm64, f: FuncId, with_link: bool) void #once = {
    // If we already emitted the target function, can just branch there directly.
    // This covers the majority of cases because I try to handle callees first.
    if get_already_jitted(self.bc, self.comp, f) { bytes |
        n := u32.int_from_ptr(self.asm.next);
        offset := bytes.int_from_rawptr() - n;
        offset = offset / 4;
        // TODO: use adr/adrp
        if offset.abs() < 1.shift_left(25) {
            @if(TRACE_ASM) println("backwards call");
            // :sema_regression
            self.asm.push(b(offset, @as(u1) @as(i64) if(with_link, => 1, => 0)));
            return(); // TODO: footgun. its too easy to `return;` which returns in other languages but here just evaluates and descards.
        };
    } else {
        // don't do this in load_fnptr_from_dispatch because it might just be because its an import and we can't jump that far.
        // only care if its something that's not ready yet. 
        self.forward_calls&.push(f);
    };

    // It's a function we haven't emitted yet, so we don't know where to jump to. (or we do know but it's far away)
    // The normal solution would be punt and let the linker deal with it or go back later and patch it ourselves.
    // But for now, I just spend a register on having a dispatch table and do an indirect call through that.
    // TODO: have a mapping. funcs take up slots even if never indirect called.
    self.load_fnptr_from_dispatch(f, x16);
    // :sema_regression
    self.asm.push(br(x16, @as(u1) @as(i64) if(with_link, => 1, => 0)));
}

fn load_fnptr_from_dispatch(self: *EmitArm64, f: FuncId, reg: i64) void = {
    self.load_imm(reg, self.asm.get_dispatch()); // NOTE: this means you can't ever resize
    if f.as_index() < 4096 {
        @if(TRACE_ASM) println("short table call");
        self.asm.push(ldr_uo(Bits.X64, reg, reg, @as(u12) @as(i64) f.as_index()));
    } else {
        @if(TRACE_ASM) println("long table call");
        full := f.as_index() * 8;
        bottom := full.bit_and(1.shift_left(12) - 1);
        top := (full - bottom).shift_right_logical(12); // TODO: don't need to minus? we're shifting it off anyway. 
        @assert(top < 4096, "not enough bits for load_fnptr_from_dispatch %", f);
        self.asm.push(add_im(Bits.X64, reg, reg, top, 1));
        self.asm.push(ldr_uo(Bits.X64, reg, reg, @as(u12) @as(i64) bottom / 8));
    };
}

fn emit_stack_fixup(self: *EmitArm64) void = {
    self.asm.push(brk(0));
    fst := self.asm.prev();
    self.asm.push(brk(0));
    self.asm.push(brk(0));
    self.release_stack&.push(fst);
}

fn emit_return(self: *EmitArm64, sig: [] Prim) bool = {
    // We have the values on virtual stack and want them in r0-r7, that's the same as making a call.
    if !sig.is_empty() {
        self.stack_to_ccall_reg(sig, false);
    };
    self.emit_stack_fixup();
    self.asm.push(ret(()));
    true
}

fn inst_intrinsic(self: *EmitArm64, op: Intrinsic) void #once = {
    // TODO: for add, include offset_bytes in the immediate if they fit or combine them and defer instead of doing 3 seperate adds.
    bin :: fn($do: @Fn(a: u5, b: u5) void) void => {
        snd: u5 = self.pop_to_reg();
        fst: u5 = self.pop_to_reg();
        do(fst, snd);
        self.drop_reg(snd);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    // Note: could use better error message when you try to factor this into a '=' but keep 'bin' a '=>',
    //       "missing value self" because 'bin' closes over the outer one, not the one you'd add as an argument to this function.
    // TODO: allow mutiple callsites that are all tail of a '=>' function to only make one duplicate of it so there's less code bloat. 
    //! IMPORTANT: cond is inverted because CSINC
    bin_cmp :: fn(inv_cond: Cond) => bin(fn(a, b) => {
        self.asm.push(cmp(Bits.X64, a, b));
        self.asm.push(cset(Bits.X64, a, inv_cond));
    });
    bin_float :: fn($do: @Fn(a: u5, b: u5) void) void => {
        snd: u5 = self.pop_to_reg_float();
        fst: u5 = self.pop_to_reg_float();
        do(fst, snd);
        self.drop_reg_float(snd);
        self.state.stack&.push(FloatReg = fst);
    };
    bin_cmp_float :: fn(self: *EmitArm64, inv_cond: Cond) void = {
        snd: u5 = self.pop_to_reg_float();
        fst: u5 = self.pop_to_reg_float();
        out := self.get_free_reg();
        self.asm.push(fcmp(FType.D64, fst, snd));
        self.asm.push(cset(Bits.X64, @as(u5) out, inv_cond));
        self.drop_reg_float(fst);
        self.drop_reg_float(snd);
        self.state.stack&.push(Increment = (reg = out, offset_bytes = 0));
    };
        
    trunc8 :: fn(self: *EmitArm64) void = {
        fst: u5 = self.pop_to_reg();
        extra: u5 = self.get_free_reg();
        self.asm.push(movz(.X64, extra, 0x00FF, .Left0));
        self.asm.push(and_sr(.X64, fst, fst, extra, Shift.LSL, @as(u6) 0));
        self.drop_reg(extra);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    
    trunc16 :: fn(self: *EmitArm64) void = {
        fst: u5 = self.pop_to_reg();
        extra: u5 = self.get_free_reg();
        self.asm.push(movz(.X64, extra, 0xFFFF, .Left0));
        self.asm.push(and_sr(.X64, fst, fst, extra, Shift.LSL, @as(u6) 0));
        self.drop_reg(extra);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    
    trunc32 :: fn(self: *EmitArm64) void = {
        fst: u5 = self.pop_to_reg();
        extra: u5 = self.get_free_reg();
        self.asm.push(movz(.X64, extra, 0xFFFF, .Left0));
        self.asm.push(movk(.X64, extra, 0xFFFF, .Left16));
        self.asm.push(and_sr(.X64, fst, fst, extra, Shift.LSL, @as(u6) 0));
        self.drop_reg(extra);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    
    float_to_int :: fn($do: @Fn(in: u5, out: u5) void) void => {
        in: u5 = self.pop_to_reg_float();
        out: u5 = self.get_free_reg();
        do(in, out);
        self.drop_reg_float(in);
        self.state.stack&.push(Increment = (reg = out, offset_bytes = 0));
    };
    int_to_float :: fn($do: @Fn(in: u5, out: u5) void) void => {
        in: u5 = self.pop_to_reg();
        out: u5 = self.get_free_reg_float();
        do(in, out);
        self.drop_reg(in);
        self.state.stack&.push(FloatReg = out);
    };
    float_to_float :: fn($do: @Fn(in: u5) void) void => {
        in: u5 = self.pop_to_reg_float();
        do(in);
        self.state.stack&.push(FloatReg = in);
    };
    int_to_int :: fn($do: @Fn(in: u5) void) void => {
        in: u5 = self.pop_to_reg();
        do(in);
        self.state.stack&.push(Increment = (reg = in, offset_bytes = 0));
    };
    
    @match(op) {
        fn Add() => bin(fn(a, b) => self.asm.push(add_sr(Bits.X64, a, a, b, Shift.LSL, 0b000000)));
        fn Sub() => bin(fn(a, b) => self.asm.push(sub_sr(Bits.X64, a, a, b, Shift.LSL, 0b000000)));
        fn Mul() => bin(fn(a, b) => self.asm.push(madd(Bits.X64, a, a, b, 0b11111)));
        fn Div() => bin(fn(a, b) => self.asm.push(sdiv(Bits.X64, a, a, b)));
        fn Eq()  => bin_cmp(.NE);
        fn Ne()  => bin_cmp(.EQ);
        fn Le()  => bin_cmp(.GT);
        fn Ge()  => bin_cmp(.LT);
        fn Lt()  => bin_cmp(.GE);
        fn Gt()  => bin_cmp(.LE);
        fn IntToPtr() => (); // no-op
        fn PtrToInt() => (); // no-op
        fn ShiftLeft()            => bin(fn(a, b) => self.asm.push(lslv(Bits.X64, a, a, b)));
        fn ShiftRightLogical()    => bin(fn(a, b) => self.asm.push(lsrv(Bits.X64, a, a, b)));
        fn ShiftRightArithmetic() => bin(fn(a, b) => self.asm.push(asrv(Bits.X64, a, a, b)));
        fn BitOr()  => bin(fn(a, b) => self.asm.push(orr(Bits.X64, a, a, b, Shift.LSL, 0b000000)));
        fn BitAnd() => bin(fn(a, b) => self.asm.push(and_sr(Bits.X64, a, a, b, Shift.LSL, @as(u6) 0)));
        fn BitXor() => bin(fn(a, b) => self.asm.push(eor(Bits.X64, a, a, b, Shift.LSL, @as(u6) 0)));
        fn FAdd() => bin_float(fn(a, b) => self.asm.push(fadd(FType.D64, a, a, b)));
        fn FSub() => bin_float(fn(a, b) => self.asm.push(fsub(FType.D64, a, a, b)));
        fn FMul() => bin_float(fn(a, b) => self.asm.push(fmul(FType.D64, a, a, b)));
        fn FDiv() => bin_float(fn(a, b) => self.asm.push(fdiv(FType.D64, a, a, b)));
        fn FEq()  => self.bin_cmp_float(.NE);
        fn FNe()  => self.bin_cmp_float(.EQ);
        fn FLe()  => self.bin_cmp_float(.GT);
        fn FGe()  => self.bin_cmp_float(.LT);
        fn FLt()  => self.bin_cmp_float(.GE);
        fn FGt()  => self.bin_cmp_float(.LE);
        fn Trunc64To32() => self.trunc32();
        fn Trunc64To16() => self.trunc16();
        fn Trunc64To8()  => self.trunc8();
        fn Trunc32To16() => self.trunc16();
        fn Trunc32To8()  => self.trunc8();
        fn Trunc16To8()  => self.trunc8();
        // TODO: this about how signed numbers are represented. 
        fn SignExtend32To64() => { // TODO: WRITE A TEST THAT RELIES ON THIS (other than new backend)
            fst: u5 = self.pop_to_reg();
            self.asm.push(bfm(Bits.X64, 0b0, 0b000000, 0b011111, fst, fst));
            self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
        };
        fn ZeroExtend32To64() => (); // no-op
        fn ZeroExtend16To64() => (); // no-op
        fn ZeroExtend8To64()  => (); // no-op
        fn ZeroExtend16To32() => (); // no-op
        fn ZeroExtend8To32()  => (); // no-op
        fn ZeroExtend8To16()  => (); // no-op
        fn IntToFloatValue()  => int_to_float(fn(in, out) => self.asm.push(scvtf(Bits.X64, FType.D64, out, in)));
        fn FloatToIntValue()  => float_to_int(fn(in, out) => self.asm.push(fcvtzs(Bits.X64, FType.D64, out, in)));
        fn IntToFloatBits()   => int_to_float(fn(in, out) => self.asm.push(fmov_to(out, in)));
        fn FloatToIntBits()   => float_to_int(fn(in, out) => self.asm.push(fmov_from(out, in)));
        fn ShrinkFloat() => float_to_float(fn(in) => self.asm.push(fcnv(.D64, .S32, in, in)));
        fn GrowFloat()   => float_to_float(fn(in) => self.asm.push(fcnv(.S32, .D64, in, in)));
        fn BitNot()      => int_to_int(fn(in) => self.asm.push(orn(Bits.X64, in, in, 0b11111, Shift.LSL, @as(u6) 0)));
        @default => {
            @panic("ICE: unimplemented aarch64 intrinsic %", op);
        };
    };
}

fn print_llvm_mc_dis(self: *EmitArm64, asm: []u32) void #inline = print_llvm_mc_dis(asm);

// It's not symetrical or perfect but its beautiful and its mine
