//! stuff for the comptime jit that's needed for both arm and x86. 

// TODO: put all these config options somewhere consistant.
ZERO_DROPPED_REG :: false;    // TODO: bring this back
ZERO_DROPPED_SLOTS :: false;  // TODO: bring this back
TRACE_ASM :: false;
TRACE_CALLS :: false; 
USE_LLVM_DIS :: true;

SpOffset :: @struct(id: i64);

IncReg :: @struct(reg: i64, offset_bytes: i64);
AsmVal :: @tagged(
    Increment: IncReg, // not a pointer derefernce, just adding a static number to the value in the register.
    Literal: i64,
    Spill: SpOffset,
    FloatReg: i64,
);
::tagged(AsmVal);
::DeriveFmt(AsmVal);

SpSlot :: @struct(at: SpOffset, size: u16, clock: u16);
BlockState :: @struct(
    stack: List(AsmVal),
    free_reg: StaticBitSet(32), // update clone if you make this not static
    free_reg_float: StaticBitSet(32), // update clone if you make this not static
    open_slots: List(SpSlot),
);

fn clone(self: *BlockState) BlockState = {
    (stack = self.stack&.clone(), free_reg = self.free_reg, free_reg_float = self.free_reg_float, open_slots = self.open_slots&.clone())
}

fn create_jit_shim(self: CompCtx, bc: *BcBackend, fid: FuncId, handler_callee_address: rawptr) Res(*FnBody) = {
    func := self.get_function(fid);
    mark := __temp_alloc.mark();
    //@println("create shim for %", func.log(self));
    fake_body := @try(self.empty_fn_body(bc, fid, .Jit)) return;
    
    // TODO: copy-paste 
    emit: EmitBc = (
        result = fake_body, 
        last_loc = func.loc,
        locals = list(temp()),
        var_lookup = init(temp()),
        inlined_return_addr = init(temp()),
        is_ssa_var = empty(),
        out_alloc = self.get_alloc(), //temp(), // TODO: temp() should be fine because we don't deduplicate shims but clearly its not -- Nov 7
        current_block = (id = 0),
        clock = 0,
        want_log = false,
        debug = false,
        program = self,
    );
    
    entry_block := emit&.push_block(empty());
    c := self.data.cast()[][];
    arg_type := c.tuple_of(@slice(c.get_or_create_type(rawptr), c.get_or_create_type(FuncId)));
    f_ty: FnType = (arg = arg_type, ret = c.get_or_create_type(rawptr), arity = 2);
    sig := @try(self.prim_sig(f_ty, .CCallReg)) return;
    emit&.push(PushConstant = (value = handler_callee_address.int_from_rawptr(), ty = .P64));
    emit&.push(PushConstant = (value = SelfHosted.int_from_ptr(c), ty = .P64));
    emit&.push(PushConstant = (value = fid.as_index(), ty = .I32));
    emit&.push(CallFnPtr = (sig = emit&.push_sig(sig)));
    // All our arguments are still v-stacked so if we've found a pointer to call by now, we can just do that.
    // Otherwise the compiler would have panicked.  
    emit&.push(.RotateForImmediateCallPtr);
    emit&.push(CallFnPtr = (sig = emit&.push_sig(emit.result.signeture)));
    @try(emit&.emit_return(func.finished_ret.unwrap())) return;
    bc.jitted&.extend_blanks(fid);
    @debug_assert_eq(bc.jitted.dispatch_table[fid.as_index()], uncompiled_function_placeholder(), "already compiled? why are we making a shim? %", self.log(func));
    __temp_alloc.reset_retaining_capacity(mark);
    (Ok = fake_body)
}

// TODO: it would be nice if i had the arguments here for reporting what call you actually tried to make. 
//       but this way will be even better once i can lazily compile it and just lean on the backend to make the args flow through. 
report_called_uncompiled_or_just_fix_the_problem :: @as(rawptr) fn(self: *SelfHosted, fid: FuncId) rawptr = {
    func := self.get_function(fid);
    if func.get_flag(.RealAsmDone) { 
        if self.get_fn_callable(fid) { addr | 
            // It's been compiled since last we checked so just use that. 
            return(addr);
        };
    };
    // TODO: maybe better to put smuggle_libc here too
    if func.name == Flag.___this_function_is_not_real_im_just_testing_the_franca_compiler.ident() {
        return(fn(a: i64, b: i64, c: i64) i64 = {
            @println("% + % + % = %", a, b, c, a + b + c);
            a + b + c
        });
    };
    
    // :ugly
    first := true;
    addr := self.poll_in_place(rawptr) { () Maybe(rawptr) |
        r :: local_return;
        if func.get_flag(.RealAsmDone) {
            r(Ok = self.get_fn_callable(fid).expect("must have pointer if done"));
        } else {
            if !first {
                // If its an import we're never gonna make progress by trying to compile it. 
                if func.get_flag(.BodyIsSpecial) {
                    @panic("Tried to call uncompiled function:\n %", self.get_function(fid).log(self));
                };  
            };
            first = false;
            func.unset_flag(.AsmDone);
            r(Suspend = self.wait_for(Jit = fid));
        };
        unreachable()
    };
    addr := self.unwrap_report_error(rawptr, addr);
    //bc.jitted&.bump_dirty();
    addr
};

fn print_llvm_mc_dis(asm: []u32) void = {
    @println("% instrucitons.", asm.len());
    hex: List(u8) = list(asm.len * 17, temp());
    for asm { op |
        range(0, 4) { _ |
            byte := op.bit_and(0x000000FF);
            op = op.shift_right_logical(8);
            hex&.push_prefixed_hex_byte(byte.trunc());
            hex&.push_all(" ");
        };
    };
    print_llvm_mc_dis("--arch=aarch64", hex.items());
}

fn print_llvm_mc_dis(asm: []u8) void = {
    @println("% bytes.", asm.len());
    hex: List(u8) = list(asm.len * 5, temp());
    for asm { byte |
        hex&.push_prefixed_hex_byte(byte);
        hex&.push_all(" ");
    };
    print_llvm_mc_dis("--arch=x86-64", hex.items());
}

fn print_llvm_mc_dis(arch: Str, hex: Str) void = {
    out := open_temp_file();
    out.fd&.write(hex);
    // TODO: have the caller pass in anything like this, 
    //       but this is just for debugging the compiler so its not a big deal. :driver_io
    // varient 1 means intel syntax. ignored when doing arm. 
    success := run_cmd_blocking("llvm-mc", @slice(arch, "--disassemble", "-output-asm-variant=1", "--show-encoding", out&.s_name()));
    if(!success, => eprintln("ICE(debugging): couldn't disassemble"));
    out.remove();
} 

PAGE_SIZE :: 16384; // TODO: ask the os for this

Jitted :: @struct(
    mmapped: []u32,
    dispatch_table: List(rawptr),
    // aarch64 instructions are always 4 bytes. 
    current_start: *u32,
    next: *u32,
    old: *u32,
);

fn new(bytes: i64) Jitted #once = {
    mmapped := page_allocator.alloc(u32, bytes / 4);
    (
        mmapped = mmapped,
        dispatch_table = list(99999, page_allocator), // Dont ever resize!
        current_start = mmapped.ptr,
        next = mmapped.ptr,
        old = mmapped.ptr,
    )
}


// This is a better marker for not compiled yet.
// Depending on your mood this could be 0x1337 or whatever so you can recognise when you hit it in a debugger.
// TODO: generate shims so we know the function id
fn uncompiled_function_placeholder() rawptr = {
    addr: rawptr = fn(a: i64, b: i64, c: i64) void = {
        @panic("ICE: Tried to call un-compiled function. (x0=%, x1=%, x2=%)", a, b, c);
    };
    addr
}

fn get_dispatch(self: *Jitted) i64 = {
    rawptr.int_from_ptr(self.dispatch_table.maybe_uninit.ptr)
}


fn get_already_jitted(self: *BcBackend, c: CompCtx, f: FuncId) ?rawptr = {
    if(f.as_index() >= self.jitted.dispatch_table.len, => return(.None));
    func := c.get_function(f);
    if(!func.get_flag(.AsmDone), => return(.None));
    addr := self.jitted.dispatch_table[f.as_index()];
    (Some = addr)
}

fn offset_words(self: *Jitted, from_ip: *u32, to_ip: *u32) i64 = {
    from_ip.ptr_diff(to_ip)
}

fn prev(self: *Jitted) *u32 = {
    self.next.offset(-1)
}

fn patch(self: *Jitted, ip: *u32, inst_value: u32) void = {
    @debug_assert_eq(ip[], brk(0), "unexpected patch");
    ip[] = inst_value;
}

::ptr_utils(u32);

fn push(self: *Jitted, inst: u32) void = {
    //println(inst);
    //@debug_assert((self.next as usize) < self.high, "OOB {} {}", self.next as usize, self.high);
    
    self.next[] = inst;
    self.next = self.next.offset(1);
}

fn push_bytes_asm(self: *Jitted, insts: []u8) void #once = {  
    byte := u32.raw_from_ptr(self.next);
    byte := u8.ptr_from_raw(byte);
    for insts { op |
        byte[] = op;
        byte = byte.offset(1);
    };
    extra := insts.len.mod(4);
    range(0, extra) { _ |
        byte[] = 0;
        byte = byte.offset(1);
    };
    
    byte := u8.raw_from_ptr(byte);
    self.next = u32.ptr_from_raw(byte);
}

// Recursion shouldn't have to slowly lookup the start address.
fn mark_start(self: *Jitted, f: FuncId) void = {
    addr := u32.int_from_ptr(self.current_start);
    self.extend_blanks(f);
    self.dispatch_table[f.as_index()] = addr.rawptr_from_int();
}

fn extend_blanks(self: *Jitted, f: FuncId) void = {
    assert(self.dispatch_table.maybe_uninit.len > f.as_index(), "cannot resize dispatch table");
    while => self.dispatch_table.len() < f.to_index().zext() + 1 {
        self.dispatch_table&.push(uncompiled_function_placeholder());
    };
}

fn save_current(self: *Jitted, f: FuncId) []u32 #once = {
    //@debug_assert_ne!(self.next as usize, self.old as usize);
    //@debug_assert_ne!(self.next as usize, self.current_start as usize);
    self.extend_blanks(f);
    func_start := self.current_start;
    addr := u32.raw_from_ptr(self.current_start);
    start := u32.int_from_ptr(self.current_start);
    end := u32.int_from_ptr(self.next);
    xx := u32.int_from_ptr(self.mmapped.ptr);
    last_space := xx + self.mmapped.len * 4;
    @debug_assert(end < last_space, "ICE: out of space for jitted code (% MB)", self.mmapped.len * 4 / 1024 / 1024);
    //@debug_assert_eq(self.current_start as usize % 4, 0);
    // just a marker to make sure someone doesn't fall off the end of the function by forgetting to return.
    // also you can see it in the debugger disassembly so you can tell if you emitted a trash instruction inside a function or you're off the end in uninit memory. -- Apr 25
    self.push(brk(0xDAED)); // TODO: x86 int3 or whatever
    self.current_start = self.next;
    // TODO: have a slice_from(start, to = end);
    (ptr = func_start, len = (end - start) / 4)
}

fn bump_dirty(self: *Jitted) void = {
    beg := u32.raw_from_ptr(self.old);
    end := u32.raw_from_ptr(self.next);
    if beg != end {
        //@debug_assert_eq(self.next as usize, self.current_start as usize);
        self.push(brk(0x3141));  // TODO: x86 int3 or whatever
        len := end.int_from_rawptr() - beg.int_from_rawptr();
        
        prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
        res := mprotect(beg, len, prot);
        assert(res.value.eq(0), "mprotect failed");
        
        // TODO: is this wrong now? if you were at the very end of a page you would have written the brk to the next one,
        //      but i guess thats find cause we rewrite anyway when we put instructions there. 
        page_start := end.int_from_rawptr() / PAGE_SIZE * PAGE_SIZE;
        self.next = u32.ptr_from_int(page_start + PAGE_SIZE);
        self.old = self.next;
        self.current_start = self.next;
        
        // Some architectures need a little help when you jit code!
        clear_instruction_cache(beg, end);
    };
}

/* // TODO: traits
    CGJ :: CodeGenJit(B);
    fn emit_switch(self: *CGJ, cases: *RsVec(SwitchPayload)) void;
    // returns the number of bytes pushed to the stack that need to be popped by the caller after the call. 
    fn stack_to_ccall_reg(self: *CGJ, types: [] Prim, extra_special_ret: bool) i64;
    fn ccall_reg_to_stack(self: *CGJ, types: [] Prim, extra_special_ret: bool) Ty(i64, i64);
    fn emit_store(self: *CGJ, addr: AsmVal, value: AsmVal, ty: Prim) void;
    fn store_u64(self: *CGJ, src_reg: i64, dest_addr_reg: i64, offset_bytes: i64) void;
    fn in_reg_float(self: *CGJ, val: AsmVal) i64;
    fn in_reg(self: *CGJ, val: AsmVal) i64;
    fn load_imm(self: *CGJ, reg: i64, value: i64) void;  // TODO: this should check if adr is close enough since it statically knows the ip cause we're jitting.
    fn pop_stacked_args(self: *CGJ, amount_stacked: i64) void;
    fn add_free_args(self: *CGJ, taken_int_count: i64, taken_float_count: i64) void;
    fn branch_func(self: *CGJ, f: FuncId, with_link: bool) void ;
    fn load_fnptr_from_dispatch(self: *CGJ, f: FuncId, reg: i64) void;
    fn emit_stack_fixup(self: *CGJ) void;
    fn emit_return(self: *CGJ, sig: [] Prim) void;
    fn inst_intrinsic(self: *CGJ, op: Intrinsic) void;
    fn is_special_reg(self: *CGJ, reg: i64) bool;
    // do_floats:false if you're just trying to free up a gpr, not saving for a call.
    fn try_spill(self: *CGJ, i: usize, do_ints: bool, do_floats: bool) bool;
    fn patch_reserve_stack(self: *CGJ, reserve_stack: *u8) void;
    fn emit_header(self: *CGJ) *InstUnit;
    fn next_inst(self: *CGJ) *InstUnit;
    fn new(compile: CompilerRs, bc: *BcBackend) B;
    fn jump_to(self: *CGJ, to_ip: *InstUnit) void;
    // fn Load(ty) => {
    // fn JumpIf(f) => {
    // fn Unreachable() => {
    // fn PeekDup(skip) => {
    // fn CopyBytesToFrom(bytes) => {
    // fn CallFnPtr(f) => 
    fn emit_other_inst(self: *CGJ, inst: Bc) bool;
*/

fn pop_to_reg();
fn get_free_reg();
fn drop_reg();
fn spill_abi_stompable();
fn restore_all_registers();
fn get_free_reg_float();
fn pop_to_reg_with_offset();
fn create_slots();
fn drop_reg_float();
fn pop_to_reg_float();
fn in_reg_with_offset();
fn compile();
fn dyn_c_call();
fn emit_block();

// ehhh i don't like this but i also don't want to write it twice. 
// ive just reinvented inheritance. but only when you want it for typing less code, not dynamic dispatch. 
// CodeGenJit is an abstract class, the trait overload sets are abstract methods, 
// B is the extra fields you add when you extend it. EmitArm64 extends CodeGenJit. 
// i wonder if i just dont like it because it goes against my brainwashing

// important: be careful not to use the wrong arch's register constants in here. 
fn CodeGenJit($B: Type, $InstUnit: Type, $SP: i64, $negative_slots: bool) Type = {
    Emit :: @struct(
        program: CompilerRs,
        comp: CompCtx,
        bc: *BcBackend,
        asm: *Jitted,
        vars: List(?SpOffset),
        next_slot: SpOffset,
        release_stack: List(*InstUnit),
        state: BlockState,
        block_ips: List(?*InstUnit),
        clock: u16,
        body: *FnBody,
        forward_calls: List(FuncId),
        backend: B,
    );
    
    fn new(compile: CompilerRs, bc: *BcBackend, body: *FnBody) Emit = {
        (
            program = compile,
            comp = compile[][].comp(),
            bc = bc,
            asm = bc.jitted&,
            vars = list(temp()),
            next_slot = (id = 0),
            release_stack = list(temp()),
            state = (stack = list(temp()), free_reg = empty(), free_reg_float = empty(), open_slots = list(temp())),
            block_ips = list(temp()),
            clock = 0,
            body = body,
            forward_calls = list(temp()),
            backend = new(compile, bc),
        )
    }
    
    // Note: you can only use Self once because this doesn't reset fields.
    fn compile(self: *Emit, f: FuncId) rawptr = {
        func := self.comp.get_function(f);
        @assert(!func.body&.is(.JittedAarch64) && !func.body&.is(.ComptimeAddr), "func want body");
    
        self.bc_to_asm(f);
        if !self.backend.valid {
            return(rawptr.zeroed());
        };
        asm := self.asm.save_current(f);
        
        // TODO: assert that we don't generate code for a bunch of empty functions (just the size of prolouge + epiloge)
        //       when doing instantiations like `::tagged(T);`. should get deduplicated in bytecode or earlier. 
        // TODO: this is kinda dumb, I could just be a bit more abstract about how im emitting instructions and then also emit text assembler.
        if TRACE_ASM || func.get_flag(.LogAsm) || self.backend.interesting {
            addr := u32.int_from_ptr(asm.ptr);
            @println("% is at addr=%", self.comp.get_string(func.name), addr);
            @if(USE_LLVM_DIS) self.print_llvm_mc_dis(asm);
        };
        u32.raw_from_ptr(asm.ptr)
    }
    
    fn bc_to_asm(self: *Emit, f: FuncId) void = {
        ff := self.comp.get_function(f);
        @if(TRACE_ASM) @println("=== F% % ===", f.as_index(), self.comp.get_string(ff.name));
    
        func := self.body;
        self.next_slot = (id = 0);
        self.vars = .None.repeated(func.vars.len, temp());
        self.block_ips = .None.repeated(self.body.blocks.len, temp());
    
        self.asm.mark_start(f); // this puts currenet address in table so recursion can be direct
        ff.set_flag(.AsmDone);  // this tells us to check the table. TODO: better to just compare table to tried_to_call_uncompiled instead of checking this flag?
        reserve_stack := self.emit_header();
    
        // TODO: assert int_count <= 8 && float_count <= 8 or impl pass on stack.
        sig := func.signeture&;
        int_count, float_count := self.ccall_reg_to_stack(sig.args, sig.first_arg_is_indirect_return);
        // Any registers not containing args can be used.
        self.add_free_args(int_count, float_count);
    
        @if(TRACE_ASM) @println("entry: (%)", self.state.stack.items());
        self.emit_block(0, false);
        if !self.backend.valid {
            return();
        };
        self.patch_reserve_stack(reserve_stack);
    }
    
    fn emit_block(self: *Emit, b: i64, args_not_vstacked: bool) void = {
        if !self.backend.valid {
            return();
        };
        if(self.block_ips[b].is_some(), => return());
        self.block_ips[b] = (Some = self.next_inst());
        block := self.body.blocks[b]&;
    
        self.clock = block.clock;
        // TODO: handle normal args here as well.
        if args_not_vstacked {
            self.state.free_reg&.clear();
            self.state.free_reg_float&.clear();
            int_count, float_count := self.ccall_reg_to_stack(block.arg_prims, false);
            self.add_free_args(int_count, float_count);
        };
        @debug_assert(self.state.stack.len >= block.arg_prims.len);
        func := self.body;
        is_done := false;
        block := self.body.blocks[b]&;
        range(0, func.blocks[b].insts.len) { i |
            if(is_done, => return());
            inst := block.insts[i];
            is_done = self.emit_inst(b, inst, i);
            @if(TRACE_ASM) @println("%:% % => %", b, i, inst&, self.state.stack.items());
        };
        @debug_assert(is_done, "emitted all blocks but no function terminator");
    }
    
    fn emit_inst(self: *Emit, b_idx: i64, inst: Bc, i: usize) bool = {
        if !self.backend.valid {
            return (true);  // TODO: remove
        };
        // TODO: bring this back 
        //if TRACE_ASM.or(self.log_asm_bc) {
        //    ins := self.asm.offset_words(self.asm.current_start, self.asm.next) - 1;
        //    self.markers.push((format!("[{b_idx}:{i}] {inst:?}: {:?}", self.state.stack), ins as usize));
        //};
        @if(TRACE_ASM) @println("[%:%] %", b_idx, i, self.state.stack.items());
        @match(inst) {
            fn Nop() => ();
            fn SaveSsa(f) => {
                @assert(self.vars[f.id.zext()].is_none(), "SaveSsa missing");
                slot := self.create_slots(8);
                reg := self.pop_to_reg();
                self.store_u64(reg, SP, slot.id);
                self.drop_reg(reg);
                self.vars[f.id.zext()] = (Some = slot);
            }
            fn LoadSsa(f) => {
                // debug_assert!(self.body.is_ssa_var.get(id as usize));
                slot := self.vars[f.id.zext()].unwrap();
                self.state.stack&.push(Spill = slot);
            }
            fn AddrVar(f) => {
                idx: i64 = f.id.zext();
                if self.vars[idx] { (slot: SpOffset) |
                    self.state.stack&.push(Increment = (
                        reg = SP,
                        offset_bytes = slot.id,
                    ));
                } else {
                    ty := self.body.vars[idx];
                    slot := self.create_slots(ty.size);
                    self.vars[idx] = (Some = slot);
                    self.state.stack&.push(Increment = (
                        reg = SP,
                        offset_bytes = slot.id,
                    ));
                };
            }
            fn GetCompCtx() => {
                addr := ptr_cast_unchecked(Compiler, i64, self.comp.data&)[];
                self.state.stack&.push(Literal = addr);
            }
            fn LastUse(f) => {
                slot := self.vars[f.id.zext()]; 
                if slot { slot |
                    ty := self.body.vars[f.id.zext()];
                    self.drop_slot(slot, ty.size);
                };
            }
            fn NoCompile() => {
                func := self.comp.get_function(self.body.func);
                @panic("tried to compile unreachable block in %", self.comp.get_string(func.name));
            }
            fn PushConstant(f) => self.state.stack&.push(Literal = f.value);
            fn PushGlobalAddr(id) => {
                ptr, _ := self.comp.get_baked(id)[];
                self.state.stack&.push(Literal = ptr.int_from_rawptr());
            }
            fn GetNativeFnPtr(fid) => {
                // TODO: use adr+adrp instead of an integer. but should do that in load_imm so it always happens.
    
                if self.program[][].get_fn_old(fid) { (ptr: rawptr) |
                    self.state.stack&.push(Literal = ptr.int_from_rawptr());
                } else {
                    reg := self.get_free_reg();
                    self.asm.extend_blanks(fid);
                    self.load_fnptr_from_dispatch(fid, reg);
                    self.state.stack&.push(Increment = (reg = reg, offset_bytes = 0));
                };
            }
            fn Goto(f) => {
                idx: i64 = f.ip.id.zext();
                block := self.body.blocks[idx]&;
                
                // TODO: fix ending with unreachable() :block_arg_count_wrong_unreachable
                //debug_assert_eq!(slots, block.arg_slots);
                if block.incoming_jumps == 1 {
                    @debug_assert(self.block_ips[idx].is_none());
                    self.emit_block(idx, false);
                } else {
                    amount_stacked := self.stack_to_ccall_reg(block.arg_prims, false);
                    @debug_assert_eq(amount_stacked, 0, "TODO: handle stacked args for '=>' call");
                    self.spill_abi_stompable();
                    if self.block_ips[idx] { (to_ip: *InstUnit) |
                        // we've already emitted it, so we know where it is, so just go there. 
                        self.jump_to(to_ip);
                    } else {
                        // If we haven't emitted it yet, it will be right after us, so just fall through.
                        self.emit_block(idx, true);
                    };
                };
                return(true);
            }
            fn CallDirect(f) => {
                sig := self.body.decode_sig(f.sig);
                // TODO: use with_link for tail calls. need to "Leave holes for stack fixup code." like below
                // TODO: if you already know the stack height of the callee, you could just fixup to that and jump in past the setup code. but lets start simple.
                self.dyn_c_call(sig) {
                    if(f.tail, => self.emit_stack_fixup());
                    self.branch_func(f.f, !f.tail)
                };
                return(f.tail);
            }
            fn Intrinsic(op) => self.inst_intrinsic(op);
            fn Ret0()     => return(self.emit_return(empty()));
            fn Ret1(prim) => return(self.emit_return(@slice(prim)));
            fn Ret2(f)    => return(self.emit_return(@slice(f._0, f._1)));
            // Note: we do this statically, it wont get actually applied to a register until its needed because loads/stores have an offset immediate.
            fn IncPtrBytes(bytes) => {
                ::?*AsmVal;
                val: *AsmVal = self.state.stack&.last().unwrap();
                @match(val) {
                    fn Increment(f) => {
                        f.offset_bytes += bytes;
                    }
                    fn Literal(v) => {
                        v[] += bytes;
                    }
                    fn Spill(_) => {
                        // TODO: should it hold the add statically? rn it does the load but doesn't need to restore because it just unspills it.
                        reg, offset_bytes := self.pop_to_reg_with_offset();
                        self.state.stack&.push(Increment = (
                            reg = reg,
                            offset_bytes = offset_bytes + bytes,
                        ));
                    }
                    fn FloatReg(_) => panic("ICE: don't try to gep a float");
                };
            }
            fn StorePost(ty) => {
                addr := self.state.stack&.pop().expect("enough stack for store");
                value := self.state.stack&.pop().expect("enough stack for store");
                self.emit_store(addr, value, ty);
            }
            fn StorePre(ty) => {
                value := self.state.stack&.pop().expect("enough stack for store");
                addr := self.state.stack&.pop().expect("enough stack for store");
                self.emit_store(addr, value, ty);
            }
            fn Snipe(skip) => {
                index := self.state.stack.len - skip.zext() - 1;
                @match(self.state.stack&.ordered_remove(index).unwrap()) {
                    fn Increment(f) => self.drop_reg(f.reg);
                    fn Literal(_)   => ();
                    fn Spill(_)     => ();
                    fn FloatReg(_)  => {
                        panic("unexpected float reg");
                    }
                };
            }
            fn Switch(i) => {
                cases := self.body.switch_payloads.index(i.zext());
                self.emit_switch(cases);
                return(true);
            }
            fn RotateForImmediateCallPtr() => { // first it just has to work 
                block := self.body.blocks.index(b_idx);
                next := block.insts[i + 1];
                assert(next&.is(.CallFnPtr), "bad RotateForImmediateCallPtr");
                sig := self.body.decode_sig(next.CallFnPtr.sig);
                arg_count: i64 = sig.arg_slots();
                stack := self.state.stack&;
                callee := stack.pop().unwrap();
                stack.insert(stack.len - arg_count, callee); 
            }
            @default => return(self.emit_other_inst(inst));
        };
    
        false
    }
    
    fn restore_all_registers(self: *Emit) void = 
        self.add_free_args(0, 0);

    fn spill_abi_stompable(self: *Emit) void = {
        range(0, self.state.stack.len) { i |
            self.try_spill(i, true, true);
        };
    }

    fn get_free_reg(self: *Emit) i64 = {
        or self.state.free_reg&.pop() {
            i := 0;
            while => i < self.state.stack.len && !self.try_spill(i, true, false) {
                i += 1;
            };
            if i == self.state.stack.len {
                if !self.backend.valid {
                    return(0);
                };
                panic("ICE: spill to stack failed");
            };
            self.state.free_reg&.pop().expect("to have registers (too many args?)")
        }
    }

    fn get_free_reg_float(self: *Emit) i64 = {
        or self.state.free_reg_float&.pop() {
            i := 0;
            while => i < self.state.stack.len && !self.try_spill(i, false, true) {
                i += 1;
            };
            if i == self.state.stack.len {
                if !self.backend.valid {
                    return(0);
                };
                panic("ICE: spill to stack failed!");
            };
            self.state.free_reg_float&.pop().expect("to have registers (too many args?)")
        } 
    }
    
    fn drop_reg(self: *Emit, reg: i64) void = {
        if !self.is_special_reg(reg) && self.backend.valid {
            @debug_assert(!self.state.free_reg&.contains(reg&), "drop r%", reg); // TODO: print bitset
            self.state.free_reg&.set(reg);
            if ZERO_DROPPED_REG {
                self.load_imm(reg, 0);
            };
        };
    }

    fn drop_reg_float(self: *Emit, reg: i64) void = {
        @debug_assert(!self.state.free_reg_float&.get(reg), "drop F%", reg);
        self.state.free_reg_float&.set(reg);
    }

    fn pop_to_reg(self: *Emit) i64 = {
        val := self.state.stack&.pop().unwrap();
        self.in_reg(val)
    }

    fn pop_to_reg_float(self: *Emit) i64 = {
        val := self.state.stack&.pop().unwrap();
        self.in_reg_float(val)
    }

    fn peek_to_reg_with_offset(self: *Emit) Ty(i64, i64) = {
        val := self.state.stack&.last().unwrap()[];
        self.in_reg_with_offset(val)
    }

    fn in_reg_with_offset(self: *Emit, val: AsmVal) Ty(i64, i64) = {
        @match(val) {
            fn Increment(f) => (f.reg, f.offset_bytes);
            fn Literal(x) => {
                r := self.get_free_reg();
                self.load_imm(r, x);
                (r, 0)
            }
            fn Spill(slot) => {
                r := self.get_free_reg();
                self.load_u64(r, SP, slot.id);
                (r, 0)
            }
            fn FloatReg(_) => panic("ICE: in_reg_with_offset on a float register");
        }
    }

    fn pop_to_reg_with_offset(self: *Emit) Ty(i64, i64) = {
        res := self.peek_to_reg_with_offset();
        self.state.stack&.pop().unwrap();
        res
    }

    fn create_slots(self: *Emit, bytes: u16) SpOffset = {
        bytes = (bytes + 7) / 8 * 8; // TODO: use alignment instead of just wasting space :aligned_stack_slots
        enumerate self.state.open_slots { i, slot |
            if self.clock >= slot.clock && slot.size == bytes {
                slot := self.state.open_slots&.unordered_remove(i).unwrap();
                return(slot.at); 
            };
        };
    
        @if (negative_slots) {
            self.next_slot.id -= bytes.zext();
        };
        made := self.next_slot;
        
        @if (!negative_slots) {
            self.next_slot.id += bytes.zext();
        };
        made
    }

    // Note: comp_ctx doesn't push the ctx here, bc needs to do that. this it just does the call with an extra argument.
    /// <arg:n> -> <ret:m>
    fn dyn_c_call(self: *Emit, sig: *PrimSig, $do_call: @Fn() void) void = {
        amount_stacked := self.stack_to_ccall_reg(sig.args, sig.first_arg_is_indirect_return);
        self.spill_abi_stompable();
        self.state.free_reg&.clear();
        self.state.free_reg_float&.clear();
        do_call();
        self.pop_stacked_args(amount_stacked);
    
        ::if_opt(Prim, i64);
        ::if_opt(Prim, Ty(i64, i64));
        int_countt, float_count := if sig.ret1 { (fst: Prim) |
            if sig.ret2 { (snd: Prim) |
                self.ccall_ret_reg_to_stack(@slice(fst, snd));
                ints := fst.int_count() + snd.int_count();
                @as(Ty(i64, i64)) (ints, 2 - ints) // TODO: annoying that you need the type here. 
            } else {
                self.ccall_ret_reg_to_stack(@slice(fst));
                ints := fst.int_count();
                (ints, 1 - ints)
            }
        } else {
            (0, 0)
        };
    
        // now the extras are usable again.
        self.add_free_rets(int_countt, float_count);
    }

    fn drop_slot(self: *Emit, slot: SpOffset, bytes: u16) void = {
        bytes = (bytes + 7) / 8 * 8; // TODO: use alignment instead of just wasting space :aligned_stack_slots
        self.state.open_slots&.push((at = slot, size = bytes, clock = self.clock)); // TODO: keep this sorted by count?
        // TODO: if ZERO_DROPPED_SLOTS
    }

    // TODO: do i need to be using this instead? -- Jul 22
    fn drop_if_unused(self: *Emit, register: i64) void = {
        for self.state.stack { r |
            @if_let(r) fn Increment(f) => {
                if(f.reg == register, => return());
            };
        };
        self.drop_reg(register);
    }
    
    Emit
}

::display_slice(AsmVal);
::DeriveFmt(AsmVal);
::DeriveFmt(IncReg);
::DeriveFmt(SpOffset);
::if(AsmVal);
::display_slice(i64);
::enum(CallConv);
