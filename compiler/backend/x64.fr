// TODO: im not following the calling convention for structs correctly. 

EmitX64    :: CodeGenJit(X64AsmData, u8, @as(i64) X86Reg.rbp, @as(i64) X86Reg.rax, true);
X64AsmData :: @struct(asm_out: List(u8), valid := true, interesting := false);

fn new(compile: CompilerRs, bc: *BcBackend) X64AsmData #once = (
    asm_out = (
        maybe_uninit = (ptr = bc.jitted.next.as_u8(), len = 1.shift_left(20)), // TODO: use real cap
        len = 0,
        gpa = panicking_allocator, // no growing!
    ),
);

x64_reserve_stack_encoding_size :: 7;
x64_release_stack_encoding_size :: 7;
x64_prelude_reserved_space :: 16;

// :StackX64
fn emit_header(self: *EmitX64) *u8 #once = {
    // ret addr is at the top of the stack, so we can just leave it there. 
    // TODO: deal with stack args somehow
    self.next_slot.id -= x64_prelude_reserved_space;
    self.out().encode_op_reg(PrimaryOp.PushBase, X86Reg.rbp);
    self.out().encode_bin(PrimaryOp.MovReg, X86Reg.rbp, X86Reg.rsp);
    self.blank_patch(x64_reserve_stack_encoding_size)
}

// :StackX64
fn patch_reserve_stack(self: *EmitX64, reserve_stack: *u8) void #once = {
    stack_size := -self.next_slot.id;
    extra := stack_size.abs().mod(16);
    if extra != 0 {
        // doesn't matter on macos but clang targeting linux assumes the stack is 16 byte aligned (it uses 128 bit xmm instructions for faster copies)
        stack_size += 16 - extra; // play by the rules
    };
    
    self.write_patch(reserve_stack, x64_reserve_stack_encoding_size) { patch |
        patch.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -stack_size);
    };

    for self.release_stack { it |
        self.write_patch(it, x64_release_stack_encoding_size) { patch |
            patch.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, stack_size);
        };
    };
    
    // TODO: you don't need to do this
    //       it's just to keep Jitted.next as an aligned u32 because it made sense for arm
    len, extra := self.backend.asm_out.len.div_mod(4);
    _ := self.blank_patch(4 - extra);  // we must of already returned by now
    self.bc.jitted.next = self.bc.jitted.next.offset(len + 1);
}

// :StackX64
fn emit_stack_fixup(self: *EmitX64) void = {
    self.release_stack&.push(self.blank_patch(x64_release_stack_encoding_size));
    self.out().encode_op_reg(PrimaryOp.PopBase, X86Reg.rbp);
}

::List(X86Reg);
x64_int_args :: clone(@slice(X86Reg.rdi, X86Reg.rsi, X86Reg.rdx, X86Reg.rcx, X86Reg.r8, X86Reg.r9), ast_alloc()).items();
x64_int_rets :: clone(@slice(X86Reg.rax, X86Reg.rdx), ast_alloc()).items();
FLOAT_ARG_REG_COUNT :: 8;

fn add_free_args(self: *EmitX64, taken_int_count: i64, taken_float_count: i64) void = {
    if taken_int_count < x64_int_args.len() {
        for x64_int_args.slice(taken_int_count, x64_int_args.len()) { r |
            self.state.free_reg&.set(@as(i64) r);
        };
    };
    self.state.free_reg&.set(@as(i64) X86Reg.rax);
    
    range(taken_float_count, FLOAT_ARG_REG_COUNT) { i | 
        self.state.free_reg_float&.set(i);
    };
}

fn add_free_rets(self: *EmitX64, taken_int_count: i64, taken_float_count: i64) void = {
    for x64_int_rets.slice(taken_int_count, x64_int_rets.len()) { r |
        self.state.free_reg&.set(@as(i64) r);
    };
    
    for (@slice(X86Reg.rdi, X86Reg.rsi, X86Reg.rcx, X86Reg.r8, X86Reg.r9)) { r | 
        self.state.free_reg&.set(@as(i64) r);
    };
    
    range(taken_float_count, FLOAT_ARG_REG_COUNT) { i | 
        self.state.free_reg_float&.set(i);
    };
}

fn emit_other_inst(self: *EmitX64, inst: Bc) bool #once = {
    @match(inst) {
        fn Unreachable() => {
            self.out().push(@as(u8) PrimaryOp.Int3);
            return(true);
        }
        fn PeekDup(skip) => {
            val := self.state.stack[self.state.stack.len - skip.zext() - 1];
            @match(val) {
                fn Increment(f) => {
                    reg := @as(X86Reg) f.reg;
                    if reg == .rbp {
                        self.state.stack&.push(val);
                    } else {
                        new := self.get_free_reg();
                        dest := @as(X86Reg) new;
                        if self.backend.valid {
                            @debug_assert_ne(reg, dest, "shouldn't have been free"); // TODO
                        };
                        self.out().encode_bin(PrimaryOp.MovReg, dest, reg);
                        self.state.stack&.push(Increment = (reg = new, offset_bytes = f.offset_bytes));
                    };
                }
                fn Literal(_)     => self.state.stack&.push(val);
                // TODO: this means we don't know when we can drop stack slots... gc them... lol
                fn Spill(_)       => self.state.stack&.push(val); 
                fn FloatReg(old_reg)  => {
                    new_reg := self.get_free_reg_float();
                    self.mov_float_from_float(new_reg, old_reg);
                    self.state.stack&.push(FloatReg = new_reg);
                }
            };
        }
        fn Load(ty) => {
            addr, offset_bytes := self.pop_to_reg_with_offset(); // get the ptr

            dest := self.get_free_reg();
            @match(ty) {
                fn I64() => self.load_u64(dest, addr, offset_bytes);
                fn P64() => self.load_u64(dest, addr, offset_bytes);
                fn F64() => self.load_u64(dest, addr, offset_bytes);  // TODO: load into float reg
                fn F32() => { // TODO: load into float reg
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    reg := @as(X86Reg) dest;
                    addr := @as(X86Reg) addr;
                    self.out().encode_non_sp_offset(false, PrimaryOp.MovRegLoad, reg, addr, offset_bytes);
                }
                fn I32() => {
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    reg := @as(X86Reg) dest;
                    addr := @as(X86Reg) addr;
                    self.out().encode_non_sp_offset(false, PrimaryOp.MovRegLoad, reg, addr, offset_bytes);
                }
                fn I16() => {
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    reg := @as(X86Reg) dest;
                    addr := @as(X86Reg) addr;
                    self.out().push(operand_16bit_prefix);
                    self.out().encode_non_sp_offset(false, PrimaryOp.MovRegLoad, reg, addr, offset_bytes);
                }
                fn I8() => {
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    self.encode_non_sp_offset(PrimaryOp.MovByteLoad, dest, addr, offset_bytes);
                }
            };
            self.state.stack&.push(Increment = (reg = dest, offset_bytes = 0));
            self.drop_reg(addr);
        }
        // TODO: not always a call!
        fn CopyBytesToFrom(bytes) => {
            // copy-paste from arm. TODO: could move to jit if i add call_const(addr)
            self.state.stack&.push(Literal = bytes.zext());
            self.stack_to_ccall_reg(@slice(Prim.P64, Prim.P64, Prim.I64), false);
            self.spill_abi_stompable();
            self.state.free_reg&.clear();
            self.state.free_reg_float&.clear();
            
            // TODO: fancy version of this?
            addr: rawptr : fn(dest: *u8, src: *u8, len: i64) void = {
                range(0, len) { i |
                    dest.offset(i)[] = src.offset(i)[];
                };
            };
            self.load_imm(@as(i64) X86Reg.r11, addr.int_from_rawptr());
            encode_call_reg(self.out(), ModrmMode.Direct, X86Reg.r11);
            self.add_free_rets(0, 0);
        }
        fn CallFnPtr(f) => {
            // TODO: tail call
            sig := self.body.decode_sig(f.sig);
            self.dyn_c_call(sig, f.context) {
                // dyn_c_call will have popped the args, so now stack is just the pointer to call
                callee := self.state.stack&.pop().expect("enough stack for call");
                // TODO: this does redundant spilling every time!
                @match(callee) {
                    fn Literal(p) => self.load_imm(@as(i64) X86Reg.r11, p);
                    fn Spill(offset) => self.load_u64(@as(i64) X86Reg.r11, @as(i64) X86Reg.rbp, offset.id);
                    @default => {
                        panic("ICE: unspilled register");
                    };
                };
                encode_call_reg(self.out(), ModrmMode.Direct, X86Reg.r11);
            };
        }
        fn JumpIf(f) => {
            cond := self.pop_to_reg();
            @debug_assert_eq(f.slots, 0); // emit_bc doesn't do this
            self.spill_abi_stompable();
            self.out().encode_bin(PrimaryOp.Test, @as(X86Reg) cond, @as(X86Reg) cond);
            // The offset is from **the end** of the jump instruction
            jump_encode_size :: 6;
            patch := self.blank_patch(jump_encode_size);
            jump_index := self.backend.asm_out.len;
            
            // branch if zero so true before false
            // we only do one branch so true block must be directly after the check.
            // this is the only shape of flow graph that its possible to generate with my ifs/whiles.
            @debug_assert(self.block_ips[f.true_ip.id.zext()].is_none());

            self.drop_reg(cond);
            state := self.state&.clone();
            self.emit_block(f.true_ip.id.zext(), true);
            if !self.backend.valid {
                return(true);
            };
            self.state = state;
            false_index := self.backend.asm_out.len;
            @debug_assert(self.block_ips[f.false_ip.id.zext()].is_none());
            self.emit_block(f.false_ip.id.zext(), true);
            if !self.backend.valid {
                return(true);
            };
            
            offset := false_index - jump_index;
            @debug_assert(offset.abs() < 1.shift_left(31), "cant jump that far");
            self.write_patch(patch, jump_encode_size) { patch |
                patch.encode_jmp(.e, offset);
            };
            return(true); 
        }
        @default => panic("unreachable: unhandled bc op");
    };

    false
}

fn blank_patch(self: *EmitX64, size: i64) *u8 = {
    start := self.next_inst();
    range(0, size) { _ |
        self.out().push(@as(u8) PrimaryOp.Int3);
    };
    start
}

fn write_patch(self: *EmitX64, start: *u8, size: i64, $f: @Fn(out: *List(u8)) void) void = {
    patch: List(u8) = (maybe_uninit = (ptr = start, len = size), len = 0, gpa = panicking_allocator);
    f(patch&);
    @debug_assert_eq(patch.len, size);
}

fn emit_switch(self: *EmitX64, cases: *RsVec(SwitchPayload)) void = {
    @debug_assert(cases.len >= 2, "switch op should have default + >=2 branches");
    inspect := self.pop_to_reg();
    // TODO: jump table? binary search? 
    // TODO: is it better to have all the jumps at the top? 
    normal_branches, default := cases.decode_switch();
    each normal_branches { option |
        self.spill_abi_stompable(); // TODO: this is dumb but you need to make sure the different branches don't spill things in different places. 
       
        jump_encode_size :: 6;
        assert(option.value < 4096 && option.value >= 0, "value must fit in u12"); // TODO: use the right number types sooner?
        self.out().encode_cmp_imm32(@as(X86Reg) inspect, option.value);
        
        patch := self.blank_patch(jump_encode_size); 
        jump_ip := self.backend.asm_out.len;
        
        @debug_assert(self.block_ips[option.block.id.zext()].is_none(), "I only generate simple control flow");

        state := self.state&.clone();
        self.drop_reg(inspect); // Inside the new block
        self.emit_block(option.block.id.zext(), true);
        if !self.backend.valid {
            return();
        };
        self.state = state;
        
        offset := self.backend.asm_out.len - jump_ip;
        @debug_assert(offset.abs() < 1.shift_left(31), "cant jump that far");
        self.write_patch(patch, jump_encode_size) { patch |
            patch.encode_jmp(.ne, offset);
        };
    };
    self.drop_reg(inspect);
    assert(default.is_some(), "i always have a default branch currently");
    default_block := default.unwrap();
    @debug_assert(self.block_ips[default_block.id.zext()].is_none(), "I only generate simple control flow!");
    self.emit_block(default_block.id.zext(), true);
}

// TODO: better error message if you do Result(void, str) // (lowercase)
fn branch_func(self: *EmitX64, f: FuncId, with_link: bool) void #once = {
    // TODO: im not super confident im doing with_link right but it seems to work. 

    // If we already emitted the target function, can just branch there directly.
    if self.program[][].get_fn_old(f) { bytes |
    //if self.program[][].get_fn_callable(f) { bytes | // no cause this would bump while we're writing
        start := self.backend.asm_out.len;
        // The offset is from **the end** of the jump instruction
        CALL_ENCODE_SIZE :: 5;
        n := u8.int_from_ptr(self.backend.asm_out.maybe_uninit.ptr) + self.backend.asm_out.len + CALL_ENCODE_SIZE;
        offset := bytes.int_from_rawptr() - n;
        if offset.abs() < 1.shift_left(31) {
            @if(TRACE_ASM) println("backwards call");
            ::if(PrimaryOp);
            op := if(with_link, => PrimaryOp.CallImm32, => PrimaryOp.JmpImm32);
            encode_imm32_only(self.out(), op, offset); 
            end := self.backend.asm_out.len;
            @debug_assert_eq(end - start, CALL_ENCODE_SIZE, "encoding wrong size???");
            return(); 
        };
    };
    
    r := X86Reg.r11;
    self.load_fnptr_from_dispatch(f, @as(i64) r);
    if with_link {
        encode_call_reg(self.out(), ModrmMode.Direct, r);
    } else {
        encode_extended_op(self.out(), ModrmMode.Direct, 0xff, 0b0100, r);
    };
}


fn emit_return(self: *EmitX64, sig: [] Prim) bool = {
    int_count := 0;
    float_count := 0;
    enumerate sig { i, ty |
        stack_index := self.state.stack.len - sig.len + i;
        if ty[].is_float() {
            goal := float_count;
            self.snipe_to_specific_reg_float(float_count, stack_index);
            float_count += 1;
        } else {
            goal := x64_int_rets[int_count];
            self.snipe_to_specific_reg(goal, stack_index);
            int_count += 1;
        };
    };
    self.state.stack.len -= int_count + float_count;
    self.emit_stack_fixup();
    self.out().push(@as(u8) PrimaryOp.Ret);
    true
}

fn ccall_ret_reg_to_stack(self: *EmitX64, types: [] Prim) void = {
    int_count := 0;
    float_count := 0;
    for types { ty |
        if ty.is_float() {
            goal := float_count;
            self.state.free_reg_float&.unset(goal); 
            self.state.stack&.push(FloatReg = goal);
            float_count += 1;
        } else {
            goal := @as(i64) @as(X86Reg) x64_int_rets[int_count];
            self.state.free_reg&.unset(goal); 
            self.state.stack&.push(Increment = (reg = goal, offset_bytes = 0));
            int_count += 1;
        };
    };
}

// :StackX64
// when you're out of registers, they come in on the stack, but in reverse order. 
// last args are pushed first, so first stacked args are closest to sp (they imagine you popping them off in order).
// TODO: this wont work for internal block calls because it will push them to the end but we try to read them where our args are. 
//       but i think that actually can't happen until i allow multiple calls to the same `=>` function,
//       because it the block will always not be emitted yet so you can just fall through with the same v-stack. 
//       TODO: assert that!
fn ccall_reg_to_stack(self: *EmitX64, types: [] Prim, extra_special_ret: bool) Ty(i64, i64) = {
    int_count := if(extra_special_ret, => 1, => 0); // x64 just passes the ret addr as first arg. 
    // TODO: i think it would be cleaner if it was included in the prims and arm just had to slice forward the 
    //       array before looping. instead of both having to do strange shit? -- Sep 12
    if extra_special_ret { 
        self.state.stack&.push(Increment = (reg = @as(i64) x64_int_args[0], offset_bytes = 0));
    };
    
    rbp_offset := 16; // ([high], <args>, return address, saved rbp, locals and spills, [low])
    float_count := 0;
    for types { type | 
        if type.is_float() {
            if float_count < FLOAT_ARG_REG_COUNT {
                self.state.stack&.push(FloatReg = float_count);
            } else {
                self.state.stack&.push(Spill = (id = rbp_offset)); // positive!
                rbp_offset += 8;
            };
            float_count += 1;
        } else {
            if int_count < x64_int_args.len() {
                self.state.stack&.push(Increment = (reg = @as(i64) x64_int_args[int_count], offset_bytes = 0));
            } else {
                self.state.stack&.push(Spill = (id = rbp_offset)); // positive!
                rbp_offset += 8;
            };
            int_count += 1;
        };
    };
    
    (int_count, float_count)
}

fn stack_to_ccall_reg(self: *EmitX64, types: [] Prim, extra_special_ret: bool) i64 = {
    int_count := if(extra_special_ret, => 1, => 0); // x64 just passes the ret addr as first arg. 
    if extra_special_ret {
        idx := self.state.stack.len - types.len - 1;
        self.snipe_to_specific_reg(x64_int_args[0], idx);
    };
    total := types.len + int_count;
    @debug_assert(total <= self.state.stack.len, "not enough stack for args");
    float_count := 0;
    reverse_stacked: List(i64) = list(temp());
    enumerate types { i, type | 
        stack_index := self.state.stack.len - types.len + i;
        if type[].is_float() {
            if float_count < FLOAT_ARG_REG_COUNT {
                self.snipe_to_specific_reg_float(float_count, stack_index);
            } else {
                reverse_stacked&.push(stack_index);
            };
            float_count += 1;
        } else {
            if int_count < x64_int_args.len() {
                self.snipe_to_specific_reg(x64_int_args[int_count], stack_index);
            } else {
                reverse_stacked&.push(stack_index);
            };
            int_count += 1;
        };
    };
    
    // :StackX64
    amount_stacked := reverse_stacked.len * 8;
    if amount_stacked.mod(16) != 0 {
        amount_stacked += 8;
        self.out().encode_op_reg(PrimaryOp.PushBase, X86Reg.r11); // junk
        // I can't quite think of a test that would need this right now, 
        // becuase it would only matter when calling between jitted and aot code, 
        // where llvm happended to use a 16 byte sse instruction with the stack, 
        // which seems to only happen when targeting linux. 
        // But im pretty sure this will save a lot of confusion at some point. 
    };
    
    for_rev reverse_stacked { stack_index | 
        self.snipe_to_specific_reg(X86Reg.r11, stack_index);
        self.out().encode_op_reg(PrimaryOp.PushBase, X86Reg.r11);
    };
    
    self.state.stack.len -= int_count + float_count;
    amount_stacked
}

fn pop_stacked_args(self: *EmitX64, amount_stacked: i64) void = {
    if amount_stacked != 0 {
        self.out().encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, amount_stacked);
    };
}

fn pop_to_specific_reg(self: *EmitX64, reg: X86Reg) void = {
    val := self.state.stack&.pop().unwrap();
    self.in_specific_reg(reg, val);
}

fn pop_to_specific_reg_float(self: *EmitX64, reg: i64) void = {
    val := self.state.stack&.pop().unwrap();
    self.in_specific_reg_float(reg, val);
}

fn snipe_to_specific_reg(self: *EmitX64, reg: X86Reg, idx: i64) void = {
    val := self.state.stack[idx];
    self.state.stack[idx] = (Literal = 0); // don't try to spill yourself
    self.in_specific_reg(reg, val);
}

fn snipe_to_specific_reg_float(self: *EmitX64, reg: i64, idx: i64) void = {
    val := self.state.stack[idx];
    self.state.stack[idx] = (Literal = 0); // don't try to spill yourself
    self.in_specific_reg_float(reg, val);
}

fn in_specific_reg(self: *EmitX64, reg: X86Reg, val: AsmVal) void = {
    @match(val) {
        fn Literal(x) => {
            self.ensure_specific_free(reg);
            self.load_imm(@as(i64) reg, x);
        }
        fn Increment(f) => {
            old_reg := @as(X86Reg) f.reg;
            if old_reg != reg {
                self.ensure_specific_free(reg);
                self.out().encode_bin(PrimaryOp.MovReg, reg, old_reg);
                self.drop_reg(f.reg);
            };
            if f.offset_bytes != 0 {
                self.out().encode_imm(PrimaryOp.AddImm32, reg, f.offset_bytes);
            };
        }
        fn Spill(slot) => {
            self.ensure_specific_free(reg);
            self.load_u64(@as(i64) reg, @as(i64) X86Reg.rbp, slot.id);
        }
        fn FloatReg(float) => {
            self.ensure_specific_free(reg);
            self.mov_int_from_float(reg, float);
            self.drop_reg_float(float);
        }
    };
}

fn in_specific_reg_float(self: *EmitX64, reg: i64, val: AsmVal) void = {
    @match(val) {
        fn Literal(x) => {
            self.ensure_specific_free_float(reg);
            int := X86Reg.r11;
            self.load_imm(@as(i64) int, x);
            self.mov_float_from_int(reg, int);
        }
        fn Increment(f) => {
            self.ensure_specific_free_float(reg);
            int := self.in_reg(val);
            self.mov_float_from_int(reg, @as(X86Reg) int);
            self.drop_reg(int);
        }
        fn Spill(slot) => {
            self.ensure_specific_free_float(reg);
            self.load_f64(reg, @as(i64) X86Reg.rbp, slot.id);
        }
        fn FloatReg(old_reg) => {
            if old_reg != reg {
                self.ensure_specific_free_float(reg);
                self.mov_float_from_float(reg, old_reg);
                self.drop_reg_float(old_reg);
            };
        }
    };
}

fn ensure_specific_free(self: *EmitX64, goal_r: X86Reg) void = {
    if(goal_r == X86Reg.r11, => return());
    goal := @as(i64) goal_r;
    if self.state.free_reg&.get(goal) { 
        self.state.free_reg&.unset(goal);
        return();
    };
    
    enumerate self.state.stack { stack_index, val |
        @if_let(val) fn Increment(f) => {
            if f.reg == goal {
                ok := self.try_spill(stack_index, true, false);
                @debug_assert(ok, "failed to spill specific");
                self.state.free_reg&.unset(goal);
                return();
            };
        };
    };
    
    @panic("ICE: lost a register %", goal_r);
}

fn ensure_specific_free_float(self: *EmitX64, goal: i64) void = {
    if self.state.free_reg_float&.get(goal) {
        self.state.free_reg_float&.unset(goal);
        return();
    };
    
    enumerate self.state.stack { stack_index, val |
        @if_let(val) fn FloatReg(f) => {
            if f[] == goal {
                ok := self.try_spill(stack_index, false, true);
                @debug_assert(ok, "failed to spill specific");
                self.state.free_reg_float&.unset(goal);
                return();
            };
        };
    };
    
    @panic("ICE: lost a register %", goal);
}


fn load_imm(self: *EmitX64, reg: i64, value: i64) void = {
    reg: X86Reg = @as(X86Reg) reg;
    fits_in_32 := value <= MAX_i32 && value >= MIN_i32;
    
    if fits_in_32 {
        encode_imm(self.out(), reg, value);
    } else {
        value: u64 = value.bitcast();
        self.out().encode_imm64(reg, value);
    };
}

fn emit_store(self: *EmitX64, addr: AsmVal, value: AsmVal, ty: Prim) void = { 
    addr_reg, offset := self.in_reg_with_offset(addr);
    value := self.in_reg(value);
    @match(ty) {
        fn I64() => self.store_u64(value, addr_reg, offset);
        fn P64() => self.store_u64(value, addr_reg, offset);
        fn F64() => self.store_u64(value, addr_reg, offset); // TODO: allow going through float reg
        fn I8() => self.encode_non_sp_offset(PrimaryOp.MovByte, value, addr_reg, offset);
        fn F32() => { // TODO: allow going through float reg
            reg := @as(X86Reg) value;
            addr := @as(X86Reg) addr_reg;
            self.out().encode_non_sp_offset(false, PrimaryOp.MovReg, reg, addr, offset);
        }
        fn I32() => {
            reg := @as(X86Reg) value;
            addr := @as(X86Reg) addr_reg;
            self.out().encode_non_sp_offset(false, PrimaryOp.MovReg, reg, addr, offset);
        }
        fn I16() => {
            reg := @as(X86Reg) value;
            addr := @as(X86Reg) addr_reg;
            self.out().push(operand_16bit_prefix);
            self.out().encode_non_sp_offset(false, PrimaryOp.MovReg, reg, addr, offset);
        }
    };
    self.drop_reg(value);
    self.drop_reg(addr_reg);
}

fn store_u64(self: *EmitX64, src_reg: i64, dest_addr_reg: i64, offset_bytes: i64) void = { 
    self.encode_non_sp_offset(PrimaryOp.MovReg, src_reg, dest_addr_reg, offset_bytes);
}

fn load_u64(self: *EmitX64, dest_reg: i64, src_addr_reg: i64, offset_bytes: i64) void = { 
    self.encode_non_sp_offset(PrimaryOp.MovRegLoad, dest_reg, src_addr_reg, offset_bytes);
}

fn encode_non_sp_offset(self: *EmitX64, op: PrimaryOp, reg: i64, addr: i64, offset_bytes: i64) void = { 
    reg := @as(X86Reg) reg;
    addr := @as(X86Reg) addr;
    if addr == X86Reg.rsp {
        self.backend.valid = false; 
        @eprintln("[TODO X64] cant encode");
        return();
    };
    if addr == X86Reg.rbp {
        @debug_assert(offset_bytes != 0, "shouldn't be using raw frame pointer");
    };
    self.out().encode_non_sp_offset(op, reg, addr, offset_bytes);
}

fn in_reg_float(self: *EmitX64, val: AsmVal) i64 = { 
    @match(val) {
        fn FloatReg(float) => float;
        @default => {
            r := self.get_free_reg_float();
            self.drop_reg_float(r);
            self.in_specific_reg_float(r, val);
            r
        };
    }
}

fn in_reg(self: *EmitX64, val: AsmVal) i64 = { 
    @match(val) {
        fn Increment(f) => {
            old := @as(X86Reg) f.reg;
            if old == X86Reg.rbp {
                f.reg = self.get_free_reg();
                self.out().encode_bin(PrimaryOp.MovReg, @as(X86Reg) f.reg, X86Reg.rbp);
            };
            if f.offset_bytes != 0 {
                self.out().encode_imm(PrimaryOp.AddImm32, @as(X86Reg) f.reg, f.offset_bytes);
            };
            f.reg
        }
        fn Literal(i) => {
            r := self.get_free_reg();
            self.load_imm(r, i);
            r
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, @as(i64) X86Reg.rbp, slot.id);
            r
        }
        fn FloatReg(float) => {
            int := self.get_free_reg();
            self.mov_int_from_float(@as(X86Reg) int, float);
            self.drop_reg_float(float);
            int
        }
    }
}

fn load_fnptr_from_dispatch(self: *EmitX64, f: FuncId, reg: i64) void = {
    self.forward_calls&.push(f); 
    self.load_imm(reg, self.asm.get_dispatch());
    self.load_u64(reg, reg, f.as_index() * 8);
}

fn is_special_reg(self: *EmitX64, reg: i64) bool = {
    reg.eq(@as(i64) X86Reg.rsp).or(reg == @as(i64) X86Reg.rbp)
}

fn try_spill(self: *EmitX64, i: usize, do_ints: bool, do_floats: bool) bool = { 
    v := self.state.stack[i];
    if do_ints {
        @if_let(v) fn Increment(f) => {
            if(f.reg == @as(i64) X86Reg.rbp, => return(false));
            
            // Note: this assumes we don't need to preserve the value in the reg other than for this one v-stack slot.
            slot := self.create_slots(8);
            @if(TRACE_ASM) @print("(spill (x% + %) -> [fp, %]) ", f.reg, f.offset_bytes, slot.id);
            if f.offset_bytes != 0 {
                self.out().encode_imm(PrimaryOp.AddImm32, @as(X86Reg) f.reg, f.offset_bytes);
            };
    
            self.store_u64(f.reg, @as(i64) X86Reg.rbp, slot.id);
            self.drop_reg(f.reg);
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    if do_floats {
        @if_let(v) fn FloatReg(freg) => {
            temp_reg := X86Reg.r11;
            slot := self.create_slots(8);
            self.store_f64(freg, @as(i64) X86Reg.rbp, slot.id);
            self.drop_reg_float(freg);
            self.state.stack[i] = (Spill = slot); // TODO: a test other than farm game that fails if you don't do this! -- Sep 12
            return(true);
        };
    };
    
    false
}

fn mov_float_from_int(self: *EmitX64, float: i64, int: X86Reg) void = {
    encode_binf(self.out(), precision_prefix, true, FloatOp.MovQTo, @as(u4) float, @as(u4) int);
}

fn mov_int_from_float(self: *EmitX64, int: X86Reg, float: i64) void = {
    encode_binf(self.out(), precision_prefix, true, FloatOp.MovQ, @as(u4) float, @as(u4) int);
}

fn mov_float_from_float(self: *EmitX64, dest: i64, src: i64) void = {
    encode_binf(self.out(), float_prefix, false, FloatOp.MovQ, @as(u4) dest, @as(u4) src);
}

// TODO: direct
fn store_f64(self: *EmitX64, float: i64, dest_addr_reg: i64, offset_bytes: i64) void = { 
    temp_reg := X86Reg.r11;
    mov_int_from_float(self, temp_reg, float);
    store_u64(self, @as(i64) temp_reg, dest_addr_reg, offset_bytes);
}

// TODO: direct
fn load_f64(self: *EmitX64, float: i64, src_addr_reg: i64, offset_bytes: i64) void = { 
    temp_reg := X86Reg.r11;
    load_u64(self, @as(i64) temp_reg, src_addr_reg, offset_bytes);
    mov_float_from_int(self, float, temp_reg);
}

// TODO: copy paste from call-short
fn jump_to(self: *EmitX64, to_ip: *u8) void = {
    start := self.backend.asm_out.len;
    // The offset is from **the end** of the jump instruction
    jump_encode_size :: 5;
    n := u8.int_from_ptr(self.next_inst()) + jump_encode_size;
    offset := u8.int_from_ptr(to_ip) - n;
    @debug_assert(offset.abs() < 1.shift_left(31), "what a big function you have");
    encode_imm32_only(self.out(), PrimaryOp.JmpImm32, offset); 
    end := self.backend.asm_out.len;
    @debug_assert_eq(end - start, jump_encode_size, "encoding wrong size???");
}

fn print_llvm_mc_dis(self: *EmitX64, asm: []u32) void #inline = {
    asm: []u8 = (ptr = asm.ptr.as_u8(), len = asm.len * 4);
    print_llvm_mc_dis(asm);
}

fn inst_intrinsic(self: *EmitX64, op: Intrinsic) void = { 
    // TODO: for add, include offset_bytes in the immediate if they fit or combine them and defer instead of doing 3 seperate adds.
    bin :: fn($do: @Fn(a: X86Reg, b: X86Reg) void) void => {
        snd := self.pop_to_reg();
        fst := self.pop_to_reg();
        do(@as(X86Reg) fst, @as(X86Reg) snd);
        self.drop_reg(snd);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    bin_cmp :: fn(cond: X86cc) => bin(fn(a, b) => {
        self.out().encode_bin(PrimaryOp.CmpReg, a, b);
        self.load_imm(@as(i64) a, 0); // setcc doesn't zero the whole register!
        self.out().encode_setcc(a, cond);
    });
    
    sane_bin :: fn(op: PrimaryOp) => bin(fn(a, b) => {
        self.out().encode_bin(op, a, b);
    });
    
    bin_rcx :: fn($do: @Fn(a: X86Reg) void) void => {
        self.pop_to_specific_reg(X86Reg.rcx);
        fst := self.pop_to_reg();
        do(@as(X86Reg) fst);
        self.drop_reg(@as(i64) X86Reg.rcx);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    dumb_trunc :: fn(size: i64) void => {
        self.state.stack&.push(Literal = 1.shift_left(size) - 1);
        sane_bin(PrimaryOp.AndReg);
    };
    
    // TODO: actually since i always produce the value, not jump, i could use CMPPD, but then i have to do VEX encoding if i want all the codes without needing to not it.          
    bin_fcmp :: fn(self: *EmitX64, cond: X86cc) = {
        snd := self.pop_to_reg_float();
        fst := self.pop_to_reg_float();
        a := self.get_free_reg();
        encode_binf(self.out(), precision_prefix, false, FloatOp.UComIsd, @as(u4) fst, @as(u4) snd);
        self.load_imm(a, 0); // setcc doesn't zero the whole register!
        self.load_imm(@as(i64) X86Reg.r11, 0); // TODO: everything is a garbage nightmare
        self.out().encode_setcc(@as(X86Reg) a, cond);
        self.out().encode_movcc(@as(X86Reg) a, X86Reg.r11, X86cc.p);  // TODO: everything is a garbage nightmare
        self.drop_reg_float(snd);
        self.drop_reg_float(fst);
        self.state.stack&.push(Increment = (reg = a, offset_bytes = 0));
    };
    
    binf :: fn($do: @Fn(a: u4, b: u4) void) void => {
        snd := self.pop_to_reg_float();
        fst := self.pop_to_reg_float();
        do(@as(u4) fst, @as(u4) snd);
        self.drop_reg_float(snd);
        self.state.stack&.push(FloatReg = fst);
    };
    
    @match(op) {
        fn Add()    => sane_bin(PrimaryOp.AddReg);
        fn Sub()    => sane_bin(PrimaryOp.SubReg);
        fn BitOr()  => sane_bin(PrimaryOp.OrReg);
        fn BitAnd() => sane_bin(PrimaryOp.AndReg);
        fn BitXor() => sane_bin(PrimaryOp.XorReg);
        fn Mul() => bin(fn(a, b) => {
            self.out().encode_bin2(TwoByteOp.IMul, b, a);
        });
        fn Div() => {  // the year is 1726, we have 4 registers, and 2 of them are reserved for division. 
            self.ensure_specific_free(.rax);
            self.ensure_specific_free(.rdx);
            divisor := self.pop_to_reg();
            self.drop_reg(@as(i64) X86Reg.rax);
            self.pop_to_specific_reg(X86Reg.rax);
            
            // :fun_with_x64_division
            // rdx has the high bits of an i128, so sign extend rax into rdx. 
            self.out().push(0b01001000); // REX W=1
            self.out().push(@as(u8) PrimaryOp.SignExtendAxToDxAx);
            
            self.out().encode_extended_op(ModrmMode.Direct, 0xf7, 0b0111, @as(X86Reg) divisor);
            self.drop_reg(@as(i64) X86Reg.rdx); // TODO: expose as div_mod
            self.drop_reg(@as(i64) divisor);
            self.state.stack&.push(Increment = (reg = @as(i64) X86Reg.rax, offset_bytes = 0));
        }
        fn BitNot() => {
            fst := self.pop_to_reg();
            self.out().encode_not(@as(X86Reg) fst);
            self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
        }
        fn ShiftLeft() => bin_rcx(fn(a) => {
            self.out().encode_extended_op(ModrmMode.Direct, @as(u8) PrimaryOp.ShiftBase, 0b0100, a);
        });
        fn ShiftRightLogical() => bin_rcx(fn(a) => {
            self.out().encode_extended_op(ModrmMode.Direct, @as(u8) PrimaryOp.ShiftBase, 0b0101, a);
        });
        fn ShiftRightArithmetic() => bin_rcx(fn(a) => {
            self.out().encode_extended_op(ModrmMode.Direct, @as(u8) PrimaryOp.ShiftBase, 0b0111, a);
        });
        fn Eq()  => bin_cmp(.e);
        fn Ne()  => bin_cmp(.ne);
        fn Le()  => bin_cmp(.le);
        fn Ge()  => bin_cmp(.ge);
        fn Lt()  => bin_cmp(.l);
        fn Gt()  => bin_cmp(.g);
        fn IntToPtr() => (); // no-op
        fn PtrToInt() => (); // no-op
        fn Trunc64To32() => dumb_trunc(32);
        fn Trunc64To16() => dumb_trunc(16);
        fn Trunc64To8()  => dumb_trunc(8);
        fn Trunc32To16() => dumb_trunc(16);
        fn Trunc32To8()  => dumb_trunc(8);
        fn Trunc16To8()  => dumb_trunc(8);
        // TODO: this about how signed numbers are represented. 
        fn SignExtend32To64() => (); // TODO: WRITE A TEST THAT FAILS BECAUSE OF THIS
        fn ZeroExtend32To64() => (); // no-op
        fn ZeroExtend16To64() => (); // no-op
        fn ZeroExtend8To64()  => (); // no-op
        fn ZeroExtend16To32() => (); // no-op
        fn ZeroExtend8To32()  => (); // no-op
        fn ZeroExtend8To16()  => (); // no-op    
        // These are noops because we do the translation when you try to use the register anyway. 
        // Once i implement load/store/imm properly we could do the translation here and ICE if its required elsewhere (since bc tracks prims).
        fn IntToFloatBits() => (); // ^
        fn FloatToIntBits() => (); // ^
        // Note: floats use the unsigned condition codes. i guess with parity being ordered-ness
        // TODO: doesn't work for is_nan tho. i think that takes 2 compares (clang -O2 does 2)
        fn FEq()  => self.bin_fcmp(.e);
        fn FNe()  => self.bin_fcmp(.ne);
        fn FLe()  => self.bin_fcmp(.be); 
        fn FGe()  => self.bin_fcmp(.ae);
        fn FLt()  => self.bin_fcmp(.b);
        fn FGt()  => self.bin_fcmp(.a);
        fn FAdd() => binf(fn(a, b) => {
            encode_binf(self.out(), double_prefix, false, FloatOp.AddSd, a, b);
        });
        fn FSub() => binf(fn(a, b) => {
            encode_binf(self.out(), double_prefix, false, FloatOp.SubSd, a, b);
        });
        fn FMul() => binf(fn(a, b) => {
            encode_binf(self.out(), double_prefix, false, FloatOp.MulSd, a, b);
        });
        fn FDiv() => binf(fn(a, b) => {
            encode_binf(self.out(), precision_prefix, false, FloatOp.DivSd, a, b);
        });
        fn IntToFloatValue() => {
            int := self.pop_to_reg();
            float := self.get_free_reg_float();
            encode_binf(self.out(), double_prefix, true, FloatOp.CVTSI2SD, @as(u4) float, @as(u4) int);
            self.drop_reg(int);
            self.state.stack&.push(FloatReg = float);
        }
        fn FloatToIntValue() => {
            float := self.pop_to_reg_float();
            int := self.get_free_reg();
            encode_binf(self.out(), double_prefix, true, FloatOp.cvttsd2si, @as(u4) int, @as(u4) float);
            self.drop_reg_float(float);
            self.state.stack&.push(Increment = (reg = int, offset_bytes = 0));
        }
        fn ShrinkFloat() => {
            float := self.pop_to_reg_float();
            encode_binf(self.out(), double_prefix, true, FloatOp.cvtsd2ss, @as(u4) float, @as(u4) float);
            self.state.stack&.push(FloatReg = float);
        }
        fn GrowFloat() => {
            float := self.pop_to_reg_float();
            encode_binf(self.out(), float_prefix, true, FloatOp.cvtsd2ss, @as(u4) float, @as(u4) float);
            self.state.stack&.push(FloatReg = float);
        }
        @default => @panic("ICE: unimplemented x64 intrinsic %", op);
    };
}

fn next_inst(self: *EmitX64) *u8 = {    
    self.backend.asm_out.maybe_uninit.ptr.offset(self.backend.asm_out.len)
}

// i cant keep typing this 
fn out(self: *EmitX64) *List(u8) #inline = 
    self.backend.asm_out&;

fn as_u8(ptr: *u32) *u8 = {
    ptr := u32.raw_from_ptr(ptr);
    ptr := u8.ptr_from_raw(ptr);
    ptr
}
