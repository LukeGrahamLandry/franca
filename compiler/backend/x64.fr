EmitX64    :: CodeGenJit(X64AsmData, u8, @as(i64) X86Reg.rbp, true);
X64AsmData :: @struct(asm_out: List(u8), valid := true, interesting := false);

fn new(compile: CompilerRs) X64AsmData #once = {
    (
        asm_out = (
            maybe_uninit = (ptr = compile.jitted.next.as_u8(), len = 1.shift_left(20)), // TODO: use real cap
            len = 0,
            gpa = panicking_allocator, // no growing!
        ),
    )
}

fn as_u8(ptr: *u32) *u8 = {
    ptr := u32.raw_from_ptr(ptr);
    ptr := u8.ptr_from_raw(ptr);
    ptr
}

x64_reserve_stack_encoding_size :: 7;
x64_release_stack_encoding_size :: 11;

x64_prelude_reserved_space :: 16;
fn emit_header(self: *EmitX64) *u8 #once = {
    // ret addr is at the top of the stack, so we can just leave it there. 
    // TODO: deal with stack args somehow
    self.next_slot.id -= x64_prelude_reserved_space;
    self.backend.asm_out&.encode_op_reg(PrimaryOp.PushBase, X86Reg.rbp);
    if self.body.signeture.first_arg_is_indirect_return {
        // "On return %rax will contain the address that has been passed in by the caller in %rdi"
        self.backend.asm_out&.encode_op_reg(PrimaryOp.PushBase, X86Reg.rax);
    };
    self.backend.asm_out&.encode_bin(PrimaryOp.MovReg, X86Reg.rbp, X86Reg.rsp);
    
    reserve_stack := self.next_inst();
    range(0, x64_reserve_stack_encoding_size) { _ |
        self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
    };
    reserve_stack
}

fn patch_reserve_stack(self: *EmitX64, reserve_stack: *u8) void #once = {
    stack_size := -@as(i64) self.next_slot.id;
    extra := stack_size.abs().mod(16);
    if extra != 0 {
        stack_size += 16 - extra; // play by the rules
    };
    used_stack := self.next_slot.id != -x64_prelude_reserved_space;
    
    patch: List(u8) = (maybe_uninit = (ptr = reserve_stack, len = x64_reserve_stack_encoding_size), len = 0, gpa = panicking_allocator);
    if used_stack {
        patch&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -stack_size);
        @debug_assert_eq(patch.maybe_uninit.len, patch.len, "confused about encoding size!");
    } else {
        range(0, x64_reserve_stack_encoding_size) { _ |
           patch&.push(@as(u8) PrimaryOp.Nop);
        };
    };
    
    for self.release_stack { it |
        patch: List(u8) = (maybe_uninit = (ptr = it, len = x64_release_stack_encoding_size), len = 0, gpa = panicking_allocator);
        if used_stack {
            patch&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, stack_size);
        } else {
            range(0, 7) { _ |
                patch&.push(@as(u8) PrimaryOp.Nop);
            };
        };
        if self.body.signeture.first_arg_is_indirect_return {
            // "On return %rax will contain the address that has been passed in by the caller in %rdi"
            // I don't use it but c code might
            patch&.encode_op_reg(PrimaryOp.PopBase, X86Reg.rax);
        } else {
            // TODO: this wont work if i want to access args by offsetting sp
            patch&.push(@as(u8) PrimaryOp.Nop);
            patch&.push(@as(u8) PrimaryOp.Nop);
        };
        patch&.encode_op_reg(PrimaryOp.PopBase, X86Reg.rbp);
        @debug_assert_eq(patch.maybe_uninit.len, patch.len, "confused about encoding size!");
    };
    
    len, extra := self.backend.asm_out.len.div_mod(4);
    range(0, 4 - extra) { _ | 
        self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
    };
    self.program.jitted.next = self.program.jitted.next.offset(len + 1);
}

fn emit_stack_fixup(self: *EmitX64) void = {
    self.release_stack&.push(self.next_inst());
    range(0, x64_release_stack_encoding_size) { _ |
        self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
    };
}

::List(X86Reg);
x64_int_args :: clone(@slice(X86Reg.rdi, X86Reg.rsi, X86Reg.rdx, X86Reg.rcx, X86Reg.r8, X86Reg.r9), ast_alloc()).items();
x64_int_rets :: clone(@slice(X86Reg.rax, X86Reg.rdx), ast_alloc()).items();

fn add_free_args(self: *EmitX64, taken_int_count: i64, taken_float_count: i64) void = {
    if taken_int_count < x64_int_args.len() {
        for x64_int_args.slice(taken_int_count, x64_int_args.len()) { r |
            self.state.free_reg&.push(@as(i64) r);
        };
    };
    self.state.free_reg&.push(@as(i64) X86Reg.rax);
}

fn add_free_rets(self: *EmitX64, taken_int_count: i64, taken_float_count: i64) void = {
    for x64_int_rets.slice(taken_int_count, x64_int_rets.len()) { r |
        self.state.free_reg&.push(@as(i64) r);
    };
    
    for (@slice(X86Reg.rdi, X86Reg.rsi, X86Reg.rcx, X86Reg.r8, X86Reg.r9)) { r | 
        self.state.free_reg&.push(@as(i64) r);
    };
}

fn emit_other_inst(self: *EmitX64, inst: Bc) bool #once = {
    @match(inst) {
        fn Unreachable() => {
            self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
            return(true);
        }
        fn PeekDup(skip) => {
            val := self.state.stack[self.state.stack.len - skip.zext() - 1];
            @match(val) {
                fn Increment(f) => {
                    reg := @as(X86Reg) f.reg;
                    if reg == .rbp {
                        self.state.stack&.push(val);
                    } else {
                        new := self.get_free_reg();
                        dest := @as(X86Reg) new;
                        if self.backend.valid {
                            @debug_assert_ne(reg, dest, "shouldn't have been free"); // TODO
                        };
                        self.backend.asm_out&.encode_bin(PrimaryOp.MovReg, dest, reg);
                        self.state.stack&.push(Increment = (reg = new, offset_bytes = f.offset_bytes));
                    };
                }
                fn Literal(_)   => self.state.stack&.push(val);
                fn Spill(_)     => self.state.stack&.push(val);
                fn FloatReg(reg)  => {
                    self.backend.valid = false;
                    self.state.stack&.push(Literal = 0);
                    return(true);
                }
            };
        }
        fn Load(ty) => {
            addr, offset_bytes := self.pop_to_reg_with_offset(); // get the ptr

            dest := self.get_free_reg();
            @match(ty) {
                fn I64() => self.load_u64(dest, addr, offset_bytes);
                fn P64() => self.load_u64(dest, addr, offset_bytes);
                fn I32() => {
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    reg := @as(X86Reg) dest;
                    addr := @as(X86Reg) addr;
                    self.backend.asm_out&.encode_non_sp_offset(false, PrimaryOp.MovRegLoad, reg, addr, offset_bytes);
                }
                fn I16() => {
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    reg := @as(X86Reg) dest;
                    addr := @as(X86Reg) addr;
                    self.backend.asm_out&.push(operand_16bit_prefix);
                    self.backend.asm_out&.encode_non_sp_offset(false, PrimaryOp.MovRegLoad, reg, addr, offset_bytes);
                }
                fn I8() => {
                    self.load_imm(dest, 0); // TODO: MOVZX?
                    self.encode_non_sp_offset(PrimaryOp.MovByteLoad, dest, addr, offset_bytes);
                };
                @default => {
                    self.backend.valid = false;
                    return(true);
                };
            };
            self.state.stack&.push(Increment = (reg = dest, offset_bytes = 0));
            self.drop_reg(addr);
        }
        // TODO: not always a call!
        fn CopyBytesToFrom(bytes) => {
            // copy-paste from arm. TODO: could move to jit if i add call_const(addr)
            self.state.stack&.push(Literal = bytes.zext());
            self.stack_to_ccall_reg(@slice(Prim.P64, Prim.P64, Prim.I64), false);
            self.spill_abi_stompable();
            self.state.free_reg&.clear();
            self.state.free_reg_float&.clear();
            
            // TODO: fancy version of this?
            addr: rawptr : fn(dest: *u8, src: *u8, len: i64) void = {
                range(0, len) { i |
                    dest.offset(i)[] = src.offset(i)[];
                };
            };
            self.load_imm(@as(i64) X86Reg.r11, addr.int_from_rawptr());
            encode_call_reg(self.backend.asm_out&, ModrmMode.Direct, X86Reg.r11);
            self.add_free_rets(0, 0);
        }
        fn CallFnPtr(f) => {
            // TODO: tail call
            sig := self.body.decode_sig(f.sig);
            self.dyn_c_call(sig) {
                // dyn_c_call will have popped the args, so now stack is just the pointer to call
                callee := self.state.stack&.pop().expect("enough stack for call");
                // TODO: this does redundant spilling every time!
                @match(callee) {
                    fn Literal(p) => self.load_imm(@as(i64) X86Reg.r11, p);
                    fn Spill(offset) => self.load_u64(@as(i64) X86Reg.r11, @as(i64) X86Reg.rbp, offset.id);
                    @default => {
                        panic("ICE: unspilled register");
                    };
                };
                encode_call_reg(self.backend.asm_out&, ModrmMode.Direct, X86Reg.r11);
            };
        }
        fn JumpIf(f) => {
            cond := self.pop_to_reg();
            @debug_assert_eq(f.slots, 0); // emit_bc doesn't do this
            self.spill_abi_stompable();
            self.backend.asm_out&.encode_bin(PrimaryOp.Test, @as(X86Reg) cond, @as(X86Reg) cond);
            // The offset is from **the end** of the jump instruction
            JUMP_ENCODE_SIZE :: 6;
            patch := self.backend.asm_out.maybe_uninit.ptr.offset(self.backend.asm_out.len);
            range(0, JUMP_ENCODE_SIZE) { _ |
                self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
            };
            jump_ip := u8.int_from_ptr(self.backend.asm_out.maybe_uninit.ptr) + self.backend.asm_out.len;
            
            // branch if zero so true before false
            // we only do one branch so true block must be directly after the check.
            // this is the only shape of flow graph that its possible to generate with my ifs/whiles.
            @debug_assert(self.block_ips[f.true_ip.id.zext()].is_none());

            self.drop_reg(cond);
            state := self.state&.clone();
            self.emit_block(f.true_ip.id.zext(), true);
            if !self.backend.valid {
                return(true);
            };
            self.state = state;
            self.emit_block(f.false_ip.id.zext(), true);
            if !self.backend.valid {
                return(true);
            };
            
            false_ip := self.block_ips[f.false_ip.id.zext()].unwrap();
            offset := u8.int_from_ptr(false_ip) - jump_ip;
            @debug_assert(offset.abs() < 1.shift_left(31), "cant jump that far");
            patch: List(u8) = (maybe_uninit = (ptr = patch, len = JUMP_ENCODE_SIZE), len = 0, gpa = panicking_allocator);
            patch&.encode_jmp(.e, offset);
            @debug_assert_eq(patch.len, JUMP_ENCODE_SIZE);
            return(true); 
        }
        @default => panic("unreachable: unhandled bc op");
    };

    false
}

fn emit_switch(self: *EmitX64, cases: *RsVec(SwitchPayload)) void = {
    @debug_assert(cases.len >= 2, "switch op should have default + >=2 branches");
    inspect := self.pop_to_reg();
    // TODO: jump table? binary search? 
    // TODO: is it better to have all the jumps at the top? 
    normal_branches, default := cases.decode_switch();
    each normal_branches { option |
        self.spill_abi_stompable(); // TODO: this is dumb but you need to make sure the different branches don't spill things in different places. 
       
        JUMP_ENCODE_SIZE :: 6;
        assert(option.value < 4096 && option.value >= 0, "value must fit in u12"); // TODO: use the right number types sooner?
        self.backend.asm_out&.encode_cmp_imm32(@as(X86Reg) inspect, option.value);
        
        patch := self.backend.asm_out.maybe_uninit.ptr.offset(self.backend.asm_out.len);
        range(0, JUMP_ENCODE_SIZE) { _ |
            self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
        };
        jump_ip := self.backend.asm_out.len;
        
        @debug_assert(self.block_ips[option.block.id.zext()].is_none(), "I only generate simple control flow");

        state := self.state&.clone();
        self.drop_reg(inspect); // Inside the new block
        self.emit_block(option.block.id.zext(), true);
        if !self.backend.valid {
            return();
        };
        self.state = state;
        
        offset := self.backend.asm_out.len - jump_ip;
        @debug_assert(offset.abs() < 1.shift_left(31), "cant jump that far");
        patch: List(u8) = (maybe_uninit = (ptr = patch, len = JUMP_ENCODE_SIZE), len = 0, gpa = panicking_allocator);
        patch&.encode_jmp(.ne, offset);
        @debug_assert_eq(patch.len, JUMP_ENCODE_SIZE);
    };
    self.drop_reg(inspect);
    assert(default.is_some(), "i always have a default branch currently");
    default_block := default.unwrap();
    @debug_assert(self.block_ips[default_block.id.zext()].is_none(), "I only generate simple control flow!");
    self.emit_block(default_block.id.zext(), true);
}

// TODO: better error message if you do Result(void, str) // (lowercase)
fn branch_func(self: *EmitX64, f: FuncId, with_link: bool) void #once = {
    if !with_link {
        self.backend.valid = false;
        return();
    };
    
    // If we already emitted the target function, can just branch there directly.
    if self.asm.get_fn_old(f) { bytes |
    //if self.program[][].get_fn_callable(f) { bytes | // no cause this would bump while we're writing
        start := self.backend.asm_out.len;
        // The offset is from **the end** of the jump instruction
        CALL_ENCODE_SIZE :: 5;
        n := u8.int_from_ptr(self.backend.asm_out.maybe_uninit.ptr) + self.backend.asm_out.len + CALL_ENCODE_SIZE;
        offset := bytes.int_from_rawptr() - n;
        if offset.abs() < 1.shift_left(31) {
            @if(TRACE_ASM) println("backwards call");
            encode_imm32_only(self.backend.asm_out&, PrimaryOp.CallImm32, offset); 
            end := self.backend.asm_out.len;
            @debug_assert_eq(end - start, CALL_ENCODE_SIZE, "encoding wrong size???");
            return(); 
        };
    };
    
    r := X86Reg.r11;
    self.load_fnptr_from_dispatch(f, @as(i64) r);
    encode_call_reg(self.backend.asm_out&, ModrmMode.Direct, r);
}

fn emit_return(self: *EmitX64, sig: [] Prim) bool = {
    @switch(sig.len) {
        @case(0) => ();
        @case(1) => {
            if !(sig[0] != .F32 && sig[0] != .F64) {
                self.backend.valid = false;
                return(true);
            };
            self.pop_to_specific_reg(X86Reg.rax);
        };
        @case(2) => {
            if !(sig[1] != .F32 && sig[1] != .F64) {
                self.backend.valid = false;
                return(true);
            };
            self.pop_to_specific_reg(X86Reg.rdx);
            if !(sig[0] != .F32 && sig[0] != .F64) {
                self.backend.valid = false;
                return(true);
            };
            self.pop_to_specific_reg(X86Reg.rax);
        };
        @default => @panic("ICE: invalid bytecode. tried to return %", sig);
    };
    self.emit_stack_fixup();
    self.backend.asm_out&.push(@as(u8) PrimaryOp.Ret);
    true
}

fn ccall_ret_reg_to_stack(self: *EmitX64, types: [] Prim) void = {
   if types.len > 2 {
        self.backend.valid = false;
        return();
    };
    enumerate types { i, ty |
        if !(ty[] != .F32 && ty[] != .F64) {
            self.backend.valid = false;
            return();
        };
        goal := @as(i64) @as(X86Reg) x64_int_rets[i];
        self.state.free_reg&.unordered_retain(fn(r) => r[] != goal); 
        self.state.stack&.push(Increment = (reg = goal, offset_bytes = 0));
    };
}

// TODO: do i have to return the ret addr? 
fn ccall_reg_to_stack(self: *EmitX64, types: [] Prim, extra_special_ret: bool) Ty(i64, i64) = {
    int_count := if(extra_special_ret, => 1, => 0); // x64 just passes the ret addr as first arg. 
    float_count := 0;
    for types { type | 
        if type.is_float() {
            self.backend.valid = false;
            float_count += 1;
        } else {
            int_count += 1;
        };
    };
    if int_count > x64_int_args.len() {
        self.backend.valid = false;
        return(0, 0);
    };
    for x64_int_args.slice(0, int_count) { r |
        self.state.stack&.push(Increment = (reg = @as(i64) r, offset_bytes = 0));
    };
    
    (int_count, float_count)
}

fn stack_to_ccall_reg(self: *EmitX64, types: [] Prim, extra_special_ret: bool) void = {
    int_count := if(extra_special_ret, => 1, => 0); // x64 just passes the ret addr as first arg. 
    total := types.len + int_count;
    float_count := 0;
    for types { type | 
        if type.is_float() {
            self.backend.valid = false;
            float_count += 1;
        } else {
            int_count += 1;
        };
    };
    if int_count > x64_int_args.len() {
        self.backend.valid = false;
        return();
    };
    
    enumerate_rev x64_int_args.slice(0, int_count) { slot_index, r |
        self.pop_to_specific_reg(@as(X86Reg) r[]);
    };
}

fn pop_to_specific_reg(self: *EmitX64, reg: X86Reg) void = {
    val := self.state.stack&.pop().unwrap();
    self.in_specific_reg(reg, val);
}

fn in_specific_reg(self: *EmitX64, reg: X86Reg, val: AsmVal) void = {
    @match(val) {
        fn Literal(x) => {
            self.ensure_specific_free(reg);
            self.load_imm(@as(i64) reg, x);
        }
        fn Increment(f) => {
            old_reg := @as(X86Reg) f.reg;
            if old_reg != reg {
                self.ensure_specific_free(reg);
                self.backend.asm_out&.encode_bin(PrimaryOp.MovReg, reg, old_reg);
                self.drop_reg(f.reg);
            };
            if f.offset_bytes != 0 {
                self.backend.asm_out&.encode_imm(PrimaryOp.AddImm32, reg, f.offset_bytes);
            };
        }
        fn Spill(slot) => {
            self.ensure_specific_free(reg);
            self.load_u64(@as(i64) reg, @as(i64) X86Reg.rbp, slot.id);
        }
        @default => panic("unhandled in_reg");
    };
}

fn ensure_specific_free(self: *EmitX64, goal_r: X86Reg) void = {
    goal := @as(i64) goal_r;
    if self.state.free_reg&.position(fn(r) => goal == r[]) { want |
        self.state.free_reg&.unordered_remove(want);
        return();
    };
    
    enumerate self.state.stack { stack_index, val |
        @if_let(val) fn Increment(f) => {
            if f.reg == goal {
                ok := self.try_spill(stack_index, true, false);
                @debug_assert(ok, "failed to spill specific");
                self.state.free_reg&.unordered_retain(fn(r) => r[] != goal);  // TODO: its always last, we just pushed it
                return();
            };
        };
    };
    
    @panic("ICE: lost a register %", goal_r);
}

i32_MAX :: 2147483647;
i32_MIN :: -2147483648;

fn load_imm(self: *EmitX64, reg: i64, value: i64) void = {
    reg: X86Reg = @as(X86Reg) reg;
    fits_in_32 := value <= i32_MAX && value >= i32_MIN;
    
    if fits_in_32 {
        encode_imm(self.backend.asm_out&, reg, value);
    } else {
        value: u64 = value.bitcast();
        self.backend.asm_out&.encode_imm64(reg, value);
    };
}

fn emit_store(self: *EmitX64, addr: AsmVal, value: AsmVal, ty: Prim) void = { 
    addr_reg, offset := self.in_reg_with_offset(addr);
    value := self.in_reg(value);
    @match(ty) {
        fn I64() => self.store_u64(value, addr_reg, offset);
        fn P64() => self.store_u64(value, addr_reg, offset);
        fn I8() => self.encode_non_sp_offset(PrimaryOp.MovByte, value, addr_reg, offset);
        fn I32() => {
            reg := @as(X86Reg) value;
            addr := @as(X86Reg) addr_reg;
            self.backend.asm_out&.encode_non_sp_offset(false, PrimaryOp.MovReg, reg, addr, offset);
        }
        fn I16() => {
            reg := @as(X86Reg) value;
            addr := @as(X86Reg) addr_reg;
            self.backend.asm_out&.push(operand_16bit_prefix);
            self.backend.asm_out&.encode_non_sp_offset(false, PrimaryOp.MovReg, reg, addr, offset);
        }
        @default => {
            self.backend.valid = false;
            return();
        };
    };
    self.drop_reg(value);
    self.drop_reg(addr_reg);
}

fn store_u64(self: *EmitX64, src_reg: i64, dest_addr_reg: i64, offset_bytes: i64) void = { 
    self.encode_non_sp_offset(PrimaryOp.MovReg, src_reg, dest_addr_reg, offset_bytes);
}

fn load_u64(self: *EmitX64, dest_reg: i64, src_addr_reg: i64, offset_bytes: i64) void = { 
    self.encode_non_sp_offset(PrimaryOp.MovRegLoad, dest_reg, src_addr_reg, offset_bytes);
}

fn encode_non_sp_offset(self: *EmitX64, op: PrimaryOp, reg: i64, addr: i64, offset_bytes: i64) void = { 
    reg := @as(X86Reg) reg;
    addr := @as(X86Reg) addr;
    if addr == X86Reg.rsp {
        self.backend.valid = false; 
        return();
    };
    if addr == X86Reg.rbp {
        @debug_assert(offset_bytes != 0, "shouldn't be using raw frame pointer");
    };
    self.backend.asm_out&.encode_non_sp_offset(op, reg, addr, offset_bytes);
}

fn in_reg_float(self: *EmitX64, val: AsmVal) i64 = { self.backend.valid = false; 0 };
fn in_reg(self: *EmitX64, val: AsmVal) i64 = { 
    @match(val) {
        fn Increment(f) => {
            old := @as(X86Reg) f.reg;
            if old == X86Reg.rbp {
                f.reg = self.get_free_reg();
                self.backend.asm_out&.encode_bin(PrimaryOp.MovReg, @as(X86Reg) f.reg, X86Reg.rbp);
            };
            if f.offset_bytes != 0 {
                self.backend.asm_out&.encode_imm(PrimaryOp.AddImm32, @as(X86Reg) f.reg, f.offset_bytes);
            };
            f.reg
        }
        fn Literal(i) => {
            r := self.get_free_reg();
            self.load_imm(r, i);
            r
        }
        fn Spill(slot) => {
            r := self.get_free_reg();
            self.load_u64(r, @as(i64) X86Reg.rbp, slot.id);
            r
        }
        fn FloatReg() => {
            self.backend.valid = false; 
            0 
        }
    }
}

fn load_fnptr_from_dispatch(self: *EmitX64, f: FuncId, reg: i64) void = { 
    self.load_imm(reg, self.asm.get_dispatch());
    self.load_u64(reg, reg, f.as_index() * 8);
}

fn is_special_reg(self: *EmitX64, reg: i64) bool = {
    reg.eq(@as(i64) X86Reg.rsp).or(reg == @as(i64) X86Reg.rbp)
}

fn try_spill(self: *EmitX64, i: usize, do_ints: bool, do_floats: bool) bool = { 
    v := self.state.stack[i];
    if do_ints {
        @if_let(v) fn Increment(f) => {
            if(f.reg == @as(i64) X86Reg.rbp, => return(false));
            
            // Note: this assumes we don't need to preserve the value in the reg other than for this one v-stack slot.
            slot := self.create_slots(8);
            @if(TRACE_ASM) @print("(spill (x% + %) -> [fp, %]) ", f.reg, f.offset_bytes, slot.id);
            if f.offset_bytes != 0 {
                self.backend.asm_out&.encode_imm(PrimaryOp.AddImm32, @as(X86Reg) f.reg, f.offset_bytes);
            };
    
            self.store_u64(f.reg, @as(i64) X86Reg.rbp, slot.id);
            self.drop_reg(f.reg);
            self.state.stack[i] = (Spill = slot);
            return(true);
        };
    };

    if do_floats {
        @if_let(v) fn FloatReg(freg) => {
            self.backend.valid = false; 
            return(true);
        };
    };
    
    false
}

fn next_inst(self: *EmitX64) *u8 = {    
    self.backend.asm_out.maybe_uninit.ptr.offset(self.backend.asm_out.len)
}

// TODO: copy paste from call-short
fn jump_to(self: *EmitX64, to_ip: *u8) void = {
    start := self.backend.asm_out.len;
    // The offset is from **the end** of the jump instruction
    JUMP_ENCODE_SIZE :: 5;
    n := u8.int_from_ptr(self.backend.asm_out.maybe_uninit.ptr) + self.backend.asm_out.len + JUMP_ENCODE_SIZE;
    offset :=  u8.int_from_ptr(to_ip) - n;
    @debug_assert(offset.abs() < 1.shift_left(31), "what a big function you have");
    encode_imm32_only(self.backend.asm_out&, PrimaryOp.JmpImm32, offset); 
    end := self.backend.asm_out.len;
    @debug_assert_eq(end - start, JUMP_ENCODE_SIZE, "encoding wrong size???");
}

fn print_llvm_mc_dis(self: *EmitX64, asm: []u32) void #inline = {
    asm: []u8 = (ptr = asm.ptr.as_u8(), len = asm.len * 4);
    print_llvm_mc_dis(asm);
}

fn inst_intrinsic(self: *EmitX64, op: Intrinsic) void = { 
    // TODO: for add, include offset_bytes in the immediate if they fit or combine them and defer instead of doing 3 seperate adds.
    bin :: fn($do: @Fn(a: X86Reg, b: X86Reg) void) void => {
        snd := self.pop_to_reg();
        fst := self.pop_to_reg();
        do(@as(X86Reg) fst, @as(X86Reg) snd);
        self.drop_reg(snd);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };
    // Note: could use better error message when you try to factor this into a '=' but keep 'bin' a '=>',
    //       "missing value self" because 'bin' closes over the outer one, not the one you'd add as an argument to this function.
    // TODO: allow mutiple callsites that are all tail of a '=>' function to only make one duplicate of it so there's less code bloat. 
    //! IMPORTANT: cond is inverted because CSINC
    bin_cmp :: fn(cond: X86cc) => bin(fn(a, b) => {
        self.backend.asm_out&.encode_bin(PrimaryOp.CmpReg, a, b);
        self.load_imm(@as(i64) a, 0); // setcc doesn't zero the whole register!
        self.backend.asm_out&.encode_setcc(a, cond);
    });
    
    sane_bin :: fn(op: PrimaryOp) => bin(fn(a, b) => {
        self.backend.asm_out&.encode_bin(op, a, b);
    });
    
    bin_rcx :: fn($do: @Fn(a: X86Reg) void) void => {
        self.pop_to_specific_reg(X86Reg.rcx);
        fst := self.pop_to_reg();
        do(@as(X86Reg) fst);
        self.drop_reg(@as(i64) X86Reg.rcx);
        self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
    };

    @match(op) {
        fn Add()    => sane_bin(PrimaryOp.AddReg);
        fn Sub()    => sane_bin(PrimaryOp.SubReg);
        fn BitOr()  => sane_bin(PrimaryOp.OrReg);
        fn BitAnd() => sane_bin(PrimaryOp.AndReg);
        fn BitXor() => sane_bin(PrimaryOp.XorReg);
        fn Mul() => bin(fn(a, b) => {
            self.backend.asm_out&.encode_bin2(TwoByteOp.IMul, b, a);
        });
        fn Div() => {  // the year is 1726, we have 4 registers, and 2 of them are reserved for division. 
            self.ensure_specific_free(.rax);
            self.ensure_specific_free(.rdx);
            divisor := self.pop_to_reg();
            self.drop_reg(@as(i64) X86Reg.rax);
            self.pop_to_specific_reg(X86Reg.rax);
            
            // :fun_with_x64_division
            // rdx has the high bits of an i128, so sign extend rax into rdx. 
            self.backend.asm_out&.push(0b01001000); // REX W=1
            self.backend.asm_out&.push(@as(u8) PrimaryOp.SignExtendAxToDxAx);
            
            self.backend.asm_out&.encode_extended_op(ModrmMode.Direct, 0xf7, 0b0111, @as(X86Reg) divisor);
            self.drop_reg(@as(i64) X86Reg.rdx); // TODO: expose as div_mod
            self.drop_reg(@as(i64) divisor);
            self.state.stack&.push(Increment = (reg = @as(i64) X86Reg.rax, offset_bytes = 0));
        }
        fn BitNot() => {
            fst := self.pop_to_reg();
            self.backend.asm_out&.encode_not(@as(X86Reg) fst);
            self.state.stack&.push(Increment = (reg = fst, offset_bytes = 0));
        }
        fn ShiftLeft() => bin_rcx(fn(a) => {
            self.backend.asm_out&.encode_extended_op(ModrmMode.Direct, @as(u8) PrimaryOp.ShiftBase, 0b0100, a);
        });
        fn ShiftRightLogical() => bin_rcx(fn(a) => {
            self.backend.asm_out&.encode_extended_op(ModrmMode.Direct, @as(u8) PrimaryOp.ShiftBase, 0b0101, a);
        });
        fn ShiftRightArithmetic() => bin_rcx(fn(a) => {
            self.backend.asm_out&.encode_extended_op(ModrmMode.Direct, @as(u8) PrimaryOp.ShiftBase, 0b0111, a);
        });
        fn Eq()  => bin_cmp(.e);
        fn Ne()  => bin_cmp(.ne);
        fn Le()  => bin_cmp(.le);
        fn Ge()  => bin_cmp(.ge);
        fn Lt()  => bin_cmp(.l);
        fn Gt()  => bin_cmp(.g);
        fn IntToPtr() => (); // no-op
        fn PtrToInt() => (); // no-op
        // TODO: this about how signed numbers are represented. 
        fn SignExtend32To64() => (); // TODO: WRITE A TEST THAT FAILS BECAUSE OF THIS
        fn ZeroExtend32To64() => (); // no-op
        fn ZeroExtend16To64() => (); // no-op
        fn ZeroExtend8To64()  => (); // no-op
        fn ZeroExtend16To32() => (); // no-op
        fn ZeroExtend8To32()  => (); // no-op
        fn ZeroExtend8To16()  => (); // no-op
        @default => {
            self.backend.valid = false; 
            // @panic("ICE: unimplemented x64 intrinsic %", op);
        };
    };
}
