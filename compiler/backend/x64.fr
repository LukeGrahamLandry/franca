
EmitX64    :: CodeGenJit(X64AsmData, u8, @as(i64) X86Reg.rsp);
X64AsmData :: @struct(asm_out: List(u8), valid := true);

fn new(compile: CompilerRs) X64AsmData #once = {
    (
        asm_out = (
            maybe_uninit = (ptr = compile.jitted.next.as_u8(), len = 1.shift_left(20)), // TODO: use real cap
            len = 0,
            gpa = panicking_allocator, // no growing!
        ),
    )
}

fn as_u8(ptr: *u32) *u8 = {
    ptr := u32.raw_from_ptr(ptr);
    ptr := u8.ptr_from_raw(ptr);
    ptr
}

fn patch_reserve_stack(self: *EmitX64, reserve_stack: *u8) void = {
    if self.next_slot.id != 0 {
        self.backend.valid = false;
    };
    
    len, extra := self.backend.asm_out.len.div_mod(4);
    range(0, 4 - extra) { _ | 
        self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
    };
    self.program.jitted.next = self.program.jitted.next.offset(len + 2);
}

fn add_free_args(self: *EmitX64, taken_int_count: i64, taken_float_count: i64) void = {
    arg_regs := @slice(X86Reg.rdi, X86Reg.rsi, X86Reg.rdx, X86Reg.rcx, X86Reg.r8, X86Reg.r9);
    for arg_regs.slice(taken_int_count, arg_regs.len) { r |
        self.state.free_reg&.push(@as(i64) r);
    };
}

fn emit_other_inst(self: *EmitX64, inst: Bc) bool #once = {
    @if(TRACE_ASM) @println("[%:%] %", b_idx, i, self.state.stack.items());
    @match(inst) {
        fn Unreachable() => {
            self.backend.asm_out&.push(@as(u8) PrimaryOp.Int3);
            return(true);
        }
        @default => {
            self.backend.valid = false;
        };
        //@default => return(Err = "unhandled inst");
    };

    false
}

// TODO: better error message if you do Result(void, str) // (lowercase)
fn branch_func(self: *EmitX64, f: FuncId, with_link: bool) void #once = {
    if !with_link {
        self.backend.valid = false;
    };
    
    // If we already emitted the target function, can just branch there directly.
    if self.asm.get_fn_old(f) { bytes |
        start := self.backend.asm_out.len;
        // The offset is from **the end** of the jump instruction
        CALL_ENCODE_SIZE :: 5;
        n := u8.int_from_ptr(self.backend.asm_out.maybe_uninit.ptr) + self.backend.asm_out.len + CALL_ENCODE_SIZE;
        offset := bytes.int_from_rawptr() - n;
        if offset.abs() < 1.shift_left(31) {
            @if(TRACE_ASM) println("backwards call");
            encode_imm32_only(self.backend.asm_out&, PrimaryOp.CallImm32, offset); 
            end := self.backend.asm_out.len;
            @debug_assert_eq(end - start, CALL_ENCODE_SIZE, "encoding wrong size???");
            return(); 
        };
    };
    self.backend.valid = false;
}

fn emit_return(self: *EmitX64, sig: [] Prim) bool = {
    @switch(sig.len) {
        @case(0) => ();
        @case(1) => {
            if !(sig[0] != .F32 && sig[0] != .F64) {
                self.backend.valid = false;
                return(true);
            };
            self.pop_to_specific_reg(X86Reg.rax);
        };
        @case(2) => {
            if !(sig[1] != .F32 && sig[1] != .F64) {
                self.backend.valid = false;
                return(true);
            };
            self.pop_to_specific_reg(X86Reg.rdx);
            if !(sig[0] != .F32 && sig[0] != .F64) {
                self.backend.valid = false;
                return(true);
            };
            self.pop_to_specific_reg(X86Reg.rax);
        };
        @default => @panic("ICE: invalid bytecode. tried to return %", sig);
    };
    self.emit_stack_fixup();
    self.backend.asm_out&.push(@as(u8) PrimaryOp.Ret);
    true
}

fn pop_to_specific_reg(self: *EmitX64, reg: X86Reg) void = {
    val := self.state.stack&.pop().unwrap();
    self.in_specific_reg(reg, val);
}

fn in_specific_reg(self: *EmitX64, reg: X86Reg, val: AsmVal) void = {
    @match(val) {
        fn Literal(x) => {
            self.load_imm(@as(i64) reg, x);
        }
        @default => panic("unhandled in_reg");
    };
}

i32_MAX :: 2147483647;
i32_MIN :: -2147483648;

fn load_imm(self: *EmitX64, reg: i64, value: i64) void = {
    reg: X86Reg = @as(X86Reg) reg;
    fits_in_32 := value <= i32_MAX && value >= i32_MIN;
    
    if fits_in_32 {
        encode_imm(self.backend.asm_out&, reg, value);
    } else {
        value: u64 = value.bitcast();
        self.backend.asm_out&.encode_imm64(reg, value);
    };
}
fn emit_switch(self: *EmitX64, cases: *RsVec(SwitchPayload)) void = { self.backend.valid = false; };
fn stack_to_ccall_reg(self: *EmitX64, types: [] Prim, extra_special_ret: bool) void = {
    if extra_special_ret.or(types.len != 0) {
        self.backend.valid = false;
    }
}
fn ccall_reg_to_stack(self: *EmitX64, types: [] Prim, extra_special_ret: bool) Ty(i64, i64) = {
    if extra_special_ret.or(types.len != 0) {
        self.backend.valid = false;
    };
    (0, 0)
}
fn emit_store(self: *EmitX64, addr: AsmVal, value: AsmVal, ty: Prim) void = { self.backend.valid = false; };
fn load_one(self: *EmitX64, register_type: i64, dest_reg: i64, src_addr_reg: i64, offset_bytes: u16) void = { self.backend.valid = false; };
fn store_u64(self: *EmitX64, src_reg: i64, dest_addr_reg: i64, offset_bytes: u16) void = { self.backend.valid = false; };
fn store_one(self: *EmitX64, register_type: i64, src_reg: i64, dest_addr_reg: i64, offset_bytes: u16) void = { self.backend.valid = false; };
fn in_reg_float(self: *EmitX64, val: AsmVal) i64 = { self.backend.valid = false; 0 };
fn in_reg(self: *EmitX64, val: AsmVal) i64 = { self.backend.valid = false; 0 };
fn load_fnptr_from_dispatch(self: *EmitX64, f: FuncId, reg: i64) void = { self.backend.valid = false; }
fn emit_stack_fixup(self: *EmitX64) void = {
    // TODO
}
fn inst_intrinsic(self: *EmitX64, op: Intrinsic) void = { self.backend.valid = false; }
fn is_special_reg(self: *EmitX64, reg: i64) bool = {
    reg == @as(i64) X86Reg.rsp
}
fn try_spill(self: *EmitX64, i: usize, do_ints: bool, do_floats: bool) bool = { self.backend.valid = false; true }
fn emit_header(self: *EmitX64) *u8 = {
    // TODO
    self.next_inst()
}
fn next_inst(self: *EmitX64) *u8 = {    
    self.backend.asm_out.maybe_uninit.ptr.offset(self.backend.asm_out.len)
}
fn jump_to(self: *EmitX64, to_ip: *u8) void = { self.backend.valid = false; }

fn load_u64(self: *EmitX64, dest_reg: i64, src_addr_reg: i64, offset_bytes: u16) void = { self.backend.valid = false; }
