// the api exposed by the compiler to comptime code. 

#noinline/* TODO: HACK: break cycle for examples/emit_c.fr */
fn init_driver_vtable($for_driver: bool) *ImportVTable = {
    @static(ImportVTable) {
        storage := ImportVTable.zeroed();
        fill_driver_vtable(storage&, for_driver);
        storage
    }
}

// the cleaner thing to do than update this is use __builtin_compiler_has_feature 
// but that requires compiling the definition of Str before you can access the value.
next_abi_version :: fn() i64 = {
    previous :: __driver_abi_version;
    @if_else {
        @if(IS_BOOTSTRAPPING) => 1119;
        @if(previous < 1349) => 1349;
        @if(previous < 1512) => 1512;
        @else => 1513;
    }
};

fn fill_driver_vtable(vtable: *ImportVTable, for_driver: bool) void = {
    vtable.driver_abi_version = next_abi_version();
    vtable.self_hash = Incr.self_hash;
    if enable_incremental() && !DISABLE_IMPORT_FRC {
        vtable.frc_module_magic_v = Incremental.MAGIC;
    };
    if !DUMB_EMIT_IR {
        fill_backend_vtable(vtable);
    };
    
    driver_vtable_exports := Type.scope_of(driver_vtable_exports);
    fields := get_fields(ImportVTable);
    fr := current_compiler_context();

    if DUMB_EMIT_IR {
        vtable.generation = fr.vtable.generation;
        vtable.generation += 1;
        if vtable.generation > 2 {
            vtable.generation = 0;
        };
    };
    
    for get_constants(driver_vtable_exports) { name |
        fid := FuncId.get_constant(driver_vtable_exports, name)
            || @panic("failed to get %", name.str());
        f := fields.find(fn(it) => it.name == name)
            || @panic("% is not a field of ImportVTable", name.str());
        off := f.byte_offset;
        p := bit_cast_unchecked(*ImportVTable, *rawptr, vtable).offset_bytes(off);
        
        if DUMB_EMIT_IR && for_driver {
            @if(@run(__driver_abi_version < 1512)) unreachable();
            if name == (@symbol get_alloc) {
                wrapped_get_alloc :: (fn(c: Compiler) Alloc = {
                    self := c.cast()[][];
                    a := self._get_alloc();
                    @if(abi_shift_easy_to_native) Vm'wrap_alloc_with_native(self, a&);
                    @if(abi_shift_native_to_easy) {
                        a.vptr = bit_cast_unchecked(rawptr, @type a.vptr, translate_arena_allocator_fn);
                    };
                    a
                });
                fid = wrapped_get_alloc;
            };
            if abi_shift_native_to_easy {
                fid = wrap_with_easy_abi(fid);
            };  // abi_shift_easy_to_native: done in current_compiler_context
        };
        
        // TODO: type check against the func's annotations
        p[] = fr.get_jitted(fid);
    };
    
    if DUMB_EMIT_IR {
        vtable.emit_qbe_included = (fn(_0, _1, _2, _3) = panic("can't emit_qbe_included because module has vtables"));
        vtable.emit_qbe_included_old = (fn() = panic("can't emit_qbe_included because module has vtables"));
    };
}

abi_shift_native_to_easy :: DUMB_EMIT_IR && { fr := current_compiler_context(); fr.vtable.generation == 0 };
abi_shift_easy_to_native :: DUMB_EMIT_IR && { fr := current_compiler_context(); fr.vtable.generation == 1 };

// TODO: this is garbage! the way my scoping works you can't access 
//       the old overload set once you shadow it with a constant. 
_get_build_options :: get_build_options;
_fmt_error :: fmt_error;
_get_function :: get_function;
_get_jitted_ptr :: get_jitted_ptr;
_get_whole_line :: get_whole_line;
_log_type :: log_type;
_intern_type :: intern_type;
_add_file :: add_file;
_get_alloc :: get_alloc;
_add_to_scope :: add_to_scope;
_reserve_baked :: reserve_baked;
_import_frc :: import_frc;
parse_stmts :: driver_vtable_exports.parse_stmts;

driver_vtable_exports :: @struct {
    find_unique_func :: fn(c: Compiler, name: Symbol) ?FuncId = { 
        c := c.cast()[][];
        f_ty := c._intern_type(Fn = (arg = void, ret = void, unary = true)); // :get_or_create_type
        
        value := c.poll_in_place(?Values, => c.find_in_scope(TOP_LEVEL_SCOPE, name, f_ty));
        @if(value&.is(.Err)) return(.None);
        value := c.unwrap_report_error(?Values, value) || return(.None);
        fid := FuncId.assume_cast(value&)[];
        (Some = fid)
    };
    
    get_jitted_ptr :: fn(c: Compiler, f: FuncId) CRes(rawptr) = {
        c.cast()[][]._get_jitted_ptr(f)
    };
    
    init_compiler :: fn(options: *BuildOptions) Compiler = {
        mem := init_self_hosted(options);
        mem.legacy_indirection
    };
    
    compile_func :: fn(c: Compiler, f: FuncId, when: ExecStyle) CRes(void) = {
        self := c.cast()[][];
        self.poll_in_place(void, => self.compile_body(f))
    };
    
    make_and_resolve_and_compile_top_level :: fn(cc: Compiler, body: Slice(FatStmt)) CRes(void) = 
        make_top_level(cc.cast()[][], body);
    
    add_file :: fn(c: Compiler, name: Str, content: Str) AbiHackSpan = {
        c := c.cast();
        loc := c.codemap._add_file(name, content);
        (low = loc.low.zext(), high = loc.high.zext())
    };
    
    parse_stmts :: fn(c: Compiler, f: *Span) Res(Slice(FatStmt)) = {
        c := c.cast();
        @debug_assert_ge(f.high, f.low);
        source := c.codemap.source_slice(f[]);
        @debug_assert(source.len == f[].len(), "source_slice failed");
        id := c.parser.push_parse(source, f[]);
        stmts := @try(c.parser.finish_pending_stmts(id)) return; // TODO: don't return RawList -- Jun 29
        (Ok = stmts.items())
    };
    
    intern_string :: fn(c: Compiler, s: Str) Symbol = {
        c := c.cast();
        c.pool.insert_borrowed(s, c[][]._get_alloc())
    };
    
    get_string :: fn(c: Compiler, s: Symbol) Str = {
        c := c.cast();
        c.pool.get(s)
    };
    
    get_fns_with_tag :: fn(c: Compiler, tag: Symbol) [] FuncId = {
        c := c.cast();
        found: List(FuncId) = list(temp());
        i := 0;
        each c.functions& { func |
            continue :: local_return;
            each func.annotations { a |
                if a.name == tag {
                    found&.push(from_index(i));
                    i += 1; 
                    continue();
                };
            };
            i += 1; 
        };
        found.items()
    };
    
    get_function :: fn(c: Compiler, f: FuncId) *Func = {
        c.cast()[][]._get_function(f)
    };
    destroy_compiler :: fn(c: Compiler) void = {
        c := c.cast()[][];
        c.drop();
    };
    
    fn drop(c: *SelfHosted) void = {
        when_debug(c, .VMem, fn(o) => dump_baked_vmem(c, o));
        print(c.log.items()); c.log&.clear();
        panic_on_volatile_bake(c);
        pop_resolver(c.backtrace_node);
        opts := c._get_build_options();
        m := c.comptime_codegen.m;
        c.comptime_codegen.join_codegen_thread();
        m.drop(); 
        if show_backend_stats() {
            ::DeriveFmt(@type c.stats);
            @eprintln("%", c.stats&);
        };
        if tls(.comptime)[].identical(SelfHosted.raw_from_ptr(c)) {
            // this mean that if init_self_hosted and destroy_compiler are paired, you don't have to push_as_tls
            tls(.comptime)[] = zeroed(rawptr);
        };
        arena := c.ast_alloc[];  // copy because the areana is inside itself!
        c[] = SelfHosted.zeroed();  // BEFORE releasing ast_arena;
        arena&.deinit();
    };
    get_build_options :: fn(c: Compiler) *BuildOptions = {
        c := c.cast()[][];
        c._get_build_options()
    };
   
    get_baked :: fn(c: Compiler, id: BakedVarId) rawptr = 
        BakedValue.raw_from_ptr(c.cast()[].baked.values&.nested_index(@as(i64) id.id.zext()));
    
    fmt_error :: fn(c: Compiler, err: *CompileError, out: *List(u8)) void = {
        if DUMB_EMIT_IR {
            out.gpa = temp();  // HACK
        };
        c.cast()[][]._fmt_error(err, out, true /*TODO*/);
    };
    
    add_comptime_library :: fn(c: Compiler, lib_name: Symbol, handle: rawptr) void = {
        handle := bit_cast_unchecked(rawptr, Dyn.Handle, handle);
        c.cast()[][].comptime_libraries&.insert(lib_name, handle); // TODO: report conflicts?
    };
    get_type_meta :: fn(c: Compiler, type: Type) *TypeMeta = {
        c.cast()[][].get_info(type)
    };
    get_type_info :: fn(c: Compiler, type: Type) *TypeInfo = {
        c.cast()[][].get_type(type)
    };
    get_whole_line :: fn(c: Compiler, span: Span) FrancaCodeLine = {
        c.cast()[][].codemap._get_whole_line(span)
    };
    report_aot_progress :: fn(c: Compiler, fid: FuncId, is_start: bool, zone_tag: i64) void = {
        @if(!ENABLE_TRACY) return();
            zone_tag := @as(TraceZone) zone_tag;
            self := c.cast()[][];
            self.report_progress(fid, is_start, zone_tag);
    };
    
    fn report_progress(self: *SelfHosted, fid: FuncId, is_start: bool, zone_tag: TraceZone) void = {
            @if(!ENABLE_TRACY) return();
            mem :: @static(?HashMap(Ty(FuncId, i64), TraceCtx)) (.None);
            if mem.is(.None) {
                mem[] = (Some = init(general_allocator()));
            };
            m := mem[].Some&;
            // :ThisIsNotOkBecauseMemoryWillBeReused
            key := (fid, bit_cast_unchecked(*SelfHosted, i64, self));
            ::AutoHash(@type key, TrivialHasher); ::AutoEq(@type key);
            if is_start {
                if m.get(key).is_none() {
                    zone := zone_begin(zone_tag);
                    func := self._get_function(fid);
                    real_name := self.pool.get(func.name);
                    ___tracy_emit_zone_name(zone, real_name);
                    m.insert(key, zone);
                };
            } else {
                if m.get(key) { zone | 
                    zone_end(zone);
                    m.remove(key);
                };
            };
    }

    log_expr :: fn(c: Compiler, e: *FatExpr) Str = {
        log(e, c.cast()[][])
    };
    log_stmt :: fn(c: Compiler, e: *FatStmt) Str = {
        log(e, c.cast()[][])
    };
    log_type :: fn(c: Compiler, e: Type) Str = {
        _log_type(c.cast()[][], e)
    };
    log_func :: fn(c: Compiler, e: *Func) Str = {
        log(e, c.cast()[][])
    };
    
    // :UpdateBoot get rid of this
    emit_qbe_included_old :: fn(mm: rawptr, comp: *CompCtx, fns: [] FuncId, entry: ProgramEntry) BucketArray(u8) = {
        chunks := emit_qbe_included(mm, comp, fns, entry);
        aaa: BucketArray(u8) = init(0, temp());
        for chunks { c |
            aaa&.push_bucket(assume_owned(c, temp()));
        };
        aaa
    };
    
    emit_qbe_included :: fn(mm: rawptr, comp: *CompCtx, fns: [] FuncId, entry: ProgramEntry) [][]u8 = {    
        m := QbeModule.ptr_from_raw(mm);
        threaded := entry != .GiveMeTheCodeAndGiveItToMeRawAlsoSingleThreadCodegen;
        shared := init_codegen(m, comp[]._get_alloc(), threaded);
        self := comp.data.cast()[][];
        main_thread_pump(self, shared, fns, entry == .WrapMain);
        opts := comp[]._get_build_options();
        ::if(Ty([]u8, []u8));
        source, files := if opts[].debug_info {
            encode_debug_files(self.codemap.files.items())
        } else {
            ("", "")
        };
        m.seal_debug_info(source, source.len != 0 || opts.retain_function_names, files);
        if m.goal.type == .JitOnly {
            each self.comptime_libraries& { _, lib |
                Qbe'fill_pending_dynamic_imports(m, lib[]);
            };
            Qbe'seal_imports(m);
            m.make_exec();
        };
        chunks := {comp.vtable.finish_qbe_module}(QbeModule.raw_from_ptr(m));
        @if(use_threads && show_backend_stats()) {
            frontend := clock_ms(MacosLibc.CLOCK_THREAD_CPUTIME_ID);// :TodoLinux
            // shared was deallocated when joined but it's in an arena so it's fine. meh
            @eprintln(">>> [CPU Time] frontend: %ms, codegen: %ms", frontend, shared.worker.codegen_time);  // :WrongClockTime
        };
        chunks
    };
    
    get_alloc :: fn(c: Compiler) Alloc = c.cast()[][]._get_alloc();
    compile_ast :: fn(c: Compiler, expr: *FatExpr, hint: Type) void = {
        self := c.cast()[][];
        res := self.poll_in_place(void, => self.compile_expr(expr, hint.want()));
        self.unwrap_report_error(void, res);
    };
    
    intern_type :: fn(c: Compiler, info: *TypeInfo) = 
        c.cast()[][]._intern_type(info[]);
        
    intern_func :: fn(c: Compiler, info: *Func) = 
        c.cast()[][].add_function(info[]);

    add_to_scope :: fn(c: Compiler, s: ScopeId, name: Symbol, type: Type, value: rawptr) void = {
        loc   := zeroed Span;
        c     := c.cast()[][];
        value := u8.ptr_from_raw(value).slice(c.get_info(type)[].stride_bytes.zext());
        e: FatExpr = (expr = (Value = (bytes = to_value(value, c._get_alloc()))), ty = type, loc = loc, done = true);
        _add_to_scope(c, s, name, (Finished = type), e);
    };
    
    // TODO: replace ^ with this one V in user space
    add_expr_to_scope :: fn(c: Compiler, s: ScopeId, name: Symbol, expr: FatExpr) void = {
        c     := c.cast()[][];
        scope := c.scopes.index(s);
        _add_to_scope(c, s, name, .Infer, expr);
    };
    // TODO: return addend like lookup_baked so you don't have to call both?
    reserve_baked :: fn(c: Compiler, jit_addr: ?[]u8) BakedVarId = {
        self := c.cast()[][];
        _, id := self._reserve_baked(jit_addr);
        id
    };
 
    import_frc :: fn(c: Compiler, bytes: []u8) CRes(ScopeId) = {
        self := c.cast()[][];
        @if(DISABLE_IMPORT_FRC) return(@err("TODO: DISABLE_IMPORT_FRC because scoping when you import the compiler is messed up"));
        self._import_frc(bytes)
    };
    
    cached_compile_module :: fn(path: Str, opts: *BuildOptions, out_alloc: Alloc) CRes([]u8) = {
        @if(DISABLE_IMPORT_FRC) return(@err("TODO: DISABLE_IMPORT_FRC because scoping when you import the compiler is messed up"));
        Incr'cached_compile_module(path, opts, out_alloc)
    };
    
    mangle_name :: fn(c: Compiler, fid: FuncId) Str = {
        self := c.cast()[][];
        self.fmt_fn_name(fid)
    };
    
    put_baked_var :: fn(c: Compiler, id: BakedVarId, v: BakedVar, type: Type) void = {
        self := c.cast()[][];
        dest := self.baked.values&.nested_index(id.id.zext());
        n := dest.name;
        dest[] = self.from_legacy(v);
        dest.type = type;
        dest.name = n;
    };
    
    get_codemap :: fn(c: Compiler) *CodeMap = {
        self := c.cast()[][];
        self.codemap
    };
    
    get_constants :: fn(c: Compiler, scope: ScopeId) []Symbol = 
        compiler_exports'get_names(scope, c.cast()[][]);
    
    get_constant :: fn(c: Compiler, scope: ScopeId, name: Symbol) ?Ty(rawptr, Type) = 
        compiler_exports'get_field(scope, name, c.cast()[][]);
};

CompilerRs :: ***SelfHosted;  // TODO: really this should be a new type so you can't accidently implement call functions by just adding extra indirection. hopefully the number of pointers makes it clear something's weird. 
fn cast(c: Compiler) CompilerRs #ir(.copy, .Kl);


/////////////////////////////////////////////////////////////////////////////////

fn current_comptime() *SelfHosted = {
    s := SelfHosted.ptr_from_raw(tls(.comptime)[]);
    ::ptr_utils(SelfHosted);
    @if(s.is_null()) return(s);
@debug_assert_eq(get_stack_base_for_tls(), s.main_thread_tls, "you can only call into comptime compiler functions on the main thread.");
    s
}

export_ffi_data :: {
    prefix :: """
        __builtin_compiler_abi :: %;
        __baked_compctx :: @builtin __baked_compctx;
        
        // TODO: take this out of the compiler
        fn size_of(T: Type) i64 #fold = { t := get_meta(T); t.stride_bytes.zext() }
    """;
        s := Type.scope_of(compiler_exports_s);
        names := get_constants(s);
        src := u8.list(names.len * 70, ast_alloc());
        @fmt(src&, prefix, next_abi_version());
        
        fns := rawptr.list(names.len, ast_alloc());
        fr := current_compiler_context();
        for names { name |
            fid := get_constant(FuncId, s, name) 
                || @panic("fill_export_ffi '%' is not a function", name.str());
            
            if generate_compctx_wrapper(fid, src&, fns.len) { p |
                fns&.push(p);
            };
        };
        (src.items(), fns.items())
};

fn fill_export_ffi(self: *SelfHosted) ExportFfi = {
    src, fns := export_ffi_data;
    (src = src, fns = fns)
}

compiler_exports :: {
    // adds type annotations for compctx par so need to run it before accessing the functions directly. 
    @run export_ffi_data;  
    compiler_exports_s
};

// TODO: instead of ComptimeOnly these should be expressed as imports that won't be filled at runtime 
//       because that way you could still cache comptime code and just provide a different import with the
//       compctx baked in when you want to run it in a new compiler. 
compiler_exports_s :: @struct {
    // TODO: fix error message if you forget a semi-colon in the string. 
    
    // TODO: do something if the bytes are too big for the allocation. 
    #export("lookup_baked_vbytes")
    lookup_baked_vbytes :: fn(vbytes: []u8, self) ?Ty(BakedVarId, i32) = {
        addr := u8.raw_from_ptr(vbytes.ptr);
        if self.baked.vmem&.get(addr) { it, addend, size |
            if size - addend < vbytes.len {
                // :fucked
            };
            @if_let(it) fn BakedVarId(it) => return(Some = (it, addend.intcast()));
        };
        .None
    };
    
    // TODO: these need to return a different BakedVarId if you had to deduplicate. 
    //       (instead of error if collide... tho rn it's just nothing)
    #export("cache_baked_vbytes")
    cache_baked_vbytes :: fn(addr: []u8, id: BakedVarId, self) void = {
        self.baked.vmem&.insert(addr, (BakedVarId = id)); // TODO: error if collide? 
    };
    
    #export("dyn_bake_relocatable_value")
    dyn_bake_value :: fn(bytes: Slice(u8), ty: Type, force_default_handling: bool, c) Slice(BakedEntry) = {
        r := c.emit_relocatable_constant_body(bytes, ty, force_default_handling); // TODO: sad extra binding forces it to instantiate the type. 
        c.unwrap_report_error(Slice(BakedEntry), r)
    };
    
    #export("if #macro")
    if_node :: fn(e: FatExpr, self) FatExpr = {
        assert(e.expr&.is(.Tuple), "@if expected tuple");
        parts := e.expr.Tuple&;
        if parts.len != 3 {
            e: CRes(void) = @err("expected @if(cond, if_true, if_false)");
            self.comp().report_error(e.Err);
        };
        (expr = (If = (cond = parts.index(0), if_true = parts.index(1), if_false = parts.index(2))), loc = e.loc, ty = UnknownType, done = false)
    };
    #export("slice #macro")
    slice_node :: fn(e: FatExpr, c) FatExpr = {
        e := c.box(e);
        (expr = (Slice = e), loc = e.loc, ty = UnknownType, done = false)
    };
    #export("uninitialized #macro")
    uninit_node :: fn(e: FatExpr, self) FatExpr = {
        ty := UnknownType;
        if !e&.is_raw_unit() {
            res := self.poll_in_place(Type) {
                self.eval(e&, Type)
            };
            ty = self.unwrap_report_error(Type, res);
        };
        (expr = .Uninitialized, loc = e.loc, ty = ty, done = false)
    };
    #export("loop #macro")
    loop_node :: fn(e: FatExpr, self) FatExpr = {
        e := self.box(e);
        (expr = (Loop = e), loc = e.loc, ty = Never, done = false)
    };
    
    #export("operator_star_prefix #fold")
    ptr :: fn(inner: Type, self) Type = {
        self.intern_type((Ptr = inner))
    };
    
    // These only exist for debugging the compiler when everything's broken so can't even compile the one defined in the language.
    #export("debug_log_int")
    debug_log_int :: fn(i: i64, _) void = println(i);
    
    #export("debug_log_str")
    debug_log_str :: fn(i: Str, _) void = println(i);
    
    #export("debug_log_bool")
    debug_log_bool :: fn(i: bool, _) void = println(i);
        
    #export("str #fold")
    str :: fn(i: Symbol, self) Str = {
        self.pool.get(i)
    };
    
    #export("sym #fold")
    sym :: fn(i: Str, self) Symbol = {
        self.pool.insert_borrowed(i, self.get_alloc())
    };
    
    #export("get_comptime_environment")
    env :: fn(self) *ComptimeEnvironment = {
        self.env
    };
    
    #export("compile_error")
    report_err :: fn(msg: Str, loc: Span, self) Never = {
        // TODO: command line arg to show more context? 
        println("=== ERROR ===");
        println(msg);
        self.codemap.show_error_line(loc, true /*TODO*/);
        panic("Comptime reported error.")
    };
  
    #export("safety_check_enabled")
    safe :: fn(check: SafetyCheck, self) bool = {
        build_options := self.get_build_options();
        if(check.raw() >= build_options.safety_checks&.len(), => return(true)); // allow adding new safety checks and then bootstrapping. 
        build_options.safety_checks&.index(check)[]
    };
    
    // TODO: if you name the thing something that collides, the error message is wrong. 
    #export("get_type_info_ref #fold")
    ty_info_r :: fn(ty: Type, self) *TypeInfo = {
        info := self.get_type(ty);
        while => info.is(.Named) {
            info = self.get_type(info.Named._0);
        };
        info
    };
    
    #export("Ty #fold")
    tuple2 :: fn(fst: Type, snd: Type, self) Type = {
        self.tuple_of(@slice(fst, snd))
    };
    #export("Ty #fold")
    tuplen :: fn(types: []Type, self) Type = {
        self.tuple_of(types)
    };
   
    #export("rawptr_from_value")
    rawptr_from_value :: fn(value: *Values, _) rawptr = value.jit_addr().rawptr_from_int();
    
    #export("Tag #fold")
    get_tag :: fn(tagged: Type, c) Type = {
        raw := c.raw_type(tagged);
        @if_let(c.get_type(raw)) fn Tagged(f) => {
            return(f.tag);
        };
        @panic("Expected @tagged found %", c.log_type(tagged));
        void
    };
    
    #export("Fn #fold")
    fn_type :: fn(arg: Type, ret: Type, c) Type = {
        c.intern_type(Fn = (arg = arg, ret = ret, unary = c.is_unary(arg)))
    };
    
    #export("IntType #fold")
    make_int_type :: fn(bit_count: i64, signed: bool, self) Type = {
        self.intern_type(Int = (bit_count = bit_count, signed = signed))
    };
    
    #export("builtin #macro")
    builtin :: fn(arg: FatExpr, self) FatExpr = {
        res := self.builtin_macro(arg);
        self.unwrap_report_error(FatExpr, res)
    };
    
    #export("unquote_macro_apply_placeholders")
    unquote_placeholders :: fn(args: []FatExpr, self) FatExpr = {
        args := args.assume_owned(temp());
        template := args&.pop().expect("ICE: unquote_placeholders always needs template arg");
        // Since we're probably going to call this with different values for the placeholders (like invoking the same macro multiple times),
        // we duplicate to not mess up the constant value of the template. 
        // TODO: i feel like the old version wasn't doing this. why did that work? 
        template := self.clone(template&);
        walk: Unquote = (compiler = self, placeholders = args);
        res := walk&.walk_expr(template&);
        self.unwrap_report_error(void, res);
        each(walk.placeholders, fn(e) => @assert(e.expr&.is(.Poison), "ice: unquote_placeholders didn't use all arguments\n%", template&.log(self)));
        self.renumber_expr(template&, .None); // :SLOW
        template
    };
    
    make_fn_type :: fn(self: *SelfHosted, arg: FatExpr, ret: FatExpr) FnType = {
        inner :: fn() Maybe(FnType) => {
            return :: local_return;
            types: []Type = @match(arg.expr&) {
                fn StructLiteralP(parts) => {
                    // TODO: ugh
                    x := parts.bindings.assume_owned(self.get_alloc());
                    x&.if_empty_add_unit();
                    parts.bindings = x.as_raw();
                    
                    @check(self.infer_type(parts)) return
                }
                fn Tuple(parts) => {
                    types: List(Type) = list(temp());
                    each parts { e | 
                        value := @check(self.immediate_eval_expr(e, self.get_or_create_type(Type))) return;
                        types&.push(Type.assume_cast(value&)[]);
                    };
                    types.items()
                }
                @default => {
                    value := @check(self.immediate_eval_expr(arg&, self.get_or_create_type(Type))) return; 
                    ty := Type.assume_cast(value&)[];
                    lst: List(Type) = list(temp());
                    lst&.push(ty);
                    lst.items()
                };
            };
            arg_ty := self.tuple_of(types);
            value := @check(self.immediate_eval_expr(ret&, self.get_or_create_type(Type))) return; 
            ret_ty := Type.assume_cast(value&)[];
            (Ok = (arg = arg_ty, ret = ret_ty, unary = types.len <= 1))
        };
        res := self.poll_in_place(FnType, => inner());
        self.unwrap_report_error(FnType, res)
    };
    
    #export("Fn #outputs(Type) #macro")
    fn_type_macro :: fn(arg: FatExpr, ret: FatExpr, c) FatExpr = {
        f_ty := make_fn_type(c, arg, ret);
        ty := c.intern_type(Fn = f_ty);
        value := c.to_values(Type, ty); 
        arg&.set(value, c.get_or_create_type(Type));
        arg
    };
    
    #export("FnPtr #outputs(Type) #macro")
    fn_ptr_type_macro :: fn(arg: FatExpr, ret: FatExpr, c) FatExpr = {
        f_ty := make_fn_type(c, arg, ret);
        ty := c.intern_type(FnPtr = (ty = f_ty));
        value := c.to_values(Type, ty); 
        arg&.set(value, c.get_or_create_type(Type));
        arg
    };
    
    #export("FnPtr #outputs(Type) #macro")
    fn_ptr_type_macro_single :: fn(ret: FatExpr, self) FatExpr = 
        fn_ptr_type_macro(empty_struct_literal(ret.loc), ret, self);
    
    #export("Fn #outputs(Type) #macro")
    fn_type_macro_single :: fn(ret: FatExpr, self) FatExpr = 
        fn_type_macro(empty_struct_literal(ret.loc), ret, self);
    
    #export("literal_ast")
    literal_ast :: fn(ty: Type, ptr: rawptr, c) FatExpr = {
        ptr := u8.ptr_from_raw(ptr);
        c.finish_layout(ty);
        bytes: i64 = c.get_info(ty)[].stride_bytes.zext();
        value := slice(ptr, bytes);
        value := c.from_bytes(value);
        
        // TODO: zero_padding?
        // TODO: caller should pass in loc?
        synthetic_ty((Value = (bytes = value)), c.last_loc, ty)
    };
    
    #export("type #outputs(Type) #macro")
    type_macro :: fn(e: FatExpr, self) FatExpr = {
        old := self.dispatch.enclosing_function;
        self.dispatch.enclosing_function = .None;
        ty := self.poll_in_place(Type) {()Maybe(Type)|
            @check(self.compile_expr(e&, .None)) local_return;
            (Ok = e.ty)
        };
        self.dispatch.enclosing_function = old;
        ty := self.unwrap_report_error(Type, ty);
        value := self.to_values(Type, ty); 
        e&.set(value, self.get_or_create_type(Type));
        e
    };
    
    #export("const_eval")
    const_eval_any :: fn(expr: FatExpr, ty: Type, result: rawptr, c) void = {
        size: i64 = c.get_info(ty)[].stride_bytes.zext();
        addr := u8.ptr_from_raw(result);
        out := slice(addr, size);
        
        res := c.poll_in_place(void) {()Maybe(void)|
            value := @check(c.immediate_eval_expr(expr&, ty)) local_return;
            b := value&.bytes();
            if b.len == size {
                out.copy_from(b);
                .Ok
            } else {
                @err("ICE: const_eval_any(%) size mismatch for value %", c.log_type(ty), log(value&, c, ty))
            }
        };
        c.unwrap_report_error(void, res);
    };
    
    #export("struct #outputs(Type) #macro")
    struct_macro_wrap :: fn(expr: FatExpr, self) FatExpr = {
        res := self.poll_in_place(FatExpr, => self.struct_macro(expr&));
        self.unwrap_report_error(FatExpr, res)
    }; 
    #export("union #outputs(Type) #macro")
    union_macro_wrap :: fn(expr: FatExpr, self) FatExpr = {
        res := self.poll_in_place(FatExpr, => self.union_macro(expr&));
        self.unwrap_report_error(FatExpr, res)
    }; 
    #export("tagged #outputs(Type) #macro")
    tagged_macro_wrap :: fn(expr: FatExpr, self) FatExpr = {
        res := self.poll_in_place(FatExpr, => self.tagged_macro(expr&));
        self.unwrap_report_error(FatExpr, res)
    };
    #export("enum #outputs(Type) #macro")
    enum_macro_wrap :: fn(arg: FatExpr, target: FatExpr, self) FatExpr = {
        res := self.poll_in_place(FatExpr, => self.enum_macro(arg&, target&));
        self.unwrap_report_error(FatExpr, res)
    };
    // note: the old verion didn't even use this ast node, it just did the work here like @struct, et al.
    //       maybe thats better, idk, lets see if i still need the name based hack this way. -- Aug 5s
    #export("as #macro")
    as_node :: fn(type: FatExpr, value: FatExpr, c) FatExpr = {
        value := c.box(value);
        type := c.box(type);
        (expr = (As = (type = type, value = value)), loc = value.loc, ty = UnknownType, done = false)
    };
    
    #export("debug_log_ast")
    debug_log_ast :: fn(e: FatExpr, self) void = {
        @println("[debug_log_ast] %; %", e.expr&.tag(), e&.log(self));
    };
    
    #export("debug_log_type")
    debug_log_type :: fn(type: Type, self) void = {
        @println("[debug_log_type %] %", type.as_index(), self.log_type(type));
    };
    
    #export("debug_log_func")
    debug_log_f :: fn(e: *Func, self) void = {
        self.codemap.show_error_line(e.loc, true /*TODO*/);
        @println("[debug_log_func] %", self.log(e));
    };
    
    #export("intern_type_ref")
    intern_type_ref :: fn(info: *TypeInfo, self) Type = {
        self.intern_type(info[]) // TODO: probably more useful if i make a deep copy, why else are you calling this. 
    };
    
    // Infers the type and avoids some redundant work if you duplicate the ast node in a bunch of places after calling this.
    // TODO: should take by reference so its clear that it mutates the deep structure of the node (not a copy)
    #export("compile_ast")
    compile_ast :: fn(expr: FatExpr, self) FatExpr = {
        old := self.dispatch.enclosing_function;
        self.dispatch.enclosing_function = .None;
        res := self.poll_in_place(void, => self.compile_expr(expr&, .None));
        self.dispatch.enclosing_function = old;
        self.unwrap_report_error(void, res);
        expr
    };
    
    // TODO: is this made redundant by the meta.fr enum helpers or do we need to use this too early? [
    #export("tag_value #fold")
    tag_value :: fn(enum_ty: Type, name: Symbol, self) i64 = {
        @match(self.get_type(enum_ty)) {
            fn Enum(it) => {
                // TODO: this is kinda dumb, could be in meta.fr but this us just easier right now. -- Jul 8
                if !self.get_type(it.raw).is(.Int) {
                    @panic("tag_value on @enum (not @tagged) that is not an int!");
                };
                enumerate it.fields { i, it | 
                    if it._0 == name {
                        @debug_assert(it._1&.is(.Small), "called tag_value but value was .Big");
                        return(it._1.Small._0);
                    };
                };
                @panic("bad enum case name % for %", self.pool.get(name), self.log_type(enum_ty))
            }
            fn Tagged(it) => {
                enumerate it.cases { i, it | 
                    if(it._0 == name, => return(i));
                };
                @panic("bad tagged case name % for %", self.pool.get(name), self.log_type(enum_ty))
            }
            fn Named(it) => tag_value(it._0, name, self);
            @default => @panic("% is not enum. (tried tag_value of %)",
                self.log_type(enum_ty),
                self.pool.get(name)
            );
        }
    };
    
    #export("c_str #fold")
    symbol_to_cstr :: fn(s: Symbol, self) CStr = 
        self.pool.get_c_str(s);
        
    #export("assert_compile_error #fold #macro #outputs(*CompileError)")
    assert_compile_error :: fn(e: FatExpr, self) FatExpr = {
        res := self.poll_in_place(void, => self.compile_expr(e&, .None));
        if !res&.is(.Err) {
            self.report_error2((ExpectedCompileError = e.loc), true /*TODO*/);
        };
        type := self.env.compile_error_pointer.expect("no @assert_compile_error before bootstrap");
        res := self.to_values(*CompileError, res.Err);
        e&.set(res, type);
        e
    };
    
    #export("FnPtr #fold")
    fn_ptr :: fn(arg: Type, ret: Type, self) Type = {
        f_ty: FnType = (arg = arg, ret = ret, unary = self.is_unary(arg));
        self.intern_type(FnPtr = (ty = f_ty))
    };
    
    #export("get_meta #fold")
    get_m :: fn(s: Type, self) TypeMeta = 
        self.get_info(s)[];
    
    #export("ast_alloc")
    get_aa :: fn(self) Alloc = {
        a := self.get_alloc();
        @if(abi_shift_native_to_easy) {
            a.vptr = bit_cast_unchecked(rawptr, @type a.vptr, translate_arena_allocator_fn);
        };
        @if(abi_shift_easy_to_native) Vm'wrap_alloc_with_native(self, a&);
        a
    };
    
    #export("current_compiler_context")
    get_cc :: fn(self) CompCtx = {
        vtable := init_driver_vtable(true);
        @if(abi_shift_easy_to_native) {
            hack :: @static(bool);  // because init_driver_vtable returns @static
            @if(!hack[]) Vm'wrap_vtable_with_native(self, vtable);
            hack[] = true;
        };
        (data = self.legacy_indirection, vtable = vtable)
    };
    
    #export("resolve_overload #fold")
    resolve_os :: fn(os: OverloadSet, arg: Type, ret: Type, loc: Span, self) FuncId = {
        f_ty: FnType = (arg = arg, ret = ret, unary = self.is_unary(arg));
        res := self.poll_in_place(FuncId, => self.resolve_by_type(os, f_ty, loc));
        self.unwrap_report_error(FuncId, res)
    };
    
    #export("get_function_ast #fold")
    get_func :: fn(fid: FuncId, resolve_sign: bool, resolve_body: bool, infer_sign: bool, infer_body: bool, self) *Func = {
        res := self.poll_in_place(void) {
            ret :: local_return;
            @if(resolve_sign) @check(self.ensure_resolved_sign(fid)) ret;
            @if(resolve_body) @try(self.ensure_resolved_body(fid)) ret;
            @if(infer_sign) {
                @check(self.infer_arguments(fid)) ret;
                @check(self.infer_return(fid)) ret;
            };
            @if(infer_body) @check(self.compile_body(fid)) ret;
            .Ok
        };
        self.unwrap_report_error(void, res);
        self.get_function(fid)
    };
    #export("require_layout_ready #fold")
    require_layout_ready :: fn(type: Type, self) void = {
        finish_layout(self, type);
    };
    
    #export("__builtin_compiler_has_feature #fold")
    has_feature_s :: fn(s: Str, _) bool = {
        if(s == "@franca/alloc_slice", => return(true)); 
        if(s == "@franca/fewer_builtins", => return(true)); 
        if(s == "@franca/no_merged_funcimpl", => return(true)); 
        if(s == "@franca/internal_pointers", => return(true)); 
        false
    };
    
    #export("import #fold")
    do_import :: fn(descriptor: Str, self) ScopeId = {
        desc := self.pool.insert_owned(descriptor.clone(self.get_alloc()).items());
        res := import_as_scope(self, desc, self.last_loc /*TODO*/, .None);
        self.unwrap_report_error(ScopeId, res)
    };
    
    #export("scope_from_value")
    scope_val :: fn(type: Type, ptr: rawptr, self) ?ScopeId = {
        ptr := u8.ptr_from_raw(ptr);
        ok := type == ScopeId || type == Type || type == FuncId || self.get_type(type).is(.Fn);
        if !ok {
            return(.None)
        };
        self.finish_layout(type);
        bytes: i64 = self.get_info(type)[].stride_bytes.zext();
        value := slice(ptr, bytes);
        value := self.from_bytes(value);
        res := value_to_scope(self, value, type, .None);
        scope := self.unwrap_report_error(ScopeId, res);
        (Some = scope)
    };
    
    #export("get_constants #fold")
    get_names :: fn(s: ScopeId, self) []Symbol = {
        if(s == NOSCOPE, => return(empty()));
        self.unwrap_report_error(void, self.fill_directory_scope(s));
        scope := self.scopes[s].vars&;
        names := Symbol.list(scope.len, self.get_alloc()); 
        // TODO: it's actually more useful for the callee if i return a []Var here and then 
        //       this could just be a slice without reallocating. that locks the Var abi
        //       which is bad but it's already locked elsewhere anyway so that's a bit of a lost cause at this point. 
        if true {
            break :: local_return;
            // scope.vars is in insertion order and constants are hoisted so they're always first. 
            each scope { v |
                v.kind == .Const || break();
                names&.push(v.name);
            };
        };
        names.items()
    };
    
    #export("get_constant")
    get_field_known :: fn(s: ScopeId, name: Symbol, type: Type, out: rawptr, self) bool = {
        if(s == NOSCOPE, => return(false));
        res := self.poll_in_place(?Values, => self.find_in_scope(s, name, type));
        value := self.unwrap_report_error(?Values, res) || return(false);
        // TODO: do we have to typecheck type_found?
        //       answer: YES
        size: i64 = self.get_info(type)[].stride_bytes.zext();
        // TODO: this needs to be based on a real coerce + type check not just size. 
        //       it's totally crippling if you get back a Type when you asked for a FuncId :FUCKED
        // :ThisIsTheProblemIdiot
        if size != value&.len() {
            return(false); // TODO: this really should be an error we catch from find_in_scope
        };
        //@debug_assert_eq(size, value&.len(), "get_constant() size mismatch");
        dest: []u8 = (ptr = u8.ptr_from_raw(out), len = size);
        dest.copy_from(value&.bytes());
        true
    };
    
    #export("get_constant")
    get_field :: fn(s: ScopeId, name: Symbol, self) ?Ty(rawptr, Type) = {
        if(s == NOSCOPE, => return(.None));
        var := self.scopes[s].lookup&.get(name) || return(.None);
        if(var.kind != .Const, => return(.None));
        res := self.poll_in_place(Ty(Values, Type), => self.find_const(var, .None));
        value, type := self.unwrap_report_error(Ty(Values, Type), res);
        value := @if(value&.is(.Big), value&, {
            self.get_alloc().boxed(Values, value)
        });
        (Some = (value.jit_addr().rawptr_from_int(), type))
    };
    
    // get a string representation of the type. 
    // we try to infer this as something reasonable based on constant declarations that had the type on the right.
    #export("typename #fold")
    typename :: fn(type: Type, self) Symbol = {
        self.pool.insert_borrowed(self.log_type(type), self.get_alloc())
    };
    
    #export("get_or_create_overloads #fold")
    gco :: fn(name: Symbol, scope: ScopeId, loc: Span, self) OverloadSet = {
        res := self.poll_in_place(OverloadSet, => self.get_or_create_overloads(name, scope, loc));
        self.unwrap_report_error(OverloadSet, res)
    };
    
    #export("add_to_overload_set #fold")
    ato :: fn(os: OverloadSet, fid: FuncId, self) void = {
        self.add_to_overload_set(os, fid);
    };
};

translate_arena_allocator_fn :: fn(r: *[]u8, a: *Ty(rawptr, Alloc.Action, rawptr, i64, i64)) void = {
    r[] = Arena'arena_allocator_fn(@splat_tuple a);
};

// reflects on the signeture to create a string that can be parsed by the runtime compiler 
// so the new compilation unit has a type safe way to call the functions. also injects an 
// extra *SelfHosted parameter so the functions are tied to thier compctx and comptime code 
// doesn't have to pass that around. 
// ctx as last param means the wrapper just has to set one register not rotate them. 
generate_compctx_wrapper :: fn(fid: FuncId, src: *List(u8), i: i64) ?rawptr = {
    func := get_function_ast(fid, false, false, false, false);
    sign := func.annotations.find(fn(it) => it.name == (@symbol export)) 
        || return(.None);
    if !func.get_flag(.ResolvedSign) {
        b := func.arg.bindings.index(func.arg.bindings.len - 1);
        @ct_assert(b.ty&.is(.Infer), func.loc, "missing implicit compctx parameter");
        b.ty = (Finished = *SelfHosted);
    };
    
    func := get_function_ast(fid, true, false, true, false);
    sign := const_eval(Str)(sign.args);
    
    tags_i := sign.index_of("#".ascii()) || sign.len;
    tags := sign.rest(tags_i);
    export_name := sign.slice(0, sign.index_of(" ".ascii()) || sign.len);
    
    args := func.arg.bindings&.items();
    args := args.slice(0, args.len - 1);
    @fmt(src, "fn %(", export_name);
    sig :: fn(extra) => {
        each args { it |
            @fmt(src, "%: %, ", it.name.unwrap().str(), it.unwrap_ty().show_type());
        };
        @fmt(src, "%) %", extra, show_type(func.finished_ret.unwrap()));
    };
    sig("");
    @fmt(src, " % #noinline = {\n", tags);
    @fmt(src, "    % :: fn(", export_name);
    sig("_ctx: i64");
    @fmt(src, " #comptime_addr(%) %;\n", i, tags);
    @fmt(src, "    %(", export_name);
    each args { it |
        @fmt(src, "%, ", it.name.unwrap().str());
    };
    @fmt(src, "__baked_compctx)\n}\n");
    
    if abi_shift_native_to_easy {
        fid = wrap_with_easy_abi(fid);
    };
    
    fr := current_compiler_context();
    (Some = fr.get_jitted(fid))
};

fn wrap_with_easy_abi(fid: FuncId) FuncId = {
    @debug_assert(abi_shift_native_to_easy);
    func := get_function_ast(fid, true, true, true, false);
    arg := func.finished_arg.unwrap();
    ret := func.finished_ret.unwrap();
    fid := FuncId.const_eval(@{ 
        R :: @[@literal ret];
        A :: @[@literal arg];
        callee :: @[@literal fid];
        (fn(r: *R, a: *A) void = {
            r[] = callee(@splat_tuple a);
        })
    });
    wrapped := get_function_ast(fid, false, false, false, false);
    wrapped.name = sym(@tfmt("easy_abi(%)", func.name.str()));
    fid
}

fn splat_tuple(arg: FatExpr) FatExpr #macro = {
    arg = compile_ast(arg);
    info := get_type_info_ref(arg.ty);
    @ct_assert(info.is(.Ptr), arg.loc, "@splat_tuple expected pointer");
    T := info.Ptr;
    info := get_type_info_ref(T);
    if !(info.is(.Struct) && info.Struct.is_tuple) {
        return @{ @[arg][] };
    };
    result := FatExpr.list(ast_alloc());
    require_layout_ready(T);
    each info.Struct.fields { f |
        result&.push(@{ @[@literal T].get_field_ptr(@[arg], @[@literal f])[] })
    };
    (expr = (Tuple = result.as_raw()), ty = UnknownType, done = false, loc = arg.loc)
}

fn show_type(it: Type) Str = {
    fr := current_compiler_context();
    info := fr.get_type(it);
    @if_let(info) fn Tagged(it) => if it.cases.len > 0 && it.cases[0]._0 == @symbol Some {
        return(@tfmt("?%", show_type(it.cases[0]._1)));
    };
    // :TodoLostTypeName
    //if __driver_abi_version < 1349 {  // :UpdateBoot typename used to not work
        names :: @const_slice(
            "Span", "BakedVarId", "ScopeId", "Symbol", "Type",
            "ComptimeEnvironment", "Func", "Values", "TypeMeta",
            "BakedEntry", "OverloadSet", "FuncId",
            "Alloc", 
        );
        for names { name |
            ty := get_constant(Type, TOP_LEVEL_SCOPE, sym(name)).unwrap();
            @if(ty == it) return(name);
        };
        @if_let(info) fn Ptr(it) => return(@tfmt("*%", show_type(it[])));
    //};
    // TODO: this should probably move to typename (but will also need to stay here for :UpdateBoot)
    @if_let(info) fn Struct(f) => {
        if get_constant(Type, Type.scope_of(it), @symbol Element) { it |
            return(@tfmt("[]%", show_type(it)));
        };
        if f.is_tuple {
            out := @ref u8.list(temp());
            @fmt(out, "Ty(");
            each f.fields { f |
                @fmt(out, "%, ", show_type(f.ty));
            };
            @fmt(out, ")");
            return out.items();
        };
    };
    
    @if(it == SafetyCheck) return("SafetyCheck");  // HACK
    fr'vtable'log_type(fr.data, it) // it.typename().str() :UpdateBoot
};
