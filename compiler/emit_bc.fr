//! Converts simple ASTs into my bytecode-ish format.
//! All comptime execution for a function is finished before it reaches this phase.
//! - Flatten nested expressions to stack operations.
//! - Convert control flow to explicit basic blocks.
//! - Bind non-local return labels.
//! - Reduce variable usage to register sized loads/stores.
//! - Convert large arguments/returns to references and remap signatures to use only register sized types.

// UNFINISHED: this cant be enabled yet!
// TODO: you want to do this so you don't have to compile things twice, it seems to not be much slower to do the constants.
//       but... you can't start doing it too early, because you have to compile stuff before you've compiled the bake functions. 
//       so regardless, you have to deal with the complexity of some functions havign two copies (jit + baked),
//       so maybe at that point you might as well not bake for anything you know will only be used at comptime 
//       (anything that calls a #ct function).   -- Jul 19
ALWAYS_BAKE_CONSTANTS :: false;
// TODO: abstract out body.mix_hash(i64) instead of pasting random numbers around. 

FnBody :: @struct(
    blocks: RsVec(BasicBlock),
    // TODO: would be nice if i could sort these so less alignment padding on my backends. 
    //       but thats a pain because then you need to map indexes.
    vars: RsVec(VarSlotType),
    var_names: RsVec(?Var),
    when: ExecStyle,
    alloc: Alloc,
    hash: i64 = 0,
    signeture: PrimSig,
    func: FuncId,
    name: Symbol,
    switch_payloads: RsVec(RsVec(SwitchPayload)),
    current_block: BbId,  // TODO: remove. only needed while building
    clock: u16,           // TODO: remove. only needed while building
    want_log: bool,       // TODO: remove. only needed while building
);
VarSlotType :: @struct(size: u16, align: u16);
SwitchPayload :: @struct(value: i64, block: BbId);

// TODO: suddenly my new PrimSig makes this super chonky. 80 bytes is not ok
Bc :: @tagged(
    CallDirect: @struct(sig: PrimSig, f: FuncId, tail: bool),     // <args:m> -> <ret:n>
    CallFnPtr: @struct(sig: PrimSig),                             // <ptr:1> <args:m> -> <ret:n>
    PushConstant: @struct(value: i64, ty: Prim),                  // _ -> <v:1>
    JumpIf: @struct(true_ip: BbId, false_ip: BbId, slots: u16),   // <args:slots> <cond:1> -> !
    Goto: @struct(ip: BbId, slots: u16),                          // <args:slots> -> !
    GetNativeFnPtr: FuncId,                                       // _ -> <ptr:1>
    Load: Prim,                                                   // <ptr:1> -> <?:n>
    StorePost: Prim,                                              // <?:n> <ptr:1> -> _
    StorePre: Prim,                                               // <ptr:1> <?:n> -> _
    AddrVar: @struct(id: u16),                                    // _ -> <ptr:1>
    SaveSsa: @struct(id: u16, ty: Prim),                          // <p:1> -> _
    LoadSsa: @struct(id: u16),                                    // _ -> <p:1>
    IncPtrBytes: u16,                                             // <ptr:1> -> <ptr:1>
    PeekDup: u16,                                                 // <x:1> <skip:n> -> <x:1> <skip:n> <x:1>,
    CopyBytesToFrom: u16,                                         // <to_ptr:1> <from_ptr:1> -> _
    LastUse: @struct(id: u16),                                    // _ -> _
    Unreachable,                                                  // _ -> !
    GetCompCtx,                                                   // _ -> <ptr:1>
    NoCompile,
    PushGlobalAddr: BakedVarId,
    Snipe: u16,
    Ret0, // big return uses this too because code has already written to indirect return address.
    Ret1: Prim,
    Ret2: Ty(Prim, Prim),
    Nop,
    Intrinsic: Intrinsic,
    Switch: i64,
);
::enum(Intrinsic);

PrimSig :: @struct(
    args: Slice(Prim) = empty(),
    ret1: ?Prim,
    ret2: ?Prim,
    return_value_bytes: u16 = 0,
    first_arg_is_indirect_return: bool = false,
    no_return: bool = false,
    arg_int_count: u8 = 0,
);

BbId :: @struct(id: u16);
BasicBlock :: @struct(
    insts: RsVec(Bc),
    arg_prims: Slice(Prim),
    incoming_jumps: u16,
    clock: u16,
);

//
// TODO: less pain
// - versions of @assert that return a compile error instead of crashing. 
// - struct init fields needing to be in order is super dumb!
// - @sanity(.DebugAsserts)? 
// - access fields on rvalues
// - index operator looks for the wrong overload when used in nested expressions
// - deal with infinite loops if you try to put from_raw in fn tagged because it triggers for the tag of ?i64 but then the raw of that needs to return a ?i64, it needs to be like @rec but just @rec doesnt wokr 
// - TODO: it seems the compiler segfaults trying to cope with this line?????????! -- Jul 7 //ty := binding.ty&.unwrap();

EmitBc :: @struct(
    program: *SelfHosted,
    last_loc: Span,
    locals: List(List(u16)),
    var_lookup: HashMap(Var, u16),
    inlined_return_addr: HashMap(LabelId, ReturnAddr),
    out_alloc: Alloc,
    is_ssa_var: BitSet,
);

fn hash(s: *LabelId) i64 #redirect(*u32, i64);

ReturnAddr :: @struct(
    block: BbId,
    result_loc: ResultLoc,
    store_res_ssa_inst: ?Ty(BbId, i64),
    res_ssa_id: ?u16,
    used: bool,
);

ResultLoc :: @enum(i64) (PushStack, ResAddr, Discard);
::enum(ResultLoc);
::enum(ExecStyle);

fn emit_bc(comp: CompilerRs, f: FuncId, when: ExecStyle) Res(*FnBody) = {
    if ALWAYS_BAKE_CONSTANTS.or(when == .Aot) {|
        // :bake_relocatable_value
        comp.check_for_new_aot_bake_overloads().unwrap(); // TODO: unify error types!
    };
    self: EmitBc = (
        program = comp[][], 
        last_loc = comp[f].loc,
        locals = list(temp()),
        var_lookup = init(temp()),
        inlined_return_addr = init(temp()),
        // TODO: change this if I want to keep them around for deduplicating. 
        //       the others are always just needed while working on it.
        //       then mark and reset temp at the end of this function. 
        //       have a pool and recycle old FnBodys that we decided we didn't need because they were duplicates.  
        out_alloc = comp[][].get_alloc(),  
        is_ssa_var = empty(),
    );
    func := comp[f]&; // TODO: you can't do this inline with a call that needs a *Func. it looks for an overload for **Func. -- Jul 5
    
    // TODO: HACK. you want to make sure anything called inside the stack tracing functions doesn't try to trace themselves,
    //      this happens naturally by just not inserting them when not compiled yet, but you might
    //      emit_bc again later for aot. this just remembers if tracing was ready the first time we saw the function.  -- Jun 26
    if comp[][].env.inject_function_header.is_none() {|
        func.set_flag(.NoStackTrace);
    };
    
    sig: PrimSig = (ret1 = .None, ret2 = .None);
    //test := comp.prim(i64);
    
    if func.has_tag(.log_ast) {|
        // TODO
        // @println("%", func.log(self.program.pool));
    };

    self.locals&.push(list(self.out_alloc));
    
    body := @try(self.program.empty_fn_body(f, when)) return;
    
    zone := zone_begin(.EmitBc); // TODO: defer
    @if(ENABLE_TRACY) {
        real_name := get_string(comp.cast(), func.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    
    res := self&.emit_body(body, f);
    zone_end(zone);
    
    if res&.is_err() {|
        res.Err.span = self.last_loc;
        return(Err = res.Err)
    };
    
    if body.hash != 0 && self.program.get_build_options()[].deduplicate_bytecode {|
        if self.program.saved_bytecodes&.get_key(body&) { check | 
            if check.func.as_index() != f.as_index() {|  // TODO: this shouldn't happen?
                // TODO: this is sketchy... maybe? had to think of a test that would hurt it tho. 
                func.body = (Redirect = check.func); 
                
                //old := check.func;
                //old_func := comp[old]&;
                
                //size := 0;
                //each check.blocks { b |
                //    size += b.insts.len;
                //};
                
                //@println("% ops saved. % F%_% <-> F%_%", size, when,
                //    old.as_index(), get_string(comp.cast(), old_func.name),
                //    f.as_index(), get_string(comp.cast(), func.name), 
                //);
            };
            return(Ok = check);
        };
        
        self.program.saved_bytecodes&.insert(body, when);
    };
    
    (Ok = body)
}

:: List(Symbol);
fn empty_fn_body(comp: *SelfHosted, func: FuncId, when: ExecStyle) Res(*FnBody) = {
    f := comp[func]&;
    f_ty := @unwrap(f.finished_ty(), "ICE: fn type not ready") return;
    ptr := comp.bytecodes&.push(
        blocks = empty(),
        vars = empty(),
        var_names = empty(),
        when = when,
        alloc = comp.get_alloc(),
        signeture = @try(comp.prim_sig(f_ty, f.cc.expect("known cc"))) return,
        func = func,
        name = f.name,
        switch_payloads = empty(),
        current_block = (id = 0),
        clock = 0,
        want_log = f.has_tag(.log_bc),
    );
    
    (Ok = ptr)
}

fn unwrap(self: *LazyType) Type = {
    // TODO: compiler segfaults if you write this with @match!! -- Jul 9 :FUCKED
    assert(self.is(.Finished), "type not ready");
    self.Finished
}

fn var(self: *Binding) ?Var = {
    @match(self.name) {
        fn Var(v) => (Some = v);
        @default => .None;
    }
}

// TODO: if you have an unclosed string it randomly decides later top level vars are undeclared? -- Jul 7 
fn bind_args(self: *EmitBc, result: *FnBody, arguments: *Pattern) Res([] Prim) #once = {
    arity: u16 = arguments.bindings.len.trunc();

    func := self.program[result.func]&;
    f_ty := func.finished_ty().expect("known function type");
    arg_ty := f_ty.arg;
    pushed := if(result.signeture.first_arg_is_indirect_return, => 1, => 0);
    already_did_prims := self.program.get_primitives(@as(PrimKey) (arg_ty, arity, false, false)).is_some(); // TODO: make it so this is always true. -- Jul 6
    :: if(?List(Prim));
    prim_args: ?List(Prim) = if(already_did_prims, => .None) {|
        (Some = list(self.program.get_alloc()))
    };
    // reversed because they're on the stack like [0, 1, 2]
    each_rev arguments.bindings { binding |
        continue :: local_return;
        ty := binding.unwrap_ty();
        
        // TODO:? probably fine, i just set to const in closure capture but then shouldn't be adding to vars below.
        // TODO: the frontend needs to remove the 'const' parts from the ast in DeclVarPattern
        ::if_opt(Var, Str);
        @err_assert(binding.kind != .Const, "arg '%' was const!", 
            if(binding.var(), fn(name: Var) => self.program.pool.get(name.name), => "_")) return;

        info := self.program.get_info(ty);

        if(info.size_slots == 0, => continue());
        :: if(u16);
        id: u16 = if info.pass_by_ref {|
            pushed += 1;
            if prim_args& { args |
                args.push(.P64);
            };
            // TODO: callee make copy if it wants to modify
            self.save_ssa_var(result)
        } else {|
            slots := self.program.slot_count(ty);
            id := result.add_var(self, ty);
            @switch(slots) {
                @case(0) => (); 
                @case(1) => {
                    ty := @unwrap(self.program.prim(ty), "expected prim but found %", self.program.log_type(ty)) return;
                    pushed += 1;
                    if prim_args& { args | 
                        args.push(ty);
                    };
                    self.addr_var(result, id);
                    result.push(StorePost = ty);
                };
                @case(2) => {
                    types := self.program.flat_tuple_types(ty);
                    offset_2 := align_to(
                        self.program.get_info(types[0]).stride_bytes().zext(),
                        self.program.get_info(types[1]).align_bytes().zext(),
                    );
                    p0 := self.program.prim(types[0]).expect("ret1 prim");
                    p1 := self.program.prim(types[1]).expect("ret2 prim");
                    self.addr_var(result, id);
                    result.inc_ptr_bytes(offset_2.trunc());
                    result.push(StorePost = p1);
                    self.addr_var(result, id);
                    result.push(StorePost = p0);
                    pushed += 2;
                    if prim_args& { args |
                        args.push(p1); //reverse cause we reverse at the end
                        args.push(p0);
                    }
                };
                @default => panic("ICE: unreachable because large args are passed by reference");
            };
            @as(u16) id
        };

        ::?*List(u16);
        locals := self.locals.last().unwrap().push(id);
        if binding.var() { name |
            prev := self.var_lookup&.insert(name, id);
            @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program.pool));
        };
    };

    //assert_eq!(pushed, result.signeture.arg_slots);
    if prim_args& { args |
        args.reverse(); // sigh
        //debug_assert_eq!(&args, result.signeture.args); // TODO: remove all this shit cause now its redundant.
        prims := self.program.primitives&;
        //debug_assert!(result.signeture.first_arg_is_indirect_return || (pushed as usize == args.len()));
        just_args := args.items().clone(self.program.get_alloc()).rs();
        prims.insert((arg_ty, arity, false, false), just_args);
        args.insert(0, .P64); // sad
        //debug_assert!(!result.signeture.first_arg_is_indirect_return || (pushed as usize == args.len()));
        prims.insert((arg_ty, arity, true, false), args[].rs());
    };

    key: PrimKey = (arg_ty, arity, result.signeture.first_arg_is_indirect_return, false);
    arg_sig := self.program.get_primitives(key).unwrap();
    (Ok = arg_sig)
}

fn get_primatives(self: *EmitBc, ty: Type) [] Prim = {
    self.program.as_primatives(ty)
}

fn compile_for_arg(self: *EmitBc, result: *FnBody, arg: *FatExpr, arity: usize) Res(bool) = {
    info := self.program.get_info(arg.ty);
    // If the whole thing is passed in registers, cool, we're done.
    if !info.pass_by_ref {|
        // arity isn't always the same as info.size_slots because of two slot structs like slices. 
        @try(self.compile_expr(result, arg, .PushStack, false)) return; // TODO: tail if its for a ret of the main function?
        return(Ok = false);
    };

    if arity == 1 {|
        id := result.add_var(self, arg.ty);
        self.addr_var(result, id);
        @try(self.compile_expr(result, arg, .ResAddr, false)) return;
        self.addr_var(result, id);
        return(Ok = true);
    };
    
    @assert(arg.expr&.is(.Tuple), "TODO: % %", self.program.log_type(arg.ty), arity);
    parts := arg.expr.Tuple;
    info := self.program.get_type(arg.ty);
    @assert(info.is(.Struct), "ICE: expected fn args to be tuple");
    // TODO: probably assert(info.Struct.is_tuple) or we're in an odd place. need to be more consistant about what an <arguments> is -- Jul 8
    fields := info.Struct.fields;
    //@debug_assert_eq(fields.len(), self.program.arity(arg).zext());   // TODO: why do we care about this? we have it from the tuple anyway. is it just about agreeing with the overloading thing? -- Jul 8
    @debug_assert_eq(fields.len(), parts.len(), "ICE?: not enough parts for type in compile_for_arg");

    _pushed := 0;
    // TODO: sad that I can't zip iterators
    enumerate parts { i, val |
        continue :: local_return;
        ty := fields[i].ty;
        info := self.program.get_info(ty);
        if !info.pass_by_ref {|
            _pushed += info.size_slots.zext();
            @try(self.compile_expr(result, val, .PushStack, false)) return;
            continue();
        };
        
        @match(val.expr&) {
            fn Deref(arg) => {
                _pushed += 1;
                // TODO: this is probably unsound.
                //       we're assuming that we can defer the load to be done by the callee but that might not be true.
                @try(self.compile_expr(result, arg[], .PushStack, false)) return;
                continue();
            }
            // TODO: factor out aot handling from main value handling so can use here too. -- Jun 3
            fn Value(f) => {
                if result.when == .Jit && !ALWAYS_BAKE_CONSTANTS {|
                    // TODO: this gets super bad if the callee isn't properly copying it because they'll be nmodifying something we think is constant
                    _pushed += 1;
                    result.push(PushConstant = (
                        value = f.bytes&.jit_addr(),  // Note: we know it's pass_by_ref so its large and not stored inline. 
                        ty = .P64,
                    ));
                    continue();
                };
                // else: fallthrough
            }
            @default => (); // fallthrough
        };

        _pushed += 1;
        id := result.add_var(self, ty);
        self.addr_var(result, id);
        @try(self.compile_expr(result, val, .ResAddr, false)) return;
        self.addr_var(result, id);
    };
    (Ok = true)
}

fn store_pre(self: *EmitBc, result: *FnBody, ty: Type) void = {
    slots := self.program.slot_count(ty);
    @switch(slots) {
        @case(0) => ();
        @case(1) => {
            ty := self.program.prim(ty).unwrap();
            result.push(StorePre = ty);
        };
        @case(2) => {
            types := self.program.flat_tuple_types(ty);
            offset_2 := align_to(
                self.program.get_info(types[0]).stride_bytes().zext(),
                self.program.get_info(types[1]).align_bytes().zext(),
            );
            result.push(PeekDup = 2); // grab the pointer
            result.inc_ptr_bytes(offset_2.trunc());
            result.push(StorePost = self.program.prim(types[1]).unwrap());
            result.push(StorePre = self.program.prim(types[0]).unwrap());
        };
        @default => panic("ICE: Tried to store big value.");
    }
}

fn load(self: *EmitBc, result: *FnBody, ty: Type) void = {
    slots := self.program.slot_count(ty);
    @switch(slots) {
        @case(0) => ();
        @case(1) => {
            ty := self.program.prim(ty).unwrap();
            result.push(Load = ty);
        };
        @case(2) => {
            types := self.program.flat_tuple_types(ty);
            offset_2 := align_to(
                self.program.get_info(types[0]).stride_bytes().zext(),
                self.program.get_info(types[1]).align_bytes().zext(),
            );
            result.push(PeekDup = 0);
            result.push(Load = self.program.prim(types[0]).unwrap());
            result.push(PeekDup = 1);
            result.inc_ptr_bytes(offset_2.trunc());
            result.push(Load = self.program.prim(types[1]).unwrap());
            result.push(Snipe = 2);
        };
        @default => panic("ICE: Tried to load big value.");
    }
}

fn emit_body(self: *EmitBc, result: *FnBody, f: FuncId) PRes #once = {
    func := self.program[f]&;

    body: *FatExpr = @match(func.body&) {
        fn Normal(body) => body;
        fn Intrinsic(op) => {
            // Direct calls will be inlined in emit_bc but someone might be trying to call through a function pointer. 
            // TODO: don't add to callees for direct calls. -- Jul 24
            entry_block := result.push_block(result.signeture.args);
            result.push(Intrinsic = op[]);
            @debug_assert(result.signeture.ret2.is_none(), "So far all intrinsics return one thing.");
            result.push(Ret1 = result.signeture.ret1.unwrap());
            result.hash = 0;
            return(.Ok)
        }
        // You should never actually try to run this code, the caller should have just done the call,
        // so there isn't an extra indirection and I don't have to deal with two bodies for comptime vs runtime,
        // just two ways of emitting the call.
        @default => return(.Ok);
    };

    entry_block := result.push_block(empty());
    entry_block_sig := @try(self.bind_args(result, func.arg&)) return;
    result[entry_block].arg_prims = entry_block_sig;
    ::if(ResultLoc); ::if([]Prim);
    // We represent the indirect return argument as the left-most thing on the stack,
    // so after popping all the args, its at the top and we can emit the thing normally.
    result_location: ResultLoc = if(result.signeture.first_arg_is_indirect_return, => .ResAddr, => .PushStack);
    // Note: this is different from the body expr type because of early returns.
    ret := self.program[f].finished_ret.unwrap();
    prims: []Prim = if(result.signeture.first_arg_is_indirect_return, => empty(), => self.get_primatives(ret));
    return_block := result.push_block(prims);

    result.current_block = entry_block;

    do_stacktrace := self.program.env.inject_function_header.is_some() && !func.get_flag(.NoStackTrace);
    // eventually this will be exposed as a language feature, but for now its just used for stack traces.
    has_defers := do_stacktrace;

    if do_stacktrace {|
        (push_cb, _) := self.program.env.inject_function_header.unwrap();
        result.push(PushConstant = (
            value = f.idx(),
            ty = .I32,
        ));
        sig := @try(self.program.prim_sig((arg = FuncId, ret = void, arity = 1), .CCallReg)) return;
        result.push(CallDirect = (sig = sig, f = push_cb, tail = false));
    };

    // TODO: indirect return tail
    @try(self.compile_expr(
        result,
        body,
        result_location,
        !result.signeture.first_arg_is_indirect_return && !has_defers,
    )) return;

    if result[return_block].incoming_jumps.gt(0).or(has_defers) {|
        result.push(Goto = (
            ip = return_block,
            slots = result.signeture&.ret_slots(),
        ));
        result[return_block].incoming_jumps += 1;
        result.current_block = return_block;
        if do_stacktrace {|
            (_, pop) := self.program.env.inject_function_header.expect("only stacktrace when ready");
            sig := @try(prim_sig(
                self.program,
                (arg = void, ret = void, arity = 1),
                .CCallReg,
            )) return;
            // TODO: this just totally kills tail calls which is sad. tho really, since it returns nothing, it should always tailcall out of itself. -- Jun 25
            // we're in the return block
            result.push(CallDirect = (sig = sig, f = pop, tail = false));
        }
    } else {|
        // Since there are no defers and no early returns, we don't need an extra trampoline block.
        result.push_to(return_block, .NoCompile);
    };

    self.locals&.pop().expect("outer scope to exist");
    assert(self.locals.is_empty(), "ICE: bc leaked scopes");

    if !ret.is_never() {|
        slots := self.program.slot_count(ret); // TODO: this is in the sig so don't look it up again. 
        op: Bc = @switch(slots) {
            @case(1) => {
                a := self.program.prim(ret).expect("return prim");
                //debug_assert_eq!(BigOption::Some(a), result.signeture.ret1); // TODO: debug_assert. derive eq
                (Ret1 = a)
            };
            @case(2) => {
                (a, b) := @try(self.program.prim_pair(ret)) return;
                //debug_assert_eq!(BigOption::Some(a), result.signeture.ret1); // TODO: debug_assert. derive eq
                //debug_assert_eq!(BigOption::Some(b), result.signeture.ret2);
                (Ret2 = (a, b))
            };
            @default => .Ret0; // void or indirect return
        };

        result.push(op);
    };
    
    // TODO: bring this back
    //if result.want_log {|
    //    @println("%", result.log(self.program));
    //};
    .Ok
}

fn emit_runtime_call(
    self: *EmitBc,
    result: *FnBody,
    f_ty: FnType,
    cc: CallConv,
    arg_expr: *FatExpr,
    result_location: ResultLoc,
    can_tail: bool,
    $do_call: @Fn(sig: *PrimSig, tail: bool) void,
) PRes = {
    sig := @try(prim_sig(self.program, f_ty, cc)) return;

    :: if(?u16);
    result_var: ?u16 = if sig.first_arg_is_indirect_return && result_location != .ResAddr {|
        id := result.add_var(self, f_ty.ret);
        self.addr_var(result, id);
        (Some = id)
    } else {|
        .None
    };

    if cc == .CCallRegCt {|
        result.push(.GetCompCtx);
    };
    
    any_by_ref := @try(self.compile_for_arg(result, arg_expr, f_ty.arity.zext())) return;
    // if any args are pointers, they might be to the stack and then you probably can't tail call.
    // TODO: can do better than this without getting too fancy, function pointers are fine, and anything in constant data is fine (we know if the arg is a Values).
    // TODO: !tail to force it when you know its fine.
    tail := can_tail && !self.program.get_info(f_ty.arg).contains_pointers() && !any_by_ref;
    
    do_call(sig&, tail);
    
    slots := self.slot_count(f_ty.ret);
    if slots > 0 {|
        @match(result_location) {
            fn PushStack() => {
                if sig.first_arg_is_indirect_return {|
                    if result_var { id |
                        self.addr_var(result, id);
                    };
                    self.load(result, f_ty.ret);
                }
            }
            fn ResAddr() => if(!sig.first_arg_is_indirect_return, => self.store_pre(result, f_ty.ret));
            fn Discard() => if(!sig.first_arg_is_indirect_return, => result.pop(slots));
        };
    } else {|
        if result_location == .ResAddr {|
            // pop dest!
            result.pop(1);
        };
    };

    if f_ty.ret.is_never() {|
        result.push(.Unreachable);
    };
    
    .Ok
}

fn unwrap_ty(self: *Binding) Type = {
    assert(self.ty&.is(.Finished), "type not ready");
    self.ty.Finished
}

fn compile_stmt(self: *EmitBc, result: *FnBody, stmt: *FatStmt) PRes = {
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Eval(expr) void => return(self.compile_expr(result, expr, .Discard, false));
        fn DeclVar(f) void => {
            @assert_ne(f.name.kind, VarType.Const);
            assert(f.ty&.is(.Finished), "type not ready");
            ty := f.ty.Finished;

            id := result.add_var(self, ty);
            if result.want_log {|
                // TODO: being this back -- Jul 8
                //extend_options2(&mut result.var_names, id as usize);
                //result.var_names[id as usize] = (Some = name);
            };
            self.addr_var(result, id);
            @try(self.compile_expr(result, f.value&, .ResAddr, false)) return;
            prev := self.var_lookup&.insert(f.name, id);
            self.locals.last().unwrap().push(id);
            @assert(prev.is_none(), "shadow is still new var");
        }
        fn Set(f) void => return(self.set_deref(result, f.place&, f.value&));
        fn DeclVarPattern(f) void => {
            // TODO: test for evaluation order
            @if_let(f.value.expr&) fn Tuple(parts) => {
                @assert_eq(parts.len(), f.binding.bindings.len(), "ICE: DeclVarPattern tuple size mismatch");
                enumerate parts { i, value |
                    b := f.binding.bindings[i]&; // TODO: zip
                    @assert_ne(b.kind, .Const);
                    @try(self.do_binding(result, b.var(), b.unwrap_ty(), value)) return;
                };
                return(.Ok);
            };
            
            if 1 == f.binding.bindings.len() {|
                b := f.binding.bindings[0]&;
                return(self.do_binding(result, b.var(), b.unwrap_ty(), f.value&));
            };
            
            // It's a destructuring (not inlined args)
            // We store the whole value in a stack slot and then save pointers into different offsets of it as thier own variables.
            full_id := result.add_var(self, f.value.ty);
            self.addr_var(result, full_id);
            @try(self.compile_expr(result, f.value&, .ResAddr, false)) return;
            info := self.program.get_type(f.value.ty);
            @err_assert(info.is(.Struct), "destructure must be tuple") return;
            fields := info.Struct.fields&;
            @assert_eq(fields.len(), f.binding.bindings.len(), "destructure size mismatch");
            enumerate f.binding.bindings.items() { i, b |
                f := fields[i]&; // TODO: zip
                continue :: local_return;
                @err_assert(f.kind != .Const, "TODO: destructuring skip const fields") return;
                
                name := b.var().unwrap();
                self.addr_var(result, full_id);
                result.inc_ptr_bytes(f.byte_offset.trunc());
                id := self.save_ssa_var(result);
                // TODO: bring this back
                //if result.want_log {
                //    extend_options2(&mut result.var_names, id as usize);
                //    result.var_names[id as usize] = BigOption::Some(name);
                //}
                // TODO: a test that fails if you typo put this line here. -- Jul 4 (see devlog.md)
                // self.addr_var(result, id);
                self.locals.last().unwrap().push(id);
                prev := self.var_lookup&.insert(name, id);
                @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program.pool));
            };
        };
        fn Noop() void => ();
        // Can't hit DoneDeclFunc because we don't re-eval constants.
        @default => panic("ICE: stmt not desugared");
    };
    // Note: ^early return!
    .Ok
}

fn do_binding(self: *EmitBc, result: *FnBody, name: ?Var, ty: Type, value: *FatExpr) PRes = {
    id := result.add_var(self, ty);
    // TODO: bring this back
    //if result.want_log {|
    //    extend_options2(&mut result.var_names, id as usize);
    //    result.var_names[id as usize] = name.into();
    //};
    self.addr_var(result, id);
    @try(self.compile_expr(result, value, .ResAddr, false)) return;
    self.locals.last().unwrap().push(id);
    if name { name |
        prev := self.var_lookup&.insert(name, id);
        @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program.pool));
    };
    .Ok
}

// This section looks like we could just use WalkAst, 
// but this operation is subtble enough that making control flow more indirect would probably make it more confusing. 
//
// If result_location == .ResAddr, the top of the stack on entry to this function has the pointer where the result should be stored.
fn compile_expr(self: *EmitBc, result: *FnBody, expr: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes = {
    @debug_assert(!expr.ty.is_unknown(), "Not typechecked: %. done: %", expr.log(self.program.pool), expr.done);
    @debug_assert(self.slot_count(expr.ty).lt(16).or(result_location != .PushStack), "% %", expr.log(self.program.pool), self.program.log_type(expr.ty));
    self.last_loc = expr.loc;

    expr_ty := expr.ty;
    @match(expr.expr&) {
        fn Cast(v)  => return(self.compile_expr(result, v[], result_location, can_tail));
        fn Call(f)  => return(self.emit_call(result, f.f, f.arg, result_location, can_tail));
        fn Block(f) => return(self.emit_block_expr(result, f.body&, f.result, f.ret_label, result_location, can_tail));
        fn Value(f) => return(self.emit_value(result, f.bytes&, result_location, expr.ty));
        fn If(_)      => return(self.emit_call_if(result, expr, result_location, can_tail));
        fn Loop(arg)  => return(self.emit_call_loop(result, arg[]));
        fn Addr(arg)  => return(self.addr_macro(result, arg[], result_location));
        fn StructLiteralP(pattern) => return(self.construct_struct(result, pattern, expr.ty, result_location));
        fn Slice(arg) => {
            container_ty := arg.ty;
            // Note: number of elements, not size of the whole array value.
            (ty, count) := @match(arg.expr) {
                fn Tuple(parts) Ty(Type, i64) => { 
                    fst := parts[0];
                    (fst.ty, parts.len) 
                };
                @default => (arg.ty, 1);
            };

            id := result.add_var(self, container_ty);
            self.addr_var(result, id);
            @try(self.compile_expr(result, arg[], .ResAddr, false)) return;

            self.addr_var(result, id);
            result.push(PushConstant = (
                value = count,
                ty = .I64,
            ));
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr() => {
                    result.push(PeekDup = 2);
                    result.inc_ptr_bytes(8); // Note: backwards!
                    result.push(StorePost = .I64);
                    result.push(PeekDup = 1);
                    result.push(StorePost = .P64);
                    result.pop(1);
                }
                fn Discard() => result.pop(2);
            };
            self.locals.last().unwrap().push(id);
            return(.Ok)
        };
        fn Deref(arg) => {
            @try(self.compile_expr(result, arg[], .PushStack, false)) return; // get the pointer
            slots := self.slot_count(expr_ty);
            // we care about the type of the pointer, not the value because there might be a cast. (// TODO: that shouldn't be true anymore because of ::Cast)
            value_type := self.program.unptr_ty(arg.ty).unwrap();
            if slots == 0 {|
                @match(result_location) {
                    fn ResAddr() => result.pop(2); // pop dest too!
                    @default     => result.push(Snipe = 0);
                };
            } else {|
                @match(result_location) {
                    fn PushStack() => self.load(result, value_type);
                    fn ResAddr() => {
                        info := self.program.get_info(expr_ty);

                        if info.size_slots == 1 {|
                            ty := self.program.prim(value_type).unwrap();
                            result.push(Load = ty);
                            result.push(StorePre = ty);
                        } else {| // TODO: else if
                            if info.size_slots == 2 && info.stride_bytes == 16 && info.align_bytes == 8 {|
                                result.push(PeekDup = 1);
                                result.inc_ptr_bytes(8);
                                result.push(PeekDup = 1);
                                result.inc_ptr_bytes(8);
                                result.push(Load = .I64);
                                result.push(StorePre = .I64);
                                result.push(Load = .I64);
                                result.push(StorePre = .I64);
                            } else {|
                                bytes := info.stride_bytes;
                                result.push(CopyBytesToFrom = bytes);
                            };
                        };
                    }
                    fn Discard() => result.push(Snipe = 0);
                };
            };
            return(.Ok)
        };
        fn FnPtr(arg) => {
            // debug_assert!(matches!(self.program[arg.ty], TypeInfo::Fn(_))); // TODO: debug_assert
            f := @unwrap(arg[].as_const(), "expected fn for ptr") return;
            f := FuncId.assume_cast(f&)[];
            // For jit, the redirects go in the dispatch table so it doesn't matter,
            // but for emitting c, you need to get the real name.
            f = self.program.follow_redirects(f);
            result.push(GetNativeFnPtr = f);
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr()   => result.push(StorePre = .P64);
                fn Discard()   => result.push(Snipe = 0);
            };
            return(.Ok)
        };
        fn Uninitialized() => {
            @assert(!expr_ty.is_never(), "call exit() to produce a value of type 'Never'");
            // Wierd special case I have mixed feelings about. should at least set to sentinal value in debug mode.
            // Now I have to not mess up the stack, tell the backend somehow.
            @match(result_location) {
                fn PushStack() => {
                    // for ty in self.program.flat_tuple_types(expr.ty) {
                    //     result.push(Bc::PushConstant {
                    //         value: 0,
                    //         ty: self.program.prim(ty).unwrap(),
                    //     });
                    // }
                    slots := self.slot_count(expr_ty);
                    range(0, slots.zext()) { _ |
                        result.push(PushConstant = (value = 0, ty = .I64));
                        // TODO: wrong prim!
                    };
                }
                fn ResAddr() => result.push(Snipe = 0); // just pop the res ptr
                fn Discard() => ();
            };
            return(.Ok)
        };
        fn Tuple(values) => {
            raw := self.program.raw_type(expr.ty); // TODO: do i need this? 
            @match(self.program.get_type(raw)) {
               fn Struct(f) => {
                    @err_assert(f.layout_done, "ICE: struct layout not ready.") return;
                    @assert_eq(f.fields.len, values.len);
                    // TODO: assert is_tuple or need to skip const fields (if allow const fields update len assertion). 
                    
                    // TODO: sad that i have to write loops like this. 
                    range(0, f.fields.len) { i |
                        f := f.fields[i]&;
                        value := values[i]&;
                        if result_location == .ResAddr {|
                            result.push(PeekDup = 0);
                            result.inc_ptr_bytes(f.byte_offset.trunc());
                        };
                        @try(self.compile_expr(result, value, result_location, false)) return;
                    };
                }
                fn Array(f) => {
                    @assert_eq(values.len(), f.len.zext());
                    element_size := self.program.get_info(f.inner).stride_bytes();
                    each values { value |
                        if result_location == .ResAddr {|
                            result.inc_ptr_bytes(element_size);
                            result.push(PeekDup = 0);
                        };
                        @try(self.compile_expr(result, value, result_location, false)) return;
                    }
                }
                @default => {
                    return(@error("Expr::Tuple should have struct type not %. (0/1 element tuple maybe?)", self.program.log_type(raw)));
                };
            };
            if(result_location == .ResAddr, => { result.pop(1); });
            return(.Ok)
        }
        fn PtrOffset(f) => {
            // TODO: compiler has to emit tagchecks for enums now!!
            @try(self.compile_expr(result, f.ptr, .PushStack, false)) return;
            result.inc_ptr_bytes(f.bytes.trunc());
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr() => result.push(StorePre = .P64);
                fn Discard() => result.push(Snipe = 0);
            };
            return(.Ok)
        }
        fn Switch(f) => {
            return(self.emit_switch(result, expr, result_location, can_tail))
        }
        @default => @panic("ICE: didn't desugar: %", expr.log(self.program.pool));
    };
    unreachable()
}

//// TODO: this would be prettier as tail calls. 
//fn follow_redirects(program: CompilerRs, f_id: FuncId) FuncId = {
//    loop {|
//        continue :: local_return;
//        @if_let(program[f_id].body&)
//            fn Redirect(target) => {
//                f_id = target[];
//                continue();
//            };
//        return(f_id);
//    }; // TODO: this should work without this shit -- Jul 87
//    unreachable()
//}

// infinetkly ibnliens itself???:>?f,.A 
//fn follow_redirects(program: CompilerRs, f_id: FuncId) FuncId = {
//    @if_let(program[f_id].body)
//        fn Redirect(target) => {
//            return(program.follow_redirects(target)); // TODO: tail
//        };
//    f_id
//}


fn follow_redirects(program: *SelfHosted, f_id: FuncId) FuncId = {
    dowhile() {|
        @match(program[f_id].body&) {
            fn Redirect(target) => {
                f_id = target[];
                true
            }
            @default => false;
        }
    };
    f_id
}

fn emit_call(self: *EmitBc, result: *FnBody, f: *FatExpr, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    // TODO: not enough bits! super dumb -- Jul 8
    //@assert(!f.ty.is_unknown(), "Not typechecked: %", f.log(self.program.pool));
    //@assert(!arg.ty.is_unknown(), "Not typechecked: %", arg.log(self.program.pool));
    @match(self.program.get_type(f.ty)) {
        fn Fn(f_ty) => {
            f_id := @unwrap(f.as_const(), "tried to call non-const fn") return;
            f_id := FuncId.assume_cast(f_id&)[];
            func := self.program[f_id]&;
            cc := self.program[f_id].cc.unwrap();
            @assert(!func.get_flag(.Generic));
            @assert(!func.get_flag(.MayHaveAquiredCaptures));
            @assert(cc != .Inline, "ICE: tried to call inlined %", self.program.pool.get(func.name));

            // TODO: ideally the redirect should just be stored in the overloadset so you don't have to have the big Func thing every time.
            // TODO: audit: is this f_ty different from the one we just got from the expression type? -- Jul 8
            func := self.program[f_id]&; // TODO: fix index overload resolution -- Jul 8
            f_ty := func.finished_ty().unwrap(); // kinda HACK to fix unaligned store? 
            f_id = self.program.follow_redirects(f_id);

            if(func.has_tag(.no_tail), => { can_tail = false; });
            
            return(self.emit_runtime_call(result, f_ty, cc, arg, result_location, can_tail) { sig, tail |
                result.mix_hash(f_id.as_index(), 436265);
                if self.program[f_id].body&.is(.Intrinsic) {|
                    op := self.program[f_id].body.Intrinsic;
                    result.push(Intrinsic = op);
                    if tail {|
                        @debug_assert(sig.ret2.is_none(), "So far all intrinsics return one thing.");
                        result.push(Ret1 = sig.ret1.unwrap());
                    };
                } else {|
                    if self.program.pool.get(self.program[f_id].name) == "zext" {|
                        @println("direct call zext! % %", f_id.as_index(), self.program.index(f_id).log(self.program.pool));
                    };
                    if self.program.index(f_id).has_tag(.intrinsic) {|
                        @println("direct call to % %!", self.program.pool.get(self.program[f_id].name), f_id.as_index());
                    };
                    result.push(CallDirect = (sig = sig[], f = f_id, tail = tail));
                };
            });
        }
        fn FnPtr(f_ty) => {
            @try(self.compile_expr(result, f, .PushStack, false)) return;
            will_use_indirect_ret := self.program.slot_count(f_ty.ty.ret) > 2; // TODO: get this from sig
            if result_location == .ResAddr && will_use_indirect_ret {|
                // grab the result pointer to the top of the stack so the layout matches a normal call.
                // however, if the function wants to push stack but we want it to a resaddr, we don't do this here because emit_runtime_call handles it which is kinda HACK.
                result.push(PeekDup = 1);
            };
            // TODO: allow tail calling through pointer depending on the calling convention 
            @try(self.emit_runtime_call(result, f_ty.ty, f_ty.cc, arg, result_location, can_tail) { sig, _tail |
                result.push(CallFnPtr = (sig = sig[]));
            }) return;
            if result_location == .ResAddr && will_use_indirect_ret {|
                result.push(Snipe = 0); // original ret ptr
            };
            return(.Ok)
        }
        fn Label(ret_ty) => {
            label := @unwrap(f.as_const(), "called label must be const") return;
            return_from := LabelId.assume_cast(label&)[];
            // result_location is the result of the ret() expression, which is Never and we don't care.
            ret := @unwrap(
                self.inlined_return_addr&.get_ptr(return_from),
                "missing return label. forgot '=>' on function?"
            ) return;
            ret.used = true;  // note: updating the one in the map! not a copy
            slots: u16 = @match(ret.result_loc) {
                fn PushStack() => self.slot_count(ret_ty[]);
                fn ResAddr() => {
                    id := ret.res_ssa_id.unwrap();
                    result.push(LoadSsa = (id = id));
                    0
                }
                fn Discard() => 0;
            };
            // TODO: sometimes can_tail, if you're returning the main function
            @try(self.compile_expr(result, arg, ret.result_loc, false)) return;
            result.push(Goto = (ip = ret.block, slots = slots));
            result[ret.block].incoming_jumps += 1;
            // TODO: :FUCKED. if you dont have the early return here, it returns uninit memory. 
            //       so like fall through doesnt get you to the same place.
            return(.Ok);
        }
        @default => @panic("ICE: non callable: %", f.log(self.program.pool));
    };
    unreachable()
}

fn emit_block_expr(self: *EmitBc, result: *FnBody, body: *RsVec(FatStmt), value: *FatExpr, ret_label: ?LabelId, result_location: ResultLoc, can_tail: bool) PRes #once = {
    self.locals&.push(list(temp()));
    // TODO: make sure value.ty here is the same as expr.ty outside which is what i used to use. -- Jul 8  :audi
    out := self.program.get_info(value.ty);
    @debug_assert(out.size_slots.lt(8).or(result_location != .PushStack), "pushing large value");

    // TODO: this shouldn't need type hint. 
    if ret_label { (ret_var: LabelId) | 
        entry_block := result.current_block;
        return_block := @match(result_location) {
            fn PushStack() => result.push_block(self.get_primatives(value.ty));
            fn ResAddr()   => result.push_block_empty();
            fn Discard()   => result.push_block_empty();
        };
        ret: ReturnAddr = (
            block = return_block,
            result_loc = result_location,
            store_res_ssa_inst = .None,
            res_ssa_id = .None,
            used = false,
        );
        result.current_block = entry_block;
        if result_location == .ResAddr {|
            result.push(PeekDup = 0);
            id := self.save_ssa_var(result);
            ret.res_ssa_id = (Some = id);
            index := result[entry_block].insts.len - 1;
            ret.store_res_ssa_inst = (Some = (entry_block, index));
        };

        prev := self.inlined_return_addr&.insert(ret_var, ret);
        @assert(prev.is_none(), "stomped ret var");

        each body { stmt | 
            @try(self.compile_stmt(result, stmt)) return;
        };
        @try(self.compile_expr(result, value, result_location, can_tail)) return;

        ret := self.inlined_return_addr&.remove(ret_var).unwrap();
        if result[return_block].incoming_jumps > 0 {|
            @debug_assert(ret.used, "expected jumps");
            slots := result[return_block].arg_prims.len;
            result.push(Goto = (ip = return_block, slots = slots.trunc()));
            result[return_block].incoming_jumps += 1;
            result.current_block = return_block;
        } else {|
            @debug_assert(!ret.used, "expected no jumps"); 
            if ret.store_res_ssa_inst { f | 
                (block, index) := f;
                // if we didn't use it, don't bother asking the backend to save the register with the result address.
                result[block].insts[index] = .Nop;
                result[block].insts[index - 1] = .Nop;
            };
            result.push_to(return_block, .NoCompile);
        };
    } else {|
        // TODO: sometimes the last one can tail, if value is Unit and we're the body of the main function.
        each body { stmt |
            @try(self.compile_stmt(result, stmt)) return;
        };
        @try(self.compile_expr(result, value, result_location, can_tail)) return;
    };

    // TODO: check if you try to let an address to a variable escape from its scope.
    slots := self.locals&.pop().expect("block to have scope");
    for slots { id |
        result.push(LastUse = (id = id ));
    };
    .Ok
}

fn emit_value(self: *EmitBc, result: *FnBody, value: *Values, result_location: ResultLoc, expr_ty: Type) PRes #once = {
    if(result_location == .Discard, => return(.Ok));
    want_emit_by_memcpy := value.len() > 16;  // dont change this to include small things unless you do something about values being stored inline.
    // TODO: you probably want to allow people to overload bake_relocatable_value even if !contains_pointers, but also there's no point. -- Jun 19
    if ALWAYS_BAKE_CONSTANTS.or(result.when == .Aot) && want_emit_by_memcpy.or(self.program.get_info(expr_ty).contains_pointers()) {|
        if result_location.eq(.PushStack).or(!want_emit_by_memcpy) {|
            out := @try(self.program.emit_relocatable_constant_body(value.bytes(), expr_ty, false)) return;

            for out { part | 
                @match(part) {
                    // TODO: now you can't have non-i64 in top level constant struct -- Jun 18
                    fn Num(f)     => {
                        result.push(PushConstant = (value = f.value, ty = f.ty));
                        result.mix_hash(f.value, 123);
                    }
                    fn FnPtr(f)   => {
                        f = self.program.follow_redirects(f);
                        result.push(GetNativeFnPtr = f);
                        result.mix_hash(f.as_index(), 1237);
                    }
                    fn AddrOf(id) => {
                        result.push(PushGlobalAddr = id);
                        result.mix_hash(id.id.zext(), 9765);
                    }
                };
            };
            if result_location == .ResAddr {|
                self.store_pre(result, expr_ty);
            };
        } else {|
            @assert_eq(result_location, .ResAddr);
            id := @try(self.program.emit_relocatable_constant(expr_ty, value)) return;

            result.mix_hash(id.id.zext(), 9765);
            result.push(PushGlobalAddr = id);
            info := self.program.get_info(expr_ty);
            result.push(CopyBytesToFrom = info.stride_bytes);
        };
        return(.Ok);
    };

    // TODO
    // debug_assert_eq!(
    //     self.program.get_info(expr.ty).stride_bytes as usize,
    //     value.0.len(),
    //     "{:?} is {}",
    //     value.0,
    //     self.program.log_type(expr.ty)
    // );
    reader: ReadBytes = (bytes = value.bytes(), i = 0);
    @match(result_location) {
        fn PushStack() => {
            parts: List(i64) = list(temp());
            info: List(Ty(Prim, u16)) = list(temp());
            @try(self.program.deconstruct_values(
                expr_ty,
                reader&,
                parts&,
                (Some = info&),
            )) return;
            @debug_assert_eq(parts.len(), info.len());
            enumerate parts { i, value | 
                (ty, _) := info[i];
                result.mix_hash(value[], 1234567);
                result.push(PushConstant = (value = value[], ty = ty));
            };
        }
        fn ResAddr() => {
            if want_emit_by_memcpy {|
                // we know the value is big!
                // TODO: HACK
                //       for a constant ast node, you need to load an enum but my deconstruct_values can't handle it.
                //       this solution is extra bad becuase it relies on the value vec not being free-ed
                result.push(PushConstant = (
                    value = value.jit_addr(),
                    ty = .P64,
                ));
                
                // Note: not the same as value.len!!!!!      // TODO: audit: why not? -- Jul 8
                result.push(CopyBytesToFrom = self.program.get_info(expr_ty).stride_bytes());
            } else {|
                parts: List(i64) = list(temp());
                offsets: List(Ty(Prim, u16)) = list(temp());
                @try(self.program.deconstruct_values(
                    expr_ty,
                    reader&,
                    parts&,
                    (Some = offsets&),
                )) return;
                @debug_assert_eq(parts.len(), offsets.len());
                enumerate parts { i, value | 
                    (ty, offset) := offsets[i];
                    result.push(PeekDup = 0);
                    result.inc_ptr_bytes(offset);
                    result.push(PushConstant = (value = value[], ty = ty));
                    result.push(StorePre = ty);
                    result.mix_hash(value[], 1234567);
                };
                result.pop(1); // res ptr
            };
        }
        fn Discard() => ();
    };
    return(.Ok)
}

// :PlaceExpr
fn addr_macro(self: *EmitBc, result: *FnBody, arg: *FatExpr, result_location: ResultLoc) PRes #once = {
    self.last_loc = arg.loc;
    // field accesses should have been desugared.
    @err_assert(arg.expr&.is(.GetVar), "took address of r-value") return;
    var := arg.expr.GetVar;
    // TODO: this shouldn't allow let either but i changed how variable refs work for :SmallTypes
    @assert_ne(var.kind, .Const, "Can only take address of var (not let/const) %", var&.log(self.program.pool));
    id := @unwrap(self.var_lookup&.get(var), "Missing var % (in !addr)", var&.log(self.program.pool)) return;
    self.addr_var(result, id);

    @match(result_location) {
        fn PushStack() => ();
        fn ResAddr()   => result.push(StorePre = .P64);
        fn Discard()   => result.push(Snipe = 0);
    };
    .Ok
}

// we never make the temp variable. if the arg is big, caller needs to setup a result location.
fn emit_call_if(self: *EmitBc, result: *FnBody, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    @err_assert(arg.expr&.is(.If), "ICE: expected if") return;
    parts := arg.expr.If;
    
    @try(self.compile_expr(result, parts.cond, .PushStack, false)) return; // cond
    (if_true, if_false) := (parts.if_true, parts.if_false);
 
    out := self.program.get_info(if_true.ty);
    @assert(out.size_slots.lt(4).or(result_location != .PushStack), "ICE: 'if' result too big to go on stack"); // now its the callers problem to deal with this case

    branch_block := result.current_block;
    true_ip := result.push_block_empty();
    @try(self.compile_expr(result, if_true, result_location, can_tail)) return;
    end_true_block := result.current_block;
    false_ip := result.push_block_empty();
    @try(self.compile_expr(result, if_false, result_location, can_tail)) return;
    end_false_block := result.current_block;

    block_slots := if(result_location == .PushStack, => out.size_slots, => 0);
    prims := if(result_location == .PushStack, => self.get_primatives(if_true.ty), => empty());
    ip := result.push_block(prims);
    result.push_to(branch_block, (JumpIf = (true_ip = true_ip, false_ip = false_ip, slots = 0 )));
    result.push_to(end_true_block, (Goto = (ip = ip, slots = block_slots)));

    result.push_to(end_false_block, (Goto = (ip = ip, slots = block_slots)));
    result[ip].incoming_jumps += 2;
    result[true_ip].incoming_jumps += 1;
    result[false_ip].incoming_jumps += 1;
    result.bump_clock(ip);

    .Ok
}

fn emit_switch(self: *EmitBc, result: *FnBody, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    @err_assert(arg.expr&.is(.Switch), "ICE: expected switch") return;
    parts := arg.expr.Switch;
    
    if parts.cases.len == 0 {|
        // There's only a default; that's not really a switch bro but ok... 
        @try(self.compile_expr(result, parts.value, .Discard, false)) return;
        @try(self.compile_expr(result, parts.default, result_location, false)) return;
        return(.Ok);
    };
    
    @try(self.compile_expr(result, parts.value, .PushStack, false)) return; //  this is the thing we're inspecting!
    entry_block := result.current_block; 
   
    // This is where we rejoin with the value of the whole switch expression. 
    out := self.program.get_info(arg.ty);
    @assert(out.size_slots.lt(4).or(result_location != .PushStack), "ICE: 'switch' result too big to go on stack"); // now its the callers problem to deal with this case
    block_slots := if(result_location == .PushStack, => out.size_slots, => 0);
    prims := if(result_location == .PushStack, => self.get_primatives(arg.ty), => empty());
    end_block := result.push_block(prims);
    result.current_block = entry_block;
    cases: List(SwitchPayload) = list(self.program.get_alloc());
    
    push_case :: fn(value: i64, body: *FatExpr) void => {
        case_block := result.push_block_empty();
        @try(self.compile_expr(result, body, result_location, false)) return; // TODO: tail call
        end_case_block := result.current_block;
        result.push_to(end_case_block, (Goto = (ip = end_block, slots = block_slots)));
        cases&.push(value = value, block = case_block); 
        result[case_block].incoming_jumps += 1;
        result[end_block].incoming_jumps += 1;
    };
    
    each parts.cases { f |
        push_case(f._0, f._1&);
    };
   
    // TODO: would it be nicer to have default branch just be the last thing in the ast node too so you could handle them uniformly? -- Jul 26
    push_case(-1, parts.default);
    // TODO: make sure fn neg is #fold
    
    result.current_block = end_block;
    result.push_to(entry_block, (Switch = result.switch_payloads.len));
    result.switch_payloads&.push(cases.rs(), self.program.get_alloc());
     
    result.bump_clock(end_block);
    .Ok
}

fn decode_switch(cases: *RsVec(SwitchPayload)) Ty([]SwitchPayload, ?BbId) = {
    has_default := cases.last().unwrap()[].value == -1;
    ::if(Ty([]SwitchPayload, ?BbId));
    if(has_default) {|
        (cases.items().slice(0, cases.len - 1), (Some = cases.last().unwrap()[].block))
    } else {|
        (cases.items(), .None)
    }
}

fn emit_call_loop(self: *EmitBc, result: *FnBody, arg: *FatExpr) PRes #once = {
    @debug_assert_eq(arg.ty, void);

    prev_block := result.current_block;
    start_body_block := result.push_block_empty();
    result.current_block = start_body_block;
    result.push_to(
        prev_block,
        (Goto = (ip = start_body_block, slots = 0)),
    );

    @try(self.compile_expr(result, arg, .Discard, false)) return;
    end_body_block := result.current_block;

    result.push_to(
        end_body_block,
        (Goto = (ip = start_body_block, slots = 0)),
    );
    result[start_body_block].incoming_jumps += 2;
    .Ok
}

// :PlaceExpr
fn set_deref(self: *EmitBc, result: *FnBody, place: *FatExpr, value: *FatExpr) PRes #once = {
    // we care about the type of the pointer, not the value because there might be a cast.
    @match(place.expr&) {
        fn GetVar(_) => panic("ICE: var set should be converted to place expr");
        fn Deref(arg) => {
            // TODO: write a test for pooiinter eval oreder. left hsould come first. -- MAy 7
            @try(self.compile_expr(result, arg[], .PushStack, false)) return;
            @try(self.compile_expr(result, value, .ResAddr, false)) return;
            return(.Ok)
        }
        @default => return(@error("TODO: other `place=e;` :("));
    };
    unreachable()
}

fn unwrap(self: Name) Symbol = {
    @match(self) {
        fn Var(v) => v.name;
        fn Ident(v) => v;
        @default => panic("Expected name!");
    }
}

// TODO: rename? also used for tagged. 
fn construct_struct(self: *EmitBc, result: *FnBody, pattern: *Pattern, requested: Type, result_location: ResultLoc) PRes #once = {
    raw_container_ty := self.program.raw_type(requested);
    slots := self.slot_count(raw_container_ty);
    @debug_assert(slots.lt(8).or(result_location != .PushStack), "too big to put on stack"); 
    
    @match(self.program.get_type(raw_container_ty)) {
        fn Struct(f) => {
            expected := f.fields.len - f.const_field_count.zext();
            @assert_eq(expected, pattern.bindings.len, "Cannot assign to type % with wrong field count", self.program.log_type(requested));
            i := 0;
            each f.fields { field | 
                continue :: local_return;
                i += 1;
                if(field.kind == .Const, => continue());
                value := pattern.bindings[i - 1]&;
                
                @assert_eq(value.name.unwrap(), field.name, "field name mismatch");
                
                if result_location == .ResAddr {|
                    result.push(PeekDup = 0);
                    result.inc_ptr_bytes(field.byte_offset.trunc());
                };
                expr := value.default.unwrap();
                @try(self.compile_expr(result, expr&, result_location, false)) return;
            };
            if result_location == .ResAddr {|
                result.pop(1); // res ptr
            };
        }
        // TODO: make this constexpr in compiler (TODO: audit: this comment is from back when i had the interp)
        fn Tagged(f) => {
            @debug_assert(result_location != .Discard, "todo");
            size := self.slot_count(raw_container_ty);
            @assert_eq(1, pattern.bindings.len, 
                "% is @tagged, value should have one active varient",
                self.program.log_type(requested)
            );
            name := pattern.bindings[0].name.unwrap();
            i := f.cases.position(fn(f) => f._0 == name).expect("case name to exist in type");
            payload_size := self.slot_count(f.cases[i]._1);
            if payload_size >= size {|
                return(@error("Enum value won't fit."));
            };
            value := pattern.bindings[0].default.unwrap();
            @match(result_location) {
                fn PushStack() => {
                    result.push(PushConstant = (
                        value = i,
                        ty = .I64,
                    ));
                    @try(self.compile_expr(result, value&, result_location, false)) return;

                    // TODO: this is a dumb hack to make the padding have the right prim types for backends that care. :SLOW -- Jun 23
                    //       This fixes test_option/option_small_payload_n/parse on llvm.
                    types := self.program.flat_tuple_types(raw_container_ty);
                    types&.ordered_remove(0);
                    range(0, self.program.slot_count(value.ty).zext()) { _ |
                        types&.ordered_remove(0);
                    };
                    expected_pad := size - (payload_size + 1);
                    @debug_assert_eq(types.len(), expected_pad.zext(), "confused about padding size");
                    // now types is just padding

                    // If this is a smaller varient, pad out the slot.
                    for types { p |
                        ty := @unwrap(self.program.prim(p), "not prim") return;
                        result.push(PushConstant = (value = 0, ty = ty));
                    };
                }
                fn ResAddr() => {
                    result.push(PeekDup = 0);
                    result.push(PushConstant = (
                        value = i,
                        ty = .I64,
                    ));
                    result.push(StorePre = .I64);
                    result.inc_ptr_bytes(8); // TODO: differetn sizes of tag
                    @try(self.compile_expr(result, value&, result_location, false)) return;
                }
                fn Discard() => {
                    @try(self.compile_expr(result, value&, result_location, false)) return;
                }
            };

            // TODO: support explicit uninit so backend doesn't emit code for the padding above.
            //       current system also means weird type stuff where you write ints into units in bc_to_asm.
            //       if we staticlly know the tag value, could only copy the size of the active varient (which would be the general case of leaving it uninit when creating one).
            //       but also eventually we probably want to define and preserve padding so tags could be stored there.
            //       even without that, maybe its a weird undefined behaviour to just drop some of your bytes,
            //       should have compiler settings about being allowed to cast *T -> *[size_of(T)]u8 because that would observe the padding.
            //       I like the idea of granular toggling ub vs optimisations and having those flags as a place to hang comments,
            //       but that adds a lot of extra work testing all the combinations which might not be worth it.
            //       -- Apr 17
            //
        }
        @default => {
            return(@error("struct literal for non-(struct/tagged)"));
        };
    };
    return(.Ok)
}

fn push_block_empty(self: *FnBody) BbId = {
    self.blocks&.push((
        insts = empty(),
        arg_prims = empty(),
        incoming_jumps = 0,
        clock = self.clock,
    ), self.alloc);
    b: BbId = (id = self.blocks.len().trunc() - 1);
    self.current_block = b;
    b
}

fn push_block(self: *FnBody, arg_prims: [] Prim) BbId = {
    self.blocks&.push((
        insts = empty(),
        arg_prims = arg_prims,
        incoming_jumps = 0,
        clock = self.clock,
    ), self.alloc);
    b: BbId = (id = self.blocks.len().trunc() - 1);
    self.current_block = b;
    b
}

fn bump_clock(self: *FnBody, b: BbId) void ={
    self.clock += 1;
    self[b].clock = self.clock;
}

fn slot_count(self: *EmitBc, ty: Type) u16 = {
    self.program.slot_count(ty)
}

fn push(self: *FnBody, inst: Bc) void = {
    self.push_to(self.current_block, inst);
}

fn addr_var(ctx: *EmitBc, self: *FnBody, id: u16) void = {
    if ctx.is_ssa_var&.get(id.zext()) {|
        self.push(LoadSsa = (id = id));
    } else {|
        self.push(AddrVar = (id = id));
    };
}

// TODO: move result to EmitBc instead of passing it around? -- Jul 8
fn save_ssa_var(ctx: *EmitBc, self: *FnBody) u16 = {
    id := self.add_var(ctx, rawptr);
    ctx.is_ssa_var&.set(id.zext(), temp()); 
    self.push(SaveSsa = (id = id, ty = .P64));
    id
}

fn inc_ptr_bytes(self: *FnBody, bytes: u16) void = {
    // TODO: also check if the last op was also incptrbytes or constant and merge them. 
    if bytes != 0 {|
        self.push(IncPtrBytes = bytes);
    };
}

::tagged(Bc);
fn push_to(self: *FnBody, b: BbId, inst: Bc) void = {
    //@println("%", inst&.tag());
    self.mix_hash(inst&.tag().ordinal(), 9801);
    self[b].insts&.push(inst, self.alloc);
}

fn mix_hash(self: *FnBody, i: i64, spice: i64) void = {
    self.hash += i;
    self.hash *= spice;
}

fn pop(self: *FnBody, slots: u16) void = {
    range(0, slots.zext()) { _ | 
        self.push(Snipe = 0);
    }
}

fn index(self: *FnBody, b: BbId) *BasicBlock = {
    self.blocks.index(b.id.zext())
}

fn add_var(self: *FnBody, ctx: *EmitBc, ty: Type) u16 = {
    info := ctx.program.get_info(ty);
    self.mix_hash(info.stride_bytes.zext(), 12345);
    self.vars&.push((size = info.stride_bytes, align = info.align_bytes), self.alloc);
    i: u16 = self.vars.len.trunc();
    i - 1
}

/////////////////////////////////////
/// Primitive Type Representation ///
// 
// The distinction between backend types and front end types is important. 
// There are different registers for storing integers/pointers vs floats so the backend really does need to know which everything is. 
// It was much more confusing before when I was keeping a mask of which parts were floats and only had one type of float/int. 
// But the current implementation is more complicated than it needs to be because I wasn't sure what was important at the time. 
// 

// This (prim_sig implementation) is insane. Caching all the variations of compctx/indirect is painful. 
// At least the interface is fairly thin. ~almost~ only emit_bc needs to deal with creating prim_sigs.
// but every backend needs to remember to handle indirect returns specially every time they touch one. 
//
// I treat the indirect seperately because the backend wants to put it in a special register. 
// But then for the block args, you do want the indirect ptr because the bc expects it. 
// And then compctx is weird because the front end doesn't want to insert it into the arguments, 
// so its not as part of the function type, its in the calling convention. -- Jul 5
//
fn prim_sig(program: *SelfHosted, f_ty: FnType, cc: CallConv) Res(PrimSig) = {
    ret := program.get_info(f_ty.ret);

    sig: PrimSig = (
        ret1 = .None,
        ret2 = .None,
        return_value_bytes = ret.stride_bytes,
        first_arg_is_indirect_return = ret.size_slots > @as(u16) 2.trunc(),
        no_return = f_ty.ret.is_never(),
    ); // TODO: if you typeo make this a '}' need to have a good error message. -- Jul 6

    arg_slots: u16 = 0;
    @switch(ret.size_slots) {
        @case(0) => ();
        @case(1) => {
            sig.ret1 = program.prim(f_ty.ret);
        };
        @case(2) => {
            (a, b) := @try(program.prim_pair(f_ty.ret)) return;
            sig.ret1 = (Some = a);
            sig.ret2 = (Some = b);
        };
        @default => {
            arg_slots += 1;
            // Note: not adding indirect pointer to sig because its handled sperately. TODO: that's kinda confusing.
        };
    };

    args: List(Prim) = list(program.get_alloc());

    :: enum(CallConv);
    comp_ctx := cc == .CCallRegCt;
    if comp_ctx {|
        arg_slots += 1;
        args&.push(.P64);
    };

    found_arity := 0;
    // TODO: compiler bug? what push_arg am i possibly shadowing?????? -- Jul 6
    push_arg__wtf :: fn(ty: Type) void => {
        found_arity += 1;
        info := program.get_info(ty);
        if info.pass_by_ref {|
            arg_slots += 1;
            args&.push(.P64);
        } else {|
            for program.flat_tuple_types(ty) { t | 
                arg_slots += 1;
                args&.push(program.prim(t).unwrap());
            };
        };
    };

    done := false;
    // TODO: if i always collapsed tuples of the same to array this would need different handling.
    @if_let(program.get_type(f_ty.arg)) 
        (fn Struct(f) => {
            if f.is_tuple {|
                done = true;
                // TODO: :const_field_fix (not done on rust side either so add test for this!)
                each f.fields { f |
                    push_arg__wtf(f.ty);
                };
            };
        });
    if !done {|
        push_arg__wtf(f_ty.arg);
    };

    // TODO: decide what i want a tuple to be. is it a real type you can have in memory or is it the thing that multiple arguments are?
    //       those ideas should extend to return values instead of them being special.
    //       should have a spread operator for calling a function on a tuple of arguments? like 'a := (1, 2); f(..a)'
    // @assert_eq(found_arity, f_ty.arity, "TODO: fn(a: Ty(i64, i64))");

    // i h a t e m y s e l f s o m e t i m e s
    // a n d i d o n t k n o w w h a t t o d o
    key := (f_ty.arg, arg_slots, false, comp_ctx);
    r := args.items().clone(args.gpa).rs();
    program.primitives&.insert(key, r);
    args&.insert(0, .P64); // sad
    key._2 = true;
    program.primitives&.insert(key, args.rs()); // TODO: stupid that i have to do this here
    key._2 = false;
    sig.args = program.get_primitives(key).unwrap();

    sig.arg_int_count = 0;
    for sig.args { p |
        if !p.is_float() {|
            sig.arg_int_count += 1;
        };
    };
    
    (Ok = sig)
}


fn prim_pair(self: *SelfHosted, ty: Type) Res(Ty(Prim, Prim)) = {
    types := self.flat_tuple_types(ty); // TODO: don't allocate
    @assert_eq(types.len(), 2);
    a : Prim = @unwrap(self.prim(types[0]), "non-prim") return;
    b := @unwrap(self.prim(types[1]), "non-prim") return;
    (Ok = @as(Ty(Prim, Prim)) (a, b))
}

PrimKey :: Ty(Type, u16, bool, bool);

fn get_primitives(self: *SelfHosted, key: PrimKey) ?[] Prim = {
    :: opt_map(RsVec(Prim), []Prim);
    found := self.primitives&.get(key&);
    found&.map(fn(v) => v.items())
}

fn as_primatives(self: *SelfHosted, ty: Type) [] Prim = {
    if(ty.is_unit().or(ty.is_never()), => return(empty()));
    slots := self.get_info(ty).size_slots();
    key := (ty, slots, false, false);
    if self.get_primitives(key) { p|
        return(p);
    };
    flat_types := self.flat_tuple_types(ty);
    types: List(Prim) = list(self.get_alloc());
    
    for flat_types { t |
        if !t.is_unit().or(t.is_never()) {|
            types&.push(self.prim(t).expect("tuple part to be prim"));
        };
    };
    self.primitives&.insert(key, types.clone().rs());

    types&.insert(0, .P64); // sad
    key._2 = true;
    self.primitives&.insert(key, types.rs()); // TODO: stupid that i have to do this here
    key._2 = false;

    self.get_primitives(key).unwrap()
}

fn prim(self: *SelfHosted, ty: Type) ?Prim = {
    p: Prim = @match(self.get_type(ty)) {
        fn F64()     => .F64;
        fn F32()     => .F32;
        fn Bool()    => .I8 ;
        fn VoidPtr() => .P64;
        fn FnPtr(_)  => .P64;
        fn Ptr(_)    => .P64;
        fn Fn(_)     => .I32;
        fn Label(_)  => .I32;
        fn Int(int) => 
            @switch(int.bit_count) { 
                @case(8)  => .I8 ;
                @case(16) => .I16;
                @case(32) => .I32;
                @default  => .I64;  // TODO: :fake_ints_too_big
            };
        fn Struct(f) => {
            if f.fields.len - f.const_field_count.zext() == 1 {|
                fst := f.fields[0]&;
                assert(fst.kind != .Const, "TODO: first field const");
                if self.slot_count(fst.ty) == 1 {|
                    return(self.prim(fst.ty));
                };
            };
            return(.None)
        }
        fn Tagged(f) => {
            each f.cases { c |
                if !c._1.is_unit() {|
                    return(.None);
                };
            };
            .I64 // TODO: :tag_is_always_i64
        }
        fn Named(f) => return(self.prim(f._0));
        fn Enum(f) => return(self.prim(f.raw));
        @default => return(.None);
    };
    (Some = p)
}

fn flat_tuple_types(self: *SelfHosted, ty: Type) List(Type) = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            out: List(Type) = list(temp());
            each f.fields { f | 
                if !f.kind.eq(.Const) {| 
                    out&.push_all(self.flat_tuple_types(f.ty).items());
                };
            };
            out
        }
        fn Enum(f) => self.flat_tuple_types(f.raw);
        fn Named(f) => self.flat_tuple_types(f._0);
        // TODO: this is sketchy
        fn Tagged(f) => {
            slots: i64 = self.slot_count(ty).zext();
            varients: List(List(Type)) = list(f.cases.len, temp());
            for f.cases { f |
                var := self.flat_tuple_types(f._1);
                if var.len > 0 {|
                    varients&.push(var);
                };
            };
            if varients.len() == 1 {|
                varients[0]&.insert(0, i64);
                @debug_assert_eq(slots, varients[0].len());
                return(varients[0]);
            };
            // TODO: hack
            i64.repeated(slots, temp())
        }
        fn Array(f) => {
            out := self.flat_tuple_types(f.inner);
            single := out.len;
            // TODO: fix underflow. better ban 0 len array -- Jul 9
            range(0, f.len.zext() - 1) { i|
                // reslice everytime because we might reallocate. TODO: just reserve at the beginning.  -- Jul 6
                out&.push_all(out.items().slice(0, single));
            };
            out
        }
        fn void() => list(temp());
        @default => ty.repeated(1, temp());
    }
}

fn idx(s: FuncId) i64 = s.to_index().zext();
fn is_unit(t: Type) bool = t == void;
fn is_unknown(t: Type) bool = t == UnknownType;
fn is_never(t: Type) bool = t == Never;

fn is_float(self: Prim) bool = 
    self.eq(.F64).or(self == .F32);
    
fn align_to(offset: i64, align: i64) i64 = {
    extra := offset.mod(align);
    if extra == 0 {|
        offset
    } else {|
        offset + align - extra
    }
}

// TODO: this is a dumb hack to make it less painful that i can't access fields on a value (because you need a pointer to offset)
//       fix in the compiler and remove this! -- Jul 7
//       now the problem is you can't access fields on a pointer if its not a dereference of it 
//       (i changed get_info when i ported it, i didn't fix the old problem yet).  -- Jul 21
fn stride_bytes(s: *TypeMeta) u16 = s.stride_bytes;
fn align_bytes(s: *TypeMeta) u16 = s.align_bytes;
fn size_slots(s: *TypeMeta) u16 = s.size_slots;
fn contains_pointers(s: *TypeMeta) bool = s.contains_pointers;
fn slot_count(self: *SelfHosted, ty: Type) u16 = {
    self.get_info(ty)[].size_slots
}

fn int_count(self: Prim) i64 = 
    if(self.is_float(), => 0, => 1);

fn ret_slots(self: *PrimSig) u16 = {
    if self.ret1.is_some() {|
        if(self.ret2.is_some(), => 2, => 1)
    } else {|
        0
    }
}

// This includes the indirect ret pointer!!
fn arg_slots(self: *PrimSig) i64 = {
    if(self.first_arg_is_indirect_return, => self.args.len + 1, => self.args.len)
}

// TODO: be able to derive but skip fields? or take off `func`?
// TODO: (also in the derived version) for aggragates like this with multiple collections, check all the lengths first before iterating. 
//       generalize quick_ne or something maybe? 
fn eq(lhs: **FnBody, rhs: **FnBody) bool = {
    //if(lhs.when != rhs.when, => return(false));
    if(lhs.blocks&.items() != rhs.blocks&.items(), => return(false));
    if(lhs.vars&.items() != rhs.vars&.items(), => return(false));
    true
}

// we do a rolling hash as we create the FnBody so just return that. 
fn hash(h: *TrivialHasher, s: **FnBody) void = {
    h.hash(s.hash&);
}

:: {
    AutoEq(BasicBlock);
    AutoEq(Bc);
    AutoEq(PrimSig);
    DerefEq(Prim);
    DerefEq(Intrinsic);
    AutoEq(?Prim);
    AutoEq(BbId);
    AutoEq(BakedVarId);
    AutoEq(VarSlotType);
    DerefEq(FuncId);
    fn eq(a: FuncId, b: FuncId) bool #redirect(Ty(u32, u32), bool);
    AutoEq(get_variant_type(Bc, Bc.Tag().CallDirect));
    AutoEq(get_variant_type(Bc, Bc.Tag().CallFnPtr));
    AutoEq(get_variant_type(Bc, Bc.Tag().PushConstant));
    AutoEq(get_variant_type(Bc, Bc.Tag().JumpIf));
    AutoEq(get_variant_type(Bc, Bc.Tag().Goto));
    AutoEq(get_variant_type(Bc, Bc.Tag().AddrVar));
    AutoEq(get_variant_type(Bc, Bc.Tag().SaveSsa));
    AutoEq(get_variant_type(Bc, Bc.Tag().LoadSsa));
    AutoEq(get_variant_type(Bc, Bc.Tag().LastUse));
    AutoEq(get_variant_type(Bc, Bc.Tag().Ret2));
};
