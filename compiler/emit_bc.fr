//! Converts simple ASTs into my bytecode-ish format.
//! All comptime execution for a function is finished before it reaches this phase.
//! - Flatten nested expressions to stack operations.
//! - Convert control flow to explicit basic blocks.
//! - Bind non-local return labels.
//! - Reduce variable usage to register sized loads/stores.
//! - Convert large arguments/returns to references and remap signatures to use only register sized types.

ENABLE_DEDUPLICATION :: true;

// note: bc structs are in lib/driver_api.fr

//
// TODO: less pain
// - versions of @assert that return a compile error instead of crashing. 
// - struct init fields needing to be in order is super dumb!
// - @sanity(.DebugAsserts)? 
// - access fields on rvalues
// - index operator looks for the wrong overload when used in nested expressions
// - deal with infinite loops if you try to put from_raw in fn tagged because it triggers for the tag of ?i64 but then the raw of that needs to return a ?i64, it needs to be like @rec but just @rec doesnt wokr 
// - TODO: it seems the compiler segfaults trying to cope with this line?????????! -- Jul 7 //ty := binding.ty&.unwrap();

EmitBc :: @struct(
    result: *FnBody,
    program: *SelfHosted,
    comp: CompCtx,
    last_loc: Span,
    locals: List(List(u16)),
    var_lookup: HashMap(Var, u16),
    inlined_return_addr: HashMap(LabelId, ReturnAddr),
    is_ssa_var: BitSet,
    result_addr_depth: i64 = 0,
    out_alloc: Alloc,
    current_block: BbId, 
    clock: u16,
    want_log: bool,
    debug: bool,
);

fn hash(s: *LabelId) i64 #redirect(*u32, i64);

ReturnAddr :: @struct(
    block: BbId,
    result_loc: ResultLoc,
    store_res_ssa_inst: ?Ty(BbId, i64),
    res_ssa_id: ?u16,
    used: bool,
    result_addr_depth: i64,
);

ResultLoc :: @enum(i64) (PushStack, ResAddr, Discard);
::enum(ResultLoc);
::enum(ExecStyle);

fn emit_bc(comp: *SelfHosted, f: FuncId, when: ExecStyle) Res(*FnBody) = {
    if when == .Aot {
        // :bake_relocatable_value
        @try(comp.check_for_new_aot_bake_overloads()) return; 
    };
    func := comp.get_function(f); // TODO: you can't do this inline with a call (if comp[f]) that needs a *Func. it looks for an overload for **Func. -- Jul 5
    
    self: EmitBc = (
        result = FnBody.ptr_from_int(0), // EVIL. filled in soon! 
        program = comp, 
        comp = comp.vtable.with(comp.legacy_indirection),
        last_loc = func.loc,
        locals = list(temp()),
        var_lookup = init(temp()),
        inlined_return_addr = init(temp()),
        is_ssa_var = empty(),
        out_alloc = comp.get_alloc(),  
        current_block = (id = 0),
        clock = 0,
        want_log = func.get_flag(.LogBc),
        debug = comp.get_build_options()[].debug_info,
    );
    //@println("emit_bc % % %", when, f, comp.pool.get(func.name));
    @debug_assert(func.get_flag(.EnsuredCompiled), "fn not compiled?");
    // TODO: this should be true
    //if func.finished_bytecode_pointer.is_some() {
    //    old := func.finished_bytecode_pointer.unwrap();
    //    old := FnBody.ptr_from_int(old);
    //    if old.when == when {
    //        //@println("compiled twice");
    ////        //return(Ok = old);
    //    };
    //};
    
    //if when == .Jit && !func.get_flag(.SyntheticImmEval) {
    //    @println("jitting % %", f, comp.log_name(f));
    //};

    sig: PrimSig = (ret1 = .None, ret2 = .None);
    //test := comp.prim(i64);
    
    if func.get_flag(.LogAst) {
        @println("[#log_ast %] %", f, func.log(comp));
        comp.codemap.show_error_line(func.loc);
    };
    self.locals&.push(list(temp()));
    body := @try(self.program.empty_fn_body(f, when)) return;
    self.result = body;
    
    zone := zone_begin(.EmitBc); // TODO: defer
    @if(ENABLE_TRACY) {
        real_name := comp.pool.get(func.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    
    res := self&.emit_body(f);
    @debug_assert(body.vars.len <= MAX_u16.zext() && body.blocks.len <= MAX_u16.zext(), "program too big for my tiny brain");
    zone_end(zone);
    
    if res&.is_err() {
        res.Err.update_main_span(self.last_loc);
        return(Err = res.Err)
    };
    
    @if(ENABLE_DEDUPLICATION) {
        if body.hash != 0 && self.program.get_build_options()[].deduplicate_bytecode {
            if self.program.saved_bytecodes&.get_key(body&) { check | 
                if check.func.as_index() != f.as_index() {|  // TODO: this shouldn't happen?
                    // TODO: this is sketchy... maybe? had to think of a test that would hurt it tho. 
                    func.body = (Redirect = check.func); 
                    
                    //old := check.func;
                    //old_func := comp[old]&;
                    
                    //size := 0;
                    //each check.blocks { b |
                    //    size += b.insts.len;
                    //};
                    
                    //@println("% ops saved. % F%_% <- F%_%", size, when,
                    //    old.as_index(), comp.pool.get(old_func.name),
                    //    f.as_index(), comp.pool.get(func.name), 
                    //);
                };
                return(Ok = check);
            };
            
            self.program.saved_bytecodes&.insert(body, when);
        };
    };
    func.finished_bytecode_pointer = (Some = FnBody.int_from_ptr(body)); 
    (Ok = body)
}

:: List(Symbol);
fn empty_fn_body(program: *SelfHosted, func: FuncId, when: ExecStyle) Res(*FnBody) = {
    f := program[func]&;
    f_ty := @unwrap(f.finished_ty(), "ICE: fn type not ready") return;
    ptr := program.bytecodes&.push(
        blocks = empty(),
        vars = empty(),
        var_names = empty(),
        when = when,
        signeture = @try(program.comp().prim_sig(f_ty, f.cc.expect("known cc"))) return,
        func = func,
        name = f.name,
        switch_payloads = empty(),
        sig_payloads = empty(),
    );
    
    (Ok = ptr)
}

// TODO: if you have an unclosed string it randomly decides later top level vars are undeclared? -- Jul 7 
fn bind_args(self: *EmitBc, arguments: *Pattern) Res(i64) #once = {
    arity: u16 = arguments.bindings.len.trunc();

    func := self.comp.get_function(self.result.func);
    f_ty := func.finished_ty().expect("known function type");
    arg_ty := f_ty.arg;
    pushed := if(self.result.signeture.first_arg_is_indirect_return, => 1, => 0);
    // reversed because they're on the stack like [0, 1, 2]
    each_rev arguments.bindings { binding |
        continue :: local_return;
        ty := binding.unwrap_ty();
        
        // TODO:? probably fine, i just set to const in closure capture but then shouldn't be adding to vars below.
        // TODO: the frontend needs to remove the 'const' parts from the ast in DeclVarPattern
        ::if_opt(Var, Str);
        @err_assert(binding.kind != .Const, "arg '%' was const!", 
            if(binding.var(), fn(name: Var) => self.comp.get_string(name.name), => "_")) return;

        info := self.comp.get_info(ty);

        if(info.size_slots == 0, => continue());
        :: if(u16);
        id: u16 = if info.pass_by_ref {
            pushed += 1;
            // TODO: callee make copy if it wants to modify
            self.save_ssa_var()
        } else {
            slots := self.slot_count(ty);
            id := self.add_var(ty);
            @switch(slots) {
                @case(0) => (); 
                @case(1) => {
                    ty := @unwrap(self.program.prim(ty), "expected prim but found %", self.program.log_type(ty)) return;
                    pushed += 1;
                    self.addr_var(id);
                    self.push(StorePost = ty);
                };
                @case(2) => {
                    types := self.comp.flat_tuple_types(ty);
                    offset_2 := align_to(
                        self.comp.get_info(types[0]).stride_bytes(),
                        self.comp.get_info(types[1]).align_bytes(),
                    );
                    p0 := self.program.prim(types[0]).expect("ret1 prim");
                    p1 := self.program.prim(types[1]).expect("ret2 prim");
                    self.addr_var(id);
                    self.inc_ptr_bytes(offset_2.trunc());
                    self.push(StorePost = p1);
                    self.addr_var(id);
                    self.push(StorePost = p0);
                    pushed += 2;
                };
                @default => panic("ICE: unreachable because large args are passed by reference");
            };
            @as(u16) id
        };

        ::?*List(u16);
        locals := self.locals.last().expect("in block").push(id);
        if binding.var() { name |
            prev := self.var_lookup&.insert(name, id);
            @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program));
        };
    };

    (Ok = pushed)
}

fn compile_for_arg(self: *EmitBc, arg: *FatExpr, arity: usize) Res(bool) = {
    info := self.comp.get_info(arg.ty);
    // If the whole thing is passed in registers, cool, we're done.
    if !info.pass_by_ref {
        // arity isn't always the same as info.size_slots because of two slot structs like slices. 
        @try(self.compile_expr(arg, .PushStack, false)) return; // TODO: tail if its for a ret of the main function?
        return(Ok = false);
    };

    if arity == 1 {
        id := self.add_var(arg.ty);
        self.addr_var(id);
        @try(self.compile_expr(arg, .ResAddr, false)) return;
        self.addr_var(id);
        return(Ok = true);
    };
    
    @assert(arg.expr&.is(.Tuple), "TODO: % %", self.program.log_type(arg.ty), arity);
    parts := arg.expr.Tuple;
    info := self.program.get_type(arg.ty);
    @assert(info.is(.Struct), "ICE: expected fn args to be tuple");
    // TODO: probably assert(info.Struct.is_tuple) or we're in an odd place. need to be more consistant about what an <arguments> is -- Jul 8
    fields := info.Struct.fields;
    //@debug_assert_eq(fields.len(), self.program.arity(arg).zext());   // TODO: why do we care about this? we have it from the tuple anyway. is it just about agreeing with the overloading thing? -- Jul 8
    @debug_assert_eq(fields.len(), parts.len(), "ICE?: not enough parts for type in compile_for_arg");

    _pushed := 0;
    // TODO: sad that I can't zip iterators
    enumerate parts { i, val |
        continue :: local_return;
        ty := fields[i].ty;
        info := self.comp.get_info(ty);
        if !info.pass_by_ref {
            _pushed += info.size_slots.zext();
            @try(self.compile_expr(val, .PushStack, false)) return;
            continue();
        };
        
        @match(val.expr&) {
            fn Deref(arg) => {
                _pushed += 1;
                // TODO: this is probably unsound.
                //       we're assuming that we can defer the load to be done by the callee but that might not be true.
                @try(self.compile_expr(arg[], .PushStack, false)) return;
                continue();
            }
            // TODO: factor out aot handling from main value handling so can use here too. -- Jun 3
            fn Value(f) => {
                if self.result.when == .Jit {
                    // TODO: this gets super bad if the callee isn't properly copying it because they'll be nmodifying something we think is constant
                    _pushed += 1;
                    self.push(PushConstant = (
                        value = f.bytes&.jit_addr(),  // Note: we know it's pass_by_ref so its large and not stored inline. 
                        ty = .P64,
                    ));
                    continue();
                };
                // else: fallthrough
            }
            @default => (); // fallthrough
        };

        _pushed += 1;
        id := self.add_var(ty);
        self.addr_var(id);
        self.result_addr_depth += 1;
        @try(self.compile_expr(val, .ResAddr, false)) return;
        self.result_addr_depth -= 1;
        self.addr_var(id);  // now we want to pass that pointer to the call, so leave it on the stack.
    };
    (Ok = true)
}

fn store_pre(self: *EmitBc, ty: Type) void = {
    if(ty.is_never(), => self.push(.Unreachable));
    slots := self.slot_count(ty);
    @switch(slots) {
        @case(0) => ();
        @case(1) => {
            ty := self.program.prim(ty).expect("store prim");
            self.push(StorePre = ty);
        };
        @case(2) => {
            types := self.comp.flat_tuple_types(ty);
            offset_2 := align_to(
                self.comp.get_info(types[0]).stride_bytes(),
                self.comp.get_info(types[1]).align_bytes(),
            );
            self.push(PeekDup = 2); // grab the pointer
            self.inc_ptr_bytes(offset_2.trunc());
            self.push(StorePost = self.program.prim(types[1]).expect("store prim!"));
            self.push(StorePre = self.program.prim(types[0]).expect("store prim!"));
        };
        @default => panic("ICE: Tried to store big value.");
    }
}

fn load(self: *EmitBc, ty: Type) void = {
    if(ty.is_never(), => self.push(.Unreachable));
    slots := self.slot_count(ty);
    @switch(slots) {
        @case(0) => ();
        @case(1) => {
            ty := self.program.prim(ty).expect("load prim");
            self.push(Load = ty);
        };
        @case(2) => {
            types := self.comp.flat_tuple_types(ty);
            offset_2 := align_to(
                self.comp.get_info(types[0]).stride_bytes(),
                self.comp.get_info(types[1]).align_bytes(),
            );
            self.push(PeekDup = 0);
            self.push(Load = self.program.prim(types[0]).expect("load prim!"));
            self.push(PeekDup = 1);
            self.inc_ptr_bytes(offset_2.trunc());
            self.push(Load = self.program.prim(types[1]).expect("load prim!"));
            self.push(Snipe = 2);
        };
        @default => panic("ICE: Tried to load big value.");
    }
}

fn emit_body(self: *EmitBc, f: FuncId) PRes #once = {
    func := self.program[f]&;

    body: *FatExpr = @match(func.body&) {
        fn Normal(body) => body;
        fn Intrinsic(op) => {
            // Direct calls will be inlined in emit_bc but someone might be trying to call through a function pointer. 
            // TODO: don't add to callees for direct calls. -- Jul 24
            entry_block := self.push_block(self.result.signeture.args);
            self.push(Intrinsic = op[]);
            @debug_assert(self.result.signeture.ret2.is_none(), "So far all intrinsics return one thing.");
            self.push(Ret1 = self.result.signeture.ret1.unwrap());
            self.result.hash = 0;
            return(.Ok)
        }
        // You should never actually try to run this code, the caller should have just done the call,
        // so there isn't an extra indirection and I don't have to deal with two bodies for comptime vs runtime,
        // just two ways of emitting the call.
        @default => return(.Ok);
    };

    entry_block := self.push_block(empty());
    args_pushed := @try(self.bind_args(func.arg&)) return;
    entry_block_sig := self.result.signeture.args;
    if self.result.signeture.first_arg_is_indirect_return {
        // :ConfusingPrims
        entry_block_sig.ptr = entry_block_sig.ptr.offset(-1);
        entry_block_sig.len += 1;
    };
    @debug_assert_eq(entry_block_sig.len, args_pushed, "ICE: confusing about function arity");
    self.result[entry_block].arg_prims = entry_block_sig;
    ::if(ResultLoc); ::if([]Prim);
    // We represent the indirect return argument as the left-most thing on the stack,
    // so after popping all the args, its at the top and we can emit the thing normally.
    result_location: ResultLoc = if(self.result.signeture.first_arg_is_indirect_return, => .ResAddr, => .PushStack);
    // Note: this is different from the body expr type because of early returns.
    ret := self.program[f].finished_ret.unwrap();
    prims: []Prim = if(self.result.signeture.first_arg_is_indirect_return, => empty(), => self.get_primatives(ret));
    return_block := self.push_block(prims);  // TODO: this is dumb. the body expression will never jump here (even with early returns as they only go to Blocks). 

    self.current_block = entry_block;

    if self.result.signeture.first_arg_is_indirect_return {|  
        self.result_addr_depth += 1;
    };

    // TODO: indirect return tail
    @try(self.compile_expr(
        body,
        result_location,
        !self.result.signeture.first_arg_is_indirect_return,
    )) return;
    
    if self.result.signeture.first_arg_is_indirect_return {
        self.result_addr_depth -= 1;
    };

    // TODO: we never actually use the return_block created here!
    assert(self.result[return_block].incoming_jumps == 0, "sure hope you did that on purpose");
    self.push_to(return_block, .NoCompile);

    self.locals&.pop().expect("outer scope to exist");
    assert(self.locals.is_empty(), "ICE: bc leaked scopes");

    @try(self.emit_return(ret)) return;
    if self.want_log {
        out: List(u8) = list(temp());
        self.program.log_bc(func, self.result, out&);
        println(out.items());
    };
    .Ok
}

fn emit_return(self: *EmitBc, ret: Type) Res(void) = {
    if !ret.is_never() {
        slots := self.slot_count(ret); // TODO: this is in the sig so don't look it up again. 
        op: Bc = @switch(slots) {
            @case(1) => {
                a := self.program.prim(ret).expect("return prim");
                //debug_assert_eq!(BigOption::Some(a), self.result.signeture.ret1); // TODO: debug_assert. derive eq
                (Ret1 = a)
            };
            @case(2) => {
                a, b := @try(self.comp.prim_pair(ret)) return;
                //debug_assert_eq!(BigOption::Some(a), self.result.signeture.ret1); // TODO: debug_assert. derive eq
                //debug_assert_eq!(BigOption::Some(b), self.result.signeture.ret2);
                (Ret2 = (a, b))
            };
            @default => .Ret0; // void or indirect return
        };

        self.push(op);
    } else {
        self.push(.Unreachable);
    };
    .Ok
}

fn log_bc(self: *SelfHosted, func: *Func, result: *FnBody, out: *List(u8)) void = {
    @if(BOOTSTRAP_ONLY_MODE) return();
    @fmt(out, "[#log_bc %] %\n", self.pool.get(func.name), result.signeture&);
    enumerate result.blocks { i, block | 
        continue :: local_return;
        if block.insts.len == 1 && block.insts.index(0).is(.NoCompile) {
            continue();
        };
        @fmt(out, "  [b %] (%)\n", i, block.arg_prims);
        each block.insts { inst | 
            @fmt(out, "    - %\n", inst);
        };
    };

}

fn emit_runtime_call(
    self: *EmitBc,
    f_ty: FnType,
    cc: CallConv,
    arg_expr: *FatExpr,
    result_location: ResultLoc,
    can_tail: bool,
    $do_call: @Fn(sig: *PrimSig, tail: bool) void,
) PRes = {
    sig := @try(prim_sig(self.comp, f_ty, cc)) return;

    :: if(?u16);
    result_var: ?u16 = if sig.first_arg_is_indirect_return && result_location != .ResAddr {
        id := self.add_var(f_ty.ret);
        self.addr_var(id);
        // TODO: :result_addr_depth?
        (Some = id)
    } else {
        .None
    };

    if cc == .CCallRegCt {
        self.push(.GetCompCtx);
    };
    
    any_by_ref := @try(self.compile_for_arg(arg_expr, f_ty.arity.zext())) return;
    // if any args are pointers, they might be to the stack and then you probably can't tail call.
    // TODO: can do better than this without getting too fancy, function pointers are fine, and anything in constant data is fine (we know if the arg is a Values).
    // TODO: !tail to force it when you know its fine.
    tail := can_tail && !self.comp.get_info(f_ty.arg).contains_pointers() && !any_by_ref;
    
    do_call(sig&, tail);
    
    slots := self.slot_count(f_ty.ret);
    if slots > 0 {
        @match(result_location) {
            fn PushStack() => {
                if sig.first_arg_is_indirect_return {
                    if result_var { id |
                        self.addr_var(id);
                    };
                    self.load(f_ty.ret);
                }
            }
            fn ResAddr() => if(!sig.first_arg_is_indirect_return, => self.store_pre(f_ty.ret));
            fn Discard() => if(!sig.first_arg_is_indirect_return, => self.pop(slots));
        };
    } else {
        if result_location == .ResAddr {
            // pop dest!
            self.pop(1);
        };
    };

    if f_ty.ret.is_never() {
        self.push(.Unreachable);
    };
    
    // :sema_regression this shouldn't need the hint
    @as(PRes) (.Ok)
}

fn unwrap_ty(self: *Binding) Type = {
    assert(self.ty&.is(.Finished), "type not ready!");
    self.ty.Finished
}

fn mark_variable(self: *EmitBc, name: Var, id: u16) void = {
    if self.want_log.or(self.comp.get_build_options()[].debug_info) {
        while => self.result.var_names.len <= id.zext() {
            self.result.var_names&.push(Flag.SYMBOL_ZERO.ident(), self.comp.get_alloc());
        };
        self.result.var_names[id.zext()] = name.name;
    };
}

fn compile_stmt(self: *EmitBc, stmt: *FatStmt) PRes = {
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Eval(expr) void => return(self.compile_expr(expr, .Discard, false));
        fn DeclVar(f) void => {
            @assert_ne(f.name.kind, VarType.Const);
            assert(f.ty&.is(.Finished), "type not ready");
            ty := f.ty.Finished;
            id := self.add_var(ty);
            self.mark_variable(f.name, id);
            self.addr_var(id);
            self.result_addr_depth += 1;
            @try(self.compile_expr(f.value&, .ResAddr, false)) return;
            self.result_addr_depth -= 1;
            if(ty.is_never(), => self.push(.Unreachable));
            prev := self.var_lookup&.insert(f.name, id);
            self.locals.last().unwrap().push(id);
            @assert(prev.is_none(), "shadow is still new var");
        }
        fn Set(f) void => return(self.set_deref(f.place&, f.value&));
        fn DeclVarPattern(f) void => {
            // TODO: test for evaluation order
            @if_let(f.value.expr&) fn Tuple(parts) => {
                @assert_eq(parts.len(), f.binding.bindings.len(), "ICE: DeclVarPattern tuple size mismatch");
                enumerate parts { i, value |
                    b := f.binding.bindings[i]&; // TODO: zip
                    @assert_ne(b.kind, .Const);
                    @try(self.do_binding(b.var(), b.unwrap_ty(), value)) return;
                };
                return(.Ok);
            };
            
            if 1 == f.binding.bindings.len() {
                b := f.binding.bindings[0]&;
                return(self.do_binding(b.var(), b.unwrap_ty(), f.value&));
            };
            
            // It's a destructuring (not inlined args)
            // We store the whole value in a stack slot and then save pointers into different offsets of it as thier own variables.
            full_id := self.add_var(f.value.ty);
            self.addr_var(full_id);
            self.result_addr_depth += 1;
            @try(self.compile_expr(f.value&, .ResAddr, false)) return;
            self.result_addr_depth -= 1;
            info := self.comp.get_type(f.value.ty);
            @err_assert(info.is(.Struct), "destructure must be tuple") return;
            fields := info.Struct.fields&;
            @assert_eq(fields.len(), f.binding.bindings.len(), "destructure size mismatch");
            enumerate f.binding.bindings.items() { i, b |
                f := fields[i]&; // TODO: zip
                continue :: local_return;
                @err_assert(f.kind != .Const, "TODO: destructuring skip const fields") return;
                
                name := b.var().unwrap();
                self.addr_var(full_id);
                self.inc_ptr_bytes(f.byte_offset.trunc());
                id := self.save_ssa_var();
                self.mark_variable(name, id);
                // TODO: a test that fails if you typo put this line here. -- Jul 4 (see devlog.md)
                // self.addr_var(id);
                self.locals.last().unwrap().push(id);
                prev := self.var_lookup&.insert(name, id);
                @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program));
            };
        };
        fn Noop() void => ();
        // Can't hit DoneDeclFunc because we don't re-eval constants.
        @default => panic("ICE: stmt not desugared");
    };
    // Note: ^early return!
    .Ok
}

fn do_binding(self: *EmitBc, name: ?Var, ty: Type, value: *FatExpr) PRes = {
    id := self.add_var(ty);
    self.addr_var(id);
    self.result_addr_depth += 1;
    @try(self.compile_expr(value, .ResAddr, false)) return;
    self.result_addr_depth -= 1;
    self.locals.last().unwrap().push(id);
    if name { name |
        self.mark_variable(name, id);
        prev := self.var_lookup&.insert(name, id);
        @assert(prev.is_none(), "overwrite arg? %", name&.log(self.program));
    };
    .Ok
}

// This section looks like we could just use WalkAst, 
// but this operation is subtble enough that making control flow more indirect would probably make it more confusing. 
//
// If result_location == .ResAddr, the top of the stack on entry to this function has the pointer where the result should be stored.
fn compile_expr(self: *EmitBc, expr: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes = {
    @debug_assert(!expr.ty.is_unknown(), "Not typechecked: %. done: %", expr.log(self.program), {
        self.program.codemap.show_error_line(expr.loc);
        expr.done
    });
    @debug_assert(self.slot_count(expr.ty).lt(16).or(result_location != .PushStack), "% %", expr.log(self.program), self.program.log_type(expr.ty));
    self.last_loc = expr.loc;
    //self.program.codemap.show_error_line(expr.loc);
    //println(expr.log(self.program));

    expr_ty := expr.ty;
    @match(expr.expr&) {
        fn Cast(v)  => return(self.compile_expr(v[], result_location, can_tail));
        fn Call(f)  => return(self.emit_call(f.f, f.arg, result_location, can_tail));
        fn Block(f) => return(self.emit_block_expr(expr, result_location, can_tail));
        fn Value(f) => return(self.emit_value(f.bytes&, result_location, expr.ty));
        fn If(_)      => return(self.emit_call_if(expr, result_location, can_tail));
        fn Loop(arg)  => return(self.emit_call_loop(arg[]));
        fn Addr(arg)  => return(self.addr_macro(arg[], result_location));
        fn StructLiteralP(pattern) => return(self.construct_struct(pattern, expr.ty, result_location));
        fn Slice(arg) => {
            container_ty := arg.ty;
            // Note: number of elements, not size of the whole array value.
            ty, count := @match(arg.expr) {
                fn Tuple(parts) Ty(Type, i64) => { 
                    fst := parts[0];
                    (fst.ty, parts.len) 
                };
                @default => (arg.ty, 1);
            };

            id := self.add_var(container_ty);
            self.addr_var(id);
            self.result_addr_depth += 1;
            @try(self.compile_expr(arg[], .ResAddr, false)) return;
            self.result_addr_depth -= 1;

            self.addr_var(id);
            self.push(PushConstant = (
                value = count,
                ty = .I64,
            ));
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr() => {
                    self.push(PeekDup = 2);
                    self.inc_ptr_bytes(8); // Note: backwards!
                    self.push(StorePost = .I64);
                    self.push(PeekDup = 1);
                    self.push(StorePost = .P64);
                    self.pop(1);
                }
                fn Discard() => self.pop(2);
            };
            ::?*List(u16);
            self.locals.last().unwrap().push(id);
            return(.Ok)
        };
        fn Deref(arg) => {
            @try(self.compile_expr(arg[], .PushStack, false)) return; // get the pointer
            slots := self.slot_count(expr_ty);
            // we care about the type of the pointer, not the value because there might be a cast. (// TODO: that shouldn't be true anymore because of ::Cast)
            value_type := self.program.unptr_ty(arg.ty).unwrap();
            if slots == 0 {
                @match(result_location) {
                    fn ResAddr() => self.pop(2); // pop dest too!
                    @default     => self.push(Snipe = 0);
                };
            } else {
                @match(result_location) {
                    fn PushStack() => self.load(value_type);
                    fn ResAddr() => {
                        info := self.comp.get_info(expr_ty);

                        if info.size_slots == 1 {
                            ty := self.comp.prim(value_type).expect("deref prim");
                            self.push(Load = ty);
                            self.push(StorePre = ty);
                        } else {| // TODO: else if
                            if info.size_slots == 2 && info.stride_bytes == 16 && info.align_bytes == 8 {
                                self.push(PeekDup = 1);
                                self.inc_ptr_bytes(8);
                                self.push(PeekDup = 1);
                                self.inc_ptr_bytes(8);
                                self.push(Load = .I64);
                                self.push(StorePre = .I64);
                                self.push(Load = .I64);
                                self.push(StorePre = .I64);
                            } else {
                                bytes := info.stride_bytes;
                                self.push(CopyBytesToFrom = bytes);
                            };
                        };
                    }
                    fn Discard() => self.push(Snipe = 0);
                };
            };
            if(expr_ty.is_never(), => self.push(.Unreachable));
            return(.Ok)
        };
        fn FnPtr(arg) => {
            // debug_assert!(matches!(self.program[arg.ty], TypeInfo::Fn(_))); // TODO: debug_assert
            f := @unwrap(arg[].as_const(), "expected fn for ptr") return;
            f := FuncId.assume_cast(f&)[];
            // For jit, the redirects go in the dispatch table so it doesn't matter,
            // but for emitting c, you need to get the real name.
            f = self.comp.follow_redirects(f);
            self.push(GetNativeFnPtr = f);
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr()   => self.push(StorePre = .P64);
                fn Discard()   => self.push(Snipe = 0);
            };
            return(.Ok)
        };
        fn Unreachable() => {
            self.push(.Unreachable);
            return(.Ok)
        }
        fn Uninitialized() => {
            @assert(!expr_ty.is_never(), "call exit() to produce a value of type 'Never'");
            // Wierd special case I have mixed feelings about. should at least set to sentinal value in debug mode.
            // Now I have to not mess up the stack, tell the backend somehow.
            @match(result_location) {
                fn PushStack() => {
                    // for ty in self.comp.flat_tuple_types(expr.ty) {
                    //     self.push(Bc::PushConstant {
                    //         value: 0,
                    //         ty: self.comp.prim(ty).unwrap(),
                    //     });
                    // }
                    slots := self.slot_count(expr_ty);
                    range(0, slots.zext()) { _ |
                        self.push(PushConstant = (value = 0, ty = .I64));
                        // TODO: wrong prim!
                    };
                }
                fn ResAddr() => {
                    opts := self.comp.get_build_options();
                    if opts.zero_init_memory {
                        self.zero_memory_at_top_of_stack(expr_ty);
                    } else {
                        self.push(Snipe = 0); // just pop the res ptr
                    };
                }  
                fn Discard() => ();
            };
            return(.Ok)
        };
        fn Tuple(values) => {
            raw := self.program.raw_type(expr.ty); // TODO: do i need this? 
            @match(self.comp.get_type(raw)) {
               fn Struct(f) => {
                    @err_assert(f.layout_done, "ICE: struct layout not ready.") return;
                    @assert_eq(f.fields.len, values.len);
                    // TODO: assert is_tuple or need to skip const fields (if allow const fields update len assertion). 
                    
                    // TODO: sad that i have to write loops like this. 
                    range(0, f.fields.len) { i |
                        f := f.fields[i]&;
                        value := values[i]&;
                        if result_location == .ResAddr {
                            self.push(PeekDup = 0);
                            self.inc_ptr_bytes(f.byte_offset.trunc());
                            self.result_addr_depth += 1;
                        };
                        @try(self.compile_expr(value, result_location, false)) return;
                        if result_location == .ResAddr {
                            self.result_addr_depth -= 1;
                        };
                    };
                }
                fn Array(f) => {
                    @assert_eq(values.len(), f.len.zext());
                    element_size := self.comp.get_info(f.inner).stride_bytes();
                    each values { value |
                        if result_location == .ResAddr {
                            self.inc_ptr_bytes(element_size);
                            self.push(PeekDup = 0);
                            self.result_addr_depth += 1;
                        };
                        @try(self.compile_expr(value, result_location, false)) return;
                        if result_location == .ResAddr {
                            self.result_addr_depth -= 1;
                        };
                    }
                }
                @default => {
                    return(@err("Expr::Tuple should have struct type not %. (0/1 element tuple maybe?)", self.program.log_type(raw)));
                };
            };
            if(result_location == .ResAddr, => { self.pop(1); });
            return(.Ok)
        }
        fn PtrOffset(f) => {
            // TODO: compiler has to emit tagchecks for enums now!!
            @try(self.compile_expr(f.ptr, .PushStack, false)) return;
            self.inc_ptr_bytes(f.bytes.trunc());
            @match(result_location) {
                fn PushStack() => ();
                fn ResAddr() => self.push(StorePre = .P64);
                fn Discard() => self.push(Snipe = 0);
            };
            return(.Ok)
        }
        fn Switch(f) => {
            return(self.emit_switch(expr, result_location, can_tail))
        }
        @default => @panic("ICE: didn't desugar: %", expr.log(self.program));
    }
}

// consumes the pointer at the top of the stack.
fn zero_memory_at_top_of_stack(self: *EmitBc, expr_ty: Type) void = {
    info := self.comp.get_info(expr_ty);
    align := info.align_bytes.min(8);
    chunks := info.stride_bytes / align;
    prim := @switch(align) {
        @case(1) => Prim.I8;
        @case(2) => Prim.I16;
        @case(4) => Prim.I32;
        @case(8) => Prim.I64;
        @default => panic("ICE: strange alignment");
    };
    range(0, chunks.zext()) { _ | 
        self.push(PeekDup = 0); 
        self.push(PushConstant = (value = 0, ty = prim));
        self.push(StorePre = prim);
        self.inc_ptr_bytes(align);
    }; 
    self.push(Snipe = 0); 
}

//// TODO: this would be prettier as tail calls. 
//fn follow_redirects(program: CompilerRs, f_id: FuncId) FuncId = {
//    loop {
//        continue :: local_return;
//        @if_let(program[f_id].body&)
//            fn Redirect(target) => {
//                f_id = target[];
//                continue();
//            };
//        return(f_id);
//    }; // TODO: this should work without this shit -- Jul 87
//    unreachable()
//}

// infinetkly ibnliens itself???:>?f,.A 
//fn follow_redirects(program: CompilerRs, f_id: FuncId) FuncId = {
//    @if_let(program[f_id].body)
//        fn Redirect(target) => {
//            return(program.follow_redirects(target)); // TODO: tail
//        };
//    f_id
//}

fn emit_call(self: *EmitBc, f: *FatExpr, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    @match(self.comp.get_type(f.ty)) {
        fn Fn(f_ty) => {
            f_id := @unwrap(f.as_const(), "ice: tried to call non-const fn %", f.log(self.program)) return;
            f_id := FuncId.assume_cast(f_id&)[];
            func := self.comp.get_function(f_id);
            cc := func.cc.unwrap();
            @assert(!func.get_flag(.Generic));
            @assert(!func.get_flag(.MayHaveAquiredCaptures));
            @assert(cc != .Inline, "ICE: tried to call inlined %", self.comp.get_string(func.name));

            // TODO: ideally the redirect should just be stored in the overloadset so you don't have to have the big Func thing every time.
            // TODO: audit: is this f_ty different from the one we just got from the expression type? -- Jul 8
            f_ty := func.finished_ty().unwrap(); // kinda HACK to fix unaligned store? 
            original_f_id := f_id;
            f_id = self.comp.follow_redirects(f_id);

            if(func.get_flag(.NoTail), => { can_tail = false; });
            
            self.emit_runtime_call(f_ty, cc, arg, result_location, can_tail) { sig, tail |
                self.result.mix_hash(f_id.as_index(), 436265);
                if self.comp.get_function(f_id)[].body&.is(.Intrinsic) {
                    op := self.comp.get_function(f_id)[].body.Intrinsic;
                    self.push(Intrinsic = op);
                    if tail {
                        @debug_assert(sig.ret2.is_none(), "So far all intrinsics return one thing.");
                        self.push(Ret1 = sig.ret1.unwrap());
                    };
                } else {
                    callee := self.comp.get_function(f_id);
                    // TODO: this didnt used to happen. :sema_regression
                    //if callee.has_tag(.intrinsic) {
                    //    @assert(callee.get_flag(.BodyIsSpecial), "intrinsic not special");
                    //    @println("direct call to (intrinsic) % % from % %!", self.comp.get_string(callee.name), f_id.as_index(), self.program.log_name(self.result.func), self.result.func);
                    //};
                    //if callee.has_tag(.redirect) {
                    //    @println("direct call to (redirect) % %! %. from %", self.comp.get_string(callee.name), f_id.as_index(), callee.get_flag(.EnsuredCompiled), self.program.log_name(self.result.func));
                    //};
                    caller := self.comp.get_function(self.result.func);
                    if !caller.get_flag(.ComptimeOnly) && original_f_id == f_id {
                        // TODO: it would be nice if this was at the beginning and you never tried to emit anything that was comptimeonly
                        //       that requires never adding them to callees and then later realizing you can inline them, which would make sense. -- Aug 29
                        @debug_assert(!(self.result.when == .Aot && callee.get_flag(.ComptimeOnly)), "cannot .Aot .ComptimeOnly % %", f_id, callee.log(self.program));
                    };
                    
                    self.push(CallDirect = (sig = self.push_sig(sig[]), f = f_id, tail = tail));
                    if self.debug {
                        self.debug_location_for_last_op(f.loc);
                    };
                };
            }
        }
        fn FnPtr(f_ty) => {
            @try(self.compile_expr(f, .PushStack, false)) return;
            will_use_indirect_ret := self.slot_count(f_ty.ty.ret) > 2; // TODO: get this from sig
            if result_location == .ResAddr && will_use_indirect_ret {
                // grab the result pointer to the top of the stack so the layout matches a normal call.
                // however, if the function wants to push stack but we want it to a resaddr, we don't do this here because emit_runtime_call handles it which is kinda HACK.
                self.push(PeekDup = 1);
            };
            // TODO: allow tail calling through pointer depending on the calling convention 
            @try(self.emit_runtime_call(f_ty.ty, f_ty.cc, arg, result_location, can_tail) { sig, _tail |
                self.push(CallFnPtr = (sig = self.push_sig(sig[])));
                if self.debug {
                    self.debug_location_for_last_op(f.loc);
                };
            }) return;
            if result_location == .ResAddr && will_use_indirect_ret {
                self.push(Snipe = 0); // original ret ptr
            };
            .Ok
        }
        fn Label(ret_ty) => {
            label := @unwrap(f.as_const(), "called label must be const") return;
            return_from := LabelId.assume_cast(label&)[];

            ret := @unwrap(
                self.inlined_return_addr&.get_ptr(return_from),
                "missing return label. forgot '=>' on function?"
            ) return;
            ret.used = true;  // note: updating the one in the map! not a copy
            ret := ret[];
            // result_location is the result of the ret() expression, which is Never and we don't care (because we're going to jump away).
            // But we have to discard it if the old expression was expecting something large. 
            // Because the incoming pointer on the stack will flow through and confuse us later. :early_return_big
            // this would make sense if we were checking result_location, but we're not
            x := (ret.result_loc == .ResAddr); // (result_location.eq(.ResAddr)).or
            if x {
                self.pop(1); 
            };
            // I feel this should be good enough without the above, but no.
            // Since we might have nested expressions that push result pointers, you not only have to re-push ours,
            // you also need to pop off anything in between that the target block isn't expecting. 
            // ie. its important that each entry to a block has the same stack height. 
            diff := self.result_addr_depth - ret.result_addr_depth;
            self.pop(diff.trunc()); 
            
            slots: u16 = @match(ret.result_loc) {
                fn PushStack() => self.slot_count(ret_ty[]);
                fn ResAddr() => {
                    id := ret.res_ssa_id.unwrap();
                    self.push(LoadSsa = (id = id));
                    self.result_addr_depth += 1;
                    0
                }
                fn Discard() => 0;
            };
            // TODO: sometimes can_tail, if you're returning the main function
            @try(self.compile_expr(arg, ret.result_loc, false)) return;
            if ret.result_loc == .ResAddr {
                self.result_addr_depth -= 1;
            };
            self.push(Goto = (ip = ret.block, slots = slots));
            self.result[ret.block].incoming_jumps += 1;
            .Ok
        }
        @default => @panic("ICE: non callable: %", f.log(self.program));
    }
}

fn push_sig(self: *EmitBc, sig: PrimSig) u32 = {
    self.result.sig_payloads&.push(sig, self.out_alloc);
    self.result.sig_payloads.len.sub(1).trunc()
}

fn emit_block_expr(self: *EmitBc, expr: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    block := expr.expr.Block&;
    body := block.body&;
    value := block.result;
    ret_label := block.ret_label;
    self.locals&.push(list(temp()));
    
    // :block_never_unify_early_return_type
    // Note: block_ty can be different from value.ty if the fall through is a Never but there's an early return to the block. 
    block_ty := expr.ty;
    out := self.comp.get_info(block_ty);
    @debug_assert(out.size_slots.lt(8).or(result_location != .PushStack), "pushing large value");

    if ret_label { ret_var | 
        entry_block := self.current_block;
        return_block := @match(result_location) {
            fn PushStack() => self.push_block(self.get_primatives(block_ty));
            fn ResAddr()   => self.push_block_empty();
            fn Discard()   => self.push_block_empty();
        };
        ret: ReturnAddr = (
            block = return_block,
            result_loc = result_location,
            store_res_ssa_inst = .None,
            res_ssa_id = .None,
            used = false,
            result_addr_depth = self.result_addr_depth,
        );
        self.current_block = entry_block;
        if result_location == .ResAddr {
            self.push(PeekDup = 0);
            id := self.save_ssa_var();
            ret.res_ssa_id = (Some = id);
            index := self.result[entry_block].insts.len - 1;
            ret.store_res_ssa_inst = (Some = (entry_block, index));
        };

        prev := self.inlined_return_addr&.insert(ret_var, ret);
        @assert(prev.is_none(), "stomped ret var");

        depth := self.result_addr_depth;
        each body { stmt | 
            @try(self.compile_stmt(stmt)) return;
            @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");
        };
        @try(self.compile_expr(value, result_location, can_tail)) return;
        @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");

        ret := self.inlined_return_addr&.remove(ret_var).unwrap();
        if self.result[return_block].incoming_jumps > 0 {
            @debug_assert(ret.used, "expected jumps");
            slots := self.result[return_block].arg_prims.len;
            self.push(Goto = (ip = return_block, slots = slots.trunc()));
            self.result[return_block].incoming_jumps += 1;
            self.current_block = return_block;
        } else {
            @debug_assert(!ret.used, "expected no jumps"); 
            if ret.store_res_ssa_inst { f | 
                block, index := f;
                // if we didn't use it, don't bother asking the backend to save the register with the result address.
                self.result[block].insts[index] = .Nop;
                self.result[block].insts[index - 1] = .Nop;
            };
            self.push_to(return_block, .NoCompile);
        };
    } else {
        // TODO: sometimes the last one can tail, if value is Unit and we're the body of the main function.
        depth := self.result_addr_depth;
        each body { stmt |
            @try(self.compile_stmt(stmt)) return;
            @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");
        };
        @try(self.compile_expr(value, result_location, can_tail)) return;
        @debug_assert(self.result_addr_depth == depth, "ICE: leak result_addr_depth");
    };

    // TODO: check if you try to let an address to a variable escape from its scope.
    slots := self.locals&.pop().expect("block to have scope");
    for slots { id |
        self.push(LastUse = (id = id ));
    };
    .Ok
}

fn emit_value(self: *EmitBc, value: *Values, result_location: ResultLoc, expr_ty: Type) PRes #once = {
    if(result_location == .Discard, => return(.Ok));
    want_emit_by_memcpy := value.len() > 16;  // dont change this to include small things unless you do something about values being stored inline.
    // TODO: you probably want to allow people to overload bake_relocatable_value even if !contains_pointers, but also there's no point. -- Jun 19
    if self.result.when == .Aot && want_emit_by_memcpy.or(self.comp.get_info(expr_ty).contains_pointers()) {
        if result_location.eq(.PushStack).or(!want_emit_by_memcpy) {
            out := @try(self.program.emit_relocatable_constant_body(value.bytes(), expr_ty, false)) return;

            for out { part | 
                @match(part) {
                    // TODO: now you can't have non-i64 in top level constant struct -- Jun 18
                    fn Num(f)     => {
                        self.push(PushConstant = (value = f.value, ty = f.ty));
                        self.result.mix_hash(f.value, 123);
                    }
                    fn FnPtr(f)   => {
                        f = self.comp.follow_redirects(f);
                        self.push(GetNativeFnPtr = f);
                        self.result.mix_hash(f.as_index(), 1237);
                    }
                    fn AddrOf(id) => {
                        self.push(PushGlobalAddr = id);
                        self.result.mix_hash(id.id.zext(), 9765);
                    }
                };
            };
            if result_location == .ResAddr {
                self.store_pre(expr_ty);
            };
        } else {
            @assert_eq(result_location, .ResAddr);
            id := @try(self.program.emit_relocatable_constant(expr_ty, value)) return;

            self.result.mix_hash(id.id.zext(), 9765);
            self.push(PushGlobalAddr = id);
            info := self.comp.get_info(expr_ty);
            self.push(CopyBytesToFrom = info.stride_bytes);
        };
        return(.Ok);
    };

    reader: ReadBytes = (bytes = value.bytes(), i = 0);
    @match(result_location) {
        fn PushStack() => {
            parts: List(i64) = list(temp());
            info: List(Ty(Prim, u16)) = list(temp());
            @try(self.program.deconstruct_values(
                expr_ty,
                reader&,
                parts&,
                (Some = info&),
            )) return;
            @debug_assert_eq(parts.len(), info.len());
            enumerate parts { i, value | 
                ty, _ := info[i];
                self.result.mix_hash(value[], 1234567);
                self.push(PushConstant = (value = value[], ty = ty));
            };
        }
        fn ResAddr() => {
            if want_emit_by_memcpy {
                // we know the value is big!
                // TODO: HACK
                //       for a constant ast node, you need to load an enum but my deconstruct_values can't handle it.
                //       this solution is extra bad becuase it relies on the value vec not being free-ed
                self.push(PushConstant = (
                    value = value.jit_addr(),
                    ty = .P64,
                ));
                
                // Note: not the same as value.len!!!!!      // TODO: audit: why not? -- Jul 8
                self.push(CopyBytesToFrom = self.comp.get_info(expr_ty).stride_bytes());
            } else {
                parts: List(i64) = list(temp());
                offsets: List(Ty(Prim, u16)) = list(temp());
                @try(self.program.deconstruct_values(
                    expr_ty,
                    reader&,
                    parts&,
                    (Some = offsets&),
                )) return;
                @debug_assert_eq(parts.len(), offsets.len());
                enumerate parts { i, value | 
                    ty, offset := offsets[i];
                    self.push(PeekDup = 0);
                    self.inc_ptr_bytes(offset);
                    self.push(PushConstant = (value = value[], ty = ty));
                    self.push(StorePre = ty);
                    self.result.mix_hash(value[], 1234567);
                };
                self.pop(1); // res ptr
            };
        }
        fn Discard() => ();
    };
    .Ok
}

// :PlaceExpr
fn addr_macro(self: *EmitBc, arg: *FatExpr, result_location: ResultLoc) PRes #once = {
    self.last_loc = arg.loc;
    // field accesses should have been desugared.
    @err_assert(arg.expr&.is(.GetVar), "took address of r-value") return;
    var := arg.expr.GetVar;
    // TODO: this shouldn't allow let either but i changed how variable refs work for :SmallTypes
    @assert_ne(var.kind, .Const, "Can only take address of var (not let/const) %", var&.log(self.program));
    id := @unwrap(self.var_lookup&.get(var), "Missing var % (in !addr) \n(missing $ when used in const context or missing '=>'? TODO: better error message)", var&.log(self.program)) return;
    self.addr_var(id);

    @match(result_location) {
        fn PushStack() => ();
        fn ResAddr()   => self.push(StorePre = .P64);
        fn Discard()   => self.push(Snipe = 0);
    };
    .Ok
}

// we never make the temp variable. if the arg is big, caller needs to setup a result location.
fn emit_call_if(self: *EmitBc, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    @err_assert(arg.expr&.is(.If), "ICE: expected if") return;
    parts := arg.expr.If;
    
    @try(self.compile_expr(parts.cond, .PushStack, false)) return; // cond
    if_true, if_false := (parts.if_true, parts.if_false);
 
    out := self.comp.get_info(if_true.ty);
    @assert(out.size_slots.lt(4).or(result_location != .PushStack), "ICE: 'if' result too big to go on stack"); // now its the callers problem to deal with this case

    branch_block := self.current_block;
    true_ip := self.push_block_empty();
    @try(self.compile_expr(if_true, result_location, can_tail)) return;
    end_true_block := self.current_block;
    false_ip := self.push_block_empty();
    @try(self.compile_expr(if_false, result_location, can_tail)) return;
    end_false_block := self.current_block;
    
    ::if(u16);
    block_slots := if(result_location == .PushStack, => out.size_slots, => 0);
    prims := if(result_location == .PushStack, => self.get_primatives(if_true.ty), => empty());
    ip := self.push_block(prims);
    self.push_to(branch_block, (JumpIf = (true_ip = true_ip, false_ip = false_ip, slots = 0 )));
    self.push_to(end_true_block, (Goto = (ip = ip, slots = block_slots)));

    self.push_to(end_false_block, (Goto = (ip = ip, slots = block_slots)));
    self.result[ip].incoming_jumps += 2;
    self.result[true_ip].incoming_jumps += 1;
    self.result[false_ip].incoming_jumps += 1;
    self.bump_clock(ip);

    .Ok
}

fn emit_switch(self: *EmitBc, arg: *FatExpr, result_location: ResultLoc, can_tail: bool) PRes #once = {
    @err_assert(arg.expr&.is(.Switch), "ICE: expected switch") return;
    parts := arg.expr.Switch;
    
    if parts.cases.len == 0 {
        // There's only a default; that's not really a switch bro but ok... 
        @try(self.compile_expr(parts.value, .Discard, false)) return;
        @try(self.compile_expr(parts.default, result_location, false)) return;
        return(.Ok);
    };
    
    @try(self.compile_expr(parts.value, .PushStack, false)) return; //  this is the thing we're inspecting!
    entry_block := self.current_block; 
   
    // This is where we rejoin with the value of the whole switch expression. 
    out := self.comp.get_info(arg.ty);
    @assert(out.size_slots.lt(4).or(result_location != .PushStack), "ICE: 'switch' result too big to go on stack"); // now its the callers problem to deal with this case
    block_slots := if(result_location == .PushStack, => out.size_slots, => 0);
    prims := if(result_location == .PushStack, => self.get_primatives(arg.ty), => empty());
    end_block := self.push_block(prims);
    self.current_block = entry_block;
    cases: List(SwitchPayload) = list(self.comp.get_alloc());
    
    push_case :: fn(value: i64, body: *FatExpr) void => {
        case_block := self.push_block_empty();
        @try(self.compile_expr(body, result_location, false)) return; // TODO: tail call
        end_case_block := self.current_block;
        self.push_to(end_case_block, (Goto = (ip = end_block, slots = block_slots)));
        cases&.push(value = value, block = case_block); 
        self.result[case_block].incoming_jumps += 1;
        self.result[end_block].incoming_jumps += 1;
    };
    
    each parts.cases { f |
        push_case(f._0, f._1&);
    };
   
    // TODO: would it be nicer to have default branch just be the last thing in the ast node too so you could handle them uniformly? -- Jul 26
    push_case(-1, parts.default);
    // TODO: make sure fn neg is #fold
    
    self.current_block = end_block;
    self.push_to(entry_block, (Switch = self.result.switch_payloads.len.trunc()));
    self.result.switch_payloads&.push(cases.rs(), self.comp.get_alloc());
     
    self.bump_clock(end_block);
    .Ok
}

fn emit_call_loop(self: *EmitBc, arg: *FatExpr) PRes #once = {
    @debug_assert_eq(arg.ty, void);

    prev_block := self.current_block;
    start_body_block := self.push_block_empty();
    self.current_block = start_body_block;
    self.push_to(
        prev_block,
        (Goto = (ip = start_body_block, slots = 0)),
    );

    @try(self.compile_expr(arg, .Discard, false)) return;
    end_body_block := self.current_block;

    self.push_to(
        end_body_block,
        (Goto = (ip = start_body_block, slots = 0)),
    );
    self.result[start_body_block].incoming_jumps += 2;
    .Ok
}

// :PlaceExpr
fn set_deref(self: *EmitBc, place: *FatExpr, value: *FatExpr) PRes #once = {
    // we care about the type of the pointer, not the value because there might be a cast.
    @match(place.expr&) {
        fn GetVar(_) => panic("ICE: var set should be converted to place expr");
        fn Deref(arg) => {
            // TODO: write a test for pooiinter eval oreder. left hsould come first. -- MAy 7
            @try(self.compile_expr(arg[], .PushStack, false)) return;
            self.result_addr_depth += 1;
            @try(self.compile_expr(value, .ResAddr, false)) return;
            self.result_addr_depth -= 1;
            return(.Ok)
        }
        @default => return(@err("TODO: other `place=e;` :("));
    }
}

fn unwrap(self: Name) Symbol = {
    @match(self) {
        fn Var(v) => v.name;
        fn Ident(v) => v;
        @default => panic("Expected name!");
    }
}

// TODO: rename? also used for tagged. 
fn construct_struct(self: *EmitBc, pattern: *Pattern, requested: Type, result_location: ResultLoc) PRes #once = {
    raw_container_ty := self.program.raw_type(requested);
    slots := self.slot_count(raw_container_ty);
    @debug_assert(slots < 8 || result_location != .PushStack, "too big to put on stack"); 
    
    @match(self.comp.get_type(raw_container_ty)) {
        fn Struct(f) => {
            expected := f.fields.len - f.const_field_count.zext();
            if !f.is_union {
                @assert_eq(expected, pattern.bindings.len, "Cannot assign to type % with wrong field count", self.program.log_type(requested));
            };
            i := 0;
            each f.fields { field | 
                continue :: local_return;
                i += 1;
                if(field.kind == .Const, => continue());
                value := pattern.bindings[i - 1]&;
                
                name := @unwrap(value.ident(), "map literal entry needs name (while initilizing %)", self.program.log_type(requested)) return;
                field := or find_struct_field(f, name, i - 1) {
                    if f.is_union {
                        continue();
                    };
                    return(@err("field name mismatch (ICE: should be checked by sema)"))
                };
                
                if result_location == .ResAddr {
                    self.push(PeekDup = 0);
                    self.inc_ptr_bytes(field.byte_offset.trunc());
                    self.result_addr_depth += 1;
                };
                expr := value.default.unwrap();
                @try(self.compile_expr(expr&, result_location, false)) return;
                if result_location == .ResAddr {
                    self.result_addr_depth -= 1;
                };
                if f.is_union { // :SLOW
                    if result_location == .ResAddr {
                        self.pop(1); // res ptr
                    };
                    return(.Ok);
                };
            };
            if result_location == .ResAddr {
                self.pop(1); // res ptr
            };
        }
        // TODO: make this constexpr in compiler (TODO: audit: this comment is from back when i had the interp)
        fn Tagged(f) => {
            @debug_assert(result_location != .Discard, "todo");
            size := self.slot_count(raw_container_ty);
            @debug_assert_eq(1, pattern.bindings.len, 
                "% is @tagged, value should have one active varient (ICE: should be checked by sema)",
                self.program.log_type(requested)
            );
            name := pattern.bindings[0].name.unwrap();
            i := f.cases.position(fn(f) => f._0 == name).expect("case name to exist in type");
            payload_size := self.slot_count(f.cases[i]._1);
            if payload_size >= size {
                return(@err("Enum value won't fit."));
            };
            value := pattern.bindings[0].default.unwrap();
            @match(result_location) {
                fn PushStack() => {
                    self.push(PushConstant = (
                        value = i,
                        ty = .I64,
                    ));
                    @try(self.compile_expr(value&, result_location, false)) return;

                    // :tagged_prims_hack
                    // TODO: this is a dumb hack to make the padding have the right prim types for backends that care. :SLOW -- Jun 23
                    //       This fixes test_option/option_small_payload_n/parse on llvm.  
                    // TODO: this is super copy-paste-y now that new sema outputs values for some tagged ctx_field
                    types := self.comp.flat_tuple_types(raw_container_ty);
                    types&.ordered_remove(0);
                    range(0, self.slot_count(value.ty).zext()) { _ |
                        types&.ordered_remove(0);
                    };
                    expected_pad := size - (payload_size + 1);
                    @debug_assert_eq(types.len(), expected_pad.zext(), "confused about padding size");
                    // now types is just padding

                    // If this is a smaller varient, pad out the slot.
                    for types { p |
                        ty := @unwrap(self.comp.prim(p), "not prim") return;
                        self.push(PushConstant = (value = 0, ty = ty));
                    };
                }
                fn ResAddr() => {
                    self.push(PeekDup = 0);
                    self.push(PushConstant = (
                        value = i,
                        ty = .I64,
                    ));
                    self.push(StorePre = .I64);
                    self.inc_ptr_bytes(8); // TODO: differetn sizes of tag
                    @try(self.compile_expr(value&, result_location, false)) return;
                }
                fn Discard() => {
                    @try(self.compile_expr(value&, result_location, false)) return;
                }
            };

            // TODO: support explicit uninit so backend doesn't emit code for the padding above.
            //       current system also means weird type stuff where you write ints into units in bc_to_asm.
            //       if we staticlly know the tag value, could only copy the size of the active varient (which would be the general case of leaving it uninit when creating one).
            //       but also eventually we probably want to define and preserve padding so tags could be stored there.
            //       even without that, maybe its a weird undefined behaviour to just drop some of your bytes,
            //       should have compiler settings about being allowed to cast *T -> *[size_of(T)]u8 because that would observe the padding.
            //       I like the idea of granular toggling ub vs optimisations and having those flags as a place to hang comments,
            //       but that adds a lot of extra work testing all the combinations which might not be worth it.
            //       -- Apr 17
            //
        }
        @default => {
            return(@err("struct literal for non-(struct/tagged)"));
        };
    };
    .Ok
}

fn push_block_empty(self: *EmitBc) BbId = {
    self.result.blocks&.push((
        insts = empty(),
        debug = empty(),
        arg_prims = empty(),
        incoming_jumps = 0,
        clock = self.clock,
    ), self.out_alloc);
    b: BbId = (id = self.result.blocks.len().trunc() - 1);
    self.current_block = b;
    b
}

fn push_block(self: *EmitBc, arg_prims: [] Prim) BbId = {
    self.result.blocks&.push((
        insts = empty(),
        debug = empty(),
        arg_prims = arg_prims,
        incoming_jumps = 0,
        clock = self.clock,
    ), self.out_alloc);
    b: BbId = (id = self.result.blocks.len().trunc() - 1);
    self.current_block = b;
    b
}

fn bump_clock(self: *EmitBc, b: BbId) void ={
    self.clock += 1;
    self.result[b].clock = self.clock;
}

fn slot_count(self: *EmitBc, ty: Type) u16 = {
    self.comp.get_info(ty)[].size_slots 
}

fn push(self: *EmitBc, inst: Bc) void = {
    self.push_to(self.current_block, inst);
}

fn addr_var(ctx: *EmitBc, id: u16) void = {
    if ctx.is_ssa_var&.get(id.zext()) {
        ctx.push(LoadSsa = (id = id));
    } else {
        ctx.push(AddrVar = (id = id));
    };
}

fn save_ssa_var(ctx: *EmitBc) u16 = {
    id := ctx.add_var(rawptr);
    ctx.is_ssa_var&.set(id.zext(), temp()); 
    ctx.push(SaveSsa = (id = id, ty = .P64));
    id
}

fn inc_ptr_bytes(self: *EmitBc, bytes: u16) void = {
    if bytes != 0 {
        self.push(IncPtrBytes = bytes.zext());
    };
}

::tagged(Bc);
fn push_to(self: *EmitBc, b: BbId, inst: Bc) void = {
    //@println("%", inst&.tag());
    self.result.mix_hash(inst&.tag().ordinal(), 9801);
    self.result[b].insts&.push(inst, self.out_alloc);
    self.program.bc_count += 1;
}

fn debug_location_for_last_op(self: *EmitBc, loc: Span) void = {
    block := self.result[self.current_block]&;
    // TODO: This representation is wasteful because most instructions aren't calls. 
    while => block.debug.len < block.insts.len { // :SLOW should at least reserve
        block.debug&.push(Span.zeroed(), self.out_alloc); 
    };
    block.debug[block.insts.len - 1] = loc;
}

fn mix_hash(self: *FnBody, i: i64, spice: i64) void = {
    self.hash += i;
    self.hash *= spice;
}

fn pop(self: *EmitBc, slots: u16) void = {
    range(0, slots.zext()) { _ | 
        self.push(Snipe = 0);
    }
}

fn index(self: *FnBody, b: BbId) *BasicBlock = {
    self.blocks.index(b.id.zext())
}
fn add_var(ctx: *EmitBc, ty: Type) u16 = {
    info := ctx.comp.get_info(ty);
    ctx.result.mix_hash(info.stride_bytes.zext(), 12345);
    ctx.result.vars&.push((size = info.stride_bytes, align = info.align_bytes), ctx.out_alloc);
    i: u16 = ctx.result.vars.len.trunc();
    i - 1
}

/////////////////////////////////////
/// Primitive Type Representation ///
// 
// The distinction between backend types and front end types is important. 
// There are different registers for storing integers/pointers vs floats so the backend really does need to know which everything is. 
// It was much more confusing before when I was keeping a mask of which parts were floats and only had one type of float/int. 
// But the current implementation is more complicated than it needs to be because I wasn't sure what was important at the time. 
// 

// This (prim_sig implementation) is insane. Caching all the variations of compctx/indirect is painful. 
// At least the interface is fairly thin. ~almost~ only emit_bc needs to deal with creating prim_sigs.
// but every backend needs to remember to handle indirect returns specially every time they touch one. 
//
// I treat the indirect seperately because the backend wants to put it in a special register. 
// But then for the block args, you do want the indirect ptr because the bc expects it. 
// And then compctx is weird because the front end doesn't want to insert it into the arguments, 
// so its not as part of the function type, its in the calling convention. -- Jul 5
//
fn prim_sig(program: CompCtx, f_ty: FnType, cc: CallConv) Res(PrimSig) = {
    @err_assert(!f_ty.arg.is_unknown() && !f_ty.ret.is_unknown(), "unknown fn types") return;
    ret := program.get_info(f_ty.ret);

    sig: PrimSig = (
        ret1 = .None,
        ret2 = .None,
        return_value_bytes = ret.stride_bytes,
        first_arg_is_indirect_return = ret.size_slots > @as(u16) 2.trunc(),
        no_return = f_ty.ret.is_never(),
    ); // TODO: if you typeo make this a '}' need to have a good error message. -- Jul 6

    @switch(ret.size_slots) {
        @case(0) => ();
        @case(1) => {
            sig.ret1 = program.prim(f_ty.ret);
        };
        @case(2) => {
            a, b := @try(program.prim_pair(f_ty.ret)) return;
            sig.ret1 = (Some = a);
            sig.ret2 = (Some = b);
        };
        @default => {
            // Note: not adding indirect pointer to sig because its handled sperately. TODO: that's kinda confusing.
        };
    };

    args: List(Prim) = list(program.get_alloc());
    // :ConfusingPrims
    args&.push(.P64); // for ret
    args&.push(.P64); // for #ct

    :: enum(CallConv);
    comp_ctx := cc == .CCallRegCt;

    found_arity := 0;
    // TODO: compiler bug? what push_arg am i possibly shadowing?????? -- Jul 6
    push_arg__wtf :: fn(ty: Type) void => {
        found_arity += 1;
        info := program.get_info(ty);
        if info.pass_by_ref {
            args&.push(.P64);
        } else {
            for program.flat_tuple_types(ty) { t | 
                args&.push(program.prim(t).expect("arg prim"));
            };
        };
    };

    done := false;
    // TODO: if i always collapsed tuples of the same to array this would need different handling.
    @if_let(program.get_type(f_ty.arg)) 
        (fn Struct(f) => {
            if f.is_tuple {
                done = true;
                // TODO: :const_field_fix (not done on rust side either so add test for this!)
                each f.fields { f |
                    push_arg__wtf(f.ty);
                };
            };
        });
    if !done {
        push_arg__wtf(f_ty.arg);
    };

    // TODO: decide what i want a tuple to be. is it a real type you can have in memory or is it the thing that multiple arguments are?
    //       those ideas should extend to return values instead of them being special.
    //       should have a spread operator for calling a function on a tuple of arguments? like 'a := (1, 2); f(..a)'
    // @assert_eq(found_arity, f_ty.arity, "TODO: fn(a: Ty(i64, i64))");

    
    // :ConfusingPrims
    prims := args.items();
    prims.ptr = prims.ptr.offset(2);
    prims.len -= 2;
    program.data.cast()[][].primitives&.insert(f_ty.arg, prims);
    
    sig.args = prims;
    if comp_ctx {
        sig.args.ptr = sig.args.ptr.offset(-1);
        sig.args.len += 1;
    };

    sig.arg_int_count = 0;
    for sig.args { p |
        if !p.is_float() {
            sig.arg_int_count += 1;
        };
    };
    
    (Ok = sig)
}

fn prim_pair(self: CompCtx, ty: Type) Res(Ty(Prim, Prim)) = {
    types := self.flat_tuple_types(ty); // TODO: don't allocate
    @debug_assert_eq(types.len(), 2);
    a : Prim = @unwrap(self.prim(types[0]), "non-prim") return;
    b := @unwrap(self.prim(types[1]), "non-prim") return;
    (Ok = @as(Ty(Prim, Prim)) (a, b))
}

fn get_primitives(self: *SelfHosted, key: Type) ?[] Prim = {
    self.primitives&.get(key&)
}

fn get_primatives(self: *EmitBc, ty: Type) [] Prim = 
    self.program.get_primatives(ty);

// note: this just gives empty for unit so you can't offset it like :ConfusingPrims
fn get_primatives(self: *SelfHosted, ty: Type) [] Prim = {
    if(ty.is_unit() || ty.is_never(), => return(empty()));
    slots := self.get_info(ty).size_slots();
    if self.get_primitives(ty) { p |
        return(p);
    };
    flat_types := self.comp().flat_tuple_types(ty);
    types: List(Prim) = list(self.get_alloc());
    // :ConfusingPrims
    types&.push(.P64);  // for ret
    types&.push(.P64);  // for #ct
    
    for flat_types { t |
        if !t.is_unit().or(t.is_never()) {
            types&.push(self.prim(t).expect("tuple part to be prim"));
        };
    };
    // :ConfusingPrims
    prims := types.items();
    prims.ptr = prims.ptr.offset(2);
    prims.len -= 2;
    self.primitives&.insert(ty, prims);
    self.get_primitives(ty).unwrap()
}

fn comp(self: *SelfHosted) CompCtx = {
    self.vtable.with(self.legacy_indirection)
}
fn prim(self: *SelfHosted, ty: Type) ?Prim = {
    self.comp().prim(ty)
}

fn prim(self: CompCtx, ty: Type) ?Prim = {
    p: Prim = @match(self.get_type(ty)) {
        fn F64()     => .F64;
        fn F32()     => .F32;
        fn Bool()    => .I8 ;
        fn VoidPtr() => .P64;
        fn FnPtr(_)  => .P64;
        fn Ptr(_)    => .P64;
        fn Fn(_)     => .I32;
        fn Label(_)  => .I32;
        fn Int(int) => 
            @switch(int.bit_count) { 
                @case(8)  => .I8 ;
                @case(16) => .I16;
                @case(32) => .I32;
                @default  => .I64;  // TODO: :fake_ints_too_big
            };
        fn Struct(f) => {
            if f.fields.len - f.const_field_count.zext() == 1 {
                fst := f.fields[0]&;
                assert(fst.kind != .Const, "TODO: first field const");
                if self.slot_count(fst.ty) == 1 {
                    return(self.prim(fst.ty));
                };
            };
            return(.None)
        }
        fn Tagged(f) => {
            each f.cases { c |
                if !c._1.is_unit() {
                    return(.None);
                };
            };
            .I64 // TODO: :tag_is_always_i64
        }
        fn Named(f) => return(self.prim(f._0));
        fn Enum(f) => return(self.prim(f.raw));
        fn Array(f) => {
            if f.len == 1 {
                return(self.prim(f.inner));
            };
            return(.None)
        }
        @default => return(.None);
    };
    (Some = p)
}

fn flat_tuple_types(self: CompCtx, ty: Type) List(Type) = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            out: List(Type) = list(temp());
            each f.fields { f | 
                if !f.kind.eq(.Const) {| 
                    out&.push_all(self.flat_tuple_types(f.ty).items());
                };
            };
            out
        }
        fn Enum(f) => self.flat_tuple_types(f.raw);
        fn Named(f) => self.flat_tuple_types(f._0);
        // TODO: this is sketchy
        fn Tagged(f) => {
            slots: i64 = self.slot_count(ty).zext();
            varients: List(List(Type)) = list(f.cases.len, temp());
            for f.cases { f |
                var := self.flat_tuple_types(f._1);
                if var.len > 0 {
                    varients&.push(var);
                };
            };
            if varients.len() == 1 {
                varients[0]&.insert(0, i64);
                @debug_assert_eq(slots, varients[0].len());
                return(varients[0]);
            };
            // TODO: hack
            i64.repeated(slots, temp())
        }
        fn Array(f) => {
            out := self.flat_tuple_types(f.inner);
            single := out.len;
            // TODO: fix underflow. better ban 0 len array -- Jul 9
            range(0, f.len.zext() - 1) { i|
                // reslice everytime because we might reallocate. TODO: just reserve at the beginning.  -- Jul 6
                out&.push_all(out.items().slice(0, single));
            };
            out
        }
        fn void() => list(temp());
        @default => ty.repeated(1, temp());
    }
}

fn idx(s: FuncId) i64 = s.to_index().zext();

// :get_or_create_type
fn is_unit(t: Type) bool = t == void;
fn is_unknown(t: Type) bool = t == UnknownType;
fn is_never(t: Type) bool = t == Never;

fn is_float(self: Prim) bool = 
    self.eq(.F64).or(self == .F32);
    
fn align_to(offset: i64, align: i64) i64 = {
    extra := offset.mod(align);
    if extra == 0 {
        offset
    } else {
        offset + align - extra
    }
}

fn align_to(offset: u16, align: u16) i64 #redirect(Ty(i64, i64), i64);

// TODO: this is a dumb hack to make it less painful that i can't access fields on a value (because you need a pointer to offset)
//       fix in the compiler and remove this! -- Jul 7
//       now the problem is you can't access fields on a pointer if its not a dereference of it 
//       (i changed get_info when i ported it, i didn't fix the old problem yet).  -- Jul 21
fn stride_bytes(s: *TypeMeta) u16 = s.stride_bytes;
fn align_bytes(s: *TypeMeta) u16 = s.align_bytes;
fn size_slots(s: *TypeMeta) u16 = s.size_slots;
fn contains_pointers(s: *TypeMeta) bool = s.contains_pointers;

fn slot_count(self: CompCtx, ty: Type) u16 = {
    self.get_info(ty)[].size_slots
}
fn int_count(self: Prim) i64 = 
    if(self.is_float(), => 0, => 1);

fn ret_slots(self: *PrimSig) u16 = {
    ::if(u16);
    if self.ret1.is_some() {
        if(self.ret2.is_some(), => 2, => 1)
    } else {
        0
    }
}


// TODO: be able to derive but skip fields? or take off `func`?
// TODO: (also in the derived version) for aggragates like this with multiple collections, check all the lengths first before iterating. 
//       generalize quick_ne or something maybe? 
fn eq(lhs: **FnBody, rhs: **FnBody) bool = {
    //if(lhs.when != rhs.when, => return(false));
    if(lhs.blocks&.items() != rhs.blocks&.items(), => return(false));
    if(lhs.vars&.items() != rhs.vars&.items(), => return(false));
    // you almost never need this... but when you do you really do! :deduplicate_checks_switch_payloads
    if(lhs.switch_payloads&.items() != rhs.switch_payloads&.items(), => return(false)); 
    // Note: not checking sig_payloads, i think its fine tho becuase we check function ids
    true
}

// we do a rolling hash as we create the FnBody so just return that. 
fn hash(h: *TrivialHasher, s: **FnBody) void = {
    h.hash(s.hash&);
}

:: {
    fn eq(a: FuncId, b: FuncId) bool #redirect(Ty(u32, u32), bool);
    DerefEq(FuncId);
    
    @if(ENABLE_DEDUPLICATION) {
        fn eq(a: *BasicBlock, b: *BasicBlock) bool = {
            // not comparing debug info, who cares. 
            a.insts& == b.insts& && a.arg_prims& == b.arg_prims& && a.incoming_jumps == b.incoming_jumps && a.clock == b.clock
        }
        AutoEq(Bc);
        AutoEq(PrimSig);
        DerefEq(Prim);
        DerefEq(Intrinsic);
        AutoEq(?Prim);
        AutoEq(SwitchPayload);
        AutoEq(BbId);
        AutoEq(BakedVarId);
        AutoEq(VarSlotType);
        AutoEq(get_variant_type(Bc, Bc.Tag().CallDirect));
        AutoEq(get_variant_type(Bc, Bc.Tag().CallFnPtr));
        AutoEq(get_variant_type(Bc, Bc.Tag().PushConstant));
        AutoEq(get_variant_type(Bc, Bc.Tag().JumpIf));
        AutoEq(get_variant_type(Bc, Bc.Tag().Goto));
        AutoEq(get_variant_type(Bc, Bc.Tag().AddrVar));
        AutoEq(get_variant_type(Bc, Bc.Tag().SaveSsa));
        AutoEq(get_variant_type(Bc, Bc.Tag().LoadSsa));
        AutoEq(get_variant_type(Bc, Bc.Tag().LastUse));
        AutoEq(get_variant_type(Bc, Bc.Tag().Ret2));
    };
};
