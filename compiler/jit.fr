//! - Call a function pointer with a dynamically known type on an array of bytes.
//! - Create a shim around an uncompiled function that redirects into the compiler and then forwards to the real callee. 
//! - Track which FuncId escape to user code as a pointer in comptime address space that might need to be baked. 
//! - Require a function be jitted to be called at comptime.

DynCallShim :: @tagged(Pending: Qbe.Sym, Ready: rawptr);

fn call_dynamic_values(self: *SelfHosted, f_ptr: i64, f_ty: FnType, args_value: []u8) Values = {
    ret_size: i64 = self.get_info(f_ty.ret)[].stride_bytes.zext();
    ret_buf := temp().alloc_zeroed(u8, ret_size);  // TODO: alignment?
    zone := self.zone_begin_dyncall(f_ptr.rawptr_from_int());
    self.stats.dyn_call += 1;
    
    @if(abi_shift_native_to_easy) {
        self.vm.eval(f_ptr, ret_buf, args_value);
        zone_end(zone);
        return self.from_bytes(ret_buf);
    };
    @if(abi_shift_easy_to_native) {
        @assert(f_ptr > 0, "%", f_ptr);
    };
    
    callee := self.dyncalls&.get_or_insert(f_ty) {
        // 
        // This is super stupid. we should reuse the shim when the types have the same repr. 
        // ie. currently, if you call some `fn(foo: *T) void` we'd make a new shim for every T, 
        // even though that doesn't change the calling convention.
        // However, we only get to this slow path 282 times (self.fr) which is nothing. 
        // The whole call_dynamic_values is 15/1258 main thread samples, and that is including the callee
        // (so any comptime code other than #macro and #x86_bytes). 
        // -- Mar 12, 2025
        //
        self.create_dyncall_shim(f_ty)
    };
    callee := @match(callee) {
        fn Ready(it)   => it[];
        fn Pending(it) => {
            c := wait_for_symbol(self.comptime_codegen, it[]);
            callee[] = (Ready = c);
            c
        }
    };
   
    // TODO: canary?
    callee := assume_types_fn(Arg = Ty(*u8, *u8, i64), Ret = void, ptr = callee);
    callee(args_value.ptr, ret_buf.ptr, f_ptr);
    zone_end(zone);
    self.from_bytes(ret_buf)
}

Futex :: import("@/lib/sys/sync/futex.fr");

// note: different from get_addr() because this auto-calls make_exec() for you on jit-wasm
// might be a jit-shim.
fn wait_for_symbol(shared: *FrontendCodegen, id: Qbe.Sym) rawptr = {
    // only used for comptime_codegen (which is single threaded currently) so never need to synchronize.
    @debug_assert(!shared.worker.threaded);
    c := rawptr.zeroed();
    use_symbol(shared.m, id) { symbol | 
        @if(TODOWASM)
        if shared.m.goal.arch == .wasm32 && symbol.kind == .Local && symbol.segment == .Code {
            shared.m.make_exec();
            @debug_assert(symbol.kind == .DynamicPatched);
        };
        c = symbol.jit_addr;  
        @debug_assert(!c.is_null(), "wait_for_symbol % % % %: %", 
            symbol.jit_addr, symbol.shim_addr, symbol.got_lookup_offset, symbol.name, symbol.kind);
    };
    c
}

fn comptime_jit_symbol_id(self: *SelfHosted, fid: FuncId) Qbe.Sym = {
    func := self.get_function(fid);
    if !func.has_comptime_jit_symbol_id {
        @debug_assert(!func.get_flag(.AnyConstArgs) && !func.get_flag(.UnboundGenerics));
        m := self.comptime_codegen.m;
        id := m.intern(self.fmt_fn_name(fid));
        func.comptime_jit_symbol_id = id.id;
        func.has_comptime_jit_symbol_id = true;
    };
    (id = func.comptime_jit_symbol_id)
}

// Creates a function with the signeture (arg_array: *u8, ret_array: *u8, callee: fnptr) -> void.
// It starts compiling on self.comptime_codegen and you have to get the result out of self.dyncalls when it's ready. 
// TODO: reuse functions when they have different nominal types but the same repr. 
fn create_dyncall_shim(self: *SelfHosted, f_ty: FnType) DynCallShim = {
    @if(abi_shift_easy_to_native) return(Vm'easy_to_native_dyncall_shim(self, f_ty));
    shared := self.comptime_codegen;
    ins: RawList(Qbe.Ins) = empty();
    id := Qbe.no_symbol_S; 
    enter_task shared { entry |
        f := temp().box_uninit(Qbe.Fn);
        m := shared.m;
        f.default_init(m);
        f.leaf = false;
        f.lnk.no_inline = true;
        id = shared.m.intern(@tfmt("dyncall%_%_%", f_ty.arg.as_index(), f_ty.ret.as_index(), int(f_ty.unary)));
        f.lnk.id = id;
        entry.task = (Func = f);
        
        // accept parameters from the compiler
        arg_p  := f.newtmp("arg", .Kl);
        ret_p  := f.newtmp("ret", .Kl);
        callee := f.newtmp("call", .Kl);
        env    := f.newtmp("context", .Kl);
        if next_abi_version() < 1513 {
            addins(ins&, make_ins(.pare, .Kl, env, QbeNull, QbeNull));
        };
        addins(ins&, make_ins(.par, .Kl, arg_p, QbeNull, QbeNull));
        addins(ins&, make_ins(.par, .Kl, ret_p, QbeNull, QbeNull));
        addins(ins&, make_ins(.par, .Kl, callee, QbeNull, QbeNull));
        
        ins = emit_dyncall_shim_epilogue(self, shared, f, f_ty, arg_p, ret_p, callee, env, ins);
    };
    (Pending = id)
}

emit_dyncall_shim_epilogue :: fn(self: *SelfHosted, shared: *FrontendCodegen, f: *Qbe.Fn, f_ty: FnType, arg_p: Qbe.Ref, ret_p: Qbe.Ref, callee: Qbe.Ref, env: Qbe.Ref, ins: RawList(Qbe.Ins)) RawList(Qbe.Ins) = {
        // :EmitIrCall
        // arg instructions that read from arg_p
        scratch: List(Qbe.Ins) = list(temp());
        EmitIr;
        push_arg :: fn(ty: Type, offset: i64) => {
            if self.get_info(ty)[].stride_bytes != 0 {
                ref := f.newtmp("a", .Kl);
                addins(ins&, make_ins(.add, .Kl, ref, arg_p, f.getcon(offset)));
                load, k := self.load_op(ty);
                ::if(Qbe.Ins);
                ins := if k != .Ke {  // scalar
                    r := f.newtmp("a", k);
                    addins(ins&, make_ins(load, k, r, ref, QbeNull));
                    make_ins(.arg, k, QbeNull, r, QbeNull)
                } else {  // aggregate
                    r_type := self.get_aggregate(shared, ty);
                    make_ins(.argc, .Kl, QbeNull, r_type, ref)
                };
                scratch&.push(ins);
            };
        };
        info := self.get_type(f_ty.arg);
        @assert(self.get_info(f_ty.arg)[].is_sized, "comptime call with unsized argument");
        if info.is(.Struct) && info.Struct.is_tuple {
            each info.Struct.fields { it | 
                push_arg(it.ty, it.byte_offset);
            };
        } else {
            push_arg(f_ty.arg, 0);
        };
        if next_abi_version() < 1513 {
            addins(ins&, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        };
        append(ins&, scratch.index_unchecked(0), scratch.index_unchecked(scratch.len));
        scratch&.clear();
        
        // do the call and write the results to ret_p
        _, k0 := self.load_op(f_ty.ret);
        info := self.get_info(f_ty.ret);
        ret_size: i64 = info.stride_bytes.zext();
        @assert(info.is_sized, "comptime call with unsized argument");
        result_r, k, type_r := @if_else{
            @if(ret_size == 0) => (QbeNull, Qbe.Cls.Kw, QbeNull);
            @if(k0 != .Ke) => {  // scalar
                r := f.newtmp("result", k0);
                o := self.store_op(f_ty.ret);
                @debug_assert(o != .Oxxx, "cannot store %", self.log_type(f_ty.ret));
                scratch&.push(make_ins(o, .Kw, QbeNull, r, ret_p));
                (r, k0, QbeNull)
            };
            @else => {
                r := f.newtmp("result", .Kl);
                scratch&.push(make_ins(.blit0, .Kw, QbeNull, r, ret_p));
                scratch&.push(make_ins(.blit1, .Kw, QbeNull, INT(ret_size), QbeNull));
                (r, Qbe.Cls.Kl, self.get_aggregate(shared, f_ty.ret))
            };
        };
        addins(ins&, make_ins(.call, k, result_r, callee, type_r));
        append(ins&, scratch.index_unchecked(0), scratch.index_unchecked(scratch.len));
        scratch&.clear();
        
        f.start = newblk();
        f.nblk = 1;
        f.start.ins = ins;
        f.start.jmp.type = .ret0;
        ins
};

// TODO: this generates shitty code. 
//       - i save the registers used only on the slow path at entry of the function. 
//       - it loads from the got twice on the fast path because i don't deduplicate RCon across blocks.
//       - redundant copies of aggregates. 
fn emit_jit_shim(c: *SelfHosted, fid: FuncId) void = {
    @if(DUMB_EMIT_IR) c.have_jit_shim&.push(fid, c.get_alloc());
    @if(abi_shift_native_to_easy) return(Vm'easy_abi_emit_shim(c, fid));
    @if(abi_shift_easy_to_native) return(Vm'native_abi_emit_shim(c, fid));
    
    self := c;
    shared := c.comptime_codegen;
    handler_callee_address: rawptr = report_called_uncompiled_or_just_fix_the_problem;
    m := shared.m; 
    func := self.get_function(fid);
    c.when_debug(.Shim, fn(o) => @fmt(o, "S: create %", fmt_func(c, fid)));
    id := Qbe.no_symbol_S;
    enter_task shared { entry |
        id = m.intern(@tfmt("%__shim", fid));
        
        // create a new function
        f := temp().box_uninit(Qbe.Fn);
        f.default_init(m);
        f.leaf = false;
        f.lnk.no_inline = true;
        f.lnk.id = id;
        f.start = newblk(); name_block(f.start, "start"); f.start.id = 0;
        do_the_call := newblk(); name_block(do_the_call, "do_the_call"); do_the_call.id = 1;
        not_ready := newblk(); name_block(not_ready, "not_ready"); not_ready.id = 2;
        
        f.nblk = 3;
        target_id := c.comptime_jit_symbol_id(fid);
        entry.task = (Shim = (f = f, shim_for = target_id));
        
        // collect parameters
        env    := f.newtmp("context", .Kl);
        if next_abi_version() < 1513 {
            push(f.start, make_ins(.pare, .Kl, env, QbeNull, QbeNull));
        };
        arg_ty := func.finished_arg.expect("arg infered");
        ret_ty := func.finished_ret.expect("ret infered");
        ::EmitIr;
        pars := emit_par_instructions(self, shared, func.arg&, arg_ty, f, .None);
        
        // If the entry in __got for the target symbol is not this shim, call it directly.
        // Otherwise, call into the compiler to ask for it to be compiled. 
        // Most calls will just go through __got directly in which case this check is wasteful,
        // but it helps in the case where you created a function pointer to the shim and stuck it in memory somewhere,
        // now not every single call to that has to go through the compiler and take a mutex to look at the symbol. 
        // It seems this doesn't make each shim only call into the compiler once (TODO: why?) but it does lower the number. 
        // -- Dec 22
        target := f.symcon(target_id);
        myself := f.symcon(id);
        is_ready := f.newtmp("ready", .Kl); 
        push(f.start, make_ins(.cnel, .Kl, is_ready, myself, target));
        // to test if this is helping, toggle this and add logging in report_called_uncompiled_or_just_fix_the_problem. 
        if m.goal.arch == .wasm32 {
            // can't do the fast path comparison on wasm because the indirect table 
            // doesn't yield observable function pointers in the way the GOT does
            // however, wasm can assign to the table slot to change which function 
            // the existing function pointer refers to, which has the same effect 
            // of only calling into the compiler the first time it's called. 
            is_ready = f.getcon(0);  
        };
        
        f.start.jmp = (arg = is_ready, type = .jnz);
        f.start.s1 = do_the_call;
        f.start.s2 = not_ready;
        
        // call into the compiler to get the real callee
        new_callee := f.newtmp("new_callee", .Kl);
        if next_abi_version() < 1513 {
            push(not_ready, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        };
        push(not_ready, make_ins(.arg,  .Kl, QbeNull, f.getcon(SelfHosted.int_from_ptr(c)), QbeNull));
        push(not_ready, make_ins(.arg,  .Kw, QbeNull, f.getcon(fid.as_index()), QbeNull));
        push(not_ready, make_ins(.call, .Kl, new_callee,  f.getcon(handler_callee_address.int_from_rawptr()), QbeNull));
        not_ready.jmp.type = .jmp;
        not_ready.s1 = do_the_call;
        
        // Rejoin and call something
        callee := f.newtmp("callee_join", .Kl);
        do_the_call.phi = temp().boxed(Qbe.Phi, (
            cls = .Kl, 
            to = callee,
            narg = 2,
            blk = new(2),
            arg = new(2),
            link = Qbe.Phi.ptr_from_int(0),
        ));
        do_the_call.phi.blk[0] = not_ready;
        do_the_call.phi.arg&[0] = new_callee;
        do_the_call.phi.blk[1] = f.start;
        do_the_call.phi.arg&[1] = target;
        
        name_block :: fn(b: *Qbe.Blk, name: Str) = {
            b.name = name;
        };
        
        // convert to arguments
        if next_abi_version() < 1513 {
            push(do_the_call, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        };
        emit_native_shim_epilogue(self, shared, f, ret_ty, do_the_call, pars, callee);
        f.start.link = do_the_call;
        do_the_call.link = not_ready;
    };
    
    @debug_assert(!shared.worker.threaded);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY || SPAM_DYN_CALL {
        // TODOWASM: don't make_exec for this, just assign a got_lookup_offset
        c.created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id));
    };
}

emit_native_shim_epilogue :: fn(self: *SelfHosted, shared: *FrontendCodegen, f: *Qbe.Fn, ret_ty: Type, do_the_call: *Qbe.Blk, pars: []ArgInfo, callee: Qbe.Ref) void = {
        each pars { p |
            @if_else {
                @if(p.scalar) => push(do_the_call, make_ins(.arg,  p.k, QbeNull, p.r, QbeNull));
                @else         => push(do_the_call, make_ins(.argc, p.k, QbeNull, self.get_aggregate(shared, p.type), p.r));
            };
        };
        
        // do the real call to the new callee 
        _, k0 := self.load_op(ret_ty);
        info  := self.get_info(ret_ty);
        @assert(info.is_sized, "comptime call with unsized argument");
        jmp, result_r, k, type_r := @if_else{
            @if(info.stride_bytes == 0) => (Qbe.J.ret0, QbeNull, Qbe.Cls.Kw, QbeNull);
            @if(k0 != .Ke)     => (k0.retk(), f.newtmp("result", k0), k0, QbeNull);
            @else              => (Qbe.J.retc, f.newtmp("result", .Kl), Qbe.Cls.Kl, self.get_aggregate(shared, ret_ty));
        };
        push(do_the_call, make_ins(.call, k, result_r, callee, type_r));
        f.ret_cls = @if(jmp != .ret0, k0, .Kx);
        
        // return the result to our caller
        f.retty = type_r;
        do_the_call.jmp = (type = jmp, arg = result_r);
};

fn create_jit_shim(self: *SelfHosted, fid: FuncId) Res(void) = {
    opts := self.get_build_options();
    func := self.get_function(fid);
    if func.get_flag(.AlreadyHasShim) || func.get_flag(.RealAsmDone) { 
        return(.Ok);
    };
    
    // Need to know types to do ABI stuff.
    res := self.poll_in_place(void) {
        r :: local_return;
        @check(self.infer_arguments(fid)) r;
        @check(self.infer_return(fid)) r;
        .Ok
    };
    @try(res) return;
    
    emit_jit_shim(self, fid);
    @debug_assert(!func.get_flag(.RealAsmDone), "we didn't actually compile it why do we think we did??"); 
    func.set_flag(.AlreadyHasShim);
    
    .Ok
}

fn shallow_jit_func(self: *SelfHosted, fid: FuncId) Res(void) = {
    func := self.get_function(fid);  
    
    if func.body&.is(.DynamicImport) && func.body.DynamicImport.comptime == 0 {
        // It's an import but we don't have a jit pointer for it (== didn't find it in a dylib). 
        // So there's nothing we can do. Just make a jit shim so trying to call it 
        // will safely error with "tried to call uncompiled". 
        // Also, it needs a unique address so it can be baked. 
        return(self.create_jit_shim(fid)); 
    };
    
    for func.mutual_callees& { fid |
        if !self.get_function(fid).get_flag(.AsmDone) {
            @try(self.create_jit_shim(fid)) return; 
        };
    };
    
    // ugh. can't be temp() because threaded=false wants to reset it. 
    pending := FuncId.list(general_allocator()); 
    
    emit_ir :: EmitIr.emit;
    @try(emit_ir(self, self.comptime_codegen, fid, .Jit, pending&, true)) return;
    
    shared := self.comptime_codegen;
    @debug_assert(!shared.worker.threaded);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY || SPAM_DYN_CALL {
        id := self.comptime_jit_symbol_id(fid);
        self.created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id));
    };
    
    func.set_flag(.RealAsmDone);
    func.set_flag(.AsmDone);
    for pending& { callee |
        if !self.get_function(callee).get_flag(.AsmDone) {
            @try(self.create_jit_shim(callee)) return; 
            func.mutual_callees&.push(callee, self.get_alloc());  // TODO: shouldn't need this its just for import_c
        };
    };
    pending&.drop();
    .Ok
}

fn save_in_got(self: *SelfHosted, fid: FuncId, addr: rawptr) void = {
    // :InsaneCaches
    m := self.comptime_codegen.m;
    id := self.comptime_jit_symbol_id(fid);
    use_symbol(m, id) { s |
        @debug_assert(s.got_lookup_offset != -1, "save_in_got missing slot for %", s.name);
        if m.goal.arch == .wasm32 {
            // TODO: might not need this. either way, clean it up!
            (m.target.wasm_jit_event)(Assign = (dest = s.got_lookup_offset, src = addr.int_from_rawptr()));
        } else {
            got := m.segment_address(.ConstantData, s.got_lookup_offset);  // :GotAtConstantDataBase 
            rawptr.ptr_from_raw(got)[] = addr;
        }
    };
}

// TODO: it would be nice if i had the arguments here for reporting what call you actually tried to make. 
//       but this way will be even better once i can lazily compile it and just lean on the backend to make the args flow through. 
report_called_uncompiled_or_just_fix_the_problem :: fn(self: *SelfHosted, fid: FuncId) rawptr = {
        @if(__driver_abi_version >= 1513)
        if get_stack_base_for_tls() != self.main_thread_tls {
            // TODO: this check might fault if not a franca thread and sp&mask happens to be unmapped 
            if get_stack_base_for_tls() != tls(.thread)[].int_from_rawptr() {            
                report_threaded_jit_shim(self, fid);
            };
            // it was a franca thread (just not the main one).
            // the frontend data structures are not thread safe. 
            // need to request the main thread compile the function and just wait for it here.
            // since its a franca thread, tls(.current_os) will be set so it's safe to futex().   
            return wait_threaded_jit_shim(self, fid);
        };      
        @debug_assert(!self.debug_deny_jit_shim_calls, "debug_deny_jit_shim_calls");
        self.stats.shim_calls += 1;
        
        m := self.comptime_codegen.m;
        // important: have to be careful about which comptime is in the context if the driver program gets compiled lazily. 
        func := self.get_function(fid);
        
        self.when_debug(.Shim, fn(o) => @fmt(o, "S: call %", fmt_func(self, fid)));
        if func.get_flag(.AvoidJitShim) {
            @panic("Called an #avoid_shim function through a jit_shim");
        };
        result := zeroed(rawptr);
        if func.get_flag(.RealAsmDone) { 
            xxx := self.get_fn_callable(fid);
            result = xxx.unwrap();
        };
    // you're required to already be on the same thread but this makes it more likely to work 
    // if you have multiple CompCtxs floating around and trying to call each other, 
    // which is a bit sketchy but would be nice if it worked. 
    push_tls_value(.comptime, SelfHosted.raw_from_ptr(self)) {
        if result.is_null() {
            mark := mark_temporary_storage();
            @debug_assert(!self.comptime_codegen.worker.no_more_functions);
            @debug_assert(!self.debug_deny_unfilled_jit_shim_calls, "expected all shims done");
            // :ugly
            first := true;
            addr := self.poll_in_place(rawptr) { () Maybe(rawptr) |
                r :: local_return;
                if func.get_flag(.RealAsmDone) {
                    p := self.get_fn_callable(fid).unwrap();
                    r(Ok = p);
                };
                if !first {
                    // If its an import we're never gonna make progress by trying to compile it. 
                    if func.get_flag(.BodyIsSpecial) {
                        @panic("Tried to call uncompiled function:\n %", self.get_function(fid).log(self));
                    };  
                };
                first = false;
                func.unset_flag(.AsmDone);
                r(Suspend = self.wait_for(Jit = fid))
            };
            result = self.unwrap_report_error(rawptr, addr);
            reset_temporary_storage(mark);
        };
    };
    // the shim itself checks that the address in GOT matches the shim address before calling into the compiler. 
    // so replacing it with the real address now makes future calls faster (even if they go through the old function pointer). 
    save_in_got(self, fid, result);
    result
};

// return existing jit_addr or shim_addr.
// currently i rely on W+X memory so this function doesn't need to change permissions. 
fn get_fn_callable(self: *SelfHosted, fid: FuncId) ?rawptr = {
    m := self.comptime_codegen.m;
    func := self.get_function(fid);
    if !func.get_flag(.RealAsmDone) && !func.get_flag(.AlreadyHasShim) {
        return(.None);
    };
    @debug_assert(!func.get_flag(.AnyConstArgs) && !func.get_flag(.UnboundGenerics));
    id := self.comptime_jit_symbol_id(fid);
    c := wait_for_symbol(self.comptime_codegen, id);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY {
        // created_jit_fn_ptr_value is done when creating it
        @debug_assert(self.baked.vmem&.get(c).is_some());
    };
    (Some = c)
}

// create shim if not yet ready. 
// assume the pointer escapes to user code and might need to be baked. 
fn get_jitted_ptr(self: *SelfHosted, f: FuncId) CRes(rawptr) = {
    self.took_pointer_value(f);
    p := self.get_fn_callable(f) || {
        @try(self.create_jit_shim(f)) return; 
        self.get_fn_callable(f).unwrap()
    };
    (Ok = p)
}

fn took_pointer_value(self: *SelfHosted, fid: FuncId) void = {
    func := self.get_function(fid);
    if func.get_flag(.TookPointerValue) {
        // not `Suspend = self.wait_for(Jit = fid)` because of cycles, we defer to the backend to deal with finializing it.
        // since the flag is set, create_jit_shim/shallow_jit_func will record when a new pointer is created. 
        return();
    };
    // We might have already compiled the function but didn't know we'd need to remember its address. (ie. #test fn large_struct_ret_return).
    // no address just means haven't get_fn_callable/shallow_jit_func (might never happen if not needed in the comptime module). 
    m := self.comptime_codegen.m;
    id := self.comptime_jit_symbol_id(fid);
    use_symbol(m, id) { s |
        // TODO: make sure im testing the case where is was already jitted so these are not null
        if !s.shim_addr.is_null() && !identical(s.shim_addr, s.jit_addr) {
            self.created_jit_fn_ptr_value(fid, s.shim_addr);
        };
        if !s.jit_addr.is_null() {
            self.created_jit_fn_ptr_value(fid, s.jit_addr);
        };
    };
    func.set_flag(.TookPointerValue);
}

// This avoids needing to linearly scan all the functions when they try to emit_relocatable_constant of a function pointer. 
// You only need to call it if they created a Values of it, so we might need to emit it as a constant later 
// (like for vtables where you construct the value in memory as with comptime pointers and then we emit relocations). 
// TODO: 72/251 times you get here is because you always do it for shims 
//       and you make a shim for each unique type in get_custom_bake_handler overload
//       even tho you know you're going to call it immediately.  -- Dec 22, 2025
fn created_jit_fn_ptr_value(self: *SelfHosted, f: FuncId, ptr: rawptr) void = {
    @debug_assert(!ptr.is_null(), "created_jit_fn_ptr_value cannot be null");
    // TODO: make vbytes from SymbolInfo.size and that can be used for faster backtraces. 
    self.baked.vmem&.insert(ptr, (FuncId = f));
}

fn zone_begin_dyncall(self: *SelfHosted, f_ptr: rawptr) TraceCtx = {
    zone := zone_begin(.CallDynamic);
    @if(ENABLE_TRACY || SPAM_DYN_CALL) {
        if self.get_debug_name(f_ptr) { name |
            @if(SPAM_DYN_CALL) println(name);
            @if(ENABLE_TRACY) ___tracy_emit_zone_name(zone, name);
        };
    };
    zone
}

fn get_debug_name(self: *SelfHosted, f_ptr: rawptr) ?Str = {
        if self.baked.vmem&.get(f_ptr) { it, _, __ |
            @if_let(it) fn FuncId(it) => {
                m := self.comptime_codegen.m;
                name := m.str(self.comptime_jit_symbol_id(it));
                return(Some = name);
            };
        };
    .None
}

wait_threaded_jit_shim :: fn(self: *SelfHosted, fid: FuncId) rawptr = {
    @debug_assert_ne(get_stack_base_for_tls(), self.main_thread_tls);
    it := self.threaded_jit_shim.slots&.items().find(fn(it) => 
        WipJitShim.State.cas(it.state&, .Uninit, .Claimed) == .Uninit)
            || @panic("TODO: wait when self.threaded_jit_shim.slots full");
    
    it.fid = fid;
    it.state = .Waiting;
    fence();
    u32.atomic_inc(self.threaded_jit_shim.kiki&);
    
    tmp := it.state;
    while => tmp != .Done {
        Futex'wait(bit_cast_unchecked(*WipJitShim.State, *u32, it.state&), tmp.raw());
        tmp = it.state;
    };
    addr := it.callable;
    fence();
    it.state = .Uninit;
    addr
};

wake_threaded_jit_shim :: fn(self: *SelfHosted) void = {
    @debug_assert_eq(get_stack_base_for_tls(), self.main_thread_tls);
    @debug_assert_ne(self.threaded_jit_shim.bouba, self.threaded_jit_shim.kiki);
    it := self.threaded_jit_shim.slots&.items().find(fn(it) => 
        WipJitShim.State.cas(it.state&, .Waiting, .Working) == .Waiting)
            || unreachable();
    self.threaded_jit_shim.bouba += 1;
    it.callable = report_called_uncompiled_or_just_fix_the_problem(self, it.fid);
    fence();
    it.state = .Done;
    Futex'wake(bit_cast_unchecked(*WipJitShim.State, *u32, it.state&), MAX_i32);
};

report_threaded_jit_shim :: fn(self: *SelfHosted, fid: FuncId) Never = {
    // @syscall will go through even if my tls is not set up (ie. someone else allocated the stack).
    os := self.comptime_codegen.m.goal.os;
    msg := "called jit-shim from non-main thread (or swapped stacks) without franca tls\nTODO: allow non-franca threads at comptime without #avoid_shim\n";
    sys := self.comptime_prefer_syscalls;
    _ := @syscall(Syscall.write, sys, os, 
        STD_ERR, msg.ptr, msg.len);
    // this check will might fault if not a franca thread and sp&mask happens to be unmapped 
    // but at least we already printed enough info to debug the problem.
        // StringPool/CodeMap aren't thread safe so it's luck if this works. 
        msg := self.pool.get(self.get_function(fid)[].name);  // luck^
        _ := @syscall(Syscall.write, sys, os, 
            STD_ERR, msg.ptr, msg.len);
        _ := @syscall(Syscall.exit, sys, os, 
            1);
    unreachable();
};
