//! - Call a function pointer with a dynamically known type on an array of bytes.
//! - Create a shim around an uncompiled function that redirects into the compiler and then forwards to the real callee. 

DynCallShim :: @tagged(Pending: Qbe.Sym, Ready: rawptr);

fn call_dynamic_values(self: *SelfHosted, f_ptr: i64, f_ty: FnType, args_value: []u8) Values = {
    zone := self.zone_begin_dyncall(f_ptr.rawptr_from_int());
    self.stats.dyn_call += 1;
    callee := self.dyncalls&.get_or_insert(f_ty) {
        // 
        // This is super stupid. we should reuse the shim when the types have the same repr. 
        // ie. currently, if you call some `fn(foo: *T) void` we'd make a new shim for every T, 
        // even though that doesn't change the calling convention.
        // However, we only get to this slow path 282 times (self.fr) which is nothing. 
        // The whole call_dynamic_values is 15/1258 main thread samples, and that is including the callee
        // (so any comptime code other than #macro and #x86_bytes). 
        // -- Mar 12, 2025
        //
        self.create_dyncall_shim(f_ty)
    };
    callee := @match(callee) {
        fn Ready(it)   => it[];
        fn Pending(it) => {
            c := wait_for_symbol(self.comptime_codegen, it[]);
            callee[] = (Ready = c);
            c
        }
    };
    callee := assume_types_fn(Arg = Ty(*u8, *u8, i64), Ret = void, ptr = callee);
   
    // TODO: canary?
    ret_size: i64 = self.get_info(f_ty.ret)[].stride_bytes.zext();
    ret_buf := temp().alloc_zeroed(u8, ret_size);  // TODO: alignment?
    callee(args_value.ptr, ret_buf.ptr, f_ptr);
    zone_end(zone);
    self.from_bytes(ret_buf)
}

Futex :: import("@/lib/sys/sync/futex.fr");

// note: different from get_addr() because this auto-calls make_exec() for you on jit-wasm
// might be a jit-shim. returns null if neither is ready (only reachable in )
fn wait_for_symbol(shared: *FrontendCodegen, id: Qbe.Sym) rawptr = {
    // only used for comptime_codegen (which is single threaded currently) so never need to synchronize.
    @debug_assert(!shared.worker.threaded);
    c := rawptr.zeroed();
    use_symbol(shared.m, id) { symbol | 
        ::enum(@type symbol.kind);
        @if(TODOWASM)
        if shared.m.goal.arch == .wasm32 && symbol.kind == .Local && symbol.segment == .Code {
            shared.m.make_exec();
            @debug_assert(symbol.kind == .DynamicPatched);
        };
        c = symbol.jit_addr;  
    };
    c
}

// Creates a function with the signeture (arg_array: *u8, ret_array: *u8, callee: fnptr) -> void.
// It starts compiling on self.comptime_codegen and you have to get the result out of self.dyncalls when it's ready. 
// TODO: reuse functions when they have different nominal types but the same repr. 
fn create_dyncall_shim(self: *SelfHosted, f_ty: FnType) DynCallShim = {
    shared := self.comptime_codegen;
    ins: RawList(Qbe.Ins) = empty();
    id := Qbe.no_symbol_S; 
    enter_task shared { entry |
        f := temp().box_uninit(Qbe.Fn);
        m := shared.m;
        f.default_init(m);
        f.leaf = false;
        f.lnk.no_inline = true;
        id = shared.m.intern(@tfmt("dyncall%_%_%", f_ty.arg.as_index(), f_ty.ret.as_index(), int(f_ty.unary)));
        f.lnk.id = id;
        entry.task = (Func = f);
        
        // accept parameters from the compiler
        arg_p  := f.newtmp("arg", .Kl);
        ret_p  := f.newtmp("ret", .Kl);
        callee := f.newtmp("call", .Kl);
        env    := f.newtmp("context", .Kl);
        if next_abi_version() < 1513 {
            addins(ins&, make_ins(.pare, .Kl, env, QbeNull, QbeNull));
        };
        addins(ins&, make_ins(.par, .Kl, arg_p, QbeNull, QbeNull));
        addins(ins&, make_ins(.par, .Kl, ret_p, QbeNull, QbeNull));
        addins(ins&, make_ins(.par, .Kl, callee, QbeNull, QbeNull));
        
        // :EmitIrCall
        // arg instructions that read from arg_p
        scratch: List(Qbe.Ins) = list(temp());
        push_arg :: fn(ty: Type, offset: i64) => {
            if self.get_info(ty)[].stride_bytes != 0 {
                ref := f.newtmp("a", .Kl);
                addins(ins&, make_ins(.add, .Kl, ref, arg_p, f.getcon(offset)));
                load, k := self.comp().load_op(ty);
                ::if(Qbe.Ins);
                ins := if k != .Ke {  // scalar
                    r := f.newtmp("a", k);
                    addins(ins&, make_ins(load, k, r, ref, QbeNull));
                    make_ins(.arg, k, QbeNull, r, QbeNull)
                } else {  // aggregate
                    r_type := self.comp().get_aggregate(shared, ty);
                    make_ins(.argc, .Kl, QbeNull, r_type, ref)
                };
                scratch&.push(ins);
            };
        };
        info := self.get_type(f_ty.arg);
        @assert(self.get_info(f_ty.arg)[].is_sized, "comptime call with unsized argument");
        if info.is(.Struct) && info.Struct.is_tuple {
            each info.Struct.fields { it | 
                push_arg(it.ty, it.byte_offset);
            };
        } else {
            push_arg(f_ty.arg, 0);
        };
        if next_abi_version() < 1513 {
            addins(ins&, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        };
        append(ins&, scratch.index_unchecked(0), scratch.index_unchecked(scratch.len));
        scratch&.clear();
        
        // do the call and write the results to ret_p
        _, k0 := self.comp().load_op(f_ty.ret);
        info := self.get_info(f_ty.ret);
        ret_size: i64 = info.stride_bytes.zext();
        @assert(info.is_sized, "comptime call with unsized argument");
        result_r, k, type_r := @if_else{
            @if(ret_size == 0) => (QbeNull, Qbe.Cls.Kw, QbeNull);
            @if(k0 != .Ke) => {  // scalar
                r := f.newtmp("result", k0);
                o := self.comp().store_op(f_ty.ret);
                @debug_assert(o != .Oxxx, "cannot store %", self.log_type(f_ty.ret));
                scratch&.push(make_ins(o, .Kw, QbeNull, r, ret_p));
                (r, k0, QbeNull)
            };
            @else => {
                r := f.newtmp("result", .Kl);
                scratch&.push(make_ins(.blit0, .Kw, QbeNull, r, ret_p));
                scratch&.push(make_ins(.blit1, .Kw, QbeNull, INT(ret_size), QbeNull));
                (r, Qbe.Cls.Kl, self.comp().get_aggregate(shared, f_ty.ret))
            };
        };
        addins(ins&, make_ins(.call, k, result_r, callee, type_r));
        append(ins&, scratch.index_unchecked(0), scratch.index_unchecked(scratch.len));
        scratch&.clear();
        
        f.start = newblk();
        f.nblk = 1;
        f.start.ins = ins;
        f.start.jmp.type = .ret0;
    };
    (Pending = id)
}

// TODO: this generates shitty code. 
//       - i save the registers used only on the slow path at entry of the function. 
//       - it loads from the got twice on the fast path because i don't deduplicate RCon across blocks.
//       - redundant copies of aggregates. 
fn create_jit_shim(self: CompCtx, shared: *FrontendCodegen, fid: FuncId, handler_callee_address: rawptr) void = {
    func := self.get_function(fid);
    
    m := shared.m;
    c := self.data.cast()[][];
    c.when_debug(.Shim, fn(o) => @fmt(o, "S: create %", fmt_func(c, fid)));
    id := Qbe.no_symbol_S;
    enter_task shared { entry |
        id = m.intern(@tfmt("%__shim", fid));
        
        // create a new function
        f := temp().box_uninit(Qbe.Fn);
        f.default_init(m);
        f.leaf = false;
        f.lnk.no_inline = true;
        f.lnk.id = id;
        f.start = newblk(); name_block(f.start, "start"); f.start.id = 0;
        do_the_call := newblk(); name_block(do_the_call, "do_the_call"); do_the_call.id = 1;
        not_ready := newblk(); name_block(not_ready, "not_ready"); not_ready.id = 2;
        
        f.nblk = 3;
        target_id := self.data.cast()[][].comptime_jit_symbol_id(fid);
        entry.task = (Shim = (f = f, shim_for = target_id));
        
        // collect parameters
        env    := f.newtmp("context", .Kl);
        if next_abi_version() < 1513 {
            push(f.start, make_ins(.pare, .Kl, env, QbeNull, QbeNull));
        };
        arg_ty := func.finished_arg.expect("arg infered");
        ret_ty := func.finished_ret.expect("ret infered");
        pars := emit_par_instructions(self, shared, func.arg&, arg_ty, f, .None);
        
        // If the entry in __got for the target symbol is not this shim, call it directly.
        // Otherwise, call into the compiler to ask for it to be compiled. 
        // Most calls will just go through __got directly in which case this check is wasteful,
        // but it helps in the case where you created a function pointer to the shim and stuck it in memory somewhere,
        // now not every single call to that has to go through the compiler and take a mutex to look at the symbol. 
        // It seems this doesn't make each shim only call into the compiler once (TODO: why?) but it does lower the number. 
        // -- Dec 22
        target := f.symcon(target_id);
        myself := f.symcon(id);
        is_ready := f.newtmp("ready", .Kl); 
        push(f.start, make_ins(.cnel, .Kl, is_ready, myself, target));
        // to test if this is helping, toggle this and add logging in report_called_uncompiled_or_just_fix_the_problem. 
        if m.goal.arch == .wasm32 {
            // can't do the fast path comparison on wasm because the indirect table 
            // doesn't yield observable function pointers in the way the GOT does
            is_ready = f.getcon(0);  
        };
        
        f.start.jmp = (arg = is_ready, type = .jnz);
        f.start.s1 = do_the_call;
        f.start.s2 = not_ready;
        
        // call into the compiler to get the real callee
        new_callee := f.newtmp("new_callee", .Kl);
        if next_abi_version() < 1513 {
            push(not_ready, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        };
        push(not_ready, make_ins(.arg,  .Kl, QbeNull, f.getcon(SelfHosted.int_from_ptr(c)), QbeNull));
        push(not_ready, make_ins(.arg,  .Kw, QbeNull, f.getcon(fid.as_index()), QbeNull));
        push(not_ready, make_ins(.call, .Kl, new_callee,  f.getcon(handler_callee_address.int_from_rawptr()), QbeNull));
        not_ready.jmp.type = .jmp;
        not_ready.s1 = do_the_call;
        
        // Rejoin and call something
        callee := f.newtmp("callee_join", .Kl);
        do_the_call.phi = temp().boxed(Qbe.Phi, (
            cls = .Kl, 
            to = callee,
            narg = 2,
            blk = new(2),
            arg = new(2),
            link = Qbe.Phi.ptr_from_int(0),
        ));
        do_the_call.phi.blk[0] = not_ready;
        do_the_call.phi.arg&[0] = new_callee;
        do_the_call.phi.blk[1] = f.start;
        do_the_call.phi.arg&[1] = target;
        
        name_block :: fn(b: *Qbe.Blk, name: Str) = {
            b.name = name;
        };
        
        // convert to arguments
        if next_abi_version() < 1513 {
            push(do_the_call, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        };
        each pars { p |
            @if_else {
                @if(p.scalar) => push(do_the_call, make_ins(.arg,  p.k, QbeNull, p.r, QbeNull));
                @else         => push(do_the_call, make_ins(.argc, p.k, QbeNull, self.get_aggregate(shared, p.type), p.r));
            };
        };
        
        // do the real call to the new callee 
        _, k0 := self.load_op(ret_ty);
        info  := self.get_info(ret_ty);
        @assert(info.is_sized, "comptime call with unsized argument");
        jmp, result_r, k, type_r := @if_else{
            @if(info.stride_bytes == 0) => (Qbe.J.ret0, QbeNull, Qbe.Cls.Kw, QbeNull);
            @if(k0 != .Ke)     => (k0.retk(), f.newtmp("result", k0), k0, QbeNull);
            @else              => (Qbe.J.retc, f.newtmp("result", .Kl), Qbe.Cls.Kl, self.get_aggregate(shared, ret_ty));
        };
        push(do_the_call, make_ins(.call, k, result_r, callee, type_r));
        ::enum(@type jmp);
        f.ret_cls = @if(jmp != .ret0, k0, .Kx);
        
        // return the result to our caller
        f.retty = type_r;
        do_the_call.jmp = (type = jmp, arg = result_r);
        
        f.start.link = do_the_call;
        do_the_call.link = not_ready;
    };
    
    @debug_assert(!shared.worker.threaded);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY {
        // TODOWASM: don't make_exec for this, just assign a got_lookup_offset
        self.data.cast()[][].created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id).int_from_rawptr());
    };
}

fn shallow_jit_func(self: *SelfHosted, fid: FuncId) Res(void) = {
    func := self.get_function(fid);  
    
    if func.body&.is(.DynamicImport) && func.body.DynamicImport.comptime == 0 {
        // It's an import but we don't have a jit pointer for it (== didn't find it in a dylib). 
        // So there's nothing we can do. Just make a jit shim so trying to call it 
        // will safely error with "tried to call uncompiled". 
        // Also, it needs a unique address so it can be baked. 
        return(self.create_jit_shim(fid)); 
    };
    
    for func.mutual_callees& { fid |
        if !self.get_function(fid).get_flag(.AsmDone) {
            @try(self.create_jit_shim(fid)) return; 
        };
    };
    
    // ugh. can't be temp() because threaded=false wants to reset it. 
    pending := FuncId.list(general_allocator()); 
    
    @try(emit_ir(self.comp(), self.comptime_codegen, fid, .Jit, pending&, true)) return;
    
    shared := self.comptime_codegen;
    @debug_assert(!shared.worker.threaded);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY {
        id := self.comptime_jit_symbol_id(fid);
        self.created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id).int_from_rawptr());
    };
    
    func.set_flag(.RealAsmDone);
    func.set_flag(.AsmDone);
    for pending& { callee |
        if !self.get_function(callee).get_flag(.AsmDone) {
            @try(self.create_jit_shim(callee)) return; 
            func.mutual_callees&.push(callee, self.get_alloc());  // TODO: shouldn't need this its just for import_c
        };
    };
    pending&.drop();
    .Ok
}

fn save_in_got(self: *SelfHosted, fid: FuncId, addr: rawptr) void = {
    // :InsaneCaches
    m := self.comptime_codegen.m;
    id := self.comptime_jit_symbol_id(fid);
    use_symbol(m, id) { s |
        @debug_assert(s.got_lookup_offset != -1, "save_in_got missing slot for %", s.name);
        if m.goal.arch == .wasm32 {
            // TODO: might not need this. either way, clean it up!
            (m.target.wasm_jit_event)(Assign = (dest = s.got_lookup_offset, src = addr.int_from_rawptr()));
        } else {
            got := m.segment_address(.ConstantData, s.got_lookup_offset);  // :GotAtConstantDataBase 
            rawptr.ptr_from_raw(got)[] = addr;
        }
    };
}

// TODO: it's a massive problem if a jitted program starts threads and calls this on one of them. :FUCKED
// TODO: it would be nice if i had the arguments here for reporting what call you actually tried to make. 
//       but this way will be even better once i can lazily compile it and just lean on the backend to make the args flow through. 
report_called_uncompiled_or_just_fix_the_problem :: fn(self: *SelfHosted, fid: FuncId) rawptr = {
    fence();
    result := rawptr_from_int(0);
    // You want to allow a shim being called from an extern function so we can't assume we have a valid context here. (ie. examples/farm_game.fr has this problem). 
    ;{
        // HACK this is not ok if the comptime code has multiple threads
        //set_dynamic_context(DefaultContext.raw_from_ptr(self.jit_shim_env));
        @debug_assert(!self.debug_deny_jit_shim_calls);
        self.stats.shim_calls += 1;
        
        m := self.comptime_codegen.m;
        // important: have to be careful about which comptime is in the context if the driver program gets compiled lazily. 
        func := self.get_function(fid);
        
        self.when_debug(.Shim, fn(o) => @fmt(o, "S: call %", fmt_func(self, fid)));
        if func.get_flag(.AvoidJitShim) {
            @panic("Called an #avoid_shim function through a jit_shim");
        };
        if func.get_flag(.RealAsmDone) { 
            if self.get_fn_callable(fid) { addr | 
                // the shim itself checks that the address in GOT matches the shim address before calling into the compiler. 
                // so replacing it with the real address now makes future calls faster (even if they go through the old function pointer). 
                save_in_got(self, fid, addr);
                result = addr;
            };
        };
        
        if result.is_null() {
            mark := mark_temporary_storage();
            @debug_assert(!self.comptime_codegen.worker.no_more_functions);
            // :ugly
            first := true;
            addr := self.poll_in_place(rawptr) { () Maybe(rawptr) |
                r :: local_return;
                if func.get_flag(.RealAsmDone) {
                    // TODO: why can't .expect("must have pointer if done")? -- Dec 15 
                    //       this changed when i stopped compiling in the old backend when NEW_COMPTIME_JIT
                    if self.get_fn_callable(fid) { p |
                        r(Ok = p);
                    };
                };
                if !first {
                    // If its an import we're never gonna make progress by trying to compile it. 
                    if func.get_flag(.BodyIsSpecial) {
                        @panic("Tried to call uncompiled function:\n %", self.get_function(fid).log(self));
                    };  
                };
                first = false;
                func.unset_flag(.AsmDone);
                r(Suspend = self.wait_for(Jit = fid))
            };
            result = self.unwrap_report_error(rawptr, addr);
            save_in_got(self, fid, result);
            reset_temporary_storage(mark);
        };
    };
    result
};

// currently i rely on W+X memory so this function doesn't need to change permissions. 
fn get_fn_callable(self: *SelfHosted, fid: FuncId) ?rawptr = {
    m := self.comptime_codegen.m;
    func := self.get_function(fid);
    id := self.comptime_jit_symbol_id(fid);
    result: ?rawptr = .None;
    if !func.get_flag(.RealAsmDone) && !func.get_flag(.AlreadyHasShim) {
        return(.None);
    };
    c := wait_for_symbol(self.comptime_codegen, id);
    if !c.is_null() {
        if func.get_flag(.TookPointerValue) || ENABLE_TRACY {
            self.created_jit_fn_ptr_value(fid, c.int_from_rawptr());
        };
        result = (Some = c);
    };
    result
}

fn zone_begin_dyncall(self: *SelfHosted, f_ptr: rawptr) TraceCtx = {
    zone := zone_begin(.CallDynamic);
    @if(ENABLE_TRACY) {
        if self.baked.vmem&.get(f_ptr) { it, _ |
            @if_let(it) fn FuncId(it) => {
                m := self.comptime_codegen.m;
                name := m.str(self.comptime_jit_symbol_id(it));
                ___tracy_emit_zone_name(zone, name);
            };
        };
    };
    zone
}
