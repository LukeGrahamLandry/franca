//! - Call a function pointer with a dynamically known type on an array of bytes.
//! - Create a shim around an uncompiled function that redirects into the compiler and then forwards to the real callee. 

TRACE_SHIMS :: false;
TRACE_CALLS :: false; 

DynCallShim :: @tagged(Pending: u32, Ready: rawptr);

fn call_dynamic_values(self: *SelfHosted, f_ptr: i64, f_ty: FnType, args_value: []u8) Values = {
    callee := self.dyncalls&.get_ptr(f_ty&) || {
        // 
        // This is super stupid. we should reuse the shim when the types have the same repr. 
        // ie. currently, if you call some `fn(foo: *T) void` we'd make a new shim for every T, 
        // even though that doesn't change the calling convention.
        // However, we only get to this slow path 282 times (self.fr) which is nothing. 
        // The whole call_dynamic_values is 15/1258 main thread samples, and that is including the callee
        // (so any comptime code other than #macro and #x86_bytes). 
        // -- Mar 12, 2025
        //
        self.create_dyncall_shim(f_ty);
        self.dyncalls&.get_ptr(f_ty&).unwrap()
    };
    callee := @match(callee) {
        fn Ready(it)   => it[];
        fn Pending(it) => {
            c := wait_for_symbol(self.comptime_codegen, it[]);
            callee[] = (Ready = c);
            c
        }
    };
    callee := assume_types_fn(Arg = Ty(*u8, *u8, i64), Ret = void, ptr = callee);
    
    @if(TRACE_CALLS) {
        self.call_depth += 1;
        @println("(%) -----", self.call_depth);
        @print("(%) Calling ", self.call_depth);
        if self.baked.functions&.get(f_ptr) { fid |
            @print("% ", self.log_name(fid));
        };
        @println("%: % -> %", f_ptr, self.log_type(f_ty.arg), self.log_type(f_ty.ret));
    };
    // TODO: canary?
    ret_size: i64 = self.get_info(f_ty.ret)[].stride_bytes.zext();
    ret_buf := temp().alloc_zeroed(u8, ret_size);  // TODO: alignment?
    @if(TRACE_CALLS) {
        @print("(%) input: ", self.call_depth);
        input := self.from_bytes(args_value);
        println(log(input&, self, f_ty.arg));
    };
    zone := zone_begin(.CallDynamic);
    callee(args_value.ptr, ret_buf.ptr, f_ptr);
    zone_end(zone);
    result := self.from_bytes(ret_buf);
    @if(TRACE_CALLS) {
        @print("(%) result: ", self.call_depth);
        println(log(result&, self, f_ty.ret));
        @println("(%) -----", self.call_depth);
        self.call_depth -= 1;
    }; 
    result
}

fn wait_for_symbol(shared: *CodegenShared, id: u32) rawptr = {
    symbol_is_ready :: fn(shared: *CodegenShared, s: u32) bool #inline = {
        r := false;
        use_symbol(shared.m, s) { s |
            r = s.size != 0 || (s.kind != .Local && !s.jit_addr.is_null());
        };
        r
    };
    
    @if(shared.threaded)
    if !symbol_is_ready(shared, id) {
        zone := zone_begin(.Wait);
        
        u32.atomic_set(shared.waiting_for_symbol&, 1);
        prev := shared.waiting_for_bouba;
        while => prev != codegen_queue_size && !symbol_is_ready(shared, id) {
            Futex'wait(shared.waiting_for_bouba&, prev);
            prev = shared.waiting_for_bouba;
        };
        u32.atomic_set(shared.waiting_for_symbol&, 0);
    
        zone_end(zone);
        lock(shared.m.icache_mutex&);
    };
    
    c := rawptr.zeroed();
    use_symbol(shared.m, id) { symbol | 
        c = symbol.jit_addr;
    };
    @if(shared.threaded) {
        shared.m.bump_dirty_new();
        unlock(shared.m.icache_mutex&);
    };
    c
}

// Creates a function with the signeture (arg_array: *u8, ret_array: *u8, callee: fnptr) -> void.
// It starts compiling on self.comptime_codegen and you have to get the result out of self.dyncalls when it's ready. 
// TODO: reuse functions when they have different nominal types but the same repr. 
fn create_dyncall_shim(self: *SelfHosted, f_ty: FnType) void = {
    shared := self.comptime_codegen;
    ins: QList(Qbe.Ins) = new(0);
    nins := 0;
    @if(TRACE_SHIMS) @println("create dyncall % -> %", self.log_type(f_ty.arg), self.log_type(f_ty.ret));
    entry := bouba_acquire(shared, shared.bouba_idx&);
    id: u32 = 0; 
    push_dynamic_context {
        context(DefaultContext)[].temporary_allocator = entry.arena&;
        f := temp().box(Qbe.Fn);
        m := shared.m;
        f.default_init(m);
        f.leaf = false;
        f.lnk.no_inline = true;
        id = shared.m.intern(@tfmt("dyncall%_%_%", f_ty.arg.as_index(), f_ty.ret.as_index(), f_ty.arity));
        f.lnk.id = id;
        entry.task = (Func = f);
        
        // accept parameters from the compiler
        arg_p  := f.newtmp("arg", .Kl);
        ret_p  := f.newtmp("ret", .Kl);
        callee := f.newtmp("call", .Kl);
        env    := f.newtmp("context", .Kl);
        addins(ins&, nins&, make_ins(.pare, .Kl, env, QbeNull, QbeNull));
        addins(ins&, nins&, make_ins(.par, .Kl, arg_p, QbeNull, QbeNull));
        addins(ins&, nins&, make_ins(.par, .Kl, ret_p, QbeNull, QbeNull));
        addins(ins&, nins&, make_ins(.par, .Kl, callee, QbeNull, QbeNull));
        
        // :EmitIrCall
        // arg instructions that read from arg_p
        scratch: List(Qbe.Ins) = list(temp());
        push_arg :: fn(ty: Type, offset: i64) => {
            if self.get_info(ty)[].stride_bytes != 0 {
                ref := f.newtmp("a", .Kl);
                addins(ins&, nins&, make_ins(.add, .Kl, ref, arg_p, f.getcon(offset)));
                load, k := self.comp().load_op(ty);
                ::if(Qbe.Ins);
                ins := if k != .Ke {  // scalar
                    r := f.newtmp("a", k);
                    addins(ins&, nins&, make_ins(load, k, r, ref, QbeNull));
                    make_ins(.arg, k, QbeNull, r, QbeNull)
                } else {  // aggregate
                    r_type := self.comp().get_aggregate(shared, ty);
                    make_ins(.argc, .Kl, QbeNull, r_type, ref)
                };
                scratch&.push(ins);
            };
        };
        info := self.get_type(f_ty.arg);
        @assert(self.get_info(f_ty.arg)[].is_sized, "comptime call with unsized argument");
        if info.is(.Struct) && info.Struct.is_tuple {
            each info.Struct.fields { it | 
                push_arg(it.ty, it.byte_offset);
            };
        } else {
            push_arg(f_ty.arg, 0);
        };
        addins(ins&, nins&, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        append(ins&, nins&, scratch.index_unchecked(0), scratch.index_unchecked(scratch.len));
        scratch&.clear();
        
        // do the call and write the results to ret_p
        _, k0 := self.comp().load_op(f_ty.ret);
        info := self.get_info(f_ty.ret);
        ret_size: i64 = info.stride_bytes.zext();
        @assert(info.is_sized, "comptime call with unsized argument");
        result_r, k, type_r := @if_else{
            @if(ret_size == 0) => (QbeNull, Qbe.Cls.Kw, QbeNull);
            @if(k0 != .Ke) => {  // scalar
                r := f.newtmp("result", k0);
                o := self.comp().store_op(f_ty.ret);
                @debug_assert(o != .Oxxx, "cannot store %", self.log_type(f_ty.ret));
                scratch&.push(make_ins(o, .Kw, QbeNull, r, ret_p));
                (r, k0, QbeNull)
            };
            @else => {
                r := f.newtmp("result", .Kl);
                scratch&.push(make_ins(.blit0, .Kw, QbeNull, r, ret_p));
                scratch&.push(make_ins(.blit1, .Kw, QbeNull, INT(ret_size), QbeNull));
                (r, Qbe.Cls.Kl, self.comp().get_aggregate(shared, f_ty.ret))
            };
        };
        addins(ins&, nins&, make_ins(.call, k, result_r, callee, type_r));
        append(ins&, nins&, scratch.index_unchecked(0), scratch.index_unchecked(scratch.len));
        scratch&.clear();
        
        f.start = newblk();
        f.nblk = 1;
        f.start.ins = ins;
        f.start.nins = nins.trunc();
        f.start.jmp.type = .ret0;
    };
    enqueue_task(shared, entry);
    self.dyncalls&.insert(f_ty, (Pending = id));
}

// TODO: this generates shitty code. 
//       - i save the registers used only on the slow path at entry of the function. 
//       - it loads from the got twice on the fast path because i don't deduplicate RCon across blocks.
//       - redundant copies of aggregates. 
fn create_jit_shim(self: CompCtx, shared: *CodegenShared, fid: FuncId, handler_callee_address: rawptr) void = {
    // TODO: replace accesses to bc.jitted.dispatch_table[fid.as_index()]?
    func := self.get_function(fid);
    @if(TRACE_SHIMS) @println("create shim %: %", fid, self.get_string(func.name));
    m := shared.m;
    entry := bouba_acquire(shared, shared.bouba_idx&);
    c := self.data.cast()[][];
    id: u32 = 0;
    push_dynamic_context {
        context(DefaultContext)[].temporary_allocator = entry.arena&;
        
        id = m.intern(@tfmt("%__shim", fid));
        
        // create a new function
        f := temp().box(Qbe.Fn);
        f.default_init(m);
        f.leaf = false;
        f.lnk.no_inline = true;
        f.lnk.id = id;
        f.start = newblk(); f.start.ins = new(0); name_block(f.start, "start"); f.start.id = 0;
        do_the_call := newblk(); do_the_call.ins = new(0); name_block(do_the_call, "do_the_call"); do_the_call.id = 1;
        not_ready := newblk(); not_ready.ins = new(0); name_block(not_ready, "not_ready"); not_ready.id = 2;
        
        f.nblk = 3;
        target_id := self.data.cast()[][].comptime_jit_symbol_id(fid);
        entry.task = (Shim = (f = f, shim_for = target_id));
        
        // collect parameters
        env    := f.newtmp("context", .Kl);
        push(f.start, make_ins(.pare, .Kl, env, QbeNull, QbeNull));
        arg_ty := func.finished_arg.expect("arg infered");
        ret_ty := func.finished_ret.expect("ret infered");
        pars := emit_par_instructions(self, shared, func.arg&, arg_ty, f);
        
        // If the entry in __got for the target symbol is not this shim, call it directly.
        // Otherwise, call into the compiler to ask for it to be compiled. 
        // Most calls will just go through __got directly in which case this check is wasteful,
        // but it helps in the case where you created a function pointer to the shim and stuck it in memory somewhere,
        // now not every single call to that has to go through the compiler and take a mutex to look at the symbol. 
        // It seems this doesn't make each shim only call into the compiler once (TODO: why?) but it does lower the number. 
        // -- Dec 22
        target := f.symcon(target_id);
        myself := f.symcon(id);
        is_ready := f.newtmp("ready", .Kl); 
        push(f.start, make_ins(.cnel, .Kl, is_ready, myself, target));
        //is_ready := f.getcon(0);  // to test if this is helping, toggle this and add logging in report_called_uncompiled_or_just_fix_the_problem. 
        f.start.jmp = (arg = is_ready, type = .jnz);
        f.start.s1 = do_the_call;
        f.start.s2 = not_ready;
        
        // call into the compiler to get the real callee
        new_callee := f.newtmp("new_callee", .Kl);
        push(not_ready, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        push(not_ready, make_ins(.arg,  .Kl, QbeNull, f.getcon(SelfHosted.int_from_ptr(c)), QbeNull));
        push(not_ready, make_ins(.arg,  .Kw, QbeNull, f.getcon(fid.as_index()), QbeNull));
        push(not_ready, make_ins(.call, .Kl, new_callee,  f.getcon(handler_callee_address.int_from_rawptr()), QbeNull));
        not_ready.jmp.type = .jmp;
        not_ready.s1 = do_the_call;
        
        // Rejoin and call something
        callee := f.newtmp("callee_join", .Kl);
        do_the_call.phi = temp().box(Qbe.Phi);
        do_the_call.phi[] = (
            cls = .Kl, 
            to = callee,
            narg = 2,
            blk = new(2),
            arg = new(2),
            link = Qbe.Phi.ptr_from_int(0),
        );
        do_the_call.phi.blk[0] = not_ready;
        do_the_call.phi.arg&[0] = new_callee;
        do_the_call.phi.blk[1] = f.start;
        do_the_call.phi.arg&[1] = target;
        
        name_block :: fn(b: *Qbe.Blk, name: Str) = {
            b.name = name;
        };
        
        // convert to arguments
        push(do_the_call, make_ins(.arge, .Kl, QbeNull, env, QbeNull));
        each pars { p |
            @if_else {
                @if(p.void)   => ();
                @if(p.scalar) => push(do_the_call, make_ins(.arg,  p.k, QbeNull, p.r, QbeNull));
                @else         => push(do_the_call, make_ins(.argc, p.k, QbeNull, self.get_aggregate(shared, p.type), p.r));
            };
        };
        
        // do the real call to the new callee 
        _, k0 := self.load_op(ret_ty);
        info  := self.get_info(ret_ty);
        @assert(info.is_sized, "comptime call with unsized argument");
        jmp, result_r, k, type_r := @if_else{
            @if(info.stride_bytes == 0) => (Qbe.J.ret0, QbeNull, Qbe.Cls.Kw, QbeNull);
            @if(k0 != .Ke)     => (@as(Qbe.J) @as(i32) Qbe.J.retw.raw() + k0.raw(), f.newtmp("result", k0), k0, QbeNull);
            @else              => (Qbe.J.retc, f.newtmp("result", .Kl), Qbe.Cls.Kl, self.get_aggregate(shared, ret_ty));
        };
        push(do_the_call, make_ins(.call, k, result_r, callee, type_r));
        
        // return the result to our caller
        if type_r != QbeNull {  
            f.retty = type_r.val().intcast();
        };
        do_the_call.jmp = (type = jmp, arg = result_r);
        
        f.start.link = do_the_call;
        do_the_call.link = not_ready;
    };
    enqueue_task(shared, entry);
    
    // in sema, we never push to pending_took_pointer when not threaded. 
    // so we need to track the shims here instead of letting get_fn_callable deal with it
    if !shared.threaded && func.get_flag(.TookPointerValue) {
        self.data.cast()[][].created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id).int_from_rawptr());
    };
}

fn shallow_jit_func(self: *SelfHosted, fid: FuncId) void = {
    pending := FuncId.list(temp());
    @if(TRACE_SHIMS) @println("shallow jit F%: %", fid.as_index(), self.log_name(fid));
    func := self.get_function(fid);  
    if func.body&.is(.DynamicImport) {
        self.create_jit_shim(fid); 
        return();
    };
    //for func.callees& { fid |
    //    if !self.get_function(fid).get_flag(.AsmDone) {
    //        self.create_jit_shim(fid); 
    //    };
    //};
    // TODO: shouldn't need this? but do for trivial_data_reloc
    for func.mutual_callees& { fid |
        if !self.get_function(fid).get_flag(.AsmDone) {
            self.create_jit_shim(fid); 
        };
    };
    
    _ := self.unwrap_report_error(void, emit_ir(self.comp(), self.comptime_codegen, fid, .Jit, self.comptime_codegen.bouba_idx&, pending&, true));
    
    shared := self.comptime_codegen;
    if !shared.threaded && func.get_flag(.TookPointerValue) {
        id := self.comptime_jit_symbol_id(fid);
        self.created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id).int_from_rawptr());
    };
    
    func.set_flag(.RealAsmDone);
    func.set_flag(.AsmDone);
    for pending& { callee |
        if !self.get_function(callee).get_flag(.AsmDone) {
            self.create_jit_shim(callee); 
            func.mutual_callees&.push(callee, self.get_alloc());  // TODO: shouldn't need this its just for import_c
        };
    };
    // TODO: this isn't what you want because it bleeds into the aot stuff.
    //       and i think you only need it for TargetOsSplit, 
    //       so now it's going to think it needs all of those??
    //       if you don't do this you can't run the qbe_frontend itself jitted because it chokes on mmap_anon
    if func.get_flag(.TargetSplit) {
        for pending& { callee |
            func.mutual_callees&.add_unique(callee, self.get_alloc());
        };
    };
    //func.generation = self.comptime_codegen.generation_pending;
}

fn bump_dirty_new(self: *SelfHosted) void = 
    bump_dirty_new(self.comptime_codegen.m);

fn bump_dirty_new(m: *QbeModule) void = {
    lock(m.icache_mutex&);
    
    start := m.start_of_writable_code;
    end   := m.last_function_end;
    if ptr_diff(start, end) > 0 {
        clear_instruction_cache(u8.raw_from_ptr(start), ptr_diff(start, end));
        m.start_of_writable_code = end;
    };
    start := m.stubs.mmapped.ptr; // m.start_of_executable_stubs;  // TODO
    end := m.stubs.next;
    if ptr_diff(start, end) > 0 {
        // TODO: this costs 20ms. would putting a bti there to let the signal handler know it's ok to let through be worth it?
        clear_instruction_cache(u8.raw_from_ptr(start), ptr_diff(start, end));
        m.start_of_executable_stubs = end;
    };
    unlock(m.icache_mutex&);
}

fn save_in_got(self: *SelfHosted, fid: FuncId, addr: rawptr) void = {
    // :InsaneCaches
    m := self.comptime_codegen.m;
    id := self.comptime_jit_symbol_id(fid);
    use_symbol(m, id) { s |
        @debug_assert(s.got_lookup_offset != -1, "save_in_got missing slot for %", s.name);
        got := m.segment_address(.ConstantData, s.got_lookup_offset);  // :GotAtConstantDataBase 
        rawptr.ptr_from_raw(got)[] = addr;
    };
}

// TODO: it's a massive problem if a jitted program starts threads and calls this on one of them. :FUCKED
// TODO: it would be nice if i had the arguments here for reporting what call you actually tried to make. 
//       but this way will be even better once i can lazily compile it and just lean on the backend to make the args flow through. 
report_called_uncompiled_or_just_fix_the_problem :: @as(rawptr) fn(self: *SelfHosted, fid: FuncId) rawptr = {
    fence();
    result := rawptr_from_int(0);
    // You want to allow a shim being called from an extern function so we can't assume we have a valid context here. (ie. examples/farm_game.fr has this problem). 
    push_zeroed_dynamic_context {
        c := context(DefaultContext);
        c.comptime = SelfHosted.raw_from_ptr(self);
        c.temporary_allocator = self.shim_callback_alloc&;
        c.general_allocator = page_allocator; // TODO
        c.panic_hook = fn(msg) = {
            println("[while handling a jit shim callback]");
            Crash'backtracing_panic_handler(msg);
        };
        c.thread_index = 1; // HACK this is not ok if the comptime code has multiple threads
        
        m := self.comptime_codegen.m;
        // important: have to be careful about which comptime is in the context if the driver program gets compiled lazily. 
        func := self.get_function(fid);
        is_target_split := false;
        //@println("shim! % % %", SelfHosted.raw_from_ptr(self), context(DefaultContext)[].comptime, self.pool.get(func.name));
        if func.get_flag(.RealAsmDone) { 
            @if(TRACE_SHIMS) @println("- ready");
            // TODO: this probably needs to be inside the push_context because it might hit an inner poll_in_place,
            //       but that's sad because most of the time its just an array lookup.  -- Nov 25
            if self.get_fn_callable(fid) { addr | 
                save_in_got(self, fid, addr);
                result = addr;
            };
            @if(TRACE_SHIMS && result.is_null()) @println("- wrong!");
        } else {
            // TODO: maybe better to put smuggle_libc here too
            if func.name == Flag.___this_function_is_not_real_im_just_testing_the_franca_compiler.ident() {
                result = (fn(a: i64, b: i64, c: i64) i64 = {
                    //@println("% + % + % = %", a, b, c, a + b + c);
                    a + b + c
                })
            };
        };
        
        if result.is_null() {
            mark := mark_temporary_storage();
            @if(TRACE_SHIMS) @println("- uncompiled");
            //@println("uncompiled shim for % %", fid, self.log_name(fid));
            
            @debug_assert(!self.comptime_codegen.no_more_functions, "no_more_functions is set so there is no worker waiting for us!");
            // :ugly
            first := true;
            addr := self.poll_in_place(rawptr) { () Maybe(rawptr) |
                r :: local_return;
                if func.get_flag(.RealAsmDone) {
                    // TODO: why can't .expect("must have pointer if done")? -- Dec 15 
                    //       this changed when i stopped compiling in the old backend when NEW_COMPTIME_JIT
                    if self.get_fn_callable(fid) { p |
                        r(Ok = p);
                    };
                };
                if !first {
                    // If its an import we're never gonna make progress by trying to compile it. 
                    if func.get_flag(.BodyIsSpecial) {
                        @panic("Tried to call uncompiled function:\n %", self.get_function(fid).log(self));
                    };  
                };
                first = false;
                func.unset_flag(.AsmDone);
                r(Suspend = self.wait_for(Jit = fid))
            };
            result = self.unwrap_report_error(rawptr, addr);
            //@println("made shim for % % %", fid, self.log_name(fid), result);
            save_in_got(self, fid, result);
            reset_temporary_storage(mark);
        };
        self.comptime_codegen.m.bump_dirty_new();  // SLOW
    };
    result
};

// If you're about to actually call the pointer, we need to mark the memory as executable.
fn get_fn_callable(self: *SelfHosted, fid: FuncId) ?rawptr = {
    m := self.comptime_codegen.m;
    func := self.get_function(fid);
    id := self.comptime_jit_symbol_id(fid);
    result: ?rawptr = .None;
    if !func.get_flag(.RealAsmDone) && !func.get_flag(.AlreadyHasShim) {
        return(.None);
    };
    self.update_pending_took_pointer();
    c := wait_for_symbol(self.comptime_codegen, id);
    if !c.is_null() {
        if func.get_flag(.TookPointerValue) || ENABLE_TRACY || TRACE_CALLS {
            self.created_jit_fn_ptr_value(fid, c.int_from_rawptr());
        };
        result = (Some = c);
    };
    result
}
