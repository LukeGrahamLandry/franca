// TODO: less stupid code for accessing constant values!
// TODO: always check for zero sized type instead of == void

EmitIr :: @struct(
    f: *Qbe.Fn,
    program: CompCtx,
    last_loc: Span,
    var_lookup: HashMap(Var, Qbe.Ref),  // TODO: just var id
    inlined_return_addr: HashMap(LabelId, ReturnTarget),
    blk: *Qbe.Blk, 
    debug: bool,
    context := QbeNull,
    dyn_context: bool,
    func: FuncId,
    blocks: List(*Qbe.Blk),
    when: ExecStyle,
    m: *QbeModule,
    entry: *CodegenEntry,
    next_block_name := 0,
    last_line := -1,
    new_constants: List(BakedVarId),
    shared: *CodegenShared,
    pending: *List(FuncId), // TODO: you wouldn't need this if sema tracked callees correctly. 
    idx: *i64,
);

ReturnTarget :: @struct(
    blk: *Qbe.Blk,
    p: Placement,
    used: bool,
);

Placement :: @tagged(Assign: Qbe.Ref, Scalar, Blit: Qbe.Ref, NewMemory, Discard);

fn emit_ir(comp: CompCtx, shared: *CodegenShared, f: FuncId, when: ExecStyle, idx: *i64, pending: *List(FuncId)) Res(void) = {
    if when == .Aot {
        // :bake_relocatable_value
        {comp.vtable.check_for_new_aot_bake_overloads}(comp.data); 
    };
    
    //@if(f.as_index() == 56053, panic("foo")); // TODO: this fails safety check here. 
    entry := bouba_acquire(shared, idx);
    m := shared.m;
    result: Res(void) = .Ok;
    push_dynamic_context {
        context(DefaultContext)[].temporary_allocator = entry.arena&;
        ::tagged(Placement);
        func := comp.get_function(f); 
        opts := comp.get_build_options();
        self: EmitIr = (
            f = temp().box_zeroed(Qbe.Fn),
            program = comp,
            func = f,
            last_loc = func.loc,
            var_lookup = init(temp()),
            inlined_return_addr = init(temp()),
            blk = zeroed(*Qbe.Blk),
            blocks = list(temp()),
            debug = opts.debug_info || func.get_flag(.LogIr),
            dyn_context = if(when == .Aot, => opts.implicit_context_runtime, => opts.implicit_context_comptime),
            when = when,
            m = m,
            entry = entry,
            new_constants = list(temp()),
            shared = shared,
            pending = pending,
            idx = idx,
        );
        @debug_assert(func.get_flag(.EnsuredCompiled), "fn not compiled?");
        self.f.default_init(m);
        self.f.leaf = true;
        self.f.lnk.id = m.intern(self&.fmt_fn_name(f));  // TODO: set export
        entry.task = (Func = self.f);
        
        res := self&.emit_body(f);
        entry := self.entry; // HACK because of how im doing imports below. 
        
        if res&.is_err() {
            res.Err.update_main_span(self.last_loc);
            result = (Err = res.Err)
        } else {
            has_context := func.get_flag(.YesContext) && !func.get_flag(.NoContext);  // :copy-pastrae
            if !has_context {
                func.set_flag(.NoContext);
            };
            
            // This is a bit painful, you want consistant order and to be working on the function on the other thread while we're setting up the constants. 
            // you want to do new_constants on this thread so the other one never has to touch the CompCtx. 
            entry2 := zeroed(*CodegenEntry);
            if self.new_constants.len != 0 {
                entry2 = bouba_acquire(shared, idx);
                self.new_constants = self.new_constants.items().clone(entry2.arena&.borrow());
            };
            enqueue_task(shared, entry);
            if self.new_constants.len != 0 {
                push_dynamic_context {
                    context(DefaultContext)[].temporary_allocator = entry2.arena&;
                    out := Qbe.Dat.list(self.new_constants.len * 4, temp());
                    for self.new_constants { id |
                        emit_constant(comp, m, id, out&);
                    };
                    entry2.task = (AotVar = out.items());
                };
                enqueue_task(shared, entry2);
            };
        };
    };
    result
}

// TODO: this is going to change the calling convention for small strange aggragates maybe (to match c). 
//       hopefully i don't use any of those for something important. 

// TODO: if you have an unclosed string it randomly decides later top level vars are undeclared? -- Jul 7 
// note: mirrors compile_for_arg
fn bind_args(self: *EmitIr, arguments: *Pattern) Res(void) #once = {
    ArgInfo :: @struct(r: Qbe.Ref, type: Type, name: ?Var, scalar: bool);
    args := ArgInfo.list(arguments.bindings.len, temp());
    
    func := self.program.get_function(self.func);
    arg_ty := func.finished_arg.unwrap();
    
    // Need to emit all the `arg` instructions at the beginning of the start block before doing any allocas. 
    each arguments.bindings { binding |
        continue :: local_return;
        @debug_assert(binding.kind != .Const, "tried to emit function with $const parameter");
        ty := binding.unwrap_ty();
        info := self.program.get_info(ty);
        name := binding.var();
        if info.stride_bytes == 0 {  // :NoRuntimeRepr
            if name { name | 
                self.var_lookup&.insert(name, QbeConZero);
            };
            continue();
        };
        
        _, kk := self.load_op(ty);
        scalar := kk != .Ke;
        
        op, k, a0 := @if(scalar, 
            (Qbe.O.par, kk, QbeNull),
            (Qbe.O.parc, Qbe.Cls.Kl, self.get_aggregate(ty))
        );
        r := self.f.newtmp("a", k);
        self.emit(op, k, r, a0, QbeNull);
        args&.push(r = r, type = ty, name = name, scalar = scalar);
    };
    
    // only arg instructions and allocas in the start block. 
    self.blk = self.new_block("body");
    
    each args { a | 
        mem := if(!a.scalar, => a.r) {
            // TODO: use mutable variables if they don't take the address of the argument. 
            mem := self.alloca(a.type);
            self.emit(self.store_op(a.type), .Kw, QbeNull, a.r, mem);
            mem
        };
        if a.name { name | 
            self.declare_var(name, mem, a.type);
        };
    };
    .Ok
}

// note: mirrors bind_args
fn compile_for_arg(self: *EmitIr, arg: *FatExpr) Res([]Qbe.Ins) = {
    parts := arg.items();
    out := Qbe.Ins.list(parts.len, temp());
    if self.program.get_info(arg.ty)[].stride_bytes == 0 {  // :NoRuntimeRepr
        _ := @try(self.compile_expr(arg, .Discard)) return;
        return(Ok = empty());
    };
    info := self.program.get_type(arg.ty);
    f := info.Struct.fields;
    if info.is(.Struct) && info.Struct.is_tuple {
        @err_assert(f.len == parts.len, "ICE: compile_for_arg arity mismatch") return;
    } else {
        @err_assert(parts.len == 1, "ICE: compile_for_arg arity mismatch!") return;
    };
    
    enumerate parts { i, val |
        ty := val.ty; // TODO: is this ok or do you have to get it from the function's arg type?
        _, k := self.load_op(ty);
        ::if(Qbe.Ins);
        ins := if k != .Ke {  // scalar
            r := @try(self.compile_expr(val, .Scalar)) return;
            make_ins(.arg, k, QbeNull, r, QbeNull)
        } else {  // aggregate
            // TODO: you really want to do `fn Deref(ref) => @try(self.compile_expr(ref, .Scalar)) return;`
            //       to skip an extra blit that the abi stuff will do for you. 
            //       but we don't know if a later argument expression will modify that memory. 
            r      := @try(self.compile_expr(val, .NewMemory)) return;
            r_type := self.get_aggregate(ty);
            make_ins(.argc, .Kl, QbeNull, r_type, r)
        };
        out&.push(ins);
    };
    (Ok = out.items())
}

fn context_par_e(self: *EmitIr) void = {
    if self.dyn_context {
        // We might remove this instruction at the end if we decide this function can be #no_context. 
        self.context = self.f.newtmp("context", .Kl);
        self.emit(.pare, .Kl, self.context, QbeNull, QbeNull);
    } else {
        self.context = QbeNull;
    };
}

fn finalize_blocks(self: *EmitIr) void = {
    self.f.nblk = self.blocks.len.trunc();
    prev := Qbe.Blk.ptr_from_int(0);
    ok := true;
    for_rev self.blocks& { b | 
        b.link = prev;
        prev = b;
        ok = ok && b.jmp.type != .Jxxx;
    };
    if !ok {
        printfn(self.f, self.f.globals.debug_out);
    };
    @debug_assert(ok, "ICE: unterminated block");
}

fn emit_body(self: *EmitIr, f: FuncId) Res(void) #once = {
    func := self.program.get_function(f);
    
    self.f.start = self.new_block("start");
    self.blk = self.f.start;
    
    T :: FuncImpl.Tag();
    prio_amd :: @const_slice(T.Intrinsic, T.Normal, T.ComptimeAddr, T.TargetOsSplit, T.X86AsmBytes, T.Redirect);
    prio_arm :: @const_slice(T.Intrinsic, T.Normal, T.ComptimeAddr, T.TargetOsSplit, T.JittedAarch64, T.Redirect);
    impl := or choose_impl(func.body&, @if(self.m.goal.arch == .aarch64, prio_arm, prio_amd)) {
        return(@err("no acceptable impl"))
    };
    
    body: *FatExpr = @match(impl) {
        fn Normal(body) => body;
        fn Intrinsic(op) => {
            // Direct calls will be inlined but someone might be trying to call through a function pointer. 
            // TODO: don't add to callees for direct calls. -- Jul 24
            
            if op.bc == .GetContext {
                self.context_par_e();
            };
            arg := func.arg.bindings.items();
            @debug_assert(arg.len == 1 || arg.len == 2);
            
            f_ty := func.finished_ty().unwrap();
            arg_ty := self.program.arg_types(f_ty.arg);
            
            _, k0 := self.load_op(arg_ty[0]);
            a0 := self.f.newtmp("a", k0);
            self.emit(.par, k0, a0, QbeNull, QbeNull);
            a1 := QbeNull;
            if arg.len == 2 {
                _, k1 := self.load_op(arg_ty[1]);
                a1 = self.f.newtmp("a", k1);
                self.emit(.par, k1, a1, QbeNull, QbeNull);
            };
            
            self.f.retty = -1;
            _, kr := self.load_op(f_ty.ret);
            self.blk.jmp.arg = @try(self.emit_intrinsic(op, .Scalar, @slice(a0, a1), func, f_ty.ret)) return;
            self.blk.jmp.type = ret_op(kr);
            self.finalize_blocks();
            return(.Ok)
        }
        fn ComptimeAddr(it) => {
            if self.when == .Jit {
                self.entry.task = (JitImport = (lnk = self.f.lnk&, addr = rawptr_from_int(it[])));
                return(.Ok)
            };
            
            // :DumbNameAlias
            // TODO: use FnFlag.NoMangle instead but that doesn't work with the old backends calling the same 
            //       fmt_fn_name until they also don't try to make bounce functions.
            
            real_name := self.program.get_string(func.name);
            real_name := self.m.intern(real_name);
            self.entry.task = (Bounce = (lnk = self.f.lnk&, target = real_name));
            enqueue_task(self.shared, self.entry);
            
            e := bouba_acquire(self.shared, self.idx);
            lnk := e.arena&.borrow().box_zeroed(Qbe.Lnk);
            lnk.id = real_name;
            e.task = (JitImport = (lnk = lnk, addr = rawptr_from_int(0))); // AOT
            self.entry = e;
            // no enqueue_task here, caller does it
            
            return(.Ok)
        }
        fn TargetOsSplit(_) => {
            @debug_assert(func.body&.is(.Merged), "ICE: #target_os split expected multiple");
            each func.body.Merged& { it |
                if it.is(.TargetOsSplit) && it.TargetOsSplit.os == self.m.goal.os {
                    fid := it.TargetOsSplit.fid;
                    self.entry.task = (Bounce = (lnk = self.f.lnk&, target = self.m.intern(self.fmt_fn_name(fid))));
                    self.pending.push(fid);
                    return(.Ok)
                };
            };
            return(@err("#target_os missing implementation"))
        }
        fn Redirect(fid) => {
            self.entry.task = (Bounce = (lnk = self.f.lnk&, target = self.m.intern(self.fmt_fn_name(fid[]))));
            self.pending.push(fid[]);
            return(.Ok)
        } 
        // TODO: use :ExprLevelAsm so you don't have to spill everything, 
        //       but that requires letting people annotate register usage.  -- Dec 3
        fn JittedAarch64(code) => { // TODO: just store this as bytes
            bytes := @as([]u8) (ptr = ptr_cast_unchecked(u32, u8, code.ptr), len = code.len * 4);
            self.entry.task = (Asm = (lnk = self.f.lnk&, bytes = bytes));
            return(.Ok)
        }
        fn X86AsmBytes(bytes) => {
            self.entry.task = (Asm = (lnk = self.f.lnk&, bytes = bytes.items()));
            return(.Ok)
        }
        @default => panic("TODO: unhandled FuncImpl");
    };
    
    self.entry.logging = self.program.get_log_types(f);
    self.context_par_e();
    // note: this rewrites self.blk
    @try(self.bind_args(func.arg&)) return;
    
    // Note: this is different from the body expr type because of early returns.
    ret := self.program.get_function(f)[].finished_ret.unwrap();
    _, k := self.load_op(ret); 
    scalar := k != .Ke;
    
    self.f.start.jmp.type = .jmp;
    self.f.start.s1 = self.blk;
    p: Placement = @if(scalar, .Scalar, .NewMemory);
    result := @try(self.compile_expr(body, p)) return;
    
    p: Placement = @if(scalar, .Scalar, .NewMemory);
    if self.program.get_info(ret)[].stride_bytes == 0 {  // :NoRuntimeRepr
        p = .Discard;
    };

    self.blk.jmp.arg = result;
    @if_else {
        @if(self.blk.jmp.type != .Jxxx) => {
            // TODO: assert ret=Never?
        };
        @if(ret.is_never()) => {
            self.blk.jmp.type = .hlt;
        };
        @if(p&.is(.Discard)) => {
            self.blk.jmp.type = .ret0;
        };
        @if(scalar) => {
            self.f.retty = -1;
            // TODO: tell it about b/h so we can fully follow the c abi :FUCKED but the old one didn't do it either. 
            self.blk.jmp.type = ret_op(k);
        };
        @else => {
            self.f.retty = self.get_aggregate(ret).val().trunc();
            self.blk.jmp.type = .retc;
        };
    };
    
    has_context := func.get_flag(.YesContext) && !func.get_flag(.NoContext);  // :copy-pastrae
    if !has_context && self.dyn_context {
        // :SLOW
        self.f.start.ins.slice(0, self.f.start.nins.zext() - 1).copy_from(self.f.start.ins.slice(1, self.f.start.nins.zext()));
        if self.context != QbeNull {
            self.f.start.ins[self.f.start.nins.zext() - 1] = make_ins(.copy, .Kl, self.context, QbeConZero, QbeNull);  // TODO: why?
        } else {
            self.f.start.nins -= 1;
        };
    };
    self.finalize_blocks();
    
    // TODO: decide if i care enough to fix old from_bc.
    if fails_typecheck(self.f) { err | 
        printfn(self.f, self.f.globals.debug_out);
        panic(err);
    };
    
    
    .Ok
}

fn ret_op(k: Qbe.Cls) Qbe.J = 
    @as(Qbe.J) @as(i32) Qbe.J.retw.raw() + k.raw()

fn emit_runtime_call(
    self: *EmitIr,
    f_ty: FnType,
    args: []Qbe.Ins,
    p: Placement,
    callee: Qbe.Ref,
    dyn_context: bool,
) Res(Qbe.Ref) = { 
    self.f.leaf = false;
    if dyn_context && self.context != QbeNull {
        self.emit(.arge, .Kl, QbeNull, self.context, QbeNull);
    };
    cast: i64 = self.blk.nins.zext();  // TODO: ugh
    append(self.blk.ins&, cast&, args.ptr, args.ptr.offset(args.len));
    self.blk.nins = cast.trunc();
    _, k := self.load_op(f_ty.ret);
    scalar := k != .Ke;
    type_r := QbeNull;
    info := self.program.get_info(f_ty.ret);
    result_r := if info.stride_bytes == 0 || (scalar && p&.is(.Discard)) {  // :get_or_create_type
        k = .Kw;
        QbeNull
    } else {
        if !scalar {
            k = .Kl;
            type_r = self.get_aggregate(f_ty.ret);
            if p&.is(.Discard) {
                p = .NewMemory;
            };
        };
        self.scalar_dest(p, k)
    };
    self.emit(.call, k, result_r, callee, type_r);
    self.hlt_if_never(f_ty.ret);
    
    if(result_r == QbeNull, => return(Ok = QbeNull));
    if scalar { 
        return(Ok = self.scalar_result(p, result_r, f_ty.ret));
    };
    (Ok = @match(p) {
        fn NewMemory() => result_r;
        fn Discard()   => QbeNull;
        fn Blit(dest) => {
            self.blit(dest, result_r, info.stride_bytes);
            dest
        }
        @default => return(@err("ICE: tried to assign aggragate call to scalar placement"));
    })
}

fn compile_stmt(self: *EmitIr, stmt: *FatStmt) PRes = {
    if self.debug && !(stmt.stmt&.is(.Eval) && stmt.stmt.Eval.expr&.is(.Block)) {
        place := self.program.get_whole_line(stmt.loc); // :SLOW
        if self.last_line != place.line {
            // TODO: dbgfile too because it might be something inlined from elsewhere. 
            // TODO: have the backend remove consecutive dbgloc instructions and just keep the last one 
            //       when all the stuff between them was optimised out. 
            self.emit(.dbgloc, .Kw, QbeNull, INT(place.line), INT(0));  
            self.last_line = place.line;
        };
    };
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Eval(expr) => {
            @try(self.compile_expr(expr, .Discard)) return;
            .Ok
        }
        fn DeclVar(f) => {
            @debug_assert_ne(f.name.kind, VarType.Const);
            @debug_assert(f.ty&.is(.Finished), "variable not typechecked");
            ptr  := @try(self.compile_expr(f.value&, .NewMemory)) return;
            self.declare_var(f.name, ptr, f.ty.Finished);
            .Ok
        }
        fn Set(f) => {   // :PlaceExpr
            // we care about the type of the pointer, not the value because there might be a cast.
            @match(f.place.expr&) {
                // TODO: take advantage of mutable variables to generate less noisy code. 
                fn GetVar(_) => panic("ICE: var set should be converted to place expr");
                fn Deref(arg) => {
                    dest := @try(self.compile_expr(arg[], .Scalar)) return;
                    @try(self.compile_expr(f.value&, (Blit = dest))) return;
                    .Ok
                }
                @default => return(@err("TODO: other `place=e;` :("));
            }
        }
        fn DeclVarPattern(f) => {
            simple := f.binding.bindings.len == 1 || f.value.expr&.is(.Tuple);
            if simple { 
                // This is the pattern we generate for capturing calls. 
                parts := f.value&.items();
                @err_assert(parts.len == f.binding.bindings.len, "ICE: DeclVarPattern tuple size mismatch") return;
                enumerate parts { i, value |
                    b   := f.binding.bindings[i]&;
                    ptr := @try(self.compile_expr(value, .NewMemory)) return;
                    @debug_assert(b.ty&.is(.Finished), "variable not typechecked");
                    if b.var() { name |
                        self.declare_var(name, ptr, b.ty.Finished);
                    };
                };
            } else {
                // it's a destructuring (not inlined args)
                // We store the whole value in a stack slot and then save pointers into different offsets of it as thier own variables.
                
                base := @try(self.compile_expr(f.value&, .NewMemory)) return;
                info := self.program.get_type(f.value.ty);
                @err_assert(info.is(.Struct), "destructure must be tuple") return;
                fields := info.Struct.fields&;
                @debug_assert_eq(fields.len, f.binding.bindings.len, "destructure size mismatch");
                enumerate f.binding.bindings.items() { i, b |
                    f       := fields[i]&;
                    @err_assert(f.kind != .Const, "TODO: destructuring skip const fields") return;
                    name    := @unwrap(b.var(), "tuple binding requires name") return;
                    element := self.offset(base, f.byte_offset);
                    @debug_assert(b.ty&.is(.Finished), "variable not typechecked");
                    self.declare_var(name, element, b.ty.Finished);
                };
            };
            .Ok
        };
        fn Noop() => .Ok;
        // Can't hit DoneDeclFunc because we don't re-eval constants.
        @default => @err("ICE: stmt not desugared");
    }
}

fn compile_expr(self: *EmitIr, expr: *FatExpr, p: Placement) Res(Qbe.Ref) = {
    @debug_assert(!expr.ty.is_unknown(), "Not typechecked: %", self.program.log(expr));
    self.last_loc = expr.loc;
    if self.blk.jmp.type == .hlt {  // TODO: do we need this or do we trust sema to always get rid of things after a Never? 
        return(Ok = QbeNull);
    };

    expr_ty := expr.ty;
    (Ok = @match(expr.expr&) {
        fn Cast(v)  => {
            // TODO: bleh, make the front end check this? 
            from := self.program.get_info(v.ty).stride_bytes();
            to   := self.program.get_info(expr_ty).stride_bytes();
            @err_assert(from == to, "@as size mismatch: % vs %", self.program.log(v.ty), self.program.log(expr_ty)) return;
            return(self.compile_expr(v[], p))
        }
        fn Call(f)    => return(self.emit_call(f.f, f.arg, p));
        fn Block(f)   => return(self.emit_block_expr(expr, p));
        fn Value(f)   => return(self.emit_value(f.bytes&, p, expr.ty));
        fn If(_)      => return(self.emit_call_if(expr, p));
        fn Loop(arg)  => return(self.emit_call_loop(arg[]));
        fn Addr(arg)  => return(self.addr_macro(arg[], p));
        fn StructLiteralP(pattern) => return(self.construct_aggregate(pattern, expr.ty, p));
        fn Slice(arg) => {
            if(p&.is(.Discard), => return(self.compile_expr(arg[], .Discard)));
            
            container_ty := arg.ty;
            // Note: number of elements, not size of the whole array value.
            ty, count := @match(arg.expr) {
                fn Tuple(parts) Ty(Type, i64) => { 
                    fst := parts[0];
                    (fst.ty, parts.len) 
                };
                @default => (arg.ty, 1);
            };
            ptr  := @try(self.compile_expr(arg[], .NewMemory)) return;
            dest := self.get_memory(p, expr_ty);
            self.emit(.storel, .Kw, QbeNull, ptr, dest);
            self.emit(.storel, .Kw, QbeNull, self.f.getcon(count), self.offset(dest, 8));
            dest
        };
        fn Deref(arg) => {
            src := @try(self.compile_expr(arg[], @if(p&.is(.Discard), .Discard, .Scalar))) return; // get the pointer
            // we care about the type of the pointer, not the value because there might be a cast. (// TODO: that shouldn't be true anymore because of ::Cast)
            value_type := self.program.unptr_ty(arg.ty).unwrap();
            self.hlt_if_never(expr_ty);
            size := self.program.get_info(value_type).stride_bytes();
            if(p&.is(.Discard) || size == 0, => return(Ok = QbeConZero)); // :NoRuntimeRepr
            if p&.is(.Scalar) || p&.is(.Assign) {
                @debug_assert(size <= 8);
                o, k := self.load_op(expr_ty);
                r := self.scalar_dest(p, k);
                self.emit(o, k, r, src, QbeNull);
                return(Ok = self.scalar_result(p, r, expr_ty));
            };
            dest := self.get_memory(p, value_type);
            self.blit(dest, src, size);
            dest
        };
        fn FnPtr(arg) => {
            f := @unwrap(arg[].as_const(), "expected fn for ptr") return;
            f := FuncId.assume_cast(f&)[];
            callee_r := self.func_ref(f);
            self.scalar_result(p, callee_r, i64)  // :get_or_create_type
        };
        fn Unreachable() => {
            self.hlt_if_never(Never); // :get_or_create_type
            QbeConZero
        }
        fn Uninitialized() => {
            @err_assert(!expr_ty.is_never(), "call exit() to produce a value of type 'Never'") return;
            // Wierd special case I have mixed feelings about. should at least set to sentinal value in debug mode.
            // TODO: if(opts.zero_init_memory, => self.zero_memory_at_top_of_stack(expr_ty));
            @match(p) {
                fn Scalar()    => QbeConZero;
                fn Blit(it)    => it;
                fn NewMemory() => self.alloca(expr_ty);
                fn Discard()   => QbeNull;
                fn Assign(it)  => it;
            }
        };
        fn Tuple(values) => {
            if values.len == 1 {
                return(self.compile_expr(values[0]&, p));
            };
            @err_assert(!(p&.is(.Scalar) || p&.is(.Assign)), "ICE: non single field struct as scalar") return;
            raw := self.program.raw_type(expr.ty); // TODO: do i need this? 
            @match(self.program.get_type(raw)) {
               fn Struct(f) => {
                    @err_assert(f.layout_done, "ICE: struct layout not ready.") return;
                    @assert_eq(f.fields.len, values.len);
                    @err_assert(f.is_tuple, "Expr::Tuple can only create tuple type") return;
                    base := self.get_memory(p, expr_ty);
                    range(0, f.fields.len) { i |
                        dest := self.offset(base, f.fields[i].byte_offset);
                        @try(self.compile_expr(values[i]&, (Blit = dest))) return;
                    };
                    base
                }
                fn Array(f) => {
                    @debug_assert_eq(values.len(), f.len.zext());
                    element_size := self.program.get_info(f.inner).stride_bytes();
                    base := self.get_memory(p, expr_ty);
                    mem  := base;
                    each values { value |
                        @try(self.compile_expr(value, (Blit = mem))) return;
                        mem = self.offset(mem, element_size.zext());
                    };
                    mem
                }
                @default => return(@err("Expr::Tuple should have struct type not %. (0/1 element tuple maybe?)", self.program.log(raw)));
            }
        }
        fn PtrOffset(f) => {
            // TODO: compiler has to emit tagchecks for enums now! but it does not!
            base  := @try(self.compile_expr(f.ptr, .Scalar)) return;
            field := self.offset(base, f.bytes);
            self.scalar_result(p, field, i64) // :get_or_create_type
        }
        fn Switch(f) => return(self.emit_switch(expr, p));
        @default => @panic("ICE: didn't desugar: %", self.program.log(expr));
    })
}

fn emit_call(self: *EmitIr, f: *FatExpr, arg: *FatExpr, p: Placement) Res(Qbe.Ref) #once = {
    caller := self.program.get_function(self.func);
    @match(self.program.get_type(f.ty)) {
        fn Fn(f_ty) => {
            f_id := @unwrap(f.as_const(), "ice: tried to call non-const fn %", self.program.log(f)) return;
            f_id := FuncId.assume_cast(f_id&)[];
            func := self.program.get_function(f_id);
            cc := func.cc.unwrap();
            @err_assert(!func.get_flag(.Generic), "tried to emit call to unlowered #generic") return;
            @debug_assert(!func.get_flag(.MayHaveAquiredCaptures), "tried to emit call to unlowered maybe '=>'");
            @assert(cc != .Inline, "ICE: tried to call inlined %", self.program.get_string(func.name));

            // TODO: ideally the redirect should just be stored in the overloadset so you don't have to have the big Func thing every time.
            // TODO: audit: is this f_ty different from the one we just got from the expression type? -- Jul 8
            f_ty := func.finished_ty().unwrap(); // kinda HACK to fix unaligned store? 
            original_f_id := f_id;
            f_id = self.program.follow_redirects(f_id);
            callee := self.program.get_function(f_id);  // fid might have changed. 
            
            // TODO: allow Merged with intrinsic
            if callee.body&.is(.Intrinsic) {
                op := callee.body.Intrinsic&;
                arg := arg.items();
                a0 := @try(self.compile_expr(arg[0]&, .Scalar)) return;
                a1 := if(arg.len == 1, => QbeNull, => @try(self.compile_expr(arg[1]&, .Scalar)) return);
                return(self.emit_intrinsic(op, p, @slice(a0, a1), caller, f_ty.ret));
            };
            if !caller.get_flag(.ComptimeOnly) && original_f_id == f_id {
                // TODO: it would be nice if this was at the beginning and you never tried to emit anything that was comptimeonly
                //       that requires never adding them to callees and then later realizing you can inline them, which would make sense. -- Aug 29
                @debug_assert(!(self.when == .Aot && callee.get_flag(.ComptimeOnly)), "cannot .Aot .ComptimeOnly % %", f_id, self.program.log(callee));
            };
            context := !callee.get_flag(.NoContext);
            if context {
                caller.set_flag(.YesContext);
            };
            if self.debug {
                // TODO: O.dbgloc would be nice
            };
            
            callee_r := self.func_ref(f_id);
            arg := @try(self.compile_for_arg(arg)) return;
            self.emit_runtime_call(f_ty, arg, p, callee_r, context)
        }
        fn FnPtr(f_ty) => {
            if self.dyn_context { 
                caller.set_flag(.YesContext);
            };
            callee := @try(self.compile_expr(f, .Scalar)) return;
            arg    := @try(self.compile_for_arg(arg)) return;
            self.emit_runtime_call(f_ty.ty, arg, p, callee, self.dyn_context)
        }
        fn Label(_) => {
            label := @unwrap(f.as_const(), "called label must be const") return;
            label := LabelId.assume_cast(label&)[];

            ret := self.inlined_return_addr&.get_ptr(label);
            ret := @unwrap(ret, "missing return label. forgot '=>' on function?") return;
            ret.used = true;  // note: updating the one in the map! not a copy
            
            result := @try(self.compile_expr(arg, ret.p)) return;
            seal self { b |
                b.jmp.type = .jmp;
                b.s1 = ret.blk;
            };
            (Ok = QbeNull)
        }
        @default => @panic("ICE: non callable: %", self.program.log(f));
    }
}
::enum(ExecStyle);

fn emit_intrinsic(self: *EmitIr, op: *IntrinsicPayload, p: Placement, arg: []Qbe.Ref, caller: *Func, expr_ty: Type) Res(Qbe.Ref) = {
    r := @match(op.bc) {
        fn GetContext() => {
            @assert(self.dyn_context, "TODO: support static context");
            caller.set_flag(.YesContext);  // TODO: you might have setcontext for yourself
            self.unify_placement(p&, expr_ty); // because we don't know if they'll set again before using. meh. 
            self.context
        }
        fn SetContext() => {
            @assert(self.dyn_context, "TODO: support static context");
            if self.context == QbeNull {
                self.context = self.f.newtmp("context", .Kl);
            };
            self.emit(.copy, .Kl, self.context, arg[0], QbeNull);
            QbeConZero
        }
        fn BitNot() => {
            if(p&.is(.Discard), => return(Ok = QbeNull));
            // TODO: .Kw
            r := self.scalar_dest(p, .Kl);
            self.emit(.xor, .Kl, r, arg[0], self.f.getcon(-1));
            r
        }
        @default => {
            if(p&.is(.Discard), => return(Ok = QbeNull));
            k := @as(Qbe.Cls) op.ir_cls;
            r := self.scalar_dest(p, k); 
            @debug_assert(op.ir_op != 0, "no ir op for %", op.bc);
            self.emit(@as(Qbe.O) op.ir_op, k, r, arg[0], arg[1]);
            r
        };
    };
    (Ok = self.scalar_result(p, r, expr_ty))
}

fn func_ref(self: *EmitIr, f_id: FuncId) Qbe.Ref #inline = {
    f_id := self.program.follow_redirects(f_id);
    self.pending.push(f_id); // HACK
    self.f.symcon(self.fmt_fn_name(f_id))
}


fn emit_block_expr(self: *EmitIr, expr: *FatExpr, p: Placement) Res(Qbe.Ref) #once = {
    block   := expr.expr.Block&;
    ret_var := or block.ret_label {
        // Simple case: its just grouping some statements into an expression. 
        each block.body& { stmt |
            @try(self.compile_stmt(stmt)) return;
        };
        return(self.compile_expr(block.result, p))
    };
    
    // :block_never_unify_early_return_type
    // Note: block_ty can be different from value.ty if the fall through is a Never but there's an early return to the block. 
    block_ty := expr.ty;
    
    // It might have an early return so we want to unify that up here and then the Placement system can deal with it. 
    dest := self.unify_placement(p&, block_ty);
    out  := self.program.get_info(block_ty);

    return_block_index := self.blocks.len;
    ret: ReturnTarget = (
        blk = self.new_block("local"),
        p = p,
        used = false,
    );
    prev := self.inlined_return_addr&.insert(ret_var, ret);
    @debug_assert(prev.is_none(), "stomped ret var");

    each block.body& { stmt | 
        // Note: these are allowed to do a local_return targeting `ret`
        @try(self.compile_stmt(stmt)) return;
    };
    _ := @try(self.compile_expr(block.result, p)) return;

    ret := self.inlined_return_addr&.remove(ret_var).unwrap();
    if ret.used {
        // They did a local_return, so our extra block was useful. 
        seal self { b |
            // There was also a fallthough expression which we convert to a local_return.
            b.s1 = ret.blk;
            b.jmp.type = .jmp;
        };
        self.blk = ret.blk;
    } else {
        // They didn't actually use a local_return so we can discard this useless block. 
        // TODO: this is wrong when nested
        //junk := self.blocks&.unordered_remove(return_block_index); 
        //@debug_assert(ret.blk.identical(junk.unwrap()));
        ret.blk.jmp.type = .hlt;
        // We did generate slightly more noisy code by `unify_placement` earlier than necessary. 
    };
    (Ok = dest)
}

fn emit_value(self: *EmitIr, value: *Values, p: Placement, expr_ty: Type) Res(Qbe.Ref) #once = {
    if(p&.is(.Discard), => return(Ok = QbeNull));  // TODO: bake_relocatable_value side effect? 
    info := self.program.get_info(expr_ty);
    
    // :SLOW all_zeroes is redundant with emit_relocatable_constant but for now we can't emit_relocatable_constant_body on something
    // with small fields + pointers, but its fine to emit as bytes if all zeros. 
    all_zero := value.all_zeroes();
    
    use_int_literal := value.is(.Small) && (!info.contains_pointers() || self.when == .Jit || all_zero);
    if use_int_literal {
        v := self.f.getcon(value.Small._0);
        return(Ok = self.scalar_result(p, v, expr_ty));
    };
    
    // TODO: zeroed(T) doesn't get here because it's a runtime load not a comptime one. 
    //      need to fix sema stuff to make that work.  -- Dec 5
    if all_zero {
        dest := self.get_memory(p, expr_ty);
        count := value.len() / 8;
        @err_assert(count * 8 == value.len(), "TODO: non*8 constant") return;
        range(0, count) { i |
            self.emit(.storel, .Kw, QbeNull, QbeConZero, self.offset(dest, i * 8));
        };
        return(Ok = dest);
    };
    
    @match(self.when) {
        fn Aot() => {
            if !info.contains_pointers() {
                id := @try(self.program.emit_relocatable_constant(expr_ty, value.bytes())) return;
                self.reference_constant(id);
                dest := self.get_memory(p, expr_ty);
                src := self.as_ref(AddrOf = id);
                self.blit(dest, src, info.stride_bytes);
                return(Ok = dest);
            };
            
            out  := @try({self.program.vtable.emit_relocatable_constant_body}(self.program.data, value.bytes(), expr_ty, false)) return;
            if out.len == 1 {
                v := self.as_ref(out[0]);
                return(Ok = self.scalar_result(p, v, expr_ty));
            };
            dest := self.get_memory(p, expr_ty); 
            enumerate out { i, part | 
                r := self.as_ref(part[]);
                self.emit(.storel, .Kw, QbeNull, r, self.offset(dest, i * 8));
            };
            (Ok = dest)
        }
        fn Jit() => {
            // TODO: this will generate garbage code for slices
            src := self.f.getcon(value.jit_addr());
            dest := self.get_memory(p, expr_ty);
            @debug_assert_eq(value.len(), info.stride_bytes.zext(), "value size mismatch");
            self.blit(dest, src, info.stride_bytes);
            (Ok = dest)
        }
    }
}

fn all_zeroes(value: *Values) bool = 
    (value.is(.Small) && value.Small._0 == 0) || value.bytes().all_zeroes();

fn reference_constant(self: *EmitIr, id: BakedVarId) void = {
    shared := self.shared;
    seen := shared.constants_used&;
    if !seen.get(id.id.zext()) {
        seen.set(id.id.zext());
        _, value := {self.program.vtable.get_baked}(self.program.data, id)[];
        @if_let(value) fn VoidPtrArray(it) => each it { it | 
            @match(it) {
                fn AddrOf(it) => self.reference_constant(it[]);
                fn FnPtr(it) => self.pending.push(it[]);  // HACK
                @default => ();
            };
        };
        self.new_constants&.push(id);
    };
}

fn as_ref(self: *EmitIr, part: BakedEntry) Qbe.Ref = @match(part) {
    // TODO: non-8 byte fields
    fn Num(f) => self.f.getcon(f.value);
    fn FnPtr(f)   => self.func_ref(f);
    fn AddrOf(id) => {
        self.reference_constant(id);
        self.f.symcon(@tfmt("g%", id.id))
    }
};

// :PlaceExpr
fn addr_macro(self: *EmitIr, arg: *FatExpr, p: Placement) Res(Qbe.Ref) #once = {
    self.last_loc = arg.loc;
    // field accesses should have been desugared.
    @err_assert(arg.expr&.is(.GetVar), "took address of r-value") return;
    var := arg.expr.GetVar;
    @assert_ne(var.kind, .Const, "Cannot take address of constant % \n(use @static if you want to create a non-threadsafe mutable global)", var&.log(self.program));
    ref := @unwrap(self.var_lookup&.get(var), "Missing var % (in !addr) \n(missing $ when used in const context or missing '=>'? TODO: better error message)", var&.log(self.program)) return;
    (Ok = self.scalar_result(p, ref, i64))
}

fn emit_call_if(self: *EmitIr, arg: *FatExpr, p: Placement) Res(Qbe.Ref) #once = {
    @debug_assert(arg.expr&.is(.If));
    parts := arg.expr.If&;
    
    cond := @try(self.compile_expr(parts.cond, .Scalar)) return;
    t_expr, f_expr := (parts.if_true, parts.if_false);
    t_blk, f_blk, join_blk := (self.new_block("yes"), self.new_block("no"), self.new_block("join"));
    seal self { b |
        b.jmp = (type = .jnz, arg = cond);
        b.s1 = t_blk;
        b.s2 = f_blk;
    };
    
    // force both branches to output to the same place. 
    join_val := self.unify_placement(p&, arg.ty);
    joined := false;
    
    self.blk = t_blk;
    t_val := @try(self.compile_expr(t_expr, p)) return;
    self.hlt_if_never(t_expr.ty);
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = join_blk;
        joined = true;
    };
    
    self.blk = f_blk;
    f_val := @try(self.compile_expr(f_expr, p)) return;
    self.hlt_if_never(f_expr.ty);
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = join_blk;
        joined = true;
    };
    
    self.blk = join_blk;
    if !joined {
        // both branches diverged. 
        join_blk.jmp.type = .hlt; // TODO: remove the useless block 
    };
    (Ok = join_val)
}

// TODO: use phi for scalars. 
// TODO: binary search or jump table or something. this is kinda sad. 
fn emit_switch(self: *EmitIr, arg: *FatExpr, p: Placement) Res(Qbe.Ref) #once = {
    @debug_assert(arg.expr&.is(.Switch));
    parts := arg.expr.Switch;
    if parts.cases.len == 0 {
        // There's only a default; that's not really a switch bro but ok... 
         _ := @try(self.compile_expr(parts.value, .Discard)) return;
        return(self.compile_expr(parts.default, p));
    };
    
    inspect := @try(self.compile_expr(parts.value, .Scalar)) return; 
    // TODO: maybe i have to zero extend b/h if other since trunc assumes other people only look at the lower part. 
    
    result := self.unify_placement(p&, arg.ty);
    
    entry_block := self.blk; 
   
    // This is where we rejoin with the value of the whole switch expression. 
    out := self.program.get_info(arg.ty);
    end_blk   := self.new_block("joins");
    rejoined := false;
    each parts.cases { f |
        value := f._0;
        body := f._1&;
        
        @err_assert(value >= 0 && value <= MAX_i32, "TODO: Expr::Switch only supports low positive integers.") return;
        case_block     := self.new_block(@tfmt("case%", value));
        next_chain_blk := self.new_block("check");
        
        matches := self.f.newtmp("m", .Kw);
        self.emit(.ceql, .Kw, matches, inspect, self.f.getcon(value)); // TODO: extend or use the right size compare
        self.blk.s1 = case_block;
        self.blk.s2 = next_chain_blk;
        self.blk.jmp.type = .jnz;
        self.blk.jmp.arg = matches;
        
        self.blk = case_block; 
        _ := @try(self.compile_expr(body, p)) return;
        seal self { b |
            b.jmp.type = .jmp;
            b.s1 = end_blk;
            rejoined = true;
        };
        self.blk = next_chain_blk;
    };
    
    // TODO: would it be nicer to have default branch just be the last thing in the ast node too so you could handle them uniformly? -- Jul 26
    _ := @try(self.compile_expr(parts.default, p)) return;
    seal self { b |
        b.s1 = end_blk;
        b.jmp.type = .jmp;
        rejoined = true;
    };
    if !rejoined {
        end_blk.jmp.type = .hlt; // TODO: remove the useless block
    };
    self.blk = end_blk;
    (Ok = result)
}

fn emit_call_loop(self: *EmitIr, arg: *FatExpr) Res(Qbe.Ref) #once = {
    @debug_assert_eq(arg.ty, void);
    start := self.new_block("loop");
    self.blk.jmp.type = .jmp;
    self.blk.s1 = start;
    self.blk    = start;
    @try(self.compile_expr(arg, .Discard)) return;
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = start;
    };
    (Ok = QbeNull)
}

fn construct_aggregate(self: *EmitIr, pattern: *Pattern, requested: Type, p: Placement) Res(Qbe.Ref) #once = {
    if p&.is(.Discard) {
        each pattern.bindings& { b | 
            @try(self.compile_expr(b.default&.unwrap(), .Discard)) return;
        };
        return(Ok = QbeNull);
    };
    raw_container_ty := self.program.raw_type(requested);
    (Ok = @match(self.program.get_type(raw_container_ty)) {
        fn Struct(f) => {
            expected := f.fields.len - f.const_field_count.zext();
            if f.is_union {
                @debug_assert_eq(pattern.bindings.len, 1, "union must have exactly one active field");
                expected = 1;
            } else {
                @debug_assert_eq(expected, pattern.bindings.len, "Cannot assign to type % with wrong field count", self.program.log(requested));
                if expected == 1 {
                    expr := pattern.bindings[0].default&.unwrap();
                    return(self.compile_expr(expr, p));
                };
            };
            if expected == 1 {
                return(self.compile_expr(pattern.bindings[0].default&.unwrap(), p));
            };
            base := self.get_memory(p, raw_container_ty);
            enumerate pattern.bindings& { i, b | 
                name := @unwrap(b.ident(), "map literal entry needs name (while initilizing %)", self.program.log(requested)) return;
                field := or find_struct_field(f, name, i - 1) {
                    return(@err("field name mismatch (ICE: should be checked by sema)"))
                };
                @debug_assert(field.kind != .Const, "cannot assign to const field");
                expr := b.default&.unwrap();
                dest := self.offset(base, field.byte_offset);
                _ := @try(self.compile_expr(expr, (Blit = dest))) return;
            };
            base
        }
        fn Tagged(f) => {
            @debug_assert_eq(pattern.bindings.len, 1, "@tagged must have one active varient");
            value := pattern.bindings[0].default&.unwrap();
            name := pattern.bindings[0].name.unwrap();
            i    := f.cases.position(fn(f) => f._0 == name).expect("case name to exist in type");
            if value.ty == void {  // :get_or_create_type
                _ := @try(self.compile_expr(value, .Discard)) return;
                return(Ok = self.scalar_result(p, self.f.getcon(i), i64));
            };
            @match(p) {
                fn Discard() => return(self.compile_expr(value, .Discard)); // just for side effects
                fn Scalar()  => return(@err("ICE: tagged union is not a scalar."));
                @default     => ();
            };
            base := self.get_memory(p, raw_container_ty);
            self.emit(.storel, .Kw, QbeNull, self.f.getcon(i), base);  // tag :get_or_create_type // TODO: smaller tag sizes than 64 bits 
            _ := @try(self.compile_expr(value, (Blit = self.offset(base, 8)))) return;
            base
        }
        @default => return(@err("struct literal for non-(struct/tagged)"));
    })
}

// For now ptr is always the address of the variable's stack slot. 
// TODO: generate less dumb code for non-escaping scalars. 
fn declare_var(self: *EmitIr, name: Var, ptr: Qbe.Ref, type: Type) void #inline = {
    if self.program.get_info(type)[].stride_bytes == 0 {
        // :NoRuntimeRepr
        // It's nice to let macros/generics operate over types uniformly.
        // Some types have no runtime representation, so any attempt to use them should compile to nothing. 
        // TODO: maybe sema should try to strip these out so we don't have to keep checking here. 
        self.var_lookup&.insert(name, QbeConZero);
        return()  
    };

    @debug_assert(rtype(ptr) == .RTmp);
    prev := self.var_lookup&.insert(name, ptr);
    @debug_assert(prev.is_none(), "shadow is still new var");
    
    if self.debug && name.name != Flag.SYMBOL_ZERO.ident() {
        name := self.program.get_string(name.name);
        name.len = name.len.min(20);
        t := self.f.get_temporary(ptr);
        @if(TRACK_IR_NAMES) {
            l: List(u8) = (maybe_uninit = t.name&.items(), len = 0, gpa = panicking_allocator);
            @fmt(l&, "%.%", name, ptr.val());
        };
    };
}

// IMPORTANT: you can't use f.emit because threads. 
fn emit(self: *EmitIr, o: Qbe.O, k: Qbe.Cls, dest: Qbe.Ref, a0: Qbe.Ref, a1: Qbe.Ref) void #inline = {
    @debug_assert(k.raw() < 4, "ICE: invalid base in emit_ir:emit %", o.get_name());
    @debug_assert(o != .Oxxx, "ICE: null inst emit_ir:emit");
    i := make_ins(o, k, dest, a0, a1); 
    //printins(self.f, i&, self.f.globals.debug_out);
    nins: i64 = self.blk.nins.zext();  // TODO: ugh
    addins(self.blk.ins&, nins&, i&);
    self.blk.nins = nins.trunc();
}

// TODO: types larger than 2^16 bytes but you probably don't want to copy those anyway
fn blit(self: *EmitIr, dest: Qbe.Ref, src: Qbe.Ref, bytes: u16) void #inline = {
    // mem.fr/promote knows about 1/2/4/8 byte blits. 
    if bytes == 16 {    
        // TODO: maybe we can't do this cause alignment but we're kinda bad at eliminating blits. :UGLY
        //       and the old backend special cased this so its sad to be worse. 
        //       should make slot coalessing not get confused by blits or lower them earlier.
        //       TODO: would sroa be that hard? i know which things don't escape. 
        v  := self.f.newtmp("b", .Kl);
        v2 := self.f.newtmp("b", .Kl);
        src2  := self.offset(src, 8);
        dest2 := self.offset(dest, 8);
        self.emit(.load, .Kl, v, src, QbeNull);
        self.emit(.load, .Kl, v2, src2, QbeNull);
        self.emit(.storel, .Kw, QbeNull, v, dest);
        self.emit(.storel, .Kw, QbeNull, v2, dest2);
        return();
    };
    self.emit(.blit0, .Kw, QbeNull, src, dest);
    self.emit(.blit1, .Kw, QbeNull, INT(bytes.zext()), QbeNull);
}

fn alloca(self: *EmitIr, ty: Type) Qbe.Ref = {
    info := self.program.get_info(ty);
    if info.stride_bytes == 0 { // :NoRuntimeRepr 
        return(QbeConZero);
    };
    t := self.f.newtmp("s", .Kl);
    @debug_assert(info.align_bytes <= 8, "i don't support high alignments yet");
    o: Qbe.O = @if(info.align_bytes <= 4, .alloc4, .alloc8);
    s := self.f.getcon(info.stride_bytes.zext());
    i := make_ins(o, .Kl, t, s, QbeNull);
    nins: i64 = self.f.start.nins.zext();  // TODO: ugh
    addins(self.f.start.ins&, nins&, i&);
    self.f.start.nins = nins.trunc();
    t
}

fn offset(self: *EmitIr, ptr: Qbe.Ref, bytes: i64) Qbe.Ref #inline = {
    if(bytes == 0, => return(ptr));
    new := self.f.newtmp("f", .Kl);
    self.emit(.add, .Kl, new, ptr, self.f.getcon(bytes));
    new
}

fn unify_placement(self: *EmitIr, p: *Placement, ty: Type) Qbe.Ref = @match(p[]) {
    fn Blit(it) => it;
    fn Discard() => QbeNull;
    fn NewMemory() => {
        ref := self.alloca(ty);
        p[] = (Blit = ref);
        ref
    }
    fn Scalar() => {   // TODO: use phi instead
        _, k := self.load_op(ty);
        @debug_assert(k != .Ke, "ICE: attempted scalar placement of aggragate type");
        dest := self.f.newtmp("unify", k);
        p[] = (Assign = dest);
        dest
    }
    fn Assign(it) => it;
};

fn get_memory(self: *EmitIr, r: Placement, ty: Type) Qbe.Ref = {
    @match(r) {
        fn NewMemory() => self.alloca(ty);
        fn Blit(it) => it;
        fn Discard() => self.alloca(ty);  // TODO: maybe force the caller to early out? 
        @default => panic("invalid Placement type for get_memory");
    }
}

// This exists to save a copy with Placement.Assign, 
// otherwise it just creates a new tmp for you to use. 
// Caller has to pass the result to scalar_result after copying in the value. 
fn scalar_dest(self: *EmitIr, p: Placement, k: Qbe.Cls) Qbe.Ref = @match(p) {
    fn Discard() => QbeNull;
    fn Assign(dest) => dest;
    @default => {
        @debug_assert(k != .Ke);
        self.f.newtmp("v", k)
    };
};

fn scalar_result(self: *EmitIr, p: Placement, r: Qbe.Ref, ty: Type) Qbe.Ref = @match(p) {
    fn NewMemory() => self.scalar_result((Blit = self.alloca(ty)), r, ty);
    fn Blit(dest)  => {
        size := self.program.get_info(ty)[].stride_bytes;
        if size == 0 {
            return(QbeConZero);  // :NoRuntimeRepr
        };
        o := self.store_op(ty);
        if o == .Oxxx {
            // It's convinent to emit constants as a literal even if we want a struct with multiple fields. 
            // TODO: is alignment real?
            @debug_assert(size <= 8, "tried to use scalar result for large aggragate");
            o = store_by_size[size.zext()];
            @debug_assert(o != .Oxxx, "tried to store strange size");
        };
        self.emit(o, .Kw, QbeNull, r, dest);
        dest
    }
    fn Scalar()  => r;
    fn Discard() => QbeNull;
    fn Assign(dest) => {
        if dest != r {
            t := self.f.get_temporary(dest);
            self.emit(.copy, t.cls, dest, r, QbeNull);
        };
        dest
    }
};

fn get_aggregate(self: *EmitIr, ty: Type) Qbe.Ref = {
    info := self.program.get_info(ty);
    // TODO: doing it this way is gonna be a nightmare when i want seperate jit and aot module for comptime/runtime. 
    if info.ir_index == -1 {
        type := self.program.get_type(ty);
        result := Qbe.Typ.zeroed();
        result.size = info.stride_bytes.zext();
        @debug_assert(result.size != 0, "ICE: treating zero size type % as an aggragate", self.program.log(ty));
        result.align_log2 = (@as(i64) info.align_bytes.zext()).trailing_zeros().intcast(); // TODO: thats dumb
        l: List(u8) = (maybe_uninit = result.name&.items(), len = 0, gpa = panicking_allocator);
        @fmt(l&, "T%", ty.as_index());
        i := @match(type) {
            fn Named(it)  => self.get_aggregate(it._0).val();
            fn Enum(it)   => self.get_aggregate(it.raw).val();
            fn Struct(it) => {
                yield :: local_return;
                if repr_transparent(it) { repr |
                    yield(self.get_aggregate(repr).val());
                };
                n := 0;
                rt_count: i64 = it.fields.len - it.const_field_count.zext();
                result.fields = new_long_life(rt_count + 1);
                result.nunion = @if(it.is_union, rt_count.trunc(), 1);
                result.is_union = it.is_union;
                @assert(it.layout_done, "tried to get_aggregate before layout %", self.program.log(ty));
                if it.is_union {
                    each it.fields { f | 
                        if f.kind != .Const {
                            field_size := self.program.get_info(f.ty)[].stride_bytes;
                            @debug_assert(field_size != 0, "TODO: allow void union?");
                            @debug_assert(f.byte_offset == 0, "union field must have offset 0");
                            result.fields[n] = self.as_field(f.ty);      n += 1;
                            result.fields[n] = (type = .FEnd, len = 0);  n += 1;
                        };
                    };
                } else {
                    off := 0;
                    each it.fields { f | 
                        if f.kind != .Const {
                            field_size := self.program.get_info(f.ty)[].stride_bytes;
                            if field_size != 0 {
                                add_padding(result.fields&, n&, off&, f.byte_offset);
                                result.fields&.grow(n);
                                result.fields[n] = self.as_field(f.ty);
                                n += 1;
                                off += field_size.zext();
                            }; // else :NoRuntimeRepr
                        };
                    };
                    add_padding(result.fields&, n&, off&, info.stride_bytes.zext());
                    result.fields&.grow(n);
                    result.fields[n] = (type = .FEnd, len = 0);
                };
                // TODO: name
                self.m.new_type(result)
            }
            fn Array(it) => if it.len == 1 {
                self.get_aggregate(it.inner).val()
            } else {
                inner_size: i64 = self.program.get_info(it.inner)[].stride_bytes.zext();
                @debug_assert_eq(inner_size * it.len.zext(), info.stride_bytes.zext(), "size calc wrong!");
                outer_size: i64 = info.stride_bytes.zext();
                field_count := 1 + it.len.zext();
                result.fields = new_long_life(field_count);
                result.nunion = 1;
                field := self.as_field(it.inner);
                range(0, it.len.zext()) { i |
                    result.fields[i] = field;
                };
                result.fields[it.len.zext()] = (type = .FEnd, len = 0);
                // TODO: name
                self.m.new_type(result)
            }
            fn Tagged(it) => {
                n := 0;
                result.fields = new_long_life(it.cases.len * 3);
                result.nunion = it.cases.len.trunc();
                each it.cases { case | 
                    if case._1 != void { // :get_or_create_type 
                        result.fields[n] = (type = .Fl, len = 8);      n += 1;
                        result.fields[n] = self.as_field(case._1);     n += 1;
                        result.fields[n] = (type = .FEnd, len = 0);    n += 1;
                        // TODO: do i need to pad them to the same size?
                    } else {
                        result.nunion -= 1;
                    };
                };
                result.is_union = result.nunion > 1;
                @debug_assert(result.nunion != 0, "ICE: get_aggregate of scalar @tagged");
                // TODO: name
                self.m.new_type(result)
            }
            @default => @panic("tried to get_aggregate of scalar %", self.program.log(ty));
        }; 
        info.ir_index = i.intcast();
    };
    
    TYPE(info.ir_index.intcast())
}

fn add_padding(fields: *QList(Qbe.Field), nfield: *i64, current_size: *i64, target_size: i64) void = {
    pad := target_size - current_size[];
    @debug_assert(pad >= 0, "trying to pad struct to smaller size");
    if pad > 0 {
        fields.grow(nfield[]);
        fields[][nfield[]] = (type = .FPad, len = pad.trunc());
        nfield[] += 1;
    };
}

// TODO: use this elsewhere. 
fn new_type(m: *QbeModule, type: Qbe.Typ) i64 = {
    @if(use_threads) pthread_mutex_lock(m.types_mutex&).unwrap();
    i := m.number_of_types;
    m.number_of_types += 1;
    m.types.grow(m.number_of_types);
    m.types[i] = type;
    @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
    i
}

fn as_field(self: *EmitIr, ty: Type) Qbe.Field = {
    ::if(Qbe.Field);
    type := self.program.get_type(ty);
    @match(type) {
        fn F64()      => (type = .Fd, len = 8);
        fn F32()      => (type = .Fd, len = 4);
        fn Ptr(_)     => (type = .Fl, len = 8);
        fn FnPtr(_)   => (type = .Fl, len = 8);
        fn VoidPtr(_) => (type = .Fl, len = 8);
        fn Label(_)   => (type = .Fw, len = 4);
        fn Fn(_)      => (type = .Fw, len = 4);
        fn Bool()     => (type = .Fb, len = 1);
        fn Named(it)  => self.as_field(it._0);
        fn Enum(it)   => self.as_field(it.raw);
        fn Int(it)    => @switch(it.bit_count) {
            @case(8)  => (type = .Fb, len = 1);
            @case(16) => (type = .Fh, len = 2);
            @case(32) => (type = .Fw, len = 4);
            @default  => (type = .Fl, len = 8);
        }
        fn Tagged(it) => {
            if self.program.is_tag_only(it) {
                return(type = .Fl, len = 8);
            };
            (type = .FTyp, len = self.get_aggregate(ty).val().trunc())
        }
        fn Struct(it) => {
            if repr_transparent(it) { inner |
                return(self.as_field(inner))
            };
            (type = .FTyp, len = self.get_aggregate(ty).val().trunc())
        }
        fn Array(it) => if it.len == 1 {
            self.as_field(it.inner)
        } else {
            (type = .FTyp, len = self.get_aggregate(ty).val().trunc())
        };
        @default => @panic("tried to as_field of unhandled %", self.program.log(ty));
    }
}

fn is_tag_only(self: CompCtx, t: *TypeInfo.get_variant_type(.Tagged)) bool = {
    each t.cases { case | 
        if case._1 != void {  // :get_or_create_type
            return(false);
        };
    };
    true
}

fn repr_transparent(it: *TypeInfo.get_variant_type(.Struct)) ?Type = {
    @switch(it.fields.len - it.const_field_count.zext()) {
        @case(0) => (Some = void); // :get_or_create_type
        @case(1) => {
            each it.fields { f |
                if f.kind != .Const {
                    return(Some = f.ty);
                };
            };
            unreachable()
        };
        @default => .None;
    }
}

fn hlt_if_never(self: *EmitIr, ty: Type) void #inline = {
    if ty.is_never() && self.blk.jmp.type == .Jxxx {
        self.blk.jmp.type = .hlt;
    };
}

// TODO: single field array

// returns .Ke for aggregates
fn load_op(self: *EmitIr, ty: Type) Ty(Qbe.O, Qbe.Cls) = @match(self.program.get_type(ty)) {
    fn Int(it)   => @switch(it.bit_count) {
        @case(8)  => (@if(it.signed, .loadsb, .loadub), .Kw);
        @case(16) => (@if(it.signed, .loadsh, .loaduh), .Kw);
        @case(32) => (@if(it.signed, .loadsw, .loaduw), .Kw);
        @default  => (.load, .Kl);
    }
    fn F32()     => (.load, .Ks);
    fn F64()     => (.load, .Kd);
    fn Ptr(_)    => (.load, .Kl);
    fn FnPtr(_)  => (.load, .Kl);
    fn VoidPtr() => (.load, .Kl);
    fn Label(_)  => (.loaduw, .Kw);
    fn Fn(_)     => (.loaduw, .Kw);
    fn Named(it) => self.load_op(it._0);
    fn Enum(it)  => self.load_op(it.raw);
    fn Bool()    => (.loadub, .Kw);
    fn Tagged(it) => {
        if self.program.is_tag_only(it) {
            return(.load, .Kl);
        };
        (.nop, .Ke)
    }
    fn Struct(it) => self.load_op(or repr_transparent(it) {
        return(.nop, .Ke)
    });
    fn Array(it) => @if(it.len == 1, self.load_op(it.inner), (.nop, .Ke));
    @default     => (.nop, .Ke);
};

fn store_op(self: *EmitIr, ty: Type) Qbe.O = @match(self.program.get_type(ty)) {
    fn Int(it)   => @switch(it.bit_count) {
        @case(8)  => .storeb;
        @case(16) => .storeh;
        @case(32) => .storew;
        @default  => .storel;
    }
    fn F32()     => .stores;
    fn F64()     => .stored;
    fn Ptr(_)    => .storel;
    fn FnPtr(_)  => .storel;
    fn VoidPtr() => .storel;
    fn Label(_)  => .storew;
    fn Fn(_)     => .storew;
    fn Named(it) => self.store_op(it._0);
    fn Enum(it)  => self.store_op(it.raw);
    fn Bool()    => .storeb;
    fn Tagged(it) => {
        if !self.program.is_tag_only(it) {
            return(.Oxxx);
        };
        .storel
    }
    fn Struct(it) => self.store_op(repr_transparent(it) || return(.Oxxx));
    @default     => .Oxxx;
};

fn new_block(self: *EmitIr, debug: Str) *Qbe.Blk = {   
    b := temp().box_zeroed(Qbe.Blk);
    @if(TRACK_IR_NAMES) {
        l: List(u8) = (maybe_uninit = b.name&.items(), len = 0, gpa = panicking_allocator); // :UnacceptablePanic
        @fmt(l&, "b%_%", self.next_block_name, debug);
        self.next_block_name += 1; // not the same as blocks.len because we remove for unused local returns. 
    };
    b.ins = new(0);
    b.visit = -1;
    self.blocks&.push(b);
    b
}

fn fmt_fn_name(self: *EmitIr, f: FuncId) []u8 = 
    self.program.fmt_fn_name(f);

fn seal(self: *EmitIr, $body: @Fn(b: *Qbe.Blk) void) void = {
    if self.blk.jmp.type == .Jxxx { 
        body(self.blk);
    };
}
