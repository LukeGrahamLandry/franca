// TODO: less stupid code for accessing constant values!
// TODO: always check for zero sized type instead of == void

EmitIr :: @struct(
    f: *Qbe.Fn,
    program: CompCtx,
    last_loc: Span,
    var_lookup: HashMap(Var, IrVar),  // TODO: just var id
    inlined_return_addr: HashMap(LabelId, ReturnTarget),
    blk: *Qbe.Blk, 
    debug: bool,
    context := QbeNull,
    static_context_var: ?BakedVarId,
    dyn_context: bool,
    func: FuncId,
    blocks: List(*Qbe.Blk),
    when: ExecStyle,
    m: *QbeModule,
    entry: *CodegenEntry,
    next_block_name := 0,
    last_line := -1,
    new_constants: List(BakedVarId),
    shared: *CodegenShared,
    pending: *List(FuncId), // TODO: you wouldn't need this if sema tracked callees correctly. 
    idx: *i64,
);

IrVar :: @tagged(Alloca: Qbe.Ref, Local: Qbe.Ref);
::tagged(IrVar);

ReturnTarget :: @struct(
    blk: *Qbe.Blk,
    p: Placement,
    used: bool,
);

Placement :: @tagged(Assign: Qbe.Ref, Scalar, Blit: Qbe.Ref, NewMemory, Discard, ScalarTransient);

fn emit_ir(comp: CompCtx, shared: *CodegenShared, f: FuncId, when: ExecStyle, idx: *i64, pending: *List(FuncId), export: bool) CRes(void) = {
    if when == .Aot {
        // :bake_relocatable_value
        {comp.vtable.check_for_new_aot_bake_overloads}(comp.data); 
    };
    // TODO
    //func := comp.get_function(f);
    //if find_impl(func.body&, .DynamicImport).is_some() && when == .Aot {
    //    return(.Ok);
    //};
    
    //@if(f.as_index() == 56053, panic("foo")); // TODO: this fails safety check here. 
    entry := bouba_acquire(shared, idx);
    m := shared.m;
    result: CRes(void) = .Ok;
    push_dynamic_context {
        context(DefaultContext)[].temporary_allocator = entry.arena&;
        ::tagged(Placement);
        func := comp.get_function(f); 
        opts := comp.get_build_options();
        env := comp.get_comptime_env();
        //@if(TRACE_SHIMS) @println("emit_ir % %", f, comp.get_string(func.name));
        self: EmitIr = (
            f = temp().box(Qbe.Fn),
            program = comp,
            func = f,
            last_loc = func.loc,
            var_lookup = init(temp()),
            inlined_return_addr = init(temp()),
            blk = zeroed(*Qbe.Blk),
            blocks = list(temp()),
            debug = opts.debug_info || func.get_flag(.LogIr),
            dyn_context = if(when == .Aot, => opts.implicit_context_runtime, => opts.implicit_context_comptime),
            static_context_var = env.static_context_var,
            when = when,
            m = m,
            entry = entry,
            new_constants = list(temp()),
            shared = shared,
            pending = pending,
            idx = idx,
        );
        if self.dyn_context && when == .Aot {
            @debug_assert(self.static_context_var.is_none(), "ICE: dyn_context has static context??");
        };
        @debug_assert(func.get_flag(.EnsuredCompiled), "fn not compiled?");
        m := self.shared.m;
        self.f.default_init(m);
        self.f.leaf = true;
        self.f.lnk.no_inline = func.get_flag(.NoInline);
        self.f.lnk.id = m.intern(self&.fmt_fn_name(self.func));  // TODO: set export 
        self.entry.logging = self.program.get_log_types(f);
        
        response := sync_driver_message(comp.data.cast()[][], 
            (EmitIrStart = (when = when, fid = self.func, ir_data = Qbe.Fn.raw_from_ptr(self.f)))
        );
        entry.task = (Func = self.f);
        res: CRes(void) = .Ok;
        ::tagged(@type response);
        if !response&.is(.ReplacedIr) {
            res = self&.emit_body(f);
        };
        self.f.lnk.export = export;
        
        if res&.is_err() {
            res.Err.update_main_span(self.last_loc);
            result = (Err = res.Err)
        } else {
            has_context := func.get_flag(.YesContext) && !func.get_flag(.NoContext);  // :copy-pastrae
            if !has_context {
                func.set_flag(.NoContext);
            };
            
            @if_let(self.entry.task) fn Func(it) => {
                sync_driver_message(comp.data.cast()[][], (EmitIrEnd = (when = when, fid = f, ir_data = Qbe.Fn.raw_from_ptr(it), ir_type = Flag.self.ident())));
            };
            enqueue_task(shared, self.entry);
            
            if self.new_constants.len != 0 {
                entry2 := bouba_acquire(shared, idx);
                self.new_constants = self.new_constants.items().clone(entry2.arena&.borrow());
                push_dynamic_context {
                    context(DefaultContext)[].temporary_allocator = entry2.arena&;
                    out := Qbe.Dat.list(self.new_constants.len * 4, temp());
                    for self.new_constants { id |
                        emit_constant(comp, m, id, out&);
                    };
                    entry2.task = (AotVar = out.items());
                };
                enqueue_task(shared, entry2);
            };
        };
    };
    result
}

// TODO: this is going to change the calling convention for small strange aggragates maybe (to match c). 
//       hopefully i don't use any of those for something important. 

ArgInfo :: @struct(r: Qbe.Ref, type: Type, name: ?Var, scalar: bool, void: bool, k: Qbe.Cls, type_index: Qbe.Ref);

// TODO: if you have an unclosed string it randomly decides later top level vars are undeclared? -- Jul 7 
// :EmitIrCall
// note: mirrors compile_for_arg
// appends par/parc instructions to f.start
fn emit_par_instructions(program: CompCtx, shared: *CodegenShared, arguments: *Pattern, arg_ty: Type, f: *Qbe.Fn) []ArgInfo = {
    count := arguments.bindings.len;
    args := ArgInfo.list(count, temp());
    grow(f.start.ins&, f.start.nins.zext() + count);
    
    // Need to emit all the `arg` instructions at the beginning of the start block before doing any allocas. 
    each arguments.bindings { binding |
        continue :: local_return;
        @debug_assert(binding.kind != .Const, "tried to emit function with $const parameter");
        ty := binding.unwrap_ty();
        info := program.get_info(ty);
        name := binding.var();
        if info.stride_bytes == 0 {  // :NoRuntimeRepr
            args&.push(r = QbeConZero, type = ty, name = name, scalar = false, void = true, k = .Ke, type_index = QbeNull);
            continue();
        };
        
        _, kk := load_op(program, ty);
        scalar := kk != .Ke;
        
        op, k, a0 := @if(scalar, 
            (Qbe.O.par, kk, QbeNull),
            (Qbe.O.parc, Qbe.Cls.Kl, get_aggregate(program, shared, ty))
        );
        r := f.newtmp("a", k);
        push(f.start, make_ins(op, k, r, a0, QbeNull));
        args&.push(r = r, type = ty, name = name, scalar = scalar, void = false, k = k, type_index = a0);
    };
    args.items()
}

fn bind_args(self: *EmitIr) CRes(void) #once = {
    func := self.program.get_function(self.func);
    arg_ty := func.finished_arg.unwrap();
    args := emit_par_instructions(self.program, self.shared, func.arg&, arg_ty, self.f);
    
    // only arg instructions and allocas in the start block. 
    self.blk = self.new_block("body");
    each args { a | 
        if a.name { name | 
            mut := a.scalar && self.program.is_local_scalar(name, a.type);
            mem := if(!a.scalar || mut, => a.r) {
                mem := self.alloca(a.type);
                self.emit(self.store_op(a.type), .Kw, QbeNull, a.r, mem);
                mem
            };
            if a.void {
                self.var_lookup&.insert(name, (Alloca = QbeConZero));
            } else {
                if mut {
                    self.record_var_decl(name, (Local = mem));
                } else {
                    self.declare_var_mem(name, mem, a.type);
                };
            };
        };
    };
    if func.get_flag(.CVariadic) {
        self.f.vararg = true;
    };
    .Ok
}

// :EmitIrCall
// note: mirrors bind_args
fn compile_for_arg(self: *EmitIr, arg: *FatExpr) CRes([]Qbe.Ins) = {
    parts := arg.items();
    out := Qbe.Ins.list(parts.len, temp());
    if self.program.get_info(arg.ty)[].stride_bytes == 0 {  // :NoRuntimeRepr
        _ := @try(self.compile_expr(arg, .Discard)) return;
        return(Ok = empty());
    };
    info := self.program.get_type(arg.ty);
    f := info.Struct.fields;
    if info.is(.Struct) && info.Struct.is_tuple {
        @err_assert(f.len == parts.len, "ICE: compile_for_arg arity mismatch") return;
    } else {
        @err_assert(parts.len == 1, "ICE: compile_for_arg arity mismatch!") return;
    };
    
    enumerate parts { i, val |
        continue :: local_return;
        if val.expr&.is(.CVariadicMarker) {
            out&.push(make_ins(.argv, .Kw, QbeNull, QbeNull, QbeNull));
        } else {
            ty := val.ty; // TODO: is this ok or do you have to get it from the function's arg type?
            _, k := self.load_op(ty);
            ::if(Qbe.Ins);
            ins := if k != .Ke {  // scalar
                r := @try(self.compile_expr(val, .Scalar)) return;
                make_ins(.arg, k, QbeNull, r, QbeNull)
            } else { 
                if self.program.get_info(ty)[].stride_bytes == 0 {
                    _ := @try(self.compile_expr(val, .Discard)) return;
                    continue();
                };
                // aggregate
                // TODO: you really want to do `fn Deref(ref) => @try(self.compile_expr(ref, .Scalar)) return;`
                //       to skip an extra blit that the abi stuff will do for you. 
                //       but we don't know if a later argument expression will modify that memory. 
                r      := @try(self.compile_expr(val, .NewMemory)) return;
                r_type := self.get_aggregate(ty);
                make_ins(.argc, .Kl, QbeNull, r_type, r)
            };
            out&.push(ins);
        };
    };
    (Ok = out.items())
}

fn context_par_e(self: *EmitIr) void = {
    if self.dyn_context {
        // We might remove this instruction at the end if we decide this function can be #no_context. 
        self.context = self.f.newtmp("context", .Kl);
        self.emit(.pare, .Kl, self.context, QbeNull, QbeNull);
    } else {
        self.context = QbeNull;
    };
}

fn finalize_blocks(self: *EmitIr) void = {
    self.f.nblk = self.blocks.len.trunc();
    prev := Qbe.Blk.ptr_from_int(0);
    ok := true;
    for_rev self.blocks& { b | 
        b.link = prev;
        prev = b;
        ok = ok && b.jmp.type != .Jxxx;
    };
    if !ok {
        printfn(self.f, self.f.globals.debug_out);
    };
    @debug_assert(ok, "ICE: unterminated block");
}

fn want_syscall(self: *EmitIr, fid: FuncId, force: bool) ?i64 = {
    fn current_target_syscall_index(self: *EmitIr) i64 = {
        if(self.m.goal.os   == .macos,   => return(2));
        if(self.m.goal.os   != .linux,   => return(-1));
        if(self.m.goal.arch == .aarch64, => return(0));
        if(self.m.goal.arch == .x86_64,  => return(1));
        -1
    }
    
    func := self.program.get_function(fid);
    
    @if(self.program.get_build_options()[].use_raw_syscalls || force) 
    if choose_impl(func.body&, @const_slice(FuncImpl.Tag().Syscall)) { impl | 
        impl := impl.Syscall;
        i := self.current_target_syscall_index();
        if i >= 0 && impl[i] != (@as(i32)-1) {
            return(Some = impl[i].intcast());
        };
    };
    .None
}

fn bounce_body(self: *EmitIr, callee: Qbe.Ref, o: Qbe.O) void = { 
    func := self.program.get_function(self.func);
    f_ty := func.finished_ty().unwrap();
    c := QbeNull;
    if !func.get_flag(.NoContext) && o != .syscall {
        c = self.f.newtmp("f", .Kl);
        self.emit(.pare, .Kl, c, QbeNull, QbeNull);
    };
    args := emit_par_instructions(self.program, self.shared, func.arg&, f_ty.arg, self.f);
    if c != QbeNull {
        self.emit(.arge, .Kl, QbeNull, c, QbeNull);
    };
    each args { a | 
        if a.scalar || a.void {
            k: Qbe.Cls = @if(a.void, .Kw, a.k);
            self.emit(.arg, k, QbeNull, a.r, QbeNull);
        } else {
            self.emit(.argc, a.k, QbeNull, a.type_index, a.r);
        };
    };
    
    _, k := self.load_op(f_ty.ret); 
    type_index := QbeNull;
    if k == .Ke && self.program.get_info(f_ty.ret)[].stride_bytes != 0 {
        k = .Kl;
        type_index = self.get_aggregate(f_ty.ret);
    };
    r := QbeNull;
    if k != .Ke {
        r = self.f.newtmp("f", k);
    } else {
        k = .Kw;
    };
    self.emit(o, k, r, callee, type_index);
    self.emit_ret(r);
    self.finalize_blocks();
}

fn aot_import_body(self: *EmitIr, name: Symbol) void = {
    // :DumbNameAlias
    // TODO: use FnFlag.NoMangle instead but that doesn't work with the old backends calling the same 
    //       fmt_fn_name until they also don't try to make bounce functions.
    g := self.m.goal&;
    func := self.program.get_function(self.func);
    real_name := self.program.check_link_rename(name, func, (arch = g.arch, os = g.os));
    target := self.m.intern(real_name);
    self.bounce_body(self.f.symcon(target), .call);
}

fn emit_body(self: *EmitIr, f: FuncId) CRes(void) #once = { // #log_ir("AIR") 
    func := self.program.get_function(f);
    
    if func.get_flag(.LogAst) {
        @println("[#log_ast %] %", f, self.program.log(func));
        self.program.data.cast()[][].codemap.show_error_line(func.loc);
    };
    m := self.shared.m;
    
    self.f.start = self.new_block("start");
    self.blk = self.f.start;

    if self.want_syscall(self.func, false) { syscall_number |
        self.bounce_body(self.f.getcon(syscall_number), .syscall);
        return(.Ok);
    };
    
    T :: FuncImpl.Tag();
    prio_amd :: @const_slice(T.NewIntrinsic, T.Normal, T.Intrinsic, T.ComptimeAddr, T.TargetOsSplit, T.X86AsmBytes, T.Redirect, T.DynamicImport);
    prio_arm :: @const_slice(T.NewIntrinsic, T.Normal, T.Intrinsic, T.ComptimeAddr, T.TargetOsSplit, T.JittedAarch64, T.Redirect, T.DynamicImport);
    impl := or choose_impl(func.body&, @if(self.m.goal.arch == .aarch64, prio_arm, prio_amd)) {
        if self.want_syscall(self.func, true) { syscall_number |
            self.bounce_body(self.f.getcon(syscall_number), .syscall);
            return(.Ok);
        };
        
        return(@err("no acceptable impl %", self.program.log(func)))
    };
    
    ::enum(Intrinsic);
    body: *FatExpr = @match(impl) {
        fn Normal(body) => body;
        fn Intrinsic(op) => {
            // Direct calls will be inlined but someone might be trying to call through a function pointer. 
            // TODO: don't add to callees for direct calls. -- Jul 24
            if op.bc == .GetContext {
                self.context_par_e();
            };
            a0, a1, ret := self.intrinsic_shim(func);
            self.blk.jmp.arg = @try(self.emit_intrinsic_old(op, .ScalarTransient, @slice(a0, a1), func, ret)) return;
            self.finalize_blocks();
            return(.Ok)
        }
        fn NewIntrinsic(op) => {
            if (@is(@as(Qbe.O) op.ir_op, .vastart, .cas0, .cas1, .sel0, .sel1, .blit0, .blit1)) {
                self.blk.jmp.type = .hlt;
                self.finalize_blocks();
                return(.Ok);
            };
            a0, a1, ret := self.intrinsic_shim(func);
            self.blk.jmp.arg = @try(self.emit_intrinsic_new(op, .ScalarTransient, @slice(a0, a1), func, ret)) return;
            self.finalize_blocks();
            return(.Ok)
        }
        fn ComptimeAddr(it) => {
            @match(self.when) {
                fn Aot() => self.aot_import_body(func.name);
                fn Jit() => {
                    self.entry.task = (JitImport = (lnk = self.f.lnk&, addr = rawptr_from_int(it[])));
                }
            };
            return(.Ok)
        }
        // copy-paste from above
        fn DynamicImport(name) => {
            @err_assert(self.when == .Aot, "we hit a dynamicimport with no comptimeaddr for jit. the frontend should make sure this doesn't happen") return;
            self.aot_import_body(name[]);
            return(.Ok)
        };
        fn TargetOsSplit(_) => {
            @debug_assert(func.body&.is(.Merged), "ICE: #target_os split expected multiple");
            each func.body.Merged& { it |
                if it.is(.TargetOsSplit) && it.TargetOsSplit.os == self.m.goal.os {
                    fid := it.TargetOsSplit.fid;
                    callee := self.program.get_function(fid);
                    if !callee.get_flag(.NoContext) {
                        func.set_flag(.YesContext);
                    };
                    self.bounce_body(self.func_ref(fid), .call);
                    self.pending.push(fid);
                    return(.Ok)
                };
            };
            return(@err("#target_os missing implementation"))
        }
        fn Redirect(fid) => {
            callee := self.program.get_function(fid[]);
            if !callee.get_flag(.NoContext) {
                func.set_flag(.YesContext);
            };
            self.bounce_body(self.func_ref(fid[]), .call);
            self.pending.push(fid[]);
            return(.Ok)
        } 
        // TODO: use :ExprLevelAsm so you don't have to spill everything, 
        //       but that requires letting people annotate register usage.  -- Dec 3
        fn JittedAarch64(code) => { // TODO: just store this as bytes
            @debug_assert(self.m.goal.arch == .aarch64);
            self.entry.task = (Asm = (lnk = self.f.lnk&, bytes = code.items().interpret_as_bytes()));
            return(.Ok)
        }
        fn X86AsmBytes(bytes) => {
            @debug_assert(self.m.goal.arch == .x86_64);
            self.entry.task = (Asm = (lnk = self.f.lnk&, bytes = bytes.items()));
            return(.Ok)
        }
        @default => panic("TODO: unhandled FuncImpl");
    };
   
    self.context_par_e();
    // note: this rewrites self.blk
    @try(self.bind_args()) return;
    
    // Note: this is different from the body expr type because of early returns.
    ret := self.program.get_function(f)[].finished_ret.unwrap();
    _, k := self.load_op(ret); 
    scalar := k != .Ke;
    
    self.f.start.jmp.type = .jmp;
    self.f.start.s1 = self.blk;
    p1: Placement = @if(scalar, .Scalar, .NewMemory);
    if self.program.get_info(ret)[].stride_bytes == 0 {  // :NoRuntimeRepr
        p1 = .Discard;
    };
    result := @try(self.compile_expr(body, p1)) return;
    
    self.emit_ret(result);
    
    has_context := func.get_flag(.YesContext) && !func.get_flag(.NoContext);  // :copy-pastrae
    if !has_context && self.dyn_context {
        // :SLOW
        self.f.start.ins.slice(0, self.f.start.nins.zext() - 1).copy_from(self.f.start.ins.slice(1, self.f.start.nins.zext()));
        if self.context != QbeNull {
            self.f.start.ins[self.f.start.nins.zext() - 1] = make_ins(.copy, .Kl, self.context, QbeConZero, QbeNull);  // TODO: why?
        } else {
            self.f.start.nins -= 1;
        };
    };
    self.finalize_blocks();
    
    .Ok
}

fn emit_ret(self: *EmitIr, result: Qbe.Ref) void = {
    ret := self.program.get_function(self.func)[].finished_ret.unwrap();
    _, k := self.load_op(ret); 
    scalar := k != .Ke;

    self.blk.jmp.arg = result;
    @if_else {
        @if(self.blk.jmp.type != .Jxxx) => {
            // TODO: assert ret=Never?
        };
        @if(ret.is_never()) => {
            self.blk.jmp.type = .hlt;
        };
        @if(self.program.get_info(ret)[].stride_bytes == 0) => {
            self.blk.jmp.type = .ret0;
        };
        @if(scalar) => {
            @debug_assert_ne(result, QbeNull, "missing return value");
            self.f.retty = -1;
            // TODO: tell it about b/h so we can fully follow the c abi :FUCKED but the old one didn't do it either. 
            self.blk.jmp.type = ret_op(k);
        };
        @else => {
            @debug_assert_ne(result, QbeNull, "missing return value");
            self.f.retty = self.get_aggregate(ret).val().trunc();
            self.blk.jmp.type = .retc;
        };
    };
}

fn intrinsic_shim(self: *EmitIr, func: *Func) Ty(Qbe.Ref, Qbe.Ref, Type) = {
    arg := func.arg.bindings.items();
    @debug_assert(arg.len == 1 || arg.len == 2);
    
    f_ty := func.finished_ty().unwrap();
    arg_ty := self.program.arg_types(f_ty.arg);
    
    _, k0 := self.load_op(arg_ty[0]);
    a0 := self.f.newtmp("a", k0);
    if k0 != .Ke {
        self.emit(.par, k0, a0, QbeNull, QbeNull);
    } else {
        a0 = QbeNull;
    };
    a1 := QbeNull;
    // :DoFoldInvalidArity 
    // `!= void` makes do_fold() work. it wants to pass extra void args to a #ir function.
    // TODO: allow any zero sized types as trailing args?
    if arg.len == 2 && arg_ty[1] != void {
        _, k1 := self.load_op(arg_ty[1]);
        a1 = self.f.newtmp("a", k1);
        self.emit(.par, k1, a1, QbeNull, QbeNull);
    };
    self.f.retty = -1;
    _, kr := self.load_op(f_ty.ret);
    self.blk.jmp.type = ret_op(kr);
    (a0, a1, f_ty.ret)
}

fn ret_op(k: Qbe.Cls) Qbe.J = 
    @as(Qbe.J) @as(i32) Qbe.J.retw.raw() + k.raw()

// :EmitIrCall TODO: it would be nice if i didn't need to repeat so many variations of the par/arg/call/ret logic. 
fn emit_runtime_call(
    self: *EmitIr,
    f_ty: FnType,
    args: []Qbe.Ins,
    p: Placement,
    callee: Qbe.Ref,
    dyn_context: bool,
    loc: Span,
) CRes(Qbe.Ref) = { 
    if self.debug {
        n: i64 = loc.low.zext();
        hi, lo := (n.shift_right_logical(16), n.bit_and(1.shift_left(16) - 1));
        self.emit(.dbgloc, .Kw, QbeNull, INT(hi), INT(lo));  
    };
    
    self.f.leaf = false;
    if dyn_context && self.context != QbeNull {
        @debug_assert(self.dyn_context && self.dyn_context, "tried to dyn context with static context");
        self.emit(.arge, .Kl, QbeNull, self.context, QbeNull);
    };
    cast: i64 = self.blk.nins.zext();  // TODO: ugh
    append(self.blk.ins&, cast&, args.ptr, args.ptr.offset(args.len));
    self.blk.nins = cast.trunc();
    _, k := self.load_op(f_ty.ret);
    scalar := k != .Ke;
    type_r := QbeNull;
    info := self.program.get_info(f_ty.ret);
    result_r := if info.stride_bytes == 0 || (scalar && p&.is(.Discard)) {  // :get_or_create_type
        k = .Kw;
        QbeNull
    } else {
        if !scalar {
            k = .Kl;
            type_r = self.get_aggregate(f_ty.ret);
            if p&.is(.Discard) {
                p = .NewMemory;
            };
        };
        self.scalar_dest(p, k)
    };
    self.emit(.call, k, result_r, callee, type_r);
    self.hlt_if_never(f_ty.ret);
    
    if(result_r == QbeNull, => return(Ok = QbeNull));
    if scalar { 
        return(Ok = self.scalar_result(p, result_r, f_ty.ret));
    };
    (Ok = @match(p) {
        fn NewMemory() => result_r;
        fn Discard()   => QbeNull;
        fn Blit(dest) => {
            self.blit(dest, result_r, info.stride_bytes.zext());
            dest
        }
        @default => return(@err("ICE: tried to assign aggragate call to scalar placement"));
    })
}

fn is_local_scalar(self: CompCtx, v: Var, ty: Type) bool #inline = {
    info := self.get_info(ty);
    info.size_slots == 1 && info.stride_bytes <= 8 && !{self.vtable.took_address}(self.data, v&)
}

fn compile_stmt(self: *EmitIr, stmt: *FatStmt) CRes(void) = {
    if self.debug && !(stmt.stmt&.is(.Eval) && stmt.stmt.Eval.expr&.is(.Block)) {
        //place := self.program.get_whole_line(stmt.loc); // :SLOW
        //if self.last_line != place.line {
        //    // TODO: dbgfile too because it might be something inlined from elsewhere. 
        //    // TODO: have the backend remove consecutive dbgloc instructions and just keep the last one 
        //    //       when all the stuff between them was optimised out. 
        //    self.emit(.dbgloc, .Kw, QbeNull, INT(place.line), INT(0));  
        //    self.last_line = place.line;
        //};
    };
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Eval(expr) => {
            @try(self.compile_expr(expr, .Discard)) return;
            .Ok
        }
        fn Decl(f) => {
            @debug_assert_ne(f.kind, VarType.Const);
            @debug_assert(f.name&.is(.Var));
            @debug_assert(f.ty&.is(.Finished), "variable not typechecked");
            type := f.ty.Finished;
            if self.program.is_local_scalar(f.name.Var, type) {
                val  := @try(self.compile_expr(f.default&, .Scalar)) return;
                if rtype(val) != .RTmp {
                    _, k := self.load_op(type);
                    var := self.f.newtmp("v", k);
                    self.emit(.copy, k, var, val, QbeNull);
                    val = var;
                };
                self.record_var_decl(f.name.Var, (Local = val));
            } else {
                ptr  := @try(self.compile_expr(f.default&, .NewMemory)) return;
                self.declare_var_mem(f.name.Var, ptr, type);
            };
            .Ok
        }
        fn Set(f) => {   // :PlaceExpr
            // we care about the type of the pointer, not the value because there might be a cast.
            dest := @match(f.place.expr&) {
                fn GetVar(it) => {
                    info := self.var_lookup&.get_ptr(it[]) || return(@err("missing runtime var %", self.program.get_string(it.name)));
                    @if_let(info) fn Local(dest) => {
                        @try(self.compile_expr(f.value&, (Assign = dest[]))) return;
                        return(.Ok);
                    };
                    @try(self.addr_macro(f.place&, .Scalar)) return
                }
                fn Deref(arg) => @try(self.compile_expr(arg[], .Scalar)) return;
                @default => return(@err("TODO: other `place=e;` :("));
            };
            @try(self.compile_expr(f.value&, (Blit = dest))) return;
            .Ok
        }
        fn DeclVarPattern(f) => {
            simple := f.binding.bindings.len == 1 || f.value.expr&.is(.Tuple);
            if simple { 
                // This is the pattern we generate for capturing calls. 
                parts := f.value&.items();
                @err_assert(parts.len == f.binding.bindings.len, "ICE: DeclVarPattern tuple size mismatch") return;
                enumerate parts { i, value |
                    b   := f.binding.bindings[i]&;
                    ptr := @try(self.compile_expr(value, .NewMemory)) return;
                    @debug_assert(b.ty&.is(.Finished), "variable not typechecked");
                    if b.var() { name |
                        self.declare_var_mem(name, ptr, b.ty.Finished);
                    };
                };
            } else {
                // it's a destructuring (not inlined args)
                // We store the whole value in a stack slot and then save pointers into different offsets of it as thier own variables.
                
                base := @try(self.compile_expr(f.value&, .NewMemory)) return;
                info := self.program.get_type(f.value.ty);
                @err_assert(info.is(.Struct), "destructure must be tuple") return;
                fields := info.Struct.fields&;
                @debug_assert_eq(fields.len, f.binding.bindings.len, "destructure size mismatch");
                enumerate f.binding.bindings.items() { i, b |
                    f       := fields[i]&;
                    name    := @unwrap(b.var(), "tuple binding requires name") return;
                    element := self.offset(base, f.byte_offset);
                    @debug_assert(b.ty&.is(.Finished), "variable not typechecked");
                    self.declare_var_mem(name, element, b.ty.Finished);
                };
            };
            .Ok
        };
        fn Noop() => .Ok;
        // Can't hit DoneDeclFunc because we don't re-eval constants.
        @default => @err("ICE: stmt not desugared %", self.program.log(stmt));
    }
}

fn compile_expr(self: *EmitIr, expr: *FatExpr, p: Placement) CRes(Qbe.Ref) = {
    @debug_assert(!expr.ty.is_unknown(), "Not typechecked: %", self.program.log(expr));
    self.last_loc = expr.loc;
    if self.blk.jmp.type == .hlt {  // TODO: do we need this or do we trust sema to always get rid of things after a Never? 
        return(Ok = QbeNull);
    };

    expr_ty := expr.ty;
    (Ok = @match(expr.expr&) {
        fn Cast(v)  => {
            // TODO: bleh, make the front end check this? 
            from : i64 = self.program.get_info(v.ty)[].stride_bytes.zext();
            to   : i64 = self.program.get_info(expr_ty)[].stride_bytes.zext();
            @err_assert(from == to, "@as size mismatch: % vs %", self.program.log(v.ty), self.program.log(expr_ty)) return;
            return(self.compile_expr(v[], p))
        }
        fn Call(f)    => return(self.emit_call(f.f, f.arg, p));
        fn Block(f)   => return(self.emit_block_expr(expr, p));
        fn Value(f)   => return(self.emit_constant(f.bytes&, p, expr.ty));
        fn If(_)      => return(self.emit_call_if(expr, p));
        fn Loop(arg)  => return(self.emit_call_loop(arg[]));
        fn Addr(arg)  => return(self.addr_macro(arg[], p));
        fn StructLiteralP(pattern) => return(self.construct_aggregate(pattern, expr.ty, p));
        fn Slice(arg) => {
            if(p&.is(.Discard), => return(self.compile_expr(arg[], .Discard)));
            
            container_ty := arg.ty;
            // Note: number of elements, not size of the whole array value.
            ty, count := @match(arg.expr) {
                fn Tuple(parts) Ty(Type, i64) => { 
                    fst := parts[0];
                    (fst.ty, parts.len) 
                };
                @default => (arg.ty, 1);
            };
            ptr  := @try(self.compile_expr(arg[], .NewMemory)) return;
            dest := self.get_memory(p, expr_ty);
            self.emit(.storel, .Kw, QbeNull, ptr, dest);
            self.emit(.storel, .Kw, QbeNull, self.f.getcon(count), self.offset(dest, 8));
            dest
        };
        fn GetVar(it) => {
            info := self.var_lookup&.get_ptr(it[]) || return(@err("missing runtime var %", self.program.get_string(it.name)));
            @if_let(info) fn Local(dest) => {
                if !p&.is(.ScalarTransient) {  // try to avoid a bunch of redundent copies. 
                    self.unify_placement(p&, expr_ty);
                };
                return(Ok = self.scalar_result(p, dest[], expr_ty));
            };
            src := @try(self.addr_macro(expr, .ScalarTransient)) return; // get the pointer
            self.deref(src, p, expr_ty)
        }
        fn DataSymbol(name) => {
            name := self.program.get_string(name[]);
            self.scalar_result(p, self.f.symcon(name), i64)  // :get_or_create_type
        }
        fn Deref(arg) => {
            src := @try(self.compile_expr(arg[], @if(p&.is(.Discard), .Discard, .Scalar))) return; // get the pointer
            self.deref(src, p, expr_ty)
        };
        fn FnPtr(arg) => {
            f := @unwrap(arg[].as_const(), "expected fn for ptr") return;
            f := FuncId.assume_cast(f&)[];
            callee_r := self.func_ref(f);
            self.scalar_result(p, callee_r, i64)  // :get_or_create_type
        };
        fn Unreachable() => {
            self.hlt_if_never(Never); // :get_or_create_type
            QbeConZero
        }
        fn Uninitialized() => {
            @err_assert(!expr_ty.is_never(), "call exit() to produce a value of type 'Never'") return;
            // Wierd special case I have mixed feelings about. should at least set to sentinal value in debug mode.
            // TODO: if(opts.zero_init_memory, => self.zero_memory_at_top_of_stack(expr_ty));
            @match(p) {
                fn Scalar()          => QbeConZero;
                fn ScalarTransient() => QbeConZero;
                fn Blit(it)    => it;
                fn NewMemory() => self.alloca(expr_ty);
                fn Discard()   => QbeNull;
                fn Assign(it)  => it;
            }
        };
        fn Tuple(values) => {
            if values.len == 1 {
                return(self.compile_expr(values[0]&, p));
            };
            @err_assert(!(p&.is(.ScalarTransient) || p&.is(.Scalar) || p&.is(.Assign)), "ICE: non single field struct as scalar") return;
            raw := self.program.raw_type(expr.ty); // TODO: do i need this? 
            @match(self.program.get_type(raw)) {
               fn Struct(f) => {
                    @err_assert(f.layout_done, "ICE: struct layout not ready.") return;
                    @assert_eq(f.fields.len, values.len, "Expr::Tuple field count");
                    @err_assert(f.is_tuple, "Expr::Tuple can only create tuple type") return;
                    base := self.get_memory(p, expr_ty);
                    range(0, f.fields.len) { i |
                        dest := self.offset(base, f.fields[i].byte_offset);
                        @try(self.compile_expr(values[i]&, (Blit = dest))) return;
                    };
                    base
                }
                fn Array(f) => {
                    @debug_assert_eq(values.len(), f.len.zext(), "array len");
                    element_size: i64 = self.program.get_info(f.inner)[].stride_bytes.zext();
                    base := self.get_memory(p, expr_ty);
                    mem  := base;
                    each values { value |
                        @try(self.compile_expr(value, (Blit = mem))) return;
                        mem = self.offset(mem, element_size);
                    };
                    mem
                }
                @default => return(@err("Expr::Tuple should have struct type not %. (0/1 element tuple maybe?)", self.program.log(raw)));
            }
        }
        fn PtrOffset(f) => {
            // TODO: compiler has to emit tagchecks for enums now! but it does not!
            base  := @try(self.compile_expr(f.ptr, .ScalarTransient)) return;
            field := self.offset(base, f.bytes);
            self.scalar_result(p, field, i64) // :get_or_create_type
        }
        fn Switch(f) => return(self.emit_switch(expr, p));
        @default => @panic("ICE: didn't desugar: %", self.program.log(expr));
    })
}

fn deref(self: *EmitIr, src: Qbe.Ref, p: Placement, expr_ty: Type) Qbe.Ref = {
    value_type := expr_ty;
    self.hlt_if_never(expr_ty);
    size: i64 = self.program.get_info(value_type)[].stride_bytes.zext();
    if(p&.is(.Discard) || size == 0, => return(QbeConZero)); // :NoRuntimeRepr
    if p&.is(.ScalarTransient) || p&.is(.Scalar) || p&.is(.Assign) {
        @debug_assert(size <= 8);
        o, k := self.load_op(expr_ty);
        r := self.scalar_dest(p, k);
        self.emit(o, k, r, src, QbeNull);
        return(self.scalar_result(p, r, expr_ty));
    };
    dest := self.get_memory(p, value_type);
    self.blit(dest, src, size);
    dest
}

fn guess_can_assign(e: *FatExpr) bool = {
    if (@is(e.expr&, .Value, .GetVar, .String)) {
        return(false)
    };
    
    // These all have a single inner expression as the first field. 
    // TODO: probably a bad idea to be this fragile with repr. 
    if (@is(e.expr&, .PtrOffset, .Cast, .Loop, .Addr, .Quote, .Slice, .Deref, .FnPtr)) {
        return(guess_can_assign(e.expr.Deref));
    };
    
    // TODO: this could recurse on call/tuple/struct, etc and get more. not sure if it's worth it. variable reads are the most offensive. 
    true
}

fn emit_call(self: *EmitIr, f: *FatExpr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    caller := self.program.get_function(self.func);
    @match(self.program.get_type(f.ty)) {
        fn Fn(f_ty) => {
            f_id := @unwrap(f.as_const(), "ice: tried to call non-const fn %", self.program.log(f)) return;
            f_id := FuncId.assume_cast(f_id&)[];
            func := self.program.get_function(f_id);
            cc := func.cc.unwrap();
            @err_assert(!func.get_flag(.Generic), "tried to emit call to unlowered #generic") return;
            @debug_assert(!func.get_flag(.MayHaveAquiredCaptures), "tried to emit call to unlowered maybe '=>'");
            @assert(cc != .Inline, "ICE: tried to call inlined %", self.program.get_string(func.name));

            // TODO: ideally the redirect should just be stored in the overloadset so you don't have to have the big Func thing every time.
            // TODO: audit: is this f_ty different from the one we just got from the expression type? -- Jul 8
            f_ty := func.finished_ty().unwrap(); // kinda HACK to fix unaligned store? 
            original_f_id := f_id;
            f_id = self.program.follow_redirects(f_id);
            callee := self.program.get_function(f_id);  // fid might have changed. 
           
            callee_r := self.func_ref(f_id);
            
            T :: FuncImpl.Tag();
            prio :: @const_slice(T.NewIntrinsic, T.Intrinsic, T.DynamicImport);
            
            @if(self.want_syscall(f_id, false).is_none())
            if choose_impl(func.body&, prio) { impl | 
                @match(impl) {
                    fn NewIntrinsic(op) => {
                        arg := arg.items();
                        a0 := if arg[0].ty == void {
                            QbeNull
                        } else {
                            p1: Placement = @if(arg.len != 1 && guess_can_assign(arg[1]&), .Scalar, .ScalarTransient);
                            @try(self.compile_expr(arg[0]&, p1)) return
                        };
                        // the || void is for :DoFoldInvalidArity
                        a1 := if(arg.len == 1 || arg[1].ty == void, => QbeNull, => @try(self.compile_expr(arg[1]&, .ScalarTransient)) return);
                        return(self.emit_intrinsic_new(op, p, @slice(a0, a1), caller, f_ty.ret));
                    }
                    fn Intrinsic(op) => {
                        arg := arg.items();
                        a0 := @try(self.compile_expr(arg[0]&, .Scalar)) return;
                        a1 := if(arg.len == 1, => QbeNull, => @try(self.compile_expr(arg[1]&, .ScalarTransient)) return);
                        return(self.emit_intrinsic_old(op, p, @slice(a0, a1), caller, f_ty.ret));
                    }
                    fn DynamicImport(name) => if self.when == .Aot {
                        g := self.m.goal&;
                        // TODO: should cache #link_rename because it's weird if i call the user's function a billion times. 
                        real_name := self.program.check_link_rename(name[], func, (arch = g.arch, os = g.os));
                        callee_r = self.f.symcon(self.m.intern(real_name));
                    };
                    @default => unreachable();
                };
            };
            
            if !caller.get_flag(.ComptimeOnly) && original_f_id == f_id {
                // TODO: it would be nice if this was at the beginning and you never tried to emit anything that was comptimeonly
                //       that requires never adding them to callees and then later realizing you can inline them, which would make sense. -- Aug 29
                @debug_assert(!(self.when == .Aot && callee.get_flag(.ComptimeOnly)), "cannot .Aot .ComptimeOnly % %", f_id, self.program.log(callee));
            };
            context := !callee.get_flag(.NoContext);
            if context {
                caller.set_flag(.YesContext);
            };
            
            args := @try(self.compile_for_arg(arg)) return;
            self.emit_runtime_call(f_ty, args, p, callee_r, context, arg.loc)
        }
        fn FnPtr(f_ty) => {
            caller.set_flag(.YesContext);
            callee := @try(self.compile_expr(f, .Scalar)) return;
            args   := @try(self.compile_for_arg(arg)) return;
            self.emit_runtime_call(f_ty.ty, args, p, callee, true, arg.loc)
        }
        fn Label(_) => {
            label := @unwrap(f.as_const(), "called label must be const") return;
            label := LabelId.assume_cast(label&)[];

            ret := self.inlined_return_addr&.get_ptr(label);
            ret := @unwrap(ret, "missing return label. forgot '=>' on function?") return;
            ret.used = true;  // note: updating the one in the map! not a copy
            
            result := @try(self.compile_expr(arg, ret.p)) return;
            seal self { b |
                b.jmp.type = .jmp;
                b.s1 = ret.blk;
            };
            (Ok = QbeNull)
        }
        @default => (@err("ICE: non callable: %", self.program.log(f)));
    }
}

fn emit_intrinsic_new(self: *EmitIr, op: *NewIntrinsicPayload, p: Placement, arg: []Qbe.Ref, caller: *Func, expr_ty: Type) CRes(Qbe.Ref) = {
    @debug_assert(op.ir_op != 0, "no ir op");
    o := @as(Qbe.O) op.ir_op;
    k := @as(Qbe.Cls) op.ir_cls;
    r := self.scalar_dest(p, k); 
    if r == QbeNull && !no_result(o) {
        r = self.f.newtmp("f", k);
    };
    self.emit(o, k, r, arg[0], arg[1]);
    (Ok = self.scalar_result(p, r, expr_ty))
}

fn emit_intrinsic_old(self: *EmitIr, op: *IntrinsicPayload, p: Placement, arg: []Qbe.Ref, caller: *Func, expr_ty: Type) CRes(Qbe.Ref) = {
    r := @match(op.bc) {
        fn GetContext() => {
            if self.dyn_context {
                caller.set_flag(.YesContext);  // TODO: you might have setcontext for yourself
                self.unify_placement(p&, expr_ty); // because we don't know if they'll set again before using. meh. 
                self.context
            } else {
                if self.when == .Jit {
                    // We want the jitted program to share the context with the compiler, 
                    // but we can't access the address of the compiler's static context slot, 
                    // so we just patch in a call to the compiler's own GetContext. 
                    // It would be correct to do it this way when using implicit_context, 
                    // but it's sad to be so wasteful when we could just read the register directly.  -- Nov 23
                    self.jit_call_1to1(@as(rawptr) builtin_get_dynamic_context, arg[0])
                } else {
                    context_var := self.static_context_var.expect("have static_context_var for aot");
                    self.reference_constant(context_var);
                    ref := self.f.symcon(@tfmt("g%", context_var.id));
                    context_val := self.f.newtmp("context", .Kl);
                    self.emit(.load, .Kl, context_val, ref, QbeNull);
                    context_val
                }
            }
        }
        fn SetContext() => if self.dyn_context {
            if self.context == QbeNull {
                self.context = self.f.newtmp("context", .Kl);
            };
            self.emit(.copy, .Kl, self.context, arg[0], QbeNull);
            QbeConZero
        } else {
            if self.when == .Jit {
                // see GetContext case
                self.jit_call_1to1(@as(rawptr) set_dynamic_context, arg[0])
            } else {
                context_var := self.static_context_var.expect("have static_context_var for aot");
                self.reference_constant(context_var);
                ref := self.f.symcon(@tfmt("g%", context_var.id));
                self.emit(.storel, .Kw, QbeNull, arg[0], ref);
                QbeConZero
            }
        };
        fn BitNot() => {
            if(p&.is(.Discard), => return(Ok = QbeNull));
            // TODO: .Kw
            r := self.scalar_dest(p, .Kl);
            self.emit(.xor, .Kl, r, arg[0], self.f.getcon(-1));
            r
        }
        @default => panic("unchandled old intrinsic");
    };
    (Ok = self.scalar_result(p, r, expr_ty))
}

fn jit_call_1to1(self: *EmitIr, callee: rawptr, arg: Qbe.Ref) Qbe.Ref = {
    result := self.f.newtmp("c", .Kl);
    callee := self.f.getcon(callee.int_from_rawptr());
    self.emit(.arg, .Kl, QbeNull, arg, QbeNull);
    self.emit(.call, .Kl, result, callee, QbeNull);
    result
}

fn func_ref(self: *EmitIr, f_id: FuncId) Qbe.Ref #inline = {
    f_id := self.program.follow_redirects(f_id);
    self.pending.push(f_id); // HACK
    self.f.symcon(self.fmt_fn_name(f_id))
}

fn emit_block_expr(self: *EmitIr, expr: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    block   := expr.expr.Block&;
    ret_var := or block.ret_label {
        // Simple case: its just grouping some statements into an expression. 
        each block.body& { stmt |
            @try(self.compile_stmt(stmt)) return;
        };
        return(self.compile_expr(block.result, p))
    };
    
    // :block_never_unify_early_return_type
    // Note: block_ty can be different from value.ty if the fall through is a Never but there's an early return to the block. 
    block_ty := expr.ty;
    
    // It might have an early return so we want to unify that up here and then the Placement system can deal with it. 
    dest := self.unify_placement(p&, block_ty);
    out  := self.program.get_info(block_ty);

    return_block_index := self.blocks.len;
    ret: ReturnTarget = (
        blk = self.new_block("local"),
        p = p,
        used = false,
    );
    prev := self.inlined_return_addr&.insert(ret_var, ret);
    @debug_assert(prev.is_none(), "stomped ret var");

    each block.body& { stmt | 
        // Note: these are allowed to do a local_return targeting `ret`
        @try(self.compile_stmt(stmt)) return;
    };
    _ := @try(self.compile_expr(block.result, p)) return;

    ret := self.inlined_return_addr&.remove(ret_var).unwrap();
    if ret.used {
        // They did a local_return, so our extra block was useful. 
        seal self { b |
            // There was also a fallthough expression which we convert to a local_return.
            b.s1 = ret.blk;
            b.jmp.type = .jmp;
        };
        self.blk = ret.blk;
    } else {
        // They didn't actually use a local_return so we can discard this useless block. 
        // TODO: this is wrong when nested
        //junk := self.blocks&.unordered_remove(return_block_index); 
        //@debug_assert(ret.blk.identical(junk.unwrap()));
        ret.blk.jmp.type = .hlt;
        // We did generate slightly more noisy code by `unify_placement` earlier than necessary. 
    };
    (Ok = dest)
}

fn emit_constant(self: *EmitIr, value: *Values, p: Placement, expr_ty: Type) CRes(Qbe.Ref) #once = {
    if(p&.is(.Discard), => return(Ok = QbeNull));  // TODO: bake_relocatable_value side effect? 
    info := self.program.get_info(expr_ty);
    
    
    // :SLOW all_zeroes is redundant with emit_relocatable_constant but for now we can't emit_relocatable_constant_body on something
    // with small fields + pointers, but its fine to emit as bytes if all zeros. 
    all_zero := value.all_zeroes();
    
    use_int_literal := value.is(.Small) && (!info.contains_pointers() || self.when == .Jit || all_zero) && store_by_size[value.Small._1.zext()] != .Oxxx;
    if use_int_literal {
        v := self.f.getcon(value.Small._0);
        return(Ok = self.scalar_result(p, v, expr_ty));
    };
    
    // TODO: zeroed(T) doesn't get here because it's a runtime load not a comptime one. 
    //      need to fix sema stuff to make that work.  -- Dec 5
    
    if all_zero {
        dest := self.get_memory(p, expr_ty);
        size := value.len();
        off := 0;
        z := self.f.getcon(0);  // QbeConZero
        each blit_op_table { o |
            while => size >= o.size {
                d := self.offset(dest, off);
                self.emit(o.store, .Kw, QbeNull, z, d);
                size -= o.size;
                off  += o.size;
            };
        };
        return(Ok = dest);
    };
    
    @match(self.when) {
        fn Aot() => {
            if !info.contains_pointers() {
                id := @try(self.program.emit_relocatable_constant(expr_ty, value.bytes())) return;
                self.reference_constant(id);
                dest := self.get_memory(p, expr_ty);
                src := self.as_ref(AddrOf = id);
                self.blit(dest, src, info.stride_bytes.zext());
                return(Ok = dest);
            };
            to_zero: ReadBytes = (bytes = value.bytes(), i = 0);
            self.program.zero_padding(expr_ty, to_zero&);  // Needed for reproducible builds!
            
            out  := @try({self.program.vtable.emit_relocatable_constant_body}(self.program.data, value.bytes(), expr_ty, false)) return;
            if out.len == 1 {
                v := self.as_ref(out[0]);
                return(Ok = self.scalar_result(p, v, expr_ty));
            };
            dest := self.get_memory(p, expr_ty); 
            enumerate out { i, part | 
                r := self.as_ref(part[]);
                self.emit(.storel, .Kw, QbeNull, r, self.offset(dest, i * 8));
            };
            (Ok = dest)
        }
        fn Jit() => {
            // TODO: this will generate garbage code for slices
            src := self.f.getcon(value.jit_addr());
            dest := self.get_memory(p, expr_ty);
            @debug_assert_eq(value.len(), info.stride_bytes.zext(), "value size mismatch");
            self.blit(dest, src, info.stride_bytes.zext());
            (Ok = dest)
        }
    }
}

fn all_zeroes(value: *Values) bool = 
    (value.is(.Small) && value.Small._0 == 0) || value.bytes().all_zeroes();

fn all_zeroes(b: []u8) bool = {
    for b { b |
        if(b != 0, => return(false));
    };
    true
}

fn reference_constant(self: *EmitIr, id: BakedVarId) void = {
    shared := self.shared;
    seen := shared.constants_used&;
    if !seen.get(id.id.zext()) {
        seen.set(id.id.zext());
        _, value := {self.program.vtable.get_baked}(self.program.data, id)[];
        @if_let(value) fn VoidPtrArray(it) => each it { it | 
            @match(it) {
                fn AddrOf(it) => self.reference_constant(it[]);
                fn FnPtr(it) => self.pending.push(it[]);  // HACK
                @default => ();
            };
        };
        self.new_constants&.push(id);
    };
}

fn as_ref(self: *EmitIr, part: BakedEntry) Qbe.Ref = @match(part) {
    // TODO: non-8 byte fields
    fn Num(f) => self.f.getcon(f.value);
    fn FnPtr(f)   => self.func_ref(f);
    fn AddrOf(id) => {
        self.reference_constant(id);
        self.f.symcon(@tfmt("g%", id.id))  // change intrinsic.context as well if you change this
    }
};

// :PlaceExpr
fn addr_macro(self: *EmitIr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    self.last_loc = arg.loc;
    // field accesses should have been desugared.
    @err_assert(arg.expr&.is(.GetVar), "took address of r-value") return;
    var := arg.expr.GetVar;
    @assert_ne(var.kind, .Const, "Cannot take address of constant % \n(use @static if you want to create a non-threadsafe mutable global)", var&.log(self.program));
    ref := @unwrap(self.var_lookup&.get_ptr(var), "Missing var % (in !addr) \n(missing $ when used in const context or missing '=>'? TODO: better error message)", var&.log(self.program)) return;
    @err_assert(ref.is(.Alloca), "addr_macro of scalar local `%`", self.program.get_string(var.name)) return;
    (Ok = self.scalar_result(p, ref.Alloca, i64))
}

fn emit_call_if(self: *EmitIr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    @debug_assert(arg.expr&.is(.If));
    parts := arg.expr.If&;
    
    cond := @try(self.compile_expr(parts.cond, .ScalarTransient)) return;
    t_expr, f_expr := (parts.if_true, parts.if_false);
    
    is_trivial :: fn(self: *EmitIr, e: *FatExpr) bool = {
        if (@is(e.expr&, .Value, .GetVar, .Addr, .FnPtr)) {
            return(true);
        };
        @if_let(e.expr&) fn Block(it) => {
            return(it.body.len == 0 && is_trivial(self, it.result));
        };
        // this doesn't really help because of the arg variables after inlining && and || 
        //@if_let(e.expr&) fn If(it) => {
        //    return(self.is_trivial(it.cond) && self.is_trivial(it.if_true) && self.is_trivial(it.if_false));
        //};
        false
    };
    
    if arg.ty != void {  // :get_or_create_type
        _, k := load_op(self, arg.ty);
        use_sel := k != .Ke && k.is_int() && self.is_trivial(t_expr) && self.is_trivial(f_expr);
        if use_sel {
            // If both sides are very cheap to evaluate, just do a cmov instead of creating a bunch of new blocks. 
            // This is not required for correctness, it's just a way to give the backend less work to do. 
            
            t_val := @try(self.compile_expr(t_expr, .Scalar)) return;
            f_val := @try(self.compile_expr(f_expr, .Scalar)) return;
            
            if(p&.tag() == .Discard, => return(Ok = QbeNull));
            r := self.scalar_dest(p, k);
            self.emit(.sel0, .Kw, QbeNull, cond, QbeNull);
            self.emit(.sel1, k, r, t_val, f_val);
            return(Ok = self.scalar_result(p, r, arg.ty));
        };
    };
    
    t_blk, f_blk, join_blk := (self.new_block("yes"), self.new_block("no"), self.new_block("join"));
    seal self { b |
        b.jmp = (type = .jnz, arg = cond);
        b.s1 = t_blk;
        b.s2 = f_blk;
    };
    
    // force both branches to output to the same place. 
    join_val := self.unify_placement(p&, arg.ty);
    joined := false;
    
    self.blk = t_blk;
    t_val := @try(self.compile_expr(t_expr, p)) return;
    self.hlt_if_never(t_expr.ty);
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = join_blk;
        joined = true;
    };
    
    self.blk = f_blk;
    f_val := @try(self.compile_expr(f_expr, p)) return;
    self.hlt_if_never(f_expr.ty);
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = join_blk;
        joined = true;
    };
    
    self.blk = join_blk;
    if !joined {
        // both branches diverged. 
        join_blk.jmp.type = .hlt; // TODO: remove the useless block 
    };
    (Ok = join_val)
}

// TODO: use if for single prong (maybe in backend?)

// TODO: use phi for scalars. 
fn emit_switch(self: *EmitIr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    @debug_assert(arg.expr&.is(.Switch));
    parts := arg.expr.Switch;
    inspect := @try(self.compile_expr(parts.value, .ScalarTransient)) return; 
    
    {
        o, k := self.load_op(parts.value.ty);
        @err_assert(k.is_int(), "can only switch over ints") return;
        if o != .load {
            o := rebase(o, .extsb, .loadsb);
            r := self.f.newtmp("switch", .Kl);
            self.emit(o, .Kl, r, inspect, QbeNull);
            inspect = r;
        };
    };
    
    //name := self.program.get_string(self.program.get_function(self.func)[].name);
    n: i64 = self.f.switch_count.zext();
    ss := self.f.switches&;
    {
        if n == 0 {
            ss[] = new(1);
        } else {
            ss.grow(n + 1);
        };
        self.f.switch_count += 1;
        self.blk.jmp = (type = .switch, arg = INT(n));
    };
    ss[n] = (
        cases = new(parts.cases.len),
        case_count = 0,
        inspect = inspect,
        src = self.blk,
        default = self.new_block("default"),
    );
    
    result := self.unify_placement(p&, arg.ty);
   
    // This is where we rejoin with the value of the whole switch expression.
    end_blk   := self.new_block("joins"); 
    out := self.program.get_info(arg.ty);
    rejoined := false;
    values := i64.list(parts.cases.len, temp());
    each parts.cases { f |
        value := f._0;
        body  := f._1&;
        
        // TODO: move this check to sema. 
        @err_assert(!values.items().contains(value&), "value % appears twice in switch node", value) return;
        values&.push(value);
        
        case_block := self.new_block(@tfmt("case%", value));
        ss[n].cases[ss[n].case_count] = (case_block, value);
        ss[n].case_count += 1;
        self.blk = case_block; 
        
        _ := @try(self.compile_expr(body, p)) return;
        seal self { b |
            b.jmp.type = .jmp;
            b.s1 = end_blk;
            rejoined = true;
        };
    };
    
    self.blk = ss[n].default;
    _ := @try(self.compile_expr(parts.default, p)) return;
    seal self { b |
        b.s1 = end_blk;
        b.jmp.type = .jmp;
        rejoined = true;
    };
    
    if !rejoined {
        end_blk.jmp.type = .hlt; // TODO: remove the useless block
    };
    self.blk = end_blk;
    @debug_assert_eq(ss[n].case_count, parts.cases.len);
    (Ok = result)
}

fn emit_call_loop(self: *EmitIr, arg: *FatExpr) CRes(Qbe.Ref) #once = {
    @debug_assert_eq(arg.ty, void);
    start := self.new_block("loop");
    self.blk.jmp.type = .jmp;
    self.blk.s1 = start;
    self.blk    = start;
    @try(self.compile_expr(arg, .Discard)) return;
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = start;
    };
    (Ok = QbeNull)
}

fn construct_aggregate(self: *EmitIr, pattern: *Pattern, requested: Type, p: Placement) CRes(Qbe.Ref) #once = {
    if p&.is(.Discard) {
        each pattern.bindings& { b | 
            @try(self.compile_expr(b.get_default().unwrap(), .Discard)) return;
        };
        return(Ok = QbeNull);
    };
    raw_container_ty := self.program.raw_type(requested);
    (Ok = @match(self.program.get_type(raw_container_ty)) {
        fn Struct(f) => {
            expected := f.fields.len;
            if f.is_union {
                @debug_assert_eq(pattern.bindings.len, 1, "union must have exactly one active field");
                expected = 1;
            } else {
                @debug_assert_eq(expected, pattern.bindings.len, "Cannot assign to type % with wrong field count", self.program.log(requested));
                if expected == 1 {
                    expr := pattern.bindings.index(0).get_default().unwrap();
                    return(self.compile_expr(expr, p));
                };
            };
            if expected == 1 {
                return(self.compile_expr(pattern.bindings.index(0).get_default().unwrap(), p));
            };
            base := self.get_memory(p, raw_container_ty);
            enumerate pattern.bindings& { i, b | 
                name := @unwrap(b.ident(), "map literal entry needs name (while initilizing %)", self.program.log(requested)) return;
                field := or find_struct_field(f, name, i - 1) {
                    return(@err("field name mismatch (ICE: should be checked by sema)"))
                };
                expr := b.get_default().unwrap();
                dest := self.offset(base, field.byte_offset);
                _ := @try(self.compile_expr(expr, (Blit = dest))) return;
            };
            base
        }
        fn Tagged(f) => {
            @debug_assert_eq(pattern.bindings.len, 1, "@tagged must have one active varient");
            value := pattern.bindings.index(0).get_default().unwrap();
            name := pattern.bindings[0].name.unwrap();
            i    := f.cases.position(fn(f) => f._0 == name).expect("case name to exist in type");
            if value.ty == void {  // :get_or_create_type
                _ := @try(self.compile_expr(value, .Discard)) return;
                return(Ok = self.scalar_result(p, self.f.getcon(i), i64));
            };
            @match(p) {
                fn Discard() => return(self.compile_expr(value, .Discard)); // just for side effects
                fn Scalar()  => return(@err("ICE: tagged union is not a scalar."));
                fn ScalarTransient() => return(@err("ICE: tagged union is not a scalar."));
                @default     => ();
            };
            base := self.get_memory(p, raw_container_ty);
            self.emit(.storel, .Kw, QbeNull, self.f.getcon(i), base);  // tag :get_or_create_type // TODO: smaller tag sizes than 64 bits 
            _ := @try(self.compile_expr(value, (Blit = self.offset(base, 8)))) return;
            base
        }
        @default => return(@err("struct literal for non-(struct/tagged)"));
    })
}

// For now ptr is always the address of the variable's stack slot. 
// TODO: generate less dumb code for non-escaping scalars. 
fn declare_var_mem(self: *EmitIr, name: Var, ptr: Qbe.Ref, type: Type) void #inline = {
    info := self.program.get_info(type);
    if info.stride_bytes == 0 {
        // :NoRuntimeRepr
        // It's nice to let macros/generics operate over types uniformly.
        // Some types have no runtime representation, so any attempt to use them should compile to nothing. 
        // TODO: maybe sema should try to strip these out so we don't have to keep checking here. 
        self.var_lookup&.insert(name, (Alloca = QbeConZero));
        return()  
    };

    @debug_assert(rtype(ptr) == .RTmp);
    self.record_var_decl(name, (Alloca = ptr));
}

fn record_var_decl(self: *EmitIr, name: Var, value: IrVar) void #inline = {
    prev := self.var_lookup&.insert(name, value);
    //@debug_assert(prev.is_none(), "shadow is still new var"); TODO farm_game gets here but clearly it's fine. idk man -- Jan 8
    
    if self.debug && name.name != Flag.SYMBOL_ZERO.ident() {
        name := self.program.get_string(name.name);
        name.len = name.len.min(20);
        ::@assert(IrVar.Tag().enum_count() == 2, "update IrVar repr");
        r := value.Local;
        t := self.f.get_temporary(r);
        @if(TRACK_IR_NAMES) {
            l := fixed_list(t.name&.items());
            @fmt(l&, "%.%", name, r.val());
        };
    };
}


// IMPORTANT: you can't use f.emit because threads. 
fn emit(self: *EmitIr, o: Qbe.O, k: Qbe.Cls, dest: Qbe.Ref, a0: Qbe.Ref, a1: Qbe.Ref) void #inline = {
    @debug_assert(k.raw() < 4, "ICE: invalid base % in emit_ir:emit %", k, o);
    @debug_assert(o != .Oxxx, "ICE: null inst emit_ir:emit");
    push(self.blk, make_ins(o, k, dest, a0, a1));
}

// TODO: types larger than 2^16 bytes but you probably don't want to copy those anyway
fn blit(self: *EmitIr, dest: Qbe.Ref, src: Qbe.Ref, bytes: i64) void #inline = {
    // mem.fr/promote knows about 1/2/4/8 byte blits. 
    if bytes == 16 {    
        // TODO: maybe we can't do this cause alignment but we're kinda bad at eliminating blits. :UGLY
        //       and the old backend special cased this so its sad to be worse. 
        //       should make slot coalessing not get confused by blits or lower them earlier.
        //       TODO: would sroa be that hard? i know which things don't escape. 
        v  := self.f.newtmp("b", .Kl);
        v2 := self.f.newtmp("b", .Kl);
        src2  := self.offset(src, 8);
        dest2 := self.offset(dest, 8);
        self.emit(.load, .Kl, v, src, QbeNull);
        self.emit(.load, .Kl, v2, src2, QbeNull);
        self.emit(.storel, .Kw, QbeNull, v, dest);
        self.emit(.storel, .Kw, QbeNull, v2, dest2);
        return();
    };
    self.emit(.blit0, .Kw, QbeNull, src, dest);
    self.emit(.blit1, .Kw, QbeNull, INT(bytes), QbeNull);
}

fn alloca(self: *EmitIr, ty: Type) Qbe.Ref = {
    info := self.program.get_info(ty);
    if info.stride_bytes == 0 { // :NoRuntimeRepr 
        return(QbeConZero);
    };
    t := self.f.newtmp("s", .Kl);
    @debug_assert(info.align_bytes <= 8, "i don't support high alignments yet");
    o: Qbe.O = @if(info.align_bytes <= 4, .alloc4, .alloc8);
    s := self.f.getcon(info.stride_bytes.zext());
    push(self.f.start, make_ins(o, .Kl, t, s, QbeNull));
    t
}

fn offset(self: *EmitIr, ptr: Qbe.Ref, bytes: i64) Qbe.Ref #inline = {
    if(bytes == 0, => return(ptr));
    new := self.f.newtmp("f", .Kl);
    self.emit(.add, .Kl, new, ptr, self.f.getcon(bytes));
    new
}

fn unify_placement(self: *EmitIr, p: *Placement, ty: Type) Qbe.Ref = @match(p[]) {
    fn Blit(it) => it;
    fn Discard() => QbeNull;
    fn NewMemory() => {
        ref := self.alloca(ty);
        p[] = (Blit = ref);
        ref
    }
    fn Scalar() => {   // TODO: use phi instead
        _, k := self.load_op(ty);
        @debug_assert(k != .Ke, "ICE: attempted scalar placement of aggragate type");
        dest := self.f.newtmp("unify", k);
        p[] = (Assign = dest);
        dest
    }
    fn ScalarTransient() => {  
        _, k := self.load_op(ty);
        @debug_assert(k != .Ke, "ICE: attempted scalar placement of aggragate type");
        dest := self.f.newtmp("unify", k);
        p[] = (Assign = dest);
        dest
    }
    fn Assign(it) => it;
};

fn get_memory(self: *EmitIr, r: Placement, ty: Type) Qbe.Ref = {
    @match(r) {
        fn NewMemory() => self.alloca(ty);
        fn Blit(it) => it;
        fn Discard() => self.alloca(ty);  // TODO: maybe force the caller to early out? 
        @default => panic("invalid Placement type for get_memory");
    }
}

// This exists to save a copy with Placement.Assign, 
// otherwise it just creates a new tmp for you to use. 
// Caller has to pass the result to scalar_result after copying in the value. 
fn scalar_dest(self: *EmitIr, p: Placement, k: Qbe.Cls) Qbe.Ref = @match(p) {
    fn Discard() => QbeNull;
    fn Assign(dest) => dest;
    @default => {
        @debug_assert(k != .Ke);
        self.f.newtmp("v", k)
    };
};

fn scalar_result(self: *EmitIr, p: Placement, r: Qbe.Ref, ty: Type) Qbe.Ref = @match(p) {
    fn NewMemory() => self.scalar_result((Blit = self.alloca(ty)), r, ty);
    fn Blit(dest)  => {
        size := self.program.get_info(ty)[].stride_bytes;
        if size == 0 {
            return(QbeConZero);  // :NoRuntimeRepr
        };
        o := self.store_op(ty);
        if o == .Oxxx {
            // TODO: this only sane if called from emit_value so it should probably just deal with this instead. 
            // It's convinent to emit constants as a literal even if we want a struct with multiple fields. 
            // TODO: is alignment real?
            @debug_assert(size <= 8, "tried to use scalar result for large aggragate");
            o = store_by_size[size.zext()];
            @debug_assert(o != .Oxxx, "tried to use scalar result for strange sized aggragate");
        };
        self.emit(o, .Kw, QbeNull, r, dest);
        dest
    }
    fn Scalar()  => r;
    fn ScalarTransient() => r;
    fn Discard() => QbeNull;
    fn Assign(dest) => {
        if dest != r {
            t := self.f.get_temporary(dest);
            self.emit(.copy, t.cls, dest, r, QbeNull);
        };
        dest
    }
};

fn get_aggregate(self: *EmitIr, ty: Type) Qbe.Ref #inline = 
    get_aggregate(self.program, self.shared, ty);

// TODO: reuse types for different nominal types with the same repr?
fn get_aggregate(self: CompCtx, m: *CodegenShared, ty: Type) Qbe.Ref = {
    info := self.get_info(ty);
    ir_index := m.type_indices&.get(ty) || {
        type := self.get_type(ty);
        result := Qbe.Typ.zeroed();
        result.size = info.stride_bytes.zext();
        @debug_assert(result.size != 0, "ICE: treating zero size type % as an aggragate", self.log(ty));
        result.align_log2 = (@as(i64) info.align_bytes.zext()).trailing_zeros().intcast(); // TODO: do this faster
        //@println("%", )
        @if(TRACK_IR_NAMES) {
            l := fixed_list(result.name&.items());
            @fmt(l&, "T%", ty.as_index());
        };
        i := @match(type) {
            fn Named(it)    => self.get_aggregate(m, it._0).val();
            fn Enum(it)     => self.get_aggregate(m, it.raw).val();
            fn Struct(it) => {
                yield :: local_return;
                if repr_transparent(it) { repr |
                    yield(self.get_aggregate(m, repr).val());
                };
                n := 0;
                rt_count: i64 = it.fields.len;
                result.fields = m.m.new_long_life(rt_count * 2 + 1);
                result.nunion = @if(it.is_union, rt_count.trunc(), 1);
                result.is_union = it.is_union;
                @assert(it.layout_done, "tried to get_aggregate before layout %", self.log(ty));
                if it.is_union {
                    each it.fields { f | 
                        field_size := self.get_info(f.ty)[].stride_bytes;
                        if field_size != 0 {
                            @debug_assert(f.byte_offset == 0, "union field must have offset 0");
                            result.fields[n] = self.as_field(m, f.ty);   n += 1;
                            result.fields[n] = (type = .FEnd, len = 0);  n += 1;
                        }; // else :NoRuntimeRepr
                    };
                } else {
                    off := 0;
                    each it.fields { f | 
                        field_size := self.get_info(f.ty)[].stride_bytes;
                        if field_size != 0 {
                            add_padding(result.fields&, n&, off&, f.byte_offset);
                            result.fields&.grow(n);
                            result.fields[n] = self.as_field(m, f.ty);
                            n += 1;
                            off += field_size.zext();
                        }; // else :NoRuntimeRepr
                    };
                    add_padding(result.fields&, n&, off&, info.stride_bytes.zext());
                    result.fields&.grow(n);
                    result.fields[n] = (type = .FEnd, len = 0);
                };
                // TODO: name
                m.m.new_type(result)
            }
            fn Array(it) => if it.len == 1 {
                self.get_aggregate(m, it.inner).val()
            } else {
                inner_size: i64 = self.get_info(it.inner)[].stride_bytes.zext();
                @debug_assert_eq(inner_size * it.len.zext(), info.stride_bytes.zext(), "size calc wrong!");
                outer_size: i64 = info.stride_bytes.zext();
                field_count := 1 + it.len.zext();
                result.fields = m.m.new_long_life(field_count);
                result.nunion = 1;
                field := self.as_field(m, it.inner);
                range(0, it.len.zext()) { i |
                    result.fields[i] = field;
                };
                result.fields[it.len.zext()] = (type = .FEnd, len = 0);
                // TODO: name
                m.m.new_type(result)
            }
            fn Tagged(it) => {
                n := 0;
                result.fields = m.m.new_long_life(it.cases.len * 3);
                result.nunion = it.cases.len.trunc();
                each it.cases { case | 
                    if self.get_info(case._1)[].stride_bytes != 0 { 
                        result.fields[n] = (type = .Fl, len = 8);      n += 1;
                        result.fields[n] = self.as_field(m, case._1);  n += 1;
                        result.fields[n] = (type = .FEnd, len = 0);    n += 1;
                        // TODO: do i need to pad them to the same size?
                    } else {
                        result.nunion -= 1;
                    };
                };
                result.is_union = result.nunion > 1;
                if result.nunion == 0 {
                    // all cases are zero sized. this is really just an enum. 
                    @debug_assert(false, "ICE: get_aggregate of scalar @tagged");
                    result.fields[n] = (type = .Fl, len = 8);      n += 1;
                    result.fields[n] = (type = .FEnd, len = 0);    n += 1;
                };
                // TODO: name
                m.m.new_type(result)
            }
            @default => @panic("tried to get_aggregate of scalar %", self.log(ty));
        }; 
        m.type_indices&.insert(ty, i);
        i
    };
    
    TYPE(ir_index)
}

fn as_field(self: *EmitIr, ty: Type) Qbe.Field #inline = 
    as_field(self.program, self.shared, ty);

fn as_field(self: CompCtx, m: *CodegenShared, ty: Type) Qbe.Field = {
    ::if(Qbe.Field);
    type := self.get_type(ty);
    @match(type) {
        fn F64()      => (type = .Fd, len = 8);
        fn F32()      => (type = .Fs, len = 4);
        fn Ptr(_)     => (type = .Fl, len = 8);
        fn FnPtr(_)   => (type = .Fl, len = 8);
        fn VoidPtr(_) => (type = .Fl, len = 8);
        fn Label(_)   => (type = .Fw, len = 4);
        fn Fn(_)      => (type = .Fw, len = 4);
        fn Bool()     => (type = .Fb, len = 1);
        fn Named(it)  => self.as_field(m, it._0);
        fn Enum(it)   => self.as_field(m, it.raw);
        fn Int(it)    => @switch(it.bit_count) {
            @case(8)  => (type = .Fb, len = 1);
            @case(16) => (type = .Fh, len = 2);
            @case(32) => (type = .Fw, len = 4);
            @default  => (type = .Fl, len = 8);
        }
        fn Tagged(it) => {
            if self.is_tag_only(it) {
                return(type = .Fl, len = 8);
            };
            (type = .FTyp, len = self.get_aggregate(m, ty).val().trunc())
        }
        fn Struct(it) => {
            if repr_transparent(it) { inner |
                return(self.as_field(m, inner))
            };
            (type = .FTyp, len = self.get_aggregate(m, ty).val().trunc())
        }
        fn Array(it) => if it.len == 1 {
            self.as_field(m, it.inner)
        } else {
            (type = .FTyp, len = self.get_aggregate(m, ty).val().trunc())
        };
        @default => @panic("tried to as_field of unhandled %", self.log(ty));
    }
}

fn is_tag_only(self: CompCtx, t: *TypeInfo.get_variant_type(.Tagged)) bool = {
    each t.cases { case | 
        if case._1 != void {  // :get_or_create_type
            return(false);
        };
    };
    true
}

fn repr_transparent(it: *TypeInfo.get_variant_type(.Struct)) ?Type = {
    @switch(it.fields.len) {
        @case(0) => (Some = void); // :get_or_create_type
        @case(1) => return(Some = it.fields[0].ty);
        @default => .None;
    }
}

fn hlt_if_never(self: *EmitIr, ty: Type) void #inline = {
    if ty.is_never() && self.blk.jmp.type == .Jxxx {
        self.blk.jmp.type = .hlt;
    };
}

// TODO: single field array

// returns .Ke for aggregates
fn load_op(self: *EmitIr, ty: Type) Ty(Qbe.O, Qbe.Cls) #inline = 
    load_op(self.program, ty);

fn load_op(self: CompCtx, ty: Type) Ty(Qbe.O, Qbe.Cls) = @match(self.get_type(ty)) {
    fn Int(it)   => @switch(it.bit_count) {
        @case(8)  => (@if(it.signed, .loadsb, .loadub), .Kw);
        @case(16) => (@if(it.signed, .loadsh, .loaduh), .Kw);
        @case(32) => (@if(it.signed, .loadsw, .loaduw), .Kw);
        @default  => (.load, .Kl);
    }
    fn F32()     => (.load, .Ks);
    fn F64()     => (.load, .Kd);
    fn Ptr(_)    => (.load, .Kl);
    fn FnPtr(_)  => (.load, .Kl);
    fn VoidPtr() => (.load, .Kl);
    fn Label(_)  => (.loaduw, .Kw);
    fn Fn(_)     => (.loaduw, .Kw);
    fn Named(it) => self.load_op(it._0);
    fn Enum(it)  => self.load_op(it.raw);
    fn Bool()    => (.loadub, .Kw);
    fn Tagged(it) => {
        if self.is_tag_only(it) {
            return(.load, .Kl);
        };
        (.nop, .Ke)
    }
    fn Struct(it) => self.load_op(or repr_transparent(it) {
        return(.nop, .Ke)
    });
    fn Array(it) => @if(it.len == 1, self.load_op(it.inner), (.nop, .Ke));
    @default     => (.nop, .Ke);
};

fn store_op(self: *EmitIr, ty: Type) Qbe.O #inline = 
    store_op(self.program, ty);

fn store_op(self: CompCtx, ty: Type) Qbe.O = @match(self.get_type(ty)) {
    fn Int(it)   => @switch(it.bit_count) {
        @case(8)  => .storeb;
        @case(16) => .storeh;
        @case(32) => .storew;
        @default  => .storel;
    }
    fn F32()     => .stores;
    fn F64()     => .stored;
    fn Ptr(_)    => .storel;
    fn FnPtr(_)  => .storel;
    fn VoidPtr() => .storel;
    fn Label(_)  => .storew;
    fn Fn(_)     => .storew;
    fn Named(it) => self.store_op(it._0);
    fn Enum(it)  => self.store_op(it.raw);
    fn Bool()    => .storeb;
    fn Tagged(it) => {
        if !self.is_tag_only(it) {
            return(.Oxxx);
        };
        .storel
    }
    fn Struct(it) => self.store_op(repr_transparent(it) || return(.Oxxx));
    fn Array(it) => @if(it.len == 1, self.store_op(it.inner), .Oxxx);
    @default     => .Oxxx;
};

fn new_block(self: *EmitIr, debug: Str) *Qbe.Blk = {   
    b := temp().box_zeroed(Qbe.Blk);
    @if(TRACK_IR_NAMES) {
        l := fixed_list(b.name&.items()); // :UnacceptablePanic
        @fmt(l&, "b%_%", self.next_block_name, debug);
        self.next_block_name += 1; // not the same as blocks.len because we remove for unused local returns. 
    };
    b.ins = new(0);
    b.visit = -1;
    self.blocks&.push(b);
    b
}

fn fmt_fn_name(self: *EmitIr, f: FuncId) []u8 = 
    self.program.fmt_fn_name(f);

fn seal(self: *EmitIr, $body: @Fn(b: *Qbe.Blk) void) void = {
    if self.blk.jmp.type == .Jxxx { 
        body(self.blk);
    };
}
