// 
// emit_ir runs after sema, on one function at a time, to convert the 
// (monomorphized, type-checked) ast into 3-register-ir for the backend. 
// - LabelId values are mapped to ReturnTarget: a basic block (Blk) and a result location (Placement)
// - Variables are mapped to IrVar: a mutable Ref or stack slot
// - Zero-sized types are discarded
// - (AOT only) Values are mapped to Dat2, 
//   recording relocations for any pointers in the data by running bake_relocatable_value overload
// - Any functions that weren't properly tracked in `callees` are returned in `pending`
// 
// Some functions do not have a normal ast as a body:
// - NewIntrinsic: maps to a single ir instruction
// - DynamicImport: (AOT) Redirect to an imported symbol OR (JIT) bind directly to a function pointer
// - Asm: some bytes inserted verbatim
// - Redirect
// 
// Codegen may run on the same thread or not, so calling enqueue_task() might 
// do the work immediately or put it on a work queue. To keep the order reproducible, 
// be careful to always pair bouba_acquire with enqueue_task, one at a time. 
//

// TODO: always check for zero sized type instead of == void

EmitIr :: @struct(
    f: *Qbe.Fn,
    program: CompCtx,
    last_loc: Span,
    var_lookup: HashMap(VarId, IrVar),
    inlined_return_addr: HashMap(LabelId, ReturnTarget),
    label_depth: u32,
    blk: *Qbe.Blk, 
    debug: bool,
    context := QbeNull,
    func: FuncId,
    blocks: List(*Qbe.Blk),
    when: ExecStyle,
    m: *QbeModule,
    entry: *CodegenEntry,
    next_block_name := 0,
    last_line := -1,
    new_constants: *List(BakedVarId),
    shared: *FrontendCodegen,
    // you wouldn't need this if sema tracked callees correctly,
    // but it's much easier to just fudge it with shims. 
    pending: *List(FuncId), 
    unreachable_block: *Qbe.Blk,
);

IrVar :: @tagged(Alloca: Qbe.Ref, Local: Qbe.Ref);
::tagged(IrVar);

ReturnTarget :: @struct(
    blk: *Qbe.Blk,
    p: Placement,
    depth: u32,
    used: bool,
);

Placement :: @tagged(Assign: Qbe.Ref, Scalar, Blit: Qbe.Ref, NewMemory, Discard, ScalarTransient);

fn emit_ir(comp: CompCtx, shared: *FrontendCodegen, f: FuncId, when: ExecStyle, pending: *List(FuncId), export: bool) CRes(void) = {
    func := comp.get_function(f); 

    if when == .Aot {
        // TODO: really you want the callgraph of why we think this is reachable 
        // TODO: this will need to change again when i want to be able to cache #macro, etc. 
        //       really #comptime_addr should be expressed as a strong import and someone needs to check 
        //       that all imports are fulfilled. But for now, it's better to give an error message than have the dynamic loader do it. 
        //       -- Jun 17, 2025 
        @err_assert(!func.get_flag(.ComptimeOnly), "cannot .Aot .ComptimeOnly % %", f, comp.log(func)) return;
        
        // :bake_relocatable_value
        {comp.vtable.check_for_new_aot_bake_overloads}(comp.data); 
    };
    
    z := zone_begin(.EmitIr);
    
    // currently this needs to make funcids eagarly so it might involve making jitshims for imports which has to happen before we grab the codegenentry
    @if(!DISABLE_IMPORT_FRC)
    @if_let(func.body&) fn FrcImport(it) => {
        _ := @try(find_module(comp.data.cast()[][], shared, Incr'FrcModule.ptr_from_int(it.module))) return;
        @if(ENABLE_TRACY) ___tracy_emit_zone_color(z, import_cache_colour);
    };
    
    m := shared.m;
    result: CRes(void) = .Ok;

    // ugh. can't be temp() because threaded=false wants to reset it.     
    new_constants := BakedVarId.list(general_allocator());
    
    enter_task shared { entry |
        ::tagged(Placement);
        opts := comp.get_build_options();
        
        env := comp.get_comptime_env();
        self: EmitIr = (
            f = temp().box_uninit(Qbe.Fn),
            program = comp,
            func = f,
            last_loc = func.loc,
            var_lookup = init(temp()),
            inlined_return_addr = init(temp()),
            label_depth = 0,
            blk = zeroed(*Qbe.Blk),
            blocks = list(temp()),
            debug = opts.debug_info || func.get_flag(.LogIr),
            when = when,
            m = m,
            entry = entry,
            new_constants = new_constants&,
            shared = shared,
            pending = pending,
            unreachable_block = zeroed(*Qbe.Blk),
        );
        @debug_assert(func.get_flag(.EnsuredCompiled), "fn not compiled?");
        self.f.default_init(m);
        self.f.leaf = true;
        self.f.lnk.no_inline = func.get_flag(.NoInline);
        self.f.lnk.id = m.intern(self&.fmt_fn_name(self.func));  // TODO: set export 
        // these are in addition to the defaults set by passing `-d logging` to default_driver
        self.entry.logging = self.program.get_log_types(f);
        entry.task = (Func = self.f);
        res: CRes(void) = .Ok;
        self.f.track_ir_names = self.entry.logging.len > 0 || opts.always_track_ir_names || m.debug != 0;
        res = self&.emit_body(f);
        self.f.lnk.export = export;
        
        if res&.is_err() {
            res.Err.update_main_span(self.last_loc);
            result = (Err = res.Err);
            entry.task = (AotVar2 = empty());  // do nothing
        };
    };
    
    if !result&.is_err() {
            has_context := func.get_flag(.YesContext) && !func.get_flag(.NoContext);  // :copy-pastrae
            if !has_context {
                func.set_flag(.NoContext);
            };
            
            // you could do this with nested enter_task instead of collecting them in reference_constant(). 
            // but thats a tiny tiny bit slower. i guess most are trivial so it's better to send them in a batch 
            // and let the worker just crank on it instead of dealing with the queues more often.   -- Jul 26, 2025
            if new_constants.len != 0 {
                enter_task shared { entry |
                    out := Dat2.list(new_constants.len, temp());
                    for new_constants { id |
                        out&.push(baked_var_to_dat(comp, m, id));
                    };
                    entry.task = (AotVar2 = out.items());
                };
                
                // if it's from a FrcImport it won't already have a place in the comptime address space, 
                // so we need to remember the jit_addr we made for it so it doesn't get rebaked (which 
                // would be extra bad if the relocations haven't fired yet) 
                if when == .Jit {
                    for new_constants { id |
                        baked := comp.get_baked(id);
                        if baked.need_reify {
                            @debug_assert(!shared.worker.threaded, "need_reify on threaded codegen of %", comp.fmt_fn_name(f));
                            baked.need_reify = false;
                            c := comp.data.cast()[][];
                            sym := m.intern(comp.mangle_name(id));
                            use_symbol(m, sym) { symbol |
                                c.baked.vmem&.insert(symbol.jit_addr, (BakedVarId = id));
                            };
                        };
                    };
                }
            };
            
            new_constants&.drop();
    };
    
    zone_end(z);
    result
}

fn set_lib(self: *EmitIr, it: Ty(Qbe.Sym, Str, bool)) void = {
    enter_task self.shared { entry |
        p := temp().boxed(@type it, it);
        entry.task = (SetLibrary = p.slice(1));
    };
}

// this puts a bunch of stuff in temp(), it's expected to be used in the context of a CodegenEntry. 
// TODO: this is so annoying. the data structures are so similar because the two sides need to know the same information. 
//       but i can't quite make them the same because you need to allow multiple modules on one frontend context. 
fn baked_var_to_dat(comp: CompCtx, m: *QbeModule, id: BakedVarId) Dat2 = {
    value := comp.get_baked(id);
    (
        id = m.intern(comp.mangle_name(id)),  
        align = value.align,
        export = false,  // TODO: allow exporting vars 
        template = @match(value.template) {  // this is ass
            fn Zeroes(it) => (Zeroes = it);
            fn Bytes(it) => (Bytes = it);
        },
        relocations = 
            temp().alloc_init(Dat2.Reloc, value.relocations.len) { i |
                src := value.relocations.index(i);
                // TODO: you can't be doing string formatting in here common man
                (
                    off = src.off,
                    id = @match(src.target) {
                        fn FuncId(f) => {
                            func := comp.get_function(f);
                            m.intern(comp.fmt_fn_name(f))
                        }
                        fn BakedVarId(id) => m.intern(comp.mangle_name(id));
                        fn Var() => unreachable();
                    },
                    addend = src.addend,
                )
            },
    )
}

// TODO: this is going to change the calling convention for small strange aggragates maybe (to match c). 
//       hopefully i don't use any of those for something important. 

ArgInfo :: @struct(r: Qbe.Ref, type: Type, name: ?Var, scalar: bool, k: Qbe.Cls, type_index: Qbe.Ref);

// :EmitIrCall
// note: mirrors compile_for_arg
// appends par/parc instructions to f.start
// zero sized types do not generate a par instruction but are added to zero_sized_vars instead. 
fn emit_par_instructions(program: CompCtx, shared: *FrontendCodegen, arguments: *Pattern, arg_ty: Type, f: *Qbe.Fn, zero_sized_vars: ?*HashMap(VarId, IrVar)) []ArgInfo = {
    count := arguments.bindings.len;
    args := ArgInfo.list(count, temp());
    reserve(f.start.ins&, count, temp());

    // Need to emit all the `arg` instructions at the beginning of the start block before doing any allocas. 
    enumerate arguments.bindings { i, binding |
        continue :: local_return;
        @debug_assert(binding.kind != .Const, "tried to emit function with $const parameter");
        ty := binding.unwrap_ty();
        info := program.get_info(ty);
        name := binding.var();
        if info.stride_bytes == 0 {  // :NoRuntimeRepr
            if zero_sized_vars { it |
                if name { name |
                    // special case here instead of in compile_expr::GetVar
                    it.insert(name.id, (Alloca = QbeConZero));
                };
            };
            if ty == CVariadicType {
                @assert(!f.vararg, "cannot have multiple CVaradic parameters");  // TODO: this check should really be in sema
                f.vararg = true;
            };
            continue();
        };
        @assert(!f.vararg, "CVariadic must be after all sized parameters");  // TODO: this check should really be in sema
        
        _, kk := load_op(program, ty);
        scalar := kk != .Ke;
        
        op, k, a0 := @if(scalar, 
            (Qbe.O.par, kk, QbeNull),
            (Qbe.O.parc, Qbe.Cls.Kl, get_aggregate(program, shared, ty))
        );
        r := f.newtmp("a", k);
        push(f.start, make_ins(op, k, r, a0, QbeNull));
        args&.push(r = r, type = ty, name = name, scalar = scalar, k = k, type_index = a0);
    };
    args.items()
}

fn bind_parameters(self: *EmitIr) CRes(void) #once = {
    func := self.program.get_function(self.func);
    arg_ty := func.finished_arg.unwrap();
    args := emit_par_instructions(self.program, self.shared, func.arg&, arg_ty, self.f, (Some = self.var_lookup&));
    
    // only arg instructions and allocas in the start block. 
    self.blk = self.new_block("body");
    each args { a | 
        if a.name { name | 
            mut := a.scalar && self.program.is_local_scalar(name, a.type);
            mem := if(!a.scalar || mut, => a.r) {
                mem := self.alloca(a.type);
                self.emit(self.store_op(a.type), .Kw, QbeNull, a.r, mem);
                mem
            };
            if mut {
                self.record_var_decl(name, (Local = mem));
            } else {
                self.declare_var_mem(name, mem, a.type);
            };
        };
    };
    .Ok
}

// :EmitIrCall
// note: mirrors bind_parameters
fn compile_for_arg(self: *EmitIr, arg: *FatExpr) CRes([]Qbe.Ins) = {
    parts := arg.items();
    out := Qbe.Ins.list(parts.len, temp());
    if self.program.get_info(arg.ty)[].stride_bytes == 0 {  // :NoRuntimeRepr
        _ := @try(self.compile_expr(arg, .Discard)) return;
        return(Ok = empty());
    };
    info := self.program.get_type(arg.ty);
    f := info.Struct.fields;
    if info.is(.Struct) && info.Struct.is_tuple {
        @err_assert(f.len == parts.len, "ICE: compile_for_arg arity mismatch") return;
    } else {
        @err_assert(parts.len == 1, "ICE: compile_for_arg arity mismatch!") return;
    };
    @try(compile_for_arg_impl(self, parts, out&)) return;
    (Ok = out.items())
}

fn compile_for_arg_impl(self: *EmitIr, parts: []FatExpr, out: *List(Qbe.Ins)) CRes(void) = {
    enumerate parts { i, val |
        continue :: local_return;
        if val.expr&.is(.CVariadic) {
            out.push(make_ins(.argv, .Kw, QbeNull, QbeNull, QbeNull));
            @try(compile_for_arg_impl(self, val.expr.CVariadic, out)) return;
        } else {
            ty := val.ty; // TODO: is this ok or do you have to get it from the function's arg type?
            msg :: "expression of type CVariadic must be an Expr::CVariadic created with @va as final parameter to a call";
            @err_assert(ty != CVariadicType, msg) return;
            _, k := self.load_op(ty);
            ::if(Qbe.Ins);
            ins := if k != .Ke {  // scalar
                r := @try(self.compile_expr(val, .Scalar)) return;
                make_ins(.arg, k, QbeNull, r, QbeNull)
            } else { 
                if self.program.get_info(ty)[].stride_bytes == 0 {
                    _ := @try(self.compile_expr(val, .Discard)) return;
                    continue();
                };
                // aggregate
                // TODO: you really want to skip an extra blit that the abi stuff will do for you. 
                //       but we don't know if a later argument expression will modify that memory.  
                //       TODO: make a test that fails if you do this
                //       currently doing it saves 30ms on self compile (and im not even doing it for GetVar) -- Mar 16
                //       alternatively, be able to tell the backend we promise the arg memory is noalias so it doesn't have to blit. 
                UNSOUND_SKIP_EXTRA_BLIT :: false;
                r := if UNSOUND_SKIP_EXTRA_BLIT && val.expr&.is(.Deref) { 
                    @try(self.compile_expr(val.expr.Deref, .Scalar)) return
                } else {
                    @try(self.compile_expr(val, .NewMemory)) return
                };
                r_type := self.get_aggregate(ty);
                make_ins(.argc, .Kl, QbeNull, r_type, r)
            };
            out.push(ins);
        };
    };
    .Ok
};

fn context_par_e(self: *EmitIr) void #inline = {
    self.context = self.f.newtmp("context", .Kl);
    self.emit(.pare, .Kl, self.context, QbeNull, QbeNull);
}

fn finalize_blocks(self: *EmitIr) void = {
    prev := Qbe.Blk.ptr_from_int(0);
    ok := true;
    self.f.nblk = 0;
    for_rev self.blocks& { b | 
        if b.visit == -1 {
            b.link = prev;
            prev = b;
            self.f.nblk += 1;
        };
        ok = ok && b.jmp.type != .Jxxx;
    };
    if !ok {
        printfn(self.f, self.f.globals.debug_out);
    };
    @debug_assert(ok, "ICE: unterminated block");
}

fn aot_import_body(self: *EmitIr, info: *DynamicImport) void = {
    // :DumbNameAlias
    // TODO: use FnFlag.NoMangle instead but that doesn't work with the old backends calling the same 
    //       fmt_fn_name until they also don't try to make bounce functions.
    g := self.m.goal&;
    func := self.program.get_function(self.func);
    @assert_eq(info.name, func.name, "this isn't the output of linkrename on the comptime arch");
    real_name := self.program.get_string(info.name);
    target := self.m.intern(real_name);
    if info.lib != Flag.SYMBOL_ZERO.ident() {
        // TODO: probably want to give the backend the path to put in the exe, not our symbolic name, 
        //       and also maybe preintern with the backend to get an ordinal that's just in lib space 
        //       instead of needing to check every time
        set_lib(self, (target, self.program.get_string(info.lib), info.weak));
    };
    
    self.bounce_body_call(target);
}

fn bounce_body_call(self: *EmitIr, target: Qbe.Sym) void = {
    func := self.program.get_function(self.func);
    f_ty := func.finished_ty().unwrap();
    save_signeture(self.program, self.shared, self.f.lnk.id, f_ty);
    save_signeture(self.program, self.shared, target, f_ty);
    self.entry.task = (Bounce = (lnk = self.f.lnk&, target = target));
}

fn save_signeture(program: CompCtx, shared: *FrontendCodegen, id: Qbe.Sym, ty: Type) void = { 
    @if(shared.m.goal.arch != .wasm32 && !@is(shared.m.goal.type, .Cached, .CachedEarly)) return();
    f_ty := @match(program.get_type(ty)) {
        fn Fn(it) => it[];
        fn FnPtr(it) => it.ty;
        @default => return();
    };
    save_signeture(program, shared, id, f_ty)
}

// kinda sucks that this is yet another mirror of emit_par_instructions
fn save_signeture(program: CompCtx, shared: *FrontendCodegen, id: Qbe.Sym, f_ty: FnType) void = { 
    @if(shared.m.goal.arch != .wasm32 && !@is(shared.m.goal.type, .Cached, .CachedEarly)) return();
    enter_task shared.worker { entry |
        f := temp().box_uninit(Qbe.Fn);
        default_init(f, shared.m);
        b := temp().box_zeroed(Qbe.Blk);
        f.start = b;
        args := @if(f_ty.unary, @slice(f_ty.arg), program.arg_types(f_ty.arg));
        for args { ty | 
            continue :: local_return;
            info := program.get_info(ty);
            if info.stride_bytes == 0 {  // :NoRuntimeRepr
                if ty == CVariadicType {
                    b.push(.argv, .Kw, QbeNull, QbeNull, QbeNull);
                };
                continue();
            };
            _, kk := load_op(program, ty);
            if kk != .Ke {
                b.push(.arg, kk, QbeNull, QbeNull, QbeNull);
            } else {
                b.push(.argc, .Kl, QbeNull, get_aggregate(program, shared, ty), QbeNull);
            };
        };
        
        _, k := load_op(program, f_ty.ret); 
        type_index := QbeNull;
        if k == .Ke && program.get_info(f_ty.ret)[].stride_bytes != 0 {
            k = .Kl;
            type_index = get_aggregate(program, shared, f_ty.ret);
        };
        r := QbeNull;
        if k != .Ke {
            r = f.newtmp("", k);
        };
        k := @if(k == .Ke, Qbe.Cls.Kw, k);
        callee := f.symcon(id);
        b.push(.call, k, r, callee, type_index);
        entry.task = (SaveSign = f);
    };
};

fn emit_body(self: *EmitIr, f: FuncId) CRes(void) #once = {
    func := self.program.get_function(f);
    
    if func.get_flag(.LogAst) {
        @eprintln("[#log_ast %] %", f, self.program.log(func));
        self.program.data.cast()[][].codemap.show_error_line(func.loc, true /*TODO*/);
    };
    m := self.shared.m;
    
    self.f.start = self.new_block("start");
    self.blk = self.f.start;

    body: *FatExpr = @match(func.body&) {
        fn Normal(body) => body;
        fn Intrinsic(op) => {
            // Direct calls will be inlined but someone might be trying to call through a function pointer. 
            // TODO: don't add to callees for direct calls. -- Jul 24
            if op[] == INTRINSIC_GET_ENV {
                self.context_par_e();
            };
            a0, a1, ret := self.intrinsic_shim(func);
            self.blk.jmp.arg = @try(self.emit_intrinsic_old(op, .ScalarTransient, @slice(a0, a1), func, ret)) return;
            self.finalize_blocks();
            return(.Ok)
        }
        fn NewIntrinsic(op) => {
            not_stand_alone := @is(@as(Qbe.O) op.ir_op, 
                .vastart, .cas0, .cas1, .sel0, .sel1, .blit0, .blit1, 
                .call, .arg, .argc, .argv, .par, .parc, .pare, 
            );
            if not_stand_alone {
                self.blk.jmp.type = .hlt;
                self.finalize_blocks();
                return(.Ok);
            };
            
            a0, a1, ret := self.intrinsic_shim(func);
            self.blk.jmp.arg = @try(self.emit_intrinsic_new(op, .ScalarTransient, @slice(a0, a1), func, ret)) return;
            self.finalize_blocks();
            return(.Ok)
        }
        fn DynamicImport(info) => {
            @match(self.when) {
                fn Aot() => self.aot_import_body(info);
                fn Jit() => {
                    @err_assert(info.comptime != 0 || info.weak, "we hit a dynamicimport ('%' from '%') with no comptimeaddr for jit\nthe frontend should make sure this doesn't happen. \nTODO: this happens when compiling targetting libc from a compiler built without libc on linux", self.program.get_string(info.name), self.program.get_string(info.lib)) return;
                    self.entry.task = (JitImport = (lnk = self.f.lnk&, addr = rawptr_from_int(info.comptime)));
                }
            };
            return(.Ok)
        };
        fn Redirect(fid) => {
            callee := self.program.get_function(fid[]);
            if !callee.get_flag(.NoContext) {
                func.set_flag(.YesContext);
            };
            self.bounce_body_call(self.func_sym(fid[]));
            self.pending.push(fid[]);
            return(.Ok)
        } 
        fn Asm(it) => {
            code := temp().box_zeroed(MultiArchAsm);
            code[.aarch64] = it.arm64;
            code[.x86_64] = it.amd64;
            code[.rv64] = it.rv64;
            code[.wasm32] = it.wasm32;
            self.entry.task = (Asm = (
                lnk = self.f.lnk&, 
                code = code,
            ));
            
            // TODO: be not fucking ugly
            f_ty := @unwrap(self.program.data.cast()[][].func_type(f), "unknown signeture") return;
            save_signeture(self.program, self.shared, self.f.lnk.id, f_ty);
            
            return(.Ok)
        }
        fn FrcImport(it) => {
            @if(DISABLE_IMPORT_FRC) return(@err("TODO: DISABLE_IMPORT_FRC because scoping when you import the compiler is messed up"));
            self.entry.task = @try(self.move_from_module(it)) return;
            return(.Ok);
        }
        @default => return(@err("no acceptable impl %", self.program.log(func)));
    };
    
    self.context_par_e();
    // note: this rewrites self.blk
    @try(self.bind_parameters()) return;
    
    // Note: this is different from the body expr type because of early returns.
    ret := self.program.get_function(f)[].finished_ret.unwrap();
    _, k := self.load_op(ret); 
    scalar := k != .Ke;
    
    self.f.start.jmp.type = .jmp;
    self.f.start.s1 = self.blk;
    p1: Placement = @if(scalar, .ScalarTransient, .NewMemory);
    if self.program.get_info(ret)[].stride_bytes == 0 {  // :NoRuntimeRepr
        p1 = .Discard;
    };
    self.unreachable_block = self.new_block("unreachable");
    self.unreachable_block.jmp.type = .hlt;
    self.unreachable_block.visit = 0;
    result := @try(self.compile_expr(body, p1)) return;
    
    self.emit_ret(result);
    self.finalize_blocks();
    
    .Ok
}

fn emit_ret(self: *EmitIr, result: Qbe.Ref) void = {
    ret := self.program.get_function(self.func)[].finished_ret.unwrap();
    _, k := self.load_op(ret); 
    scalar := k != .Ke;

    ::enum(Qbe.J);
    if self.blk.jmp.type == .hlt {
        return();
    };
    
    self.blk.jmp.arg = result;
    self.f.ret_cls = .Kx;
    @if_else {
        @if(self.blk.jmp.type != .Jxxx) => {
            // TODO: assert ret=Never?
        };
        @if(ret.is_never()) => {
            self.blk.jmp = (type = .hlt, arg = QbeNull);
        };
        @if(self.program.get_info(ret)[].stride_bytes == 0) => {
            self.blk.jmp = (type = .ret0, arg = QbeNull);
        };
        @if(scalar) => {
            @debug_assert_ne(result, QbeNull, "missing return value");
            self.f.retty = QbeNull;
            // TODO: tell it about b/h so we can fully follow the c abi :FUCKED but the old one didn't do it either. 
            self.blk.jmp.type = ret_op(k);
            self.f.ret_cls = k;
        };
        @else => {
            @debug_assert_ne(result, QbeNull, "missing return value");
            self.f.retty = TYPE(self.get_aggregate(ret).val());
            self.blk.jmp.type = .retc;
            self.f.ret_cls = .Ke;
        };
    };
}

fn intrinsic_shim(self: *EmitIr, func: *Func) Ty(Qbe.Ref, Qbe.Ref, Type) = {
    arg := func.arg.bindings.items();
    @debug_assert(arg.len == 1 || arg.len == 2);
    
    f_ty := func.finished_ty().unwrap();
    arg_ty := self.program.arg_types(f_ty.arg);
    
    _, k0 := self.load_op(arg_ty[0]);
    a0 := self.f.newtmp("a", k0);
    if k0 != .Ke {
        self.emit(.par, k0, a0, QbeNull, QbeNull);
    } else {
        a0 = QbeNull;
    };
    a1 := QbeNull;
    // :DoFoldInvalidArity 
    // `!= void` makes do_fold() work. it wants to pass extra void args to a #ir function.
    // TODO: allow any zero sized types as trailing args?
    if arg.len == 2 && arg_ty[1] != void {
        _, k1 := self.load_op(arg_ty[1]);
        a1 = self.f.newtmp("a", k1);
        self.emit(.par, k1, a1, QbeNull, QbeNull);
    };
    self.f.retty = QbeNull;
    _, kr := self.load_op(f_ty.ret);
    self.blk.jmp.type = ret_op(kr);
    self.f.ret_cls = kr;
    (a0, a1, f_ty.ret)
}

fn ret_op(k: Qbe.Cls) Qbe.J = 
    @as(Qbe.J) @as(i32) Qbe.J.retw.raw() + k.raw()

fn dbgloc(self: *EmitIr, loc: Span, tag: i64) void = {
    if self.debug {
        n: i64 = loc.low.zext();
        // :DebugTag
        // TODO: factor out all the places i do this bit fiddling. this is getting stupid. 
        hi, lo := (n.shift_right_logical(16).bit_or(tag.shift_left(16)), n.bit_and(1.shift_left(16) - 1));
        self.emit(.dbgloc, .Kw, QbeNull, INT(hi), INT(lo));  
    };
}

// :EmitIrCall TODO: it would be nice if i didn't need to repeat so many variations of the par/arg/call/ret logic. 
fn emit_runtime_call(
    self: *EmitIr,
    f_ty_ret: Type,
    args: []Qbe.Ins,
    p: Placement,
    callee: Qbe.Ref,
    dyn_context: bool,
    loc: Span,
) CRes(Qbe.Ref) = { 
    self.dbgloc(loc, 0);
    
    self.f.leaf = false;
    if dyn_context && self.context != QbeNull {
        self.emit(.arge, .Kl, QbeNull, self.context, QbeNull);
    };
    append(self.blk.ins&, args.ptr, args.ptr.offset(args.len));
    _, k := self.load_op(f_ty_ret);
    scalar := k != .Ke;
    type_r := QbeNull;
    info := self.program.get_info(f_ty_ret);
    result_r := if info.stride_bytes == 0 {  // :get_or_create_type
        k = .Kw;
        QbeNull
    } else {
        if !scalar {
            k = .Kl;
            type_r = self.get_aggregate(f_ty_ret);
            if p&.is(.Discard) {
                p = .NewMemory;
            };
        } else {
            if p&.is(.Discard) {
                // on native targets it would be fine to use Qbe.Null here 
                // but on wasm the signatures must match exactly. 
                p = .ScalarTransient;
            };
        };
        self.scalar_dest(p, k)
    };
    self.emit(.call, k, result_r, callee, type_r);
    self.hlt_if_never(f_ty_ret);
    
    if(result_r == QbeNull, => return(Ok = QbeNull));
    if scalar { 
        return(Ok = self.scalar_result(p, result_r, f_ty_ret));
    };
    (Ok = @match(p) {
        fn NewMemory() => result_r;
        fn Discard()   => QbeNull;
        fn Blit(dest) => {
            self.blit(dest, result_r, info.stride_bytes.zext());
            dest
        }
        @default => return(@err("ICE: tried to assign aggragate call to scalar placement"));
    })
}

fn is_local_scalar(self: CompCtx, v: Var, ty: Type) bool #inline = {
    _, k := load_op(self, ty);
    k != .Ke && !{self.vtable.took_address}(self.data, v&)
}

fn compile_stmt(self: *EmitIr, stmt: *FatStmt) CRes(void) = {
    if self.debug && !(stmt.stmt&.is(.Eval) && stmt.stmt.Eval.expr&.is(.Block)) {
        //place := self.program.get_whole_line(stmt.loc); // :SLOW
        //if self.last_line != place.line {
        //    // TODO: dbgfile too because it might be something inlined from elsewhere. 
        //    // TODO: have the backend remove consecutive dbgloc instructions and just keep the last one 
        //    //       when all the stuff between them was optimised out. 
        //    self.emit(.dbgloc, .Kw, QbeNull, INT(place.line), INT(0));  
        //    self.last_line = place.line;
        //};
    };
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Eval(expr) => {
            @try(self.compile_expr(expr, .Discard)) return;
            .Ok
        }
        fn Decl(f) => {
            @debug_assert_ne(f.kind, VarType.Const);
            @debug_assert(f.name&.is(.Var));
            @debug_assert(f.ty&.is(.Finished), "variable not typechecked");
            type := f.ty.Finished;
            if self.program.is_local_scalar(f.name.Var, type) {
                val  := @try(self.compile_expr(f.default&, .Scalar)) return;
                if rtype(val) != .RTmp {
                    _, k := self.load_op(type);
                    var := self.f.newtmp("v", k);
                    self.emit(.copy, k, var, val, QbeNull);
                    val = var;
                };
                self.record_var_decl(f.name.Var, (Local = val));
            } else {
                ptr  := @try(self.compile_expr(f.default&, .NewMemory)) return;
                self.declare_var_mem(f.name.Var, ptr, type);
            };
            .Ok
        }
        fn Set(f) => {   // :PlaceExpr
            // it feels a bit sketchy to be looking at the value's type not the dest type,
            // but there's always a Cast node that changes the type if needed. 
            size: i64 = self.program.get_info(f.value.ty)[].stride_bytes.zext();
            @match(f.place.expr&) {
                fn GetVar(it) => {
                    info := self.var_lookup&.get_ptr(it.id) 
                        || return(@err("missing runtime var %", self.program.get_string(it.name)));
                    @if_let(info) fn Local(dest) => {
                        @try(self.compile_expr(f.value&, (Assign = dest[]))) return;
                        return(.Ok);
                    };
                    src := @try(self.compile_expr(f.value&, .NewMemory)) return;
                    dest := @try(self.addr_macro(f.place&, .Scalar)) return;
                    self.blit(dest, src, size);
                }
                fn Deref(arg) => {
                    o := self.store_op(f.value.ty);
                    if o != .Oxxx { 
                        // This special case reduces ir_ops by 1.6% and makes mostly the same code. -- Aug 6, 2025
                        src := @try(self.compile_expr(f.value&, .Scalar)) return;
                        dest := @try(self.compile_expr(arg[], .Scalar)) return;
                        self.emit(o, .Kw, Qbe.Null, src, dest);
                        return(.Ok)
                    };
                    
                    src := @try(self.compile_expr(f.value&, .NewMemory)) return;
                    dest := @try(self.compile_expr(arg[], .Scalar)) return;
                    @debug_assert(size == self.program.get_info(f.place.ty)[].stride_bytes.zext() || { size == 0 && self.blk.jmp.type == .hlt });
                    self.blit(dest, src, size);
                }
                @default => return(@err("TODO: other `place=e;` :("));
            };
            .Ok
        }
        fn DeclVarPattern(f) => {
            simple := f.binding.bindings.len == 1 || f.value.expr&.is(.Tuple);
            if simple { 
                // This is the pattern we generate for capturing calls. 
                parts := f.value&.items();
                @err_assert(parts.len == f.binding.bindings.len, "ICE: DeclVarPattern tuple size mismatch") return;
                enumerate parts { i, value |
                    b   := f.binding.bindings[i]&;
                    ptr := @try(self.compile_expr(value, .NewMemory)) return;
                    @debug_assert(b.ty&.is(.Finished), "variable not typechecked");
                    if b.var() { name |
                        self.declare_var_mem(name, ptr, b.ty.Finished);
                    };
                };
            } else {
                // it's a destructuring (not inlined args)
                // We store the whole value in a stack slot and then save pointers into different offsets of it as thier own variables.
                
                base := @try(self.compile_expr(f.value&, .NewMemory)) return;
                info := self.program.get_type(f.value.ty);
                @err_assert(info.is(.Struct), "destructure must be tuple") return;
                @debug_assert(info.Struct.layout_done, "ICE: layout not done (DeclVarPattern)");
                fields := info.Struct.fields&;
                @debug_assert_eq(fields.len, f.binding.bindings.len, "destructure size mismatch");
                enumerate f.binding.bindings.items() { i, b |
                    f       := fields[i]&;
                    name    := @unwrap(b.var(), "tuple binding requires name") return;
                    element := self.offset(base, f.byte_offset);
                    @debug_assert(b.ty&.is(.Finished), "ICE: variable not typechecked");
                    @debug_assert_ne(f.byte_offset, FIELD_LAYOUT_NOT_DONE, "ICE: field % not ready (DeclVarPattern)", self.program.get_string(f.name));
                    self.declare_var_mem(name, element, b.ty.Finished);
                };
            };
            .Ok
        };
        fn Noop() => .Ok;
        // Can't hit DoneDeclFunc because we don't re-eval constants.
        @default => @err("ICE: stmt not desugared %", self.program.log(stmt));
    }
}

fn compile_expr(self: *EmitIr, expr: *FatExpr, p: Placement) CRes(Qbe.Ref) = {
    @debug_assert(!expr.ty.is_unknown(), "Not typechecked: %", self.program.log(expr));
    @debug_assert_ne(expr.ty, CVariadicType, "ICE: expression of type CVariadic must be an Expr::CVariadic as final parameter to a call");
    self.last_loc = expr.loc;
    if self.blk.jmp.type == .hlt {  // TODO: do we need this or do we trust sema to always get rid of things after a Never? 
        return(Ok = QbeNull);
    };

    expr_ty := expr.ty;
    (Ok = @match(expr.expr&) {
        fn Cast(v)  => {
            // TODO: bleh, make the front end check this? 
            from : i64 = self.program.get_info(v.ty)[].stride_bytes.zext();
            to   : i64 = self.program.get_info(expr_ty)[].stride_bytes.zext();
            @err_assert(from == to, "@as size mismatch: % vs %", self.program.log(v.ty), self.program.log(expr_ty)) return;
            return(self.compile_expr(v[], p))
        }
        fn Call(f)    => return(self.emit_call(f.f, f.arg, p));
        fn Block(f)   => return(self.emit_block_expr(expr, p));
        fn Value(f)   => return(self.emit_constant(f.bytes&, p, expr.ty, expr.loc));
        fn If(_)      => return(self.emit_call_if(expr, p));
        fn Loop(arg)  => return(self.emit_call_loop(arg[]));
        fn Addr(arg)  => return(self.addr_macro(arg[], p));
        fn StructLiteralP(pattern) => return(self.construct_aggregate(pattern, expr.ty, p));
        fn Slice(arg) => {
            if(p&.is(.Discard), => return(self.compile_expr(arg[], .Discard)));
            
            container_ty := arg.ty;
            // Note: number of elements, not size of the whole array value.
            ty, count := @match(arg.expr) {
                fn Tuple(parts) Ty(Type, i64) => { 
                    fst := parts[0];
                    (fst.ty, parts.len) 
                };
                @default => (arg.ty, 1);
            };
            ptr  := @try(self.compile_expr(arg[], .NewMemory)) return;
            dest := self.get_memory(p, expr_ty);
            self.emit(.storel, .Kw, QbeNull, ptr, dest);
            self.emit(.storel, .Kw, QbeNull, self.f.getcon(count), self.offset(dest, 8));
            dest
        };
        fn GetVar(it) => {
            info := self.var_lookup&.get_ptr(it.id) 
                || return(@err("missing runtime var %", self.program.get_string(it.name)));
            @if_let(info) fn Local(dest) => {
                // ScalarTransient means we promise the ref will not be held past a statement that could assign to a variable. 
                if !p&.is(.ScalarTransient) {  // try to avoid a bunch of redundent copies. 
                    self.unify_placement(p&, expr_ty);
                };
                return(Ok = self.scalar_result(p, dest[], expr_ty));
            };
            src := @try(self.addr_macro(expr, .ScalarTransient)) return; // get the pointer
            self.deref(src, p, expr_ty)
        }
        fn DataSymbol(info) => {
            name := self.program.get_string(info.name);
            id := self.m.intern(name);
            
            if info.lib != Flag.SYMBOL_ZERO.ident() {
                if self.when == .Jit && info.comptime != 0 {
                    return(Ok = self.scalar_result(p, self.f.getcon(info.comptime), i64));   // :get_or_create_type
                };
                // TODO: if jitting and not ready yet, we could make this be a call to a shim that returns the symbol?
                set_lib(self, (id, self.program.get_string(info.lib), info.weak));
            };
            
            save_signeture(self.program, self.shared, id, expr.ty);
            self.scalar_result(p, self.f.symcon(id), i64)  // :get_or_create_type
        }
        fn Deref(arg) => {
            src := @try(self.compile_expr(arg[], @if(p&.is(.Discard), .Discard, .Scalar))) return; // get the pointer
            self.deref(src, p, expr_ty)
        };
        fn FnPtr(arg) => {
            f := @unwrap(arg[].as_const(), "expected fn for ptr") return;
            f := FuncId.assume_cast(f&)[];
            callee_r := self.func_ref(f);
            self.scalar_result(p, callee_r, i64)  // :get_or_create_type
        };
        fn Unreachable() => {
            self.hlt_if_never(Never); // :get_or_create_type
            QbeConZero
        }
        fn Uninitialized() => {
            @err_assert(!expr_ty.is_never(), "call exit() to produce a value of type 'Never'") return;
            // Wierd special case I have mixed feelings about. should at least set to sentinal value in debug mode.
            @match(p) {
                fn Scalar()          => QbeConZero;
                fn ScalarTransient() => QbeConZero;
                fn Blit(it)    => it;
                fn NewMemory() => self.alloca(expr_ty);
                fn Discard()   => QbeNull;
                fn Assign(it)  => it;
            }
        };
        fn Tuple(values) => {
            if values.len == 1 {
                return(self.compile_expr(values[0]&, p));
            };
            @err_assert(!(p&.is(.ScalarTransient) || p&.is(.Scalar) || p&.is(.Assign)), "ICE: non single field struct as scalar") return;
            raw := self.program.raw_type(expr.ty); // TODO: do i need this? 
            // TODO: this is sad/pastey
            @match(self.program.get_type(raw)) {
               fn Struct(f) => {
                    @err_assert(f.layout_done, "ICE: struct layout not ready.") return;
                    @assert_eq(f.fields.len, values.len, "Expr::Tuple field count");
                    @err_assert(f.is_tuple, "Expr::Tuple can only create tuple type") return;
                    base := self.get_memory(p, expr_ty);
                    range(0, f.fields.len) { i |
                        dest := self.offset(base, f.fields[i].byte_offset);
                        @debug_assert_ne(f.fields[i].byte_offset, FIELD_LAYOUT_NOT_DONE, "field % not ready", self.program.get_string(f.fields[i].name));
                        @try(self.compile_expr(values[i]&, (Blit = dest))) return;
                    };
                    base
                }
                fn Array(f) => {
                    @debug_assert_eq(values.len(), f.len.zext(), "array len");
                    element_size: i64 = self.program.get_info(f.inner)[].stride_bytes.zext();
                    base := self.get_memory(p, expr_ty);
                    mem  := base;
                    each values { value |
                        @try(self.compile_expr(value, (Blit = mem))) return;
                        mem = self.offset(mem, element_size);
                    };
                    base
                }
                @default => return(@err("Expr::Tuple should have struct type not %. (0/1 element tuple maybe?)", self.program.log(raw)));
            }
        }
        fn PtrOffset(f) => {
            // TODO: compiler has to emit tagchecks for enums now! but it does not!
            base  := @try(self.compile_expr(f.ptr, .ScalarTransient)) return;
            field := self.offset(base, f.bytes);
            self.scalar_result(p, field, i64) // :get_or_create_type
        }
        fn Switch(f) => return(self.emit_switch(expr, p));
        fn CVariadic() => return(@err("Expr::CVariadic can only be used as the last parameter to a call"));
        fn FrcImport(it) => {
            @if(DISABLE_IMPORT_FRC) return(@err("TODO: DISABLE_IMPORT_FRC because scoping when you import the compiler is messed up"));
            return(self.emit_import_ref(it, p));
        }
        @default => return(@err("ICE: didn't desugar: %", self.program.log(expr)));
    })
}

fn deref(self: *EmitIr, src: Qbe.Ref, p: Placement, expr_ty: Type) Qbe.Ref = {
    value_type := expr_ty;
    self.hlt_if_never(expr_ty);
    size: i64 = self.program.get_info(value_type)[].stride_bytes.zext();
    if(p&.is(.Discard) || size == 0, => return(QbeConZero)); // :NoRuntimeRepr
    if p&.is(.ScalarTransient) || p&.is(.Scalar) || p&.is(.Assign) {
        @debug_assert(size <= 8);
        o, k := self.load_op(expr_ty);
        r := self.scalar_dest(p, k);
        self.emit(o, k, r, src, QbeNull);
        return(self.scalar_result(p, r, expr_ty));
    };
    dest := self.get_memory(p, value_type);
    self.blit(dest, src, size);
    dest
}

fn guess_can_assign(e: *FatExpr) bool = {
    if (@is(e.expr&, .Value, .GetVar, .String)) {
        return(false)
    };
    
    // These all have a single inner expression as the first field. 
    // TODO: probably a bad idea to be this fragile with repr. 
    if (@is(e.expr&, .PtrOffset, .Cast, .Loop, .Addr, .Quote, .Slice, .Deref, .FnPtr)) {
        return(guess_can_assign(e.expr.Deref));
    };
    
    // TODO: this could recurse on call/tuple/struct, etc and get more. not sure if it's worth it. variable reads are the most offensive. 
    true
}

fn emit_call(self: *EmitIr, f: *FatExpr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    caller := self.program.get_function(self.func);
    @match(self.program.get_type(f.ty)) {
        fn Fn(f_ty) => {
            f_id := @unwrap(f.as_const(), "ice: tried to call non-const fn %", self.program.log(f)) return;
            f_id := FuncId.assume_cast(f_id&)[];
            func := self.program.get_function(f_id);
            @err_assert(!func.get_flag(.Generic), "tried to emit call to unlowered #generic") return;
            @debug_assert(!func.get_flag(.MayHaveAquiredCaptures), "tried to emit call to unlowered maybe '=>'");
            @assert(!func.get_flag(.Inline), "ICE: tried to call inlined %", self.program.get_string(func.name));

            // TODO: audit: is this f_ty different from the one we just got from the expression type? -- Jul 8
            f_ty := func.finished_ty().unwrap(); // kinda HACK to fix unaligned store? 
            callee := func;
           
            callee_r := self.func_ref(f_id);
            
            @match(func.body&) {
                fn NewIntrinsic(op) => {
                    arg := arg.items();
                    a0 := if arg[0].ty == void {
                        QbeNull
                    } else {
                        p1: Placement = @if(arg.len != 1 && guess_can_assign(arg[1]&), .Scalar, .ScalarTransient);
                        @try(self.compile_expr(arg[0]&, p1)) return
                    };
                    // the || void is for :DoFoldInvalidArity
                    a1 := if(arg.len == 1 || arg[1].ty == void, => QbeNull, => @try(self.compile_expr(arg[1]&, .ScalarTransient)) return);
                    return(self.emit_intrinsic_new(op, p, @slice(a0, a1), caller, f_ty.ret));
                }
                fn Intrinsic(op) => {
                    // i kinda want to get rid of this and let the backend handle all inlining,
                    // and see if that's slower but you can't for set_dynamic_context. see :CompileIntrinsicsFirst

                    ::if(Qbe.Ref);
                    arg := arg.items();
                    a0 := @try(self.compile_expr(arg[0]&, .Scalar)) return;
                    a1 := if(arg.len == 1, => QbeNull, => @try(self.compile_expr(arg[1]&, .ScalarTransient)) return);
                    return(self.emit_intrinsic_old(op, p, @slice(a0, a1), caller, f_ty.ret));
                }
                fn DynamicImport(info) => if self.when == .Aot {
                    @err_assert(info.name == func.name, "this isn't the output of linkrename on the comptime arch") return;
                    real_name := self.program.get_string(info.name);
                    callee_r = self.f.symcon(self.m.intern(real_name));
                };
                @default => ();
            };
            context := !callee.get_flag(.NoContext);
            if context {
                caller.set_flag(.YesContext);
            };
            
            args := @try(self.compile_for_arg(arg)) return;
            self.emit_runtime_call(f_ty.ret, args, p, callee_r, context, arg.loc)
        }
        //
        // :ADistinctionWithoutADifference  -- May 31, 2025 
        // f: rawptr = fid;        // Expr.FnPtr
        // f := :: @as(rawptr) fid; // Expr.Values
        // do not try to represent this as an Expr.Values (even tho you almost could now that shims work well) 
        // because objc_msgSend_stret needs to be #avoid_shim and also #linkrename-ed to the same address on arm but 
        // not on amd. So if you have a comptime rawptr for that function, you can't tell which function it should bake to.
        // (see also objective_c.fr/choose_objc_dispatcher())
        //
        fn FnPtr(f_ty) => {
            fn is_variadic(self: *SelfHosted, f_ty: FnType) bool = {
                args := self.arg_types(f_ty.arg);
                for(args, fn(a) => @if(a == CVariadicType) return(true));
                false
            }
            context := !self.program.data.cast()[][].is_variadic(f_ty.ty);
            
            @if(context) caller.set_flag(.YesContext);
            callee := @try(self.compile_expr(f, .Scalar)) return;
            args   := @try(self.compile_for_arg(arg)) return;
            self.emit_runtime_call(f_ty.ty.ret, args, p, callee, context, arg.loc)
        }
        fn Label(_) => {
            label := @unwrap(f.as_const(), "called label must be const") return;
            label := LabelId.assume_cast(label&)[];

            ret := self.inlined_return_addr&.get_ptr(label);
            ret := @unwrap(ret, "missing return label. forgot '=>' on function?") return;
            ret.used = true;  // note: updating the one in the map! not a copy
            
            // TODO: show the location of the @must_return call as well. 
            @err_assert(ret.depth == self.label_depth, "tried to local_return past a call marked @must_return") return;
            result := @try(self.compile_expr(arg, ret.p)) return;
            seal self { b |
                b.jmp.type = .jmp;
                b.s1 = ret.blk;
                self.blk = self.unreachable_block;
            };
            (Ok = QbeNull)
        }
        @default => (@err("ICE: non callable: %", self.program.log(f)));
    }
}

fn emit_intrinsic_new(self: *EmitIr, op: *NewIntrinsicPayload, p: Placement, arg: []Qbe.Ref, caller: *Func, expr_ty: Type) CRes(Qbe.Ref) = {
    @debug_assert(op.ir_op != 0, "no ir op");
    o := @as(Qbe.O) op.ir_op;
    k := @as(Qbe.Cls) op.ir_cls;
    r := self.scalar_dest(p, k); 
    if r == QbeNull && !no_result(o) {
        r = self.f.newtmp("f", k);
    };
    self.emit(o, k, r, arg[0], arg[1]);
    (Ok = self.scalar_result(p, r, expr_ty))
}

fn emit_intrinsic_old(self: *EmitIr, op: *i64, p: Placement, arg: []Qbe.Ref, caller: *Func, expr_ty: Type) CRes(Qbe.Ref) = {
    r := @switch(op[]) {
        @case(INTRINSIC_GET_ENV) => {
            caller.set_flag(.YesContext);  // TODO: you might have setcontext for yourself
            self.unify_placement(p&, expr_ty); // because we don't know if they'll set again before using. meh. 
            self.context
        };
        @case(INTRINSIC_SET_ENV) => {
            if self.context == QbeNull {
                self.context = self.f.newtmp("context", .Kl);
            };
            self.emit(.copy, .Kl, self.context, arg[0], QbeNull);
            QbeConZero
        };
        @default => @panic("unchandled old intrinsic %", op[]);
    };
    (Ok = self.scalar_result(p, r, expr_ty))
}

fn func_sym(self: *EmitIr, f_id: FuncId) Qbe.Sym #inline = {
    self.pending.push(f_id); // HACK
    self.f.globals.intern(self.fmt_fn_name(f_id))
}

fn func_ref(self: *EmitIr, f_id: FuncId) Qbe.Ref #inline = 
    self.f.symcon(self.func_sym(f_id));

fn emit_block_expr(self: *EmitIr, expr: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    block   := expr.expr.Block&;
    self.dbgloc(expr.loc, 1);
    ret_var := or block.ret_label {
        // Simple case: its just grouping some statements into an expression. 
        each block.body& { stmt |
            @try(self.compile_stmt(stmt)) return;
        };
        result := self.compile_expr(block.result, p);
        self.dbgloc(expr.loc, 2);
        return result;
    };
    must_return: u32 = int(block.flags.bit_and(1.shift_left(@as(i64) BlockFlags.MustReturn)) != 0).trunc();
    self.label_depth += must_return;
    
    // :block_never_unify_early_return_type
    // Note: block_ty can be different from value.ty if the fall through is a Never but there's an early return to the block. 
    block_ty := expr.ty;
    
    // It might have an early return so we want to unify that up here and then the Placement system can deal with it. 
    dest := self.unify_placement(p&, block_ty);
    out  := self.program.get_info(block_ty);

    return_block_index := self.blocks.len;
    ret: ReturnTarget = (
        blk = self.new_block("local"),
        p = p,
        used = false,
        depth = self.label_depth,
    );
    prev := self.inlined_return_addr&.insert(ret_var, ret);
    @debug_assert(prev.is_none(), "stomped ret var");

    each block.body& { stmt | 
        // Note: these are allowed to do a local_return targeting `ret`
        @try(self.compile_stmt(stmt)) return;
    };
    _ := @try(self.compile_expr(block.result, p)) return;

    ret := self.inlined_return_addr&.remove(ret_var).unwrap();
    if ret.used {
        // They did a local_return, so our extra block was useful. 
        seal self { b |
            // There was also a fallthough expression which we convert to a local_return.
            b.s1 = ret.blk;
            b.jmp.type = .jmp;
        };
        self.blk = ret.blk;
    } else {
        // They didn't actually use a local_return so we can discard this useless block. 
        ret.blk.jmp = (type = .hlt, arg = QbeNull);
        ret.blk.visit = 0;
        // We did generate slightly more noisy code by `unify_placement` earlier than necessary. 
    };
    self.label_depth -= must_return;
    self.dbgloc(expr.loc, 2);
    (Ok = dest)
}

stores :: import("@/backend/opt/mem.fr")'store_by_size;

fn emit_constant(self: *EmitIr, value: *Values, p: Placement, expr_ty: Type, loc: Span) CRes(Qbe.Ref) #once = {
    if(p&.is(.Discard), => return(Ok = QbeNull));  // TODO: bake_relocatable_value side effect? 
    info := self.program.get_info(expr_ty);
    use_int_literal := value.is(.Small) && (!info.contains_pointers || self.when == .Jit || value.Small._0 == 0) && stores[value.Small._1.zext()] != .Oxxx;
    if use_int_literal {
        v := self.f.getcon(value.Small._0);
        return(Ok = self.scalar_result(p, v, expr_ty));
    };
    
    @match(self.when) {
        fn Aot() => {
            id := @try(self.program.emit_relocatable_constant(expr_ty, value.bytes(), loc)) return;
            baked := self.program.get_baked(id);
            if value.len() > 64 || value.len().mod(8) != 0 {  // last one unrechable because alignment?
                self.reference_constant(id);
                dest := self.get_memory(p, expr_ty);
                src := self.as_ref(AddrOf = id);
                self.blit(dest, src, info.stride_bytes.zext());
                return(Ok = dest);
            };
            
            // if it's small, load 8 byte chunks now instead of doing a runtime blit. 
            // This generates better code for constant slices/allocators/etc 
            // (lets the backend see the individual fields are constant instead of treating it as a mutable static). 
            if baked.relocations.len == 1 && baked.len() == 8 {
                bake_iter_legacy baked { part |
                    v := self.as_ref(part);
                    return(Ok = self.scalar_result(p, v, expr_ty));
                };
            };
            
            dest := self.get_memory(p, expr_ty); 
            i := 0;
            bake_iter_legacy baked { part |
                r := self.as_ref(part);
                self.emit(.storel, .Kw, QbeNull, r, self.offset(dest, i * 8));
                i += 1;
            };
            (Ok = dest)
        }
        fn Jit() => {
            dest := self.get_memory(p, expr_ty);
            @debug_assert_eq(value.len(), info.stride_bytes.zext(), "value size mismatch");
            if value.len() < 64 && value.len().mod(8) == 0 {
                // if it's not that big, hoist the load to right now so the backend knows the parts are constant. 
                parts: []i64 = (ptr = i64.ptr_from_int(value.jit_addr()), len = value.len() / 8);
                enumerate parts { i, value |
                    self.emit(.storel, .Kw, QbeNull, self.f.getcon(value[]), self.offset(dest, i * 8));
                };
            } else {
                // this is a bit sketchy because it could still be Values.Small (and stored inline),
                // but since it's in a ast node that lasts forever, it should be fine. 
                src := self.f.getcon(value.jit_addr());
                self.blit(dest, src, info.stride_bytes.zext());
            };
            (Ok = dest)
        }
    }
}

fn all_zeroes(value: *Values) bool = 
    (value.is(.Small) && value.Small._0 == 0) || value.bytes().all_zeroes();

fn all_zeroes(b: []u8) bool = {
    for b { b |
        if(b != 0, => return(false));
    };
    true
}

fn reference_constant(self: *EmitIr, id: BakedVarId) void = {
    shared := self.shared;
    seen := shared.constants_used&;
    if !seen.get(id.id.zext()) {
        seen.set(id.id.zext());
        value := self.program.get_baked(id);
        
        for value.relocations { it |
            @match(it.target) {
                fn BakedVarId(it) => self.reference_constant(it);
                fn FuncId(it) => self.pending.push(it);  // HACK
                fn Var() => unreachable();
            };
        };
        
        m := self.m;
        @if(m.goal.arch == .x86_64) 
        use_symbol(m, m.intern(self.program.mangle_name(id))) { s |
            // :NotForCorrectness
            s.pledge_local = true;
        };
        
        self.new_constants.push(id);
    };
}

fn as_ref(self: *EmitIr, part: BakedEntry) Qbe.Ref = @match(part) {
    // TODO: non-8 byte fields
    fn Num(f) => self.f.getcon(f.value);
    fn FnPtr(f) => self.func_ref(f);
    fn AddrOf(id) => {
        self.reference_constant(id);
        name := self.program.mangle_name(id);
        self.f.symcon(name)
    }
    fn AddrOfA(it) => {
        self.reference_constant(it.base);
        name := self.program.mangle_name(it.base);
        self.offset(self.f.symcon(name), it.addend.intcast())
        
    }
};

// :PlaceExpr
fn addr_macro(self: *EmitIr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    self.last_loc = arg.loc;
    // field accesses should have been desugared.
    @err_assert(arg.expr&.is(.GetVar), "took address of r-value") return;
    var := arg.expr.GetVar;
    @assert_ne(var.kind, .Const, "Cannot take address of constant % \n(use @static if you want to create a non-threadsafe mutable global)", var&.log(self.program));
    ref := @unwrap(self.var_lookup&.get_ptr(var.id), "Missing var % (in !addr) \n(missing $ when used in const context or missing '=>'? TODO: better error message)", var&.log(self.program)) return;
    @err_assert(ref.is(.Alloca), "addr_macro of scalar local `%`", self.program.get_string(var.name)) return;
    (Ok = self.scalar_result(p, ref.Alloca, i64))
}

fn emit_call_if(self: *EmitIr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    @debug_assert(arg.expr&.is(.If));
    parts := arg.expr.If&;
    
    cond := @try(self.compile_expr(parts.cond, .ScalarTransient)) return;
    t_expr, f_expr := (parts.if_true, parts.if_false);
    
    is_trivial :: fn(self: *EmitIr, e: *FatExpr) bool = {
        if (@is(e.expr&, .Value, .GetVar, .Addr, .FnPtr)) {
            return(true);
        };
        @if_let(e.expr&) fn Block(it) => {
            return(it.body.len == 0 && is_trivial(self, it.result));
        };
        // this doesn't really help because of the arg variables after inlining && and || 
        //@if_let(e.expr&) fn If(it) => {
        //    return(self.is_trivial(it.cond) && self.is_trivial(it.if_true) && self.is_trivial(it.if_false));
        //};
        false
    };
    
    if arg.ty != void {  // :get_or_create_type
        _, k := load_op(self, arg.ty);
        use_sel := k != .Ke && k.is_int() && self.is_trivial(t_expr) && self.is_trivial(f_expr);
        if use_sel {
            // If both sides are very cheap to evaluate, just do a cmov instead of creating a bunch of new blocks. 
            // This is not required for correctness, it's just a way to give the backend less work to do. 
            
            t_val := @try(self.compile_expr(t_expr, .Scalar)) return;
            f_val := @try(self.compile_expr(f_expr, .ScalarTransient)) return;
            
            if(p&.tag() == .Discard, => return(Ok = QbeNull));
            r := self.scalar_dest(p, k);
            self.emit(.sel0, .Kw, QbeNull, cond, QbeNull);
            self.emit(.sel1, k, r, t_val, f_val);
            return(Ok = self.scalar_result(p, r, arg.ty));
        };
    };
    
    t_blk, f_blk, join_blk := (self.new_block("yes"), self.new_block("no"), self.new_block("join"));
    seal self { b |
        b.jmp = (type = .jnz, arg = cond);
        b.s1 = t_blk;
        b.s2 = f_blk;
    };
    
    // force both branches to output to the same place. 
    join_val := self.unify_placement(p&, arg.ty);
    joined := false;
    
    self.blk = t_blk;
    t_val := @try(self.compile_expr(t_expr, p)) return;
    self.hlt_if_never(t_expr.ty);
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = join_blk;
        joined = true;
    };
    
    self.blk = f_blk;
    f_val := @try(self.compile_expr(f_expr, p)) return;
    self.hlt_if_never(f_expr.ty);
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = join_blk;
        joined = true;
    };
    
    self.blk = join_blk;
    if !joined {
        // both branches diverged. 
        join_blk.jmp = (type = .hlt, arg = QbeNull); // TODO: remove the useless block 
    };
    (Ok = join_val)
}

// TODO: use phi for scalars. 
fn emit_switch(self: *EmitIr, arg: *FatExpr, p: Placement) CRes(Qbe.Ref) #once = {
    @debug_assert(arg.expr&.is(.Switch));
    parts := arg.expr.Switch;
    inspect := @try(self.compile_expr(parts.value, .ScalarTransient)) return; 
    
    {
        o, k := self.load_op(parts.value.ty);
        @err_assert(k.is_int(), "can only switch over ints") return;
        if o != .load {
            o := rebase(o, .extsb, .loadsb);
            r := self.f.newtmp("switch", .Kl);
            self.emit(o, .Kl, r, inspect, QbeNull);
            inspect = r;
        };
    };
    
    //name := self.program.get_string(self.program.get_function(self.func)[].name);
    n: i64 = self.f.switch_count.zext();
    ss := self.f.switches&;
    {
        if n == 0 {
            ss[] = new(1);
        } else {
            ss.grow(n + 1);
        };
        self.f.switch_count += 1;
        self.blk.jmp = (type = .switch, arg = INT(n));
    };
    ss[n] = (
        cases = new(parts.cases.len),
        case_count = 0,
        inspect = inspect,
        src = self.blk,
        default = self.new_block("default"),
    );
    
    result := self.unify_placement(p&, arg.ty);
   
    // This is where we rejoin with the value of the whole switch expression.
    end_blk   := self.new_block("joins"); 
    rejoined := false;
    values := i64.list(parts.cases.len, temp());
    each parts.cases { f |
        value := f._0;
        body  := f._1&;
        
        // TODO: move this check to sema. 
        @err_assert(!values.items().contains(value&), "value % appears twice in switch node", value) return;
        values&.push(value);
        
        case_block := self.new_block(@tfmt("case%", value));
        ss[n].cases[ss[n].case_count] = (case_block, value);
        ss[n].case_count += 1;
        self.blk = case_block; 
        
        _ := @try(self.compile_expr(body, p)) return;
        seal self { b |
            b.jmp.type = .jmp;
            b.s1 = end_blk;
            rejoined = true;
        };
    };
    
    self.blk = ss[n].default;
    _ := @try(self.compile_expr(parts.default, p)) return;
    seal self { b |
        b.s1 = end_blk;
        b.jmp.type = .jmp;
        rejoined = true;
    };
    
    if !rejoined {
        end_blk.jmp = (type = .hlt, arg = QbeNull); // TODO: remove the useless block
    };
    self.blk = end_blk;
    @debug_assert_eq(ss[n].case_count, parts.cases.len);
    (Ok = result)
}

fn emit_call_loop(self: *EmitIr, arg: *FatExpr) CRes(Qbe.Ref) #once = {
    @debug_assert_eq(arg.ty, void);
    start := self.new_block("loop");
    self.blk.jmp.type = .jmp;
    self.blk.s1 = start;
    self.blk    = start;
    @try(self.compile_expr(arg, .Discard)) return;
    seal self { b |
        b.jmp.type = .jmp;
        b.s1 = start;
    };
    (Ok = QbeNull)
}

fn construct_aggregate(self: *EmitIr, pattern: *Pattern, requested: Type, p: Placement) CRes(Qbe.Ref) #once = {
    if p&.is(.Discard) {
        each pattern.bindings& { b | 
            @try(self.compile_expr(b.get_default().unwrap(), .Discard)) return;
        };
        return(Ok = QbeNull);
    };
    raw_container_ty := self.program.raw_type(requested);
    (Ok = @match(self.program.get_type(raw_container_ty)) {
        fn Struct(f) => {
            @debug_assert(f.layout_done, "ICE: layout not done (construct_aggregate)");
            expected := f.fields.len;
            if f.is_union {
                @debug_assert_eq(pattern.bindings.len, 1, "union must have exactly one active field");
                expected = 1;
            } else {
                @debug_assert_eq(expected, pattern.bindings.len, "Cannot assign to type % with wrong field count", self.program.log(requested));
                if expected == 1 {
                    expr := pattern.bindings.index(0).get_default().unwrap();
                    return(self.compile_expr(expr, p));
                };
            };
            
            // This is perhaps more hassle than it's worth. 
            if @is(p&.tag(), .ScalarTransient, .Assign, .Scalar) {
                _, k := self.program.load_op(raw_container_ty);
                @err_assert(k != .Ke, "trying to construct non-trivial struct into a scalar") return;
                result := QbeNull;
                enumerate pattern.bindings& { i, b | 
                    expr := b.get_default().unwrap();
                    if self.program.get_info(expr.ty)[].stride_bytes == 0 {
                        _ := @try(self.compile_expr(expr, .Discard)) return;
                    } else {
                        @debug_assert_eq(result, QbeNull);
                        result = @try(self.compile_expr(expr, p)) return;
                    };
                };
                return(Ok = result);
            };
            
            base := self.get_memory(p, raw_container_ty);
            enumerate pattern.bindings& { i, b | 
                name := @unwrap(b.ident(), "map literal entry needs name (while initilizing %)", self.program.log(requested)) return;
                field := or find_struct_field(f, name, i) {
                    return(@err("field name mismatch (ICE: should be checked by sema)"))
                };
                expr := b.get_default().unwrap();
                @debug_assert_ne(field.byte_offset, FIELD_LAYOUT_NOT_DONE, "field % not ready (construct_aggregate)", self.program.get_string(field.name));
                dest := self.offset(base, field.byte_offset);
                _ := @try(self.compile_expr(expr, (Blit = dest))) return;
            };
            base
        }
        fn Tagged(f) => {
            @debug_assert_eq(pattern.bindings.len, 1, "@tagged must have one active varient");
            value := pattern.bindings.index(0).get_default().unwrap();
            name := pattern.bindings[0].name.unwrap();
            i    := f.cases.index_of(fn(f) => f._0 == name).expect("case name to exist in type");
            if self.program.get_info(value.ty)[].stride_bytes == 0 {  // :get_or_create_type
                _ := @try(self.compile_expr(value, .Discard)) return;
                return(Ok = self.scalar_result(p, self.f.getcon(i), i64));
            };
            @match(p) {
                fn Discard() => return(self.compile_expr(value, .Discard)); // just for side effects
                fn Scalar()  => return(@err("ICE: tagged union is not a scalar."));
                fn ScalarTransient() => return(@err("ICE: tagged union is not a scalar."));
                @default     => ();
            };
            base := self.get_memory(p, raw_container_ty);
            self.emit(.storel, .Kw, QbeNull, self.f.getcon(i), base);  // tag :get_or_create_type // TODO: smaller tag sizes than 64 bits 
            _ := @try(self.compile_expr(value, (Blit = self.offset(base, 8)))) return;
            base
        }
        @default => return(@err("struct literal for non-(struct/tagged)"));
    })
}

// note: this is not called for scalars the frontend can trivially tell don't escape 
fn declare_var_mem(self: *EmitIr, name: Var, ptr: Qbe.Ref, type: Type) void #inline = {
    info := self.program.get_info(type);
    if info.stride_bytes == 0 {
        // :NoRuntimeRepr
        // It's nice to let macros/generics operate over types uniformly.
        // Some types have no runtime representation, so any attempt to use them should compile to nothing. 
        // TODO: maybe sema should try to strip these out so we don't have to keep checking here. 
        self.var_lookup&.insert(name.id, (Alloca = QbeConZero));
        return()  
    };

    @debug_assert_eq(rtype(ptr), .RTmp, "declare_var_mem %", ptr);
    self.record_var_decl(name, (Alloca = ptr));
}

fn record_var_decl(self: *EmitIr, name: Var, value: IrVar) void #inline = {
    prev := self.var_lookup&.insert(name.id, value);
    //@debug_assert(prev.is_none(), "shadow is still new var"); TODO farm_game gets here but clearly it's fine. idk man -- Jan 8
    
    if self.debug && name.name != Flag.SYMBOL_ZERO.ident() {
        name := self.program.get_string(name.name);
        name.len = name.len.min(20);
        ::@assert(IrVar.Tag().enum_count() == 2, "update IrVar repr");
        r := value.Local;
        t := self.f.get_temporary(r);
        @if(self.f.track_ir_names()) {
            t.name = @tfmt("%.%", name, r.val());
        };
    };
}

ir_op_count :: @static(i64);

// IMPORTANT: you can't use f.emit because threads. 
fn emit(self: *EmitIr, o: Qbe.O, k: Qbe.Cls, dest: Qbe.Ref, a0: Qbe.Ref, a1: Qbe.Ref) void #inline = {
    self.program.data.cast()[][].stats.ir_ops += 1;
    @debug_assert(k.raw() < 4, "ICE: invalid base % in emit_ir:emit %", k, o);
    @debug_assert(o != .Oxxx, "ICE: null inst emit_ir:emit");
    push(self.blk, make_ins(o, k, dest, a0, a1));
}

// TODO: types larger than 2^16 bytes but you probably don't want to copy those anyway
fn blit(self: *EmitIr, dest: Qbe.Ref, src: Qbe.Ref, bytes: i64) void #inline = {
    if(bytes == 0, => return());
    // mem.fr/promote knows about 1/2/4/8 byte blits. 
    if bytes == 16 {    
        // TODO: maybe we can't do this cause alignment but we're kinda bad at eliminating blits. :UGLY
        //       and the old backend special cased this so its sad to be worse. 
        //       should make slot coalessing not get confused by blits or lower them earlier.
        //       TODO: would sroa be that hard? i know which things don't escape. 
        v  := self.f.newtmp("b", .Kl);
        v2 := self.f.newtmp("b", .Kl);
        src2  := self.offset(src, 8);
        dest2 := self.offset(dest, 8);
        self.emit(.load, .Kl, v, src, QbeNull);
        self.emit(.load, .Kl, v2, src2, QbeNull);
        self.emit(.storel, .Kw, QbeNull, v, dest);
        self.emit(.storel, .Kw, QbeNull, v2, dest2);
        return();
    };
    self.emit(.blit0, .Kw, QbeNull, src, dest);
    self.emit(.blit1, .Kw, QbeNull, INT(bytes), QbeNull);
}

fn alloca(self: *EmitIr, ty: Type) Qbe.Ref = {
    info := self.program.get_info(ty);
    if info.stride_bytes == 0 { // :NoRuntimeRepr 
        return(QbeConZero);
    };
    t := self.f.newtmp("s", .Kl);
    @debug_assert(info.align_bytes <= 8, "i don't support high alignments yet");
    o: Qbe.O = @if(info.align_bytes <= 4, .alloc4, .alloc8);
    s := self.f.getcon(info.stride_bytes.zext());
    push(self.f.start, make_ins(o, .Kl, t, s, QbeNull));
    t
}

fn offset(self: *EmitIr, ptr: Qbe.Ref, bytes: i64) Qbe.Ref #inline = {
    if(bytes == 0, => return(ptr));
    new := self.f.newtmp("f", .Kl);
    self.emit(.add, .Kl, new, ptr, self.f.getcon(bytes));
    new
}

fn unify_placement(self: *EmitIr, p: *Placement, ty: Type) Qbe.Ref = @match(p[]) {
    fn Blit(it) => it;
    fn Discard() => QbeNull;
    fn NewMemory() => {
        ref := self.alloca(ty);
        p[] = (Blit = ref);
        ref
    }
    fn Scalar() => {   // TODO: use phi instead
        _, k := self.load_op(ty);
        @debug_assert(k != .Ke, "ICE: attempted scalar placement of aggragate type");
        dest := self.f.newtmp("unify", k);
        p[] = (Assign = dest);
        dest
    }
    fn ScalarTransient() => {  
        _, k := self.load_op(ty);
        @debug_assert(k != .Ke, "ICE: attempted scalar placement of aggragate type");
        dest := self.f.newtmp("unify", k);
        p[] = (Assign = dest);
        dest
    }
    fn Assign(it) => it;
};

fn get_memory(self: *EmitIr, r: Placement, ty: Type) Qbe.Ref = {
    @match(r) {
        fn NewMemory() => self.alloca(ty);
        fn Blit(it) => it;
        fn Discard() => self.alloca(ty);  // TODO: maybe force the caller to early out? 
        @default => @panic("invalid Placement type for get_memory: %", r&.tag());
    }
}

// This exists to save a copy with Placement.Assign, 
// otherwise it just creates a new tmp for you to use. 
// Caller has to pass the result to scalar_result after copying in the value. 
fn scalar_dest(self: *EmitIr, p: Placement, k: Qbe.Cls) Qbe.Ref = @match(p) {
    fn Discard() => QbeNull;
    fn Assign(dest) => dest;
    @default => {
        @debug_assert(k != .Ke);
        self.f.newtmp("v", k)
    };
};

fn scalar_result(self: *EmitIr, p: Placement, r: Qbe.Ref, ty: Type) Qbe.Ref = @match(p) {
    fn NewMemory() => self.scalar_result((Blit = self.alloca(ty)), r, ty);
    fn Blit(dest)  => {
        size := self.program.get_info(ty)[].stride_bytes;
        if size == 0 {
            return(QbeConZero);  // :NoRuntimeRepr
        };
        o := self.store_op(ty);
        if o == .Oxxx {
            // TODO: this only sane if called from emit_value so it should probably just deal with this instead. 
            // It's convinent to emit constants as a literal even if we want a struct with multiple fields. 
            // TODO: is alignment real?
            @debug_assert(size <= 8, "tried to use scalar result for large aggragate");
            stores;
            o = stores[size.zext()];
            @debug_assert(o != .Oxxx, "tried to use scalar result for strange sized aggragate");
        };
        self.emit(o, .Kw, QbeNull, r, dest);
        dest
    }
    fn Scalar()  => r;
    fn ScalarTransient() => r;
    fn Discard() => QbeNull;
    fn Assign(dest) => {
        if dest != r {
            t := self.f.get_temporary(dest);
            self.emit(.copy, t.cls, dest, r, QbeNull);
        };
        dest
    }
};

fn get_aggregate(self: *EmitIr, ty: Type) Qbe.Ref #inline = 
    get_aggregate(self.program, self.shared, ty);

// TODO: reuse types for different nominal types with the same repr?  -- new_type does this now. might not be worth it tho. 
// if you fix this to set the name don't forget to update the hash in new_type
fn get_aggregate(self: CompCtx, m: *FrontendCodegen, ty: Type) Qbe.Ref = {
    info := self.get_info(ty);
    // not get_or_insert because this is recursive
    ir_index := m.type_indices&.get(ty) || {
        type := self.get_type(ty);
        result := Qbe.Typ.zeroed();
        result.header.size = info.stride_bytes;
        @debug_assert(result.size != 0, "ICE: treating zero size type % as an aggragate", self.log(ty));
        result.header.align_log2 = (@as(i64) info.align_bytes.zext()).trailing_zeros().trunc(); // TODO: do this faster
        // TODO: track names
        i := @match(type) {
            fn Named(it)    => self.get_aggregate(m, it._0).val();
            fn Enum(it)     => self.get_aggregate(m, it.raw).val();
            fn Struct(it) => {
                yield :: local_return;
                if self.repr_transparent(it) { repr |
                    yield(self.get_aggregate(m, repr).val());
                };
                n := 0;
                rt_count: i64 = it.fields.len;
                f := u32.list(rt_count * 2 + 1, m.m.forever&.borrow()); f := f&;
                @assert(!it.is_union || it.fields.len <= 0xFFFF, "too many union fields");
                result.header.nunion = @if(it.is_union, rt_count.trunc(), 1);
                result&.set_is_union(it.is_union);
                @assert(it.layout_done, "tried to get_aggregate before layout %", self.log(ty));
                if it.is_union {
                    each it.fields { it | 
                        field_size := self.get_info(it.ty)[].stride_bytes;
                        if field_size != 0 {
                            @debug_assert(it.byte_offset == 0, "union field must have offset 0");
                            push(f, pack self.as_field(m, it.ty));
                            push(f, pack (type = .FEnd, len = 0)); // TODO: a test that hits this!
                        } else {
                            result.header.nunion -= 1; // :NoRuntimeRepr
                        };
                    };
                } else {
                    off := 0;
                    each it.fields { it | 
                        field_size := self.get_info(it.ty)[].stride_bytes;
                        if field_size != 0 {
                            add_padding(f, off&, it.byte_offset);
                            push(f, pack self.as_field(m, it.ty));
                            off += field_size.zext();
                        }; // else :NoRuntimeRepr
                    };
                    add_padding(f, off&, info.stride_bytes.zext());
                    f.push(pack(type = .FEnd, len = 0));
                };
                result.fields = f.items();
                // TODO: name
                m.m.new_type(result)
            }
            fn Array(it) => if it.len == 1 {
                self.get_aggregate(m, it.inner).val()
            } else {
                inner_size: i64 = self.get_info(it.inner)[].stride_bytes.zext();
                @debug_assert_eq(inner_size * it.len.zext(), info.stride_bytes.zext(), "size calc wrong!");
                outer_size: i64 = info.stride_bytes.zext();
                f := u32.list(1 + it.len.zext() + 1, m.m.forever&.borrow()); f := f&;
                result.header.nunion = 1;
                f.push_repeated(it.len.zext(), self.as_field(m, it.inner).pack());
                f.push(pack(type = .FEnd, len = 0));
                result.fields = f.items();
                // TODO: name
                m.m.new_type(result)
            }
            fn Tagged(it) => {
                n := 0;
                f := u32.list(it.cases.len * 3 + 3, m.m.forever&.borrow()); f := f&;
                result.header.nunion = it.cases.len.trunc();
                each it.cases { case | 
                    if self.get_info(case._1)[].stride_bytes != 0 { 
                        f.push(pack(type = .Fl, len = 8));
                        f.push(pack self.as_field(m, case._1));
                        f.push(pack(type = .FEnd, len = 0));
                        // TODO: do i need to pad them to the same size?
                    } else {
                        result.header.nunion -= 1;
                    };
                };
                result&.set_is_union(result.header.nunion > 1);
                if result.header.nunion == 0 {
                    // all cases are zero sized. this is really just an enum. 
                    @debug_assert(false, "ICE: get_aggregate of scalar @tagged");
                    f.push(pack(type = .Fl, len = 8));
                    f.push(pack(type = .FEnd, len = 0));
                };
                result.fields = f.items();
                // TODO: name
                m.m.new_type(result)
            }
            @default => @panic("tried to get_aggregate of scalar %", self.log(ty));
        }; 
        m.type_indices&.insert(ty, i);
        i
    };
    
    TYPE(ir_index)
}

fn as_field(self: *EmitIr, ty: Type) Qbe.Field #inline = 
    as_field(self.program, self.shared, ty);

fn as_field(self: CompCtx, m: *FrontendCodegen, ty: Type) Qbe.Field = {
    ::if(Qbe.Field);
    type := self.get_type(ty);
    @match(type) {
        fn F64()      => (type = .Fd, len = 8);
        fn F32()      => (type = .Fs, len = 4);
        fn Ptr(_)     => (type = .Fl, len = 8);
        fn FnPtr(_)   => (type = .Fl, len = 8);
        fn VoidPtr(_) => (type = .Fl, len = 8);
        fn Label(_)   => (type = .Fw, len = 4);
        fn Fn(_)      => (type = .Fw, len = 4);
        fn Bool()     => (type = .Fb, len = 1);
        fn Named(it)  => self.as_field(m, it._0);
        fn Enum(it)   => self.as_field(m, it.raw);
        fn Int(it)    => @switch(it.bit_count) {
            @case(8)  => (type = .Fb, len = 1);
            @case(16) => (type = .Fh, len = 2);
            @case(32) => (type = .Fw, len = 4);
            @default  => (type = .Fl, len = 8);
        }
        fn Tagged(it) => {
            if self.is_tag_only(it) {
                return(type = .Fl, len = 8);
            };
            (type = .FTyp, len = self.get_aggregate(m, ty).val().trunc())
        }
        fn Struct(it) => {
            if self.repr_transparent(it) { inner |
                return(self.as_field(m, inner))
            };
            (type = .FTyp, len = self.get_aggregate(m, ty).val().trunc())
        }
        fn Array(it) => if it.len == 1 {
            self.as_field(m, it.inner)
        } else {
            (type = .FTyp, len = self.get_aggregate(m, ty).val().trunc())
        };
        @default => @panic("tried to as_field of unhandled %", self.log(ty));
    }
}

fn is_tag_only(self: CompCtx, t: *TypeInfo.get_variant_type(.Tagged)) bool = {
    each t.cases { case | 
        if case._1 != void {  // :get_or_create_type
            return(false);
        };
    };
    true
}

fn repr_transparent(self: CompCtx, it: *TypeInfo.get_variant_type(.Struct)) ?Type = {
    @switch(it.fields.len) {
        @case(0) => (Some = void); // :get_or_create_type
        @case(1) => return(Some = it.fields[0].ty);
        @default => { 
            sized_field := void;
            if(it.is_union, => return(.None));
            each it.fields { f | 
                field_size := self.get_info(f.ty)[].stride_bytes;
                if field_size != 0 {
                    if(sized_field != void, => return(.None));
                    sized_field = f.ty;
                }; // else :NoRuntimeRepr
            }; 
            (Some = sized_field)
        };
    }
}

fn hlt_if_never(self: *EmitIr, ty: Type) void #inline = {
    if ty.is_never() && self.blk.jmp.type == .Jxxx {
        self.blk.jmp = (type = .hlt, arg = QbeNull);
    };
}

// TODO: single field array

// returns .Ke for aggregates
fn load_op(self: *EmitIr, ty: Type) Ty(Qbe.O, Qbe.Cls) #inline = 
    load_op(self.program, ty);

fn load_op(self: CompCtx, ty: Type) Ty(Qbe.O, Qbe.Cls) = @match(self.get_type(ty)) {
    fn Int(it)   => @switch(it.bit_count) {
        @case(8)  => (@if(it.signed, .loadsb, .loadub), .Kw);
        @case(16) => (@if(it.signed, .loadsh, .loaduh), .Kw);
        @case(32) => (@if(it.signed, .loadsw, .loaduw), .Kw);
        @default  => (.load, .Kl);
    }
    fn F32()     => (.load, .Ks);
    fn F64()     => (.load, .Kd);
    fn Ptr(_)    => (.load, .Kl);
    fn FnPtr(_)  => (.load, .Kl);
    fn VoidPtr() => (.load, .Kl);
    fn Label(_)  => (.loaduw, .Kw);
    fn Fn(_)     => (.loaduw, .Kw);
    fn Named(it) => self.load_op(it._0);
    fn Enum(it)  => self.load_op(it.raw);
    fn Bool()    => (.loadub, .Kw);
    fn Tagged(it) => {
        if self.is_tag_only(it) {
            return(.load, .Kl);
        };
        (.nop, .Ke)
    }
    fn Struct(it) => self.load_op(or self.repr_transparent(it) {
        return(.nop, .Ke)
    });
    fn Array(it) => @if(it.len == 1, self.load_op(it.inner), (.nop, .Ke));
    @default     => (.nop, .Ke);
};

fn store_op(self: *EmitIr, ty: Type) Qbe.O #inline = 
    store_op(self.program, ty);

fn store_op(self: CompCtx, ty: Type) Qbe.O = @match(self.get_type(ty)) {
    fn Int(it)   => @switch(it.bit_count) {
        @case(8)  => .storeb;
        @case(16) => .storeh;
        @case(32) => .storew;
        @default  => .storel;
    }
    fn F32()     => .stores;
    fn F64()     => .stored;
    fn Ptr(_)    => .storel;
    fn FnPtr(_)  => .storel;
    fn VoidPtr() => .storel;
    fn Label(_)  => .storew;
    fn Fn(_)     => .storew;
    fn Named(it) => self.store_op(it._0);
    fn Enum(it)  => self.store_op(it.raw);
    fn Bool()    => .storeb;
    fn Tagged(it) => {
        if !self.is_tag_only(it) {
            return(.Oxxx);
        };
        .storel
    }
    fn Struct(it) => self.store_op(self.repr_transparent(it) || return(.Oxxx));
    fn Array(it) => @if(it.len == 1, self.store_op(it.inner), .Oxxx);
    @default     => .Oxxx;
};

fn new_block(self: *EmitIr, debug: Str) *Qbe.Blk = {   
    b := temp().box_zeroed(Qbe.Blk);
    @if(self.f.track_ir_names()) {
        b.name = @tfmt("b%_%", self.next_block_name, debug);
        self.next_block_name += 1; // not the same as blocks.len because we remove for unused local returns. 
    };
    b.visit = -1;
    self.blocks&.push(b);
    b
}

fn fmt_fn_name(self: *EmitIr, f: FuncId) []u8 = 
    self.program.fmt_fn_name(f);

fn seal(self: *EmitIr, $body: @Fn(b: *Qbe.Blk) void) void = {
    if self.blk.jmp.type == .Jxxx { 
        body(self.blk);
    };
}
