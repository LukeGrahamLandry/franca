CompilerDispatch :: @struct(    
    overloads: BucketArray(OverloadSetData),
    enclosing_function: ?FuncId,  // for adding runtime callees. 
    const_bound_memo: HashMap(MemoKey, FuncId),
    yield_count: i64 = 0,
    return_labels: BucketArray(FuncId), // [LabelId]
    wip_tasks: List(*Task),
);

Task :: @rec @struct(
    action: *Action,
    trace: ?TraceCtx,
    loc: Span,
);

ExprAttempt :: @struct(expr: *FatExpr, requested: ResultType);
VarAttempt :: @struct(name: Var, requested: ResultType);
Action :: @rec @tagged(
    CompileBody: FuncId,  // After this, we can emit_ir.
    Jit: FuncId, // After this, it's safe to call a function pointer. 
    EvalConstant: VarAttempt,  // find_const. 
    ResolveOverload: *OverloadAttempt,
    FinishRecType: @struct(hole: Type, name: Var, value: *FatExpr),
    // Used for function callees, you can't call a function until ~all~ are ready.
    All: []*Task,
);

OverloadAttempt :: @struct(
    call: ExprAttempt, 
    callsite: ?FuncId,
    options: List(FuncId),
    last_ready_count: u32,
    prev_index: u32 = 0,
    os: OverloadSet, 
    state: u8 = 0,
);

fn init(a: Alloc) CompilerDispatch = {
    (
        overloads = init(10, a),
        enclosing_function = .None,
        const_bound_memo = init(a),
        return_labels = init(10, a),
        wip_tasks = list(10, a),
    )
}

poll_in_place :: fn(self: *SelfHosted, $T: Type, $f: @Fn() Maybe(T)) Res(T) #generic = {
    loop {
        value := f();
        @match(value&) {
            fn Ok(it)  => return(Ok = it[]);
            fn Err(it) => return(Err = it[]);
            fn Suspend(new_dep) => {
                t := self.new_task(new_dep[]);
                @try(self.poll_until_finished(t)) return;
            };
        };
    }
};

fn new_task(self: *SelfHosted, action: *Action) *Task = {
    new_task := self.get_alloc().boxed(Task, 
        (action = action, trace = .None, loc = self.last_loc));
    new_task
}

fn poll_until_finished(self: *SelfHosted, task: *Task) Result(void, *CompileError) = {
    the_mark := mark_temporary_storage();
    i := self.dispatch.wip_tasks.len;
    self.dispatch.wip_tasks&.push(task);
    while => self.dispatch.wip_tasks.len > i {
        continue :: local_return;
        reset_temporary_storage(the_mark);
        
        task := self.dispatch.wip_tasks[self.dispatch.wip_tasks.len - 1];
        
        if task.trace.is_none() {
            task.trace = (Some = zone_begin(.Task));
            @if(ENABLE_TRACY) {
                out := u8.list(temp());  // it seems to make a copy so this is fine
                log(task.action&, self, out&);
                out&.push(0);
                ___tracy_emit_zone_name(task.trace.Some, out.items());
            };
        };
        
        @if_let(task.action) fn All(tasks) => {
            while => tasks.len > 0 {
                it := tasks[tasks.len - 1];
                tasks.len -= 1;
                self.dispatch.wip_tasks&.push(it);
                continue();
            };
            x := self.dispatch.wip_tasks&.pop().unwrap();
            ::[]Task;
            @if(@run safety_check_enabled(.DebugAssertions)) x.slice(1).set_zeroed();
            continue();
        };
        
        res := self.exec_task(task);
        if !res&.is(.Suspend) {
            zone := task.trace.unwrap();
            zone_end(zone);
            task.trace = .None;
        };
        
        @match(res) {
            fn Err(e) => {
                // TODO: you want to not pop them so it shows in error trace but some errors are ok? like overloading?
                //       not popping makes @assert_type_error bubble up
                //       (also if not pop need to delay recycle_overloadattempt)
                self.dispatch.wip_tasks.len = i;
                return(Err = e);
            }
            fn Ok() => {
                x := self.dispatch.wip_tasks&.pop().unwrap();
                @if(@run safety_check_enabled(.DebugAssertions)) x.slice(1).set_zeroed();
                continue();
            }
            fn Suspend(next) => {
                t := self.new_task(next);
                self.dispatch.wip_tasks&.push(t);
                continue();
            }
        };
        unreachable();
    };
    
    .Ok
}

::tagged(Action);
fn exec_task(self: *SelfHosted, task: *Task) Maybe(void) = {
    self.dispatch.enclosing_function = .None;
    
    @match(task.action) {
        fn EvalConstant(v) => {
            name := v.name;
            var  := self.scopes.get_constant(name);
            var  := var.expect("var should always be known");
            type := var.maybe_stack_allocated_type();
            @check(self.handle_declare_constant(name, type, var.expr&)) return;
        }
        fn CompileBody(fid) => {
            fid := fid[];
            func := self.get_function(fid);
            if !func.get_flag(.EnsuredCompiled) {
                zone := zone_begin(.SemaFunction); // TODO: defer
                @if(ENABLE_TRACY) {
                    real_name := self.pool.get(func.name);
                    ___tracy_emit_zone_name(zone, real_name);
                };
                res := self.handle_compile_func_body(fid);
                zone_end(zone);
                @check(res) return;
            };
        }
        fn Jit(fid) => {
            fid := fid[];
            func := self.get_function(fid);
            
            if !func.get_flag(.EnsuredCompiled) {
                return(Suspend = self.wait_for(CompileBody = fid));
            };
            
            // when possible, you want to emit callees first so they can be inlined. 
            // but anything you miss here will still get a jit-shim in create_jit_shim (via EmitIr.pending) so it's not a big deal
            waiting: List(*Task) = list(self.get_alloc());
            for func.callees { callee |
                callee_func := self.get_function(callee);
                if !callee_func.get_flag(.AsmDone) {
                    if is_mutually_recursive(self, callee) {
                        // if it's AvoidJitShim there's kinda no hope but you'll find out if you try to call it :)
                    } else {
                        t := self.new_task(self.wait_for(Jit = callee));
                        waiting&.push(t);
                    };
                } else {
                    func := self.get_function(callee);
                    @debug_assert(callee_func.get_flag(.EnsuredCompiled) || callee_func.get_flag(.AlreadyHasShim), "direct callee should be compiled. % calls %.", self.log_name(fid), self.log_name(callee));
                };
            };
            if waiting.len != 0 {
                return(Suspend = self.wait_for(All = waiting.items()));
            };
            
            @debug_assert(func.finished_arg.is_some() && func.finished_ret.is_some());
            
            func := self.get_function(fid);
            if !func.get_flag(.RealAsmDone) {
                @try(shallow_jit_func(self, fid)) return;
            };
            
            // Previously we would push Jit tasks for func.mutual_callees here, 
            // but now we rely on shims being created since you often put a function 
            // pointer in a vtable without actually calling it at comptime -- May 22, 2025
            //
            // HACK: #avoid_shim is needed for obj_msgSend because it needs to lie about its parameter types so they can't be passed on correctly by the shim. 
            for func.mutual_callees { callee |
                callee_func := self.get_function(callee);
                if callee_func.get_flag(.AvoidJitShim) && !callee_func.get_flag(.AsmDone) {
                    t := self.new_task(self.wait_for(Jit = callee));
                    waiting&.push(t);
                };
            };
            if waiting.len != 0 {
                return(Suspend = self.wait_for(All = waiting.items()));
            };

            // :AssumeDone
            func.set_flag(.AsmDone);
        }
        fn ResolveOverload(f) => {
            self.dispatch.enclosing_function = f.callsite;
            @debug_assert(f.call.expr.expr&.is(.Call), "overload resolve must be call");
            call := f.call.expr.expr.Call;
            zone := zone_begin(.SemaOverloads); // TODO: defer
            @if(ENABLE_TRACY) {
                overloads := self.dispatch.overloads&.nested_index(f.os.as_index());
                real_name := self.pool.get(overloads.name);
                ___tracy_emit_zone_name(zone, real_name);
            };
            res := self.resolve_in_overload_set_new(call.arg, f[]);
            zone_end(zone);
            if !res&.is(.Suspend) {
                self.recycle_overloadattempt&.push(f[]);
            };
            fid := @check(res) return;
            f_value := self.to_values(FuncId, fid);
            call.f.set(f_value, self.get_or_create_type(FuncId));
        }
        fn FinishRecType(it) => {
            @check(self.handle_finish_rec_type(it.hole, it.name, it.value)) return;
        }
        fn All() => unreachable();
    };
    .Ok
}

fn is_mutually_recursive(self: *SelfHosted, callee: FuncId) bool = {
    for self.dispatch.wip_tasks { task |
        @if_let(task.action) fn Jit(other) => {
            if other[] == callee {
                return(true);
            };
        };
    };
    false
}

fn Maybe($T: Type) Type = {
    Self :: @tagged(
        Ok: T,
        Err: *CompileError,
        Suspend: *Action,
    );
    ::tagged(Self);
    Self
}

fn wait_for(s: *SelfHosted, t: Action) *Action = {
    s.stats.yield_count += 1;
    s.get_alloc().boxed(Action, t)
}
