//! Assorted functions work working with the bytes of comptime values.
//! - Emitting relocatable baked constants for AOT (fn bake_relocatable_value).
//! - Splitting aggragate values for function calls with partially $const parameters. 
//! - Track the mapping between comptime and runtime address spaces. 
//! - Determining struct size/layout

//////////////////////
/// Bake Constants ///
// See compiler/ast_external.fr/{BakedValue, BakedReloc}

/*
TODO: 
    - update the userspace apis to use the new representation. 
        table.emit_relocatable_constant_body, fn bake_relocatable_value(), fn dyn_bake_relocatable_value() #ct,
    - allow the frontend to ask for certain alignment of constant data and to ask for zero init on large 
        areas without actually allocating them at comptime. 
    - :ScaryIfAliasWrongType
        this is scary if you lie about the type so they don't match and would have expected to be baked differently. 
        once we allow internal pointers, the simple case would be if we see a pointer to the first field of a struct 
        and then later see a pointer to the whole struct. somehow we have to go back and fix the old thing to point 
        to the beginning of a larger memory area. for now we're just super cringe and declare that IFNDR   -- Mar 9, 2025 
        or another example would be: the same string being baked as CStr and Str.
    - i should have constant pointers as a concept in my language.
        right now i could put anything i show should be constant 
        (@const_slice, StringPool, TypeInfo, etc) in a special arena. 
        and then check where the pointer is for a hint that should catch a few of them.
    - life gets very confusing if someone mutates the value later after we think it's safe to emit it. 
        right now we're being very careful in bake_translate_legacy to allow a user bake impl that 
        modifies the bytes for AOT but doesn't change the comptime bytes if the same value is used again 
        later (which ArenaAlloc relies on). it would feel more robust if all the user bakes were delayed until 
        the very end of compilation but my general goal is to be more unordered not less so that would be unfortunate. 
    - some memory is expected to alias (stack, temp()) so should be unbakable. 
*/

// since pointers might have cycles, forced_id is used so you can add a cache entry for and address before it's been emitted. 
fn bake_relocatable_constant(c: *SelfHosted, value: []u8, type: Type, loc: Span, forced_id: ?BakedVarId) Result(Ty(BakedVarId, i64), *CompileError) = {
    to_zero: ReadBytes = (bytes = value, i = 0);
    c.comp().zero_padding(type, to_zero&);  // Needed for reproducible builds!
    
    jit_ptr := u8.int_from_ptr(value.ptr);
    // - values <= 8 bytes are stored inline so thier jit_addr is not useful. 
    // - when forced_id is set, it's a pointer that we've already checked the cache for (but now the placeholder will be there)
    if value.len() > 8 && forced_id.is_none() {
        // TODO: this barely does anything! cause like cstr and slice don't go through this function. they call dyn_bake_relocatable_value. -- Jun 30
        //       this is the right idea, they just need to check thier pointers against the cache first. 
        // TODO: be careful about aliasing as different types with different custom_bake_constant? -- Jun 30 
        if c.baked.vmem&.get(rawptr_from_int jit_ptr) { it, addend |
            @if_let(it) fn BakedVarId(it) => return(Ok = (it, addend));
        };
    };
    
    info := c.get_info(type);
    finish :: fn(v: BakedValue) Never => {
        v.align = info.align_bytes.zext();
        if forced_id { id |
            dest := c.baked.values&.nested_index(id.id.zext());
            n := dest.name;
            dest[] = v;
            dest.name = n;
            return(Ok = (id, 0))
        } else {
            return(Ok = (c.put_baked(v, (Some = value)), 0))
        };
        unreachable()  // TODO: this shouldn't be needed
    };
    
    // TODO: storing jit_ptr in put_baked is sketchy for small values,
    //       but currently it's only used for jit constant access and if it were small you'd have just loaded it directly in emit_bc. -- Nov 3
    
    // TODO: is it really that helpful to tell the backend to use zeroinitilizer? could at least have a faster memcmp. 
    if value.all_zeroes() {
        finish(template = (Zeroes = value.len()), loc = loc);
    };
    
    // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
    if !info.contains_pointers {
        // If this is actually a top level Value and the is less than 8, 
        // this is very very bad because it would be stored inline and not have a stable address. 
        // BUT we also recurse here when following pointers so we can't deny small values here. 
        finish(template = (Bytes = value), loc = loc);
    };
    
    out := BakedReloc.list(c.get_alloc());
    start_id := c.baked.values.len;
    @try(c.bake_relocatable_body(value, type, out&, 0, false)) return;
    
    // It's really irritating to force the user code to pass the span around, 
    // so instead, just assume anything made inside that call can be attributed to this constant. 
    if !loc.is_null() {
        end_id   := c.baked.values.len;
        range(start_id, end_id) { i | 
            value := c.baked.values&.nested_index(i);
            if value.loc.is_null() {
                value.loc = loc;
            }
        }
    };
    
    finish(template = (Bytes = value), loc = loc, relocations = out&.items())
}

// TODO: make sure baked.lookup actually ever helps. might need to add checks in more places.
fn bake_relocatable_body(c: *SelfHosted, value: []u8, type: Type, out: *List(BakedReloc), reloc_off: u32, force_default_handling: bool) Result(void, *CompileError) = {
    info          := c.get_info(type);
    first_word    := if(value.len < 8, => 0 /*don't read OOB*/, => ptr_cast_unchecked(u8, i64, value.ptr)[]);
    only_one_word := value.len == 8;
    @debug_assert_eq(value.len(), info.stride_bytes.zext(), "Tried to emit constant value of wrong size.");
    treat_as_raw_bytes := false 
        // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
        || !info.contains_pointers 
        // We only support 64-bit targets so it contains_pointers and is 8 bytes then it's exactly one pointer. 
        // Eventually i want ?*T to use zero as .None but for now allow null pointers. 
        || (only_one_word && first_word == 0)
    ;
    if(treat_as_raw_bytes, => return(.Ok));
    
    // :ScaryIfAliasWrongType
    // If we've already seen this pointer, don't emit it again, just alias the old one. 
    if only_one_word {
        if c.baked.vmem&.get(rawptr_from_int first_word) { it, addend |
            @match(it) {
                fn FuncId(fid) => {
                    out.push(off = reloc_off, target = (FuncId = fid), addend = addend);
                    return(.Ok);
                }
                fn BakedVarId(v) => {
                    out.push(off = reloc_off, target = (BakedVarId = v), addend = addend);
                    return(.Ok);
                }
                fn Var() => todo();
            };
        };
    };
    @debug_assert_ge(value.len, 8, "we only support 64-bit targets so how can it contains_pointers but be small?");
    
    // We don't bother allowing user overloads if there's no pointers in the type.
    // Maybe we should but it seems a bit creepy to encourage you to use a magic hook that runs
    // at a random time during compilation to do something that can only be a spooky action at a distance side effect. idk. 
    // :bake_relocatable_value
    if !force_default_handling {
        xx := @try(c.get_custom_bake_handler(type)) return;
        if xx { f |
            values := f(u8.raw_from_ptr(value.ptr));
            bytes_out := fixed_list(value);
            bake_translate_legacy(values, bytes_out&, out, reloc_off);
            return(.Ok);
        };
    };
    
    @match(c.get_type(type)) {
        fn FnPtr() => {
            // This is super slow but its only the error path so its fine
            if find_function_aa(c, first_word.rawptr_from_int()) { name |
                return(@err("ICE: we should have noticed that we took the address of % but somehow no", name));
            };
            
            return(@err(
                "function not found for constant\naddr=% type=%. we can't even find it if we scan all jitted functions.",
                first_word, c.log_type(type),
            ));
        }
        fn Ptr(inner) => {
            // First time seeing this address. Recurse and bake whatever it points to. 
            inner_info := c.get_info(inner[]);
            ptr := u8.ptr_from_int(first_word);
            data := slice(ptr, inner_info.stride_bytes.zext());
            _, id := c.reserve_baked(Some = data);  // avoid cycles
            id2, addend := @try(c.bake_relocatable_constant(data, inner[], Span.zeroed(), (Some = id))) return;
            @debug_assert(id.id == id2.id && addend == 0, "thought this was a fresh pointer but it wasn't!");
            out.push(off = reloc_off, target = (BakedVarId = id), addend = 0);
        }
        fn Struct(f) => {
            @err_assert(!f.is_union, "TODO: union in a constant. but its hard cause what if theres pointers in there but we don't know which is active") return;
            
            // Just do all the fields.
            off := 0;
            each f.fields { f |
                info := c.get_info(f.ty);
                @err_assert(off <= f.byte_offset, "struct fields must be in order (%)", c.log_type(type)) return;
                off = f.byte_offset + info.stride_bytes.zext();
                v  := value.slice(f.byte_offset, off);
                @try(c.bake_relocatable_body(v, f.ty, out, reloc_off + f.byte_offset.trunc(), false)) return;
            };
        }
        fn Tagged(f) => {
            tag := first_word; // For now tag is always 8 bytes. this is stupid but requires more thought to change. -- Mar 9, 2025. 
            @err_assert(f.cases.len > tag, "invalid tag % in constant @tagged.", tag) return;
            name, inner := f.cases.items()[tag];
            case_info := c.get_info(inner);
            payload_size: i64 = case_info.stride_bytes.zext();
            b := value;
            tag_size :: 8;
            payload  := value.slice(tag_size, tag_size + payload_size);
            @try(c.bake_relocatable_body(payload, inner, out, reloc_off + tag_size, false)) return;
            
            padding := value.rest(tag_size + payload_size);
            if !is_all_zeroes(padding) {
                return(@err("ICE: zero_padding() didn't work on a @tagged constant. This is required for reproducibility."));
            };
        }
        fn Enum(f)  => return(c.bake_relocatable_body(value, f.raw, out, reloc_off, false));
        fn Named(f) => return(c.bake_relocatable_body(value, f._0, out, reloc_off, force_default_handling));
        // TODO: explain that it's fine if it's reachable by an alias that knows the type
        // TODO: record it and see if we find an alias that knows the type later? 
        //       that sounds slow but wouldn't happen very often. 
        fn VoidPtr() => return(@err("You can't have a void pointer % as a constant. The compiler can't tell how many bytes to put in the final executable.", first_word));
        fn Array(_) => @panic("unrechable because Array has a bake_relocatable_value overload");
        @default => return(@err("ICE: constant that contains_pointers is a type % that doesn't make any sense.", c.log_type(type)));
    };
    .Ok
}

fn from_legacy(self: *SelfHosted, legacy: BakedVar) BakedValue = {
    @match(legacy) {
        fn VoidPtrArray(legacy) => {
            bytes := u8.list(self.get_alloc());
            relocs := BakedReloc.list(self.get_alloc());
            bake_translate_legacy(legacy.items(), bytes&, relocs&, 0);
            (template = (Bytes = bytes.items()), relocations = relocs.items(), loc = Span.zeroed())
        };
        fn Bytes(bytes) => (template = (Bytes = bytes.items()), loc = Span.zeroed());
        fn Zeros(len) => (template = (Zeroes = len), loc = Span.zeroed());
    }
}

fn bake_legacy(self: *SelfHosted, legacy: BakedVar, jit_addr: ?[]u8) BakedVarId = {
    value: BakedValue = self.from_legacy(legacy);
    self.put_baked(value, jit_addr)
}

// This avoids needing to linearly scan all the functions when they try to emit_relocatable_constant of a function pointer. 
// You only need to call it if they created a Values of it, so we might need to emit it as a constant later 
// (like for vtables where you construct the value in memory as with comptime pointers and then we emit relocations). 
// [Jun 30] building the compiler ends up with ~17000 FuncIds created and 21 calls to this function. So thats pretty good. 
// TODO: tho that means we wont find it if someone cheats somehow and gets the pointer in a way i didn't think of. 
//       it relies on them only being created through '!fn_ptr'/const_coerce expressions. 
fn created_jit_fn_ptr_value(self: *SelfHosted, f: FuncId, ptr: i64) void = {
    @debug_assert(ptr != 0, "created_jit_fn_ptr_value cannot be null");
    
    @if(false)
    if self.baked.functions&.get(ptr) { old_f | 
        if old_f != f {
            // TODO: i don't want to have warning!
            @eprintln("warn: the address % is baked to multiple functions:\n- %\n- %", 
                ptr,
                self.comp().log(self.get_function(f)), 
                self.comp().log(self.get_function(old_f)),
            );
        }
    };
    
    // TODO: make vbytes from SymbolInfo.size and that can be used for faster backtraces. 
    self.baked.vmem&.insert(rawptr_from_int ptr, (FuncId = f));
}

fn reserve_baked(c: *SelfHosted, jit_ptr: ?[]u8) Ty(*BakedValue, BakedVarId) #inline = {
    v := c.baked.values&;
    id: BakedVarId = (id = v.len.trunc());
    v.push(zeroed BakedValue);
    dest := v.nested_index(id.id.zext());
    if jit_ptr { ptr | 
        prev := c.baked.vmem&.insert(ptr, (BakedVarId = id));
        // TODO: this should be true but it's not
        //@debug_assert(prev.is_none(), "reserved_baked stomped an address");
        
        // :BringBackDataNames 
        // but have to get more serious about it
        // because the old version only got like 20 of them which wasn't super useful. 
        // was going to use SymIdx.Var for that, but if not, don't forget to get rid of it. 
        @if(false)
        if c.baked.vmem.data_names&.get(ptr) { name | 
            dest.name = name;
        };
    };
    (dest, id)
}

fn put_baked(c: *SelfHosted, v: BakedValue, jit_ptr: ?[]u8) BakedVarId #inline = {
    dest, id := c.reserve_baked(jit_ptr);
    if jit_ptr { ptr | 
        //v.jit_addr = ptr;
    };
    n := dest.name;
    dest[] = v;
    dest.name = n;
    id
}

// TODO: change the api we expose so you don't need this anymore
fn emit_relocatable_constant_body(c: *SelfHosted, bytes: Slice(u8), ty: Type, force_default_handling: bool) Res(Slice(BakedEntry)) = {
    out := BakedReloc.list(temp());  // doesn't escape!
    // TODO: zero here? 
    @try(c.bake_relocatable_body(bytes, ty, out&, 0, force_default_handling)) return;
    
    value: BakedValue = (template = (Bytes = bytes), relocations = out.items(), loc = Span.zeroed());
    (Ok = bake_collect_legacy(value&, c.get_alloc()))
}

Baked :: @struct(
    values: BucketArray(BakedValue),
    vmem: AddressSpace.Map,
    // Some types need special handling (Slice, CStr, etc). This are called at comptime and might call back into the auto dynamic versions to handle thier fields. 
    custom_bake_constant: HashMap(Type, FuncId),
);

BakeHandler :: @FnPtr(self: rawptr) Slice(BakedEntry);

// :bake_relocatable_value
fn get_custom_bake_handler(c: *SelfHosted, ty: Type) Res(?BakeHandler) = {
    if c.baked.custom_bake_constant&.get(ty) { fid |
        c := c.comp();
        res := c'vtable'get_jitted_ptr(c.data, fid);
        fptr := @try(res) return;
        fptr := assume_types_fn(rawptr, Slice(BakedEntry), fptr);
        return(Ok = (Some = fptr));
    };
    (Ok = .None)
}

// TODO: do this lazily and as part of the event loop. 
fn check_for_new_aot_bake_overloads(self: *SelfHosted) PRes = {
    os := self.env.bake_os.expect("cannot aot during bootstrapping.");
    
    overloads := self.dispatch.overloads&.nested_index(os.as_index());
    prev := overloads.ready.len();
    self.compute_new_overloads(overloads);
    // TODO: this will miss them if someone caused new things to resolve outside this function.
    //       so if you ever call bake_relocatable_value manually. :FUCKED -- Jun 19
    current := overloads.ready.len();
    new := overloads.ready.items().slice(prev, current).clone(temp());
    for new { f |
        continue :: local_return;
        func := self.get_function(f);
        args := func.arg.bindings&;
        if args.len() != 1 {
            continue();
        };
        
        value := self.poll_in_place(void) {()Maybe(void)|
            return :: local_return;
            arg_ty := @check(self.infer_arguments(f)) return;
            ret_ty := @check(self.infer_return(f)) return;
            .Ok
        };
        @try(value) return;
        f_ty := func.finished_ty().expect("bake overload fn type");
        info := self.get_type(f_ty.ret);
        if !info.is(.Struct) {
            continue();
        };
        fields := info.Struct.fields&;
        // TODO: make sure its actually a Slice(BakedEntry) :FUCKED this will be so confusing someday 
        if fields.len() != 2 {
            continue();
        };

        first := f_ty.arg; 
        ty := self.unptr_ty(first).or(=> continue());
        prev := self.baked.custom_bake_constant&.insert(ty, f);
        if !prev.is_none() {
            // TODO: return this as a CompileError instead of printing here
            self.codemap.show_error_line(func.loc, true /*TODO*/);
            self.codemap.show_error_line(self.get_function(prev.Some)[].loc, true /*TODO*/);
            @eprintln("% % %", prev.Some, f, self.log_type(ty));
        };
        
        @err_assert(prev.is_none(), "conflicting overload for bake AOT constant") return;
    };
    .Ok
};

fn Res($T: Type) Type = Result(T, *CompileError);

fn to_value(bytes: Slice(u8)) Values = {
    ::if(Values);
    if bytes.len <= 8 {
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or((@as(i64) b.zext()).shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {
        (Big = (cap = bytes.len, ptr = bytes.ptr, len = bytes.len))
    }
}

fn to_value(bytes: Slice(u8), a: Alloc) Values = {
    ::if(Values);
    if bytes.len > 8 {
        bytes = bytes.shallow_copy(a);
    };
    bytes.to_value()
}

////////////////////////
/// Splitting Values /// 

// When binding const arguments you want to split a large value into smaller ones that can be referred to by name.
fn chop_prefix(self: *SelfHosted, prefix: Type, t: *ReadBytes) ?Values #once = {
    info := self.get_info(prefix);
    @debug_assert_eq(0, t.i.mod(info.align_bytes.zext()));
    if t.take(info.stride_bytes.zext()) { bytes | 
        return(Some = bytes.to_value(self.get_alloc()));
    };
    .None
}

// You need this for reproducible builds!
// Without calling this on when baking constants, you can leak comptime addresses in @tagged padding (which happened in get_or_create_type in the compiler).
// TODO: decide if i need to do this for baked arg key hashing. 
// TODO: this should probably go through pointers but for baking constants that's handled as we recurse. 
fn zero_padding(program: CompCtx, ty: Type, bytes: *ReadBytes) PRes = {
    info := program.get_info(ty);

    @debug_assert_eq(bytes.i.mod(info.align_bytes.zext()), 0);
    found_len := bytes.bytes.len() - bytes.i;
    @debug_assert(
        found_len >= info.stride_bytes.zext(),
        "zero_padding of % wants % bytes but found %",
        program.log(ty), info.stride_bytes, found_len
    );
    
    set_zeros :: fn(padding: i64) void => {
        bytes.bytes.subslice(bytes.i, padding).set_zeroed();
        bytes.i += padding;
    };
    
    @match(program.get_type(ty)) {
        fn Tagged(f) => {
            start := bytes.i;
            tag := bytes.read_next(i64);
            @err_assert(tag.ult(f.cases.len), "Invalid tag % in constant", tag) return;
            case := f.cases.index(tag);
            @try(zero_padding(program, case._1, bytes)) return;
            padding := (@as(i64) info.stride_bytes.zext()) - (bytes.i - start);
            @err_assert(padding >= 0, "ICE: confused about @tagged size.") return;
            set_zeros(padding);
        }
        fn Enum(f)   => return(zero_padding(program, f.raw, bytes));
        fn Named(f)  => return(zero_padding(program, f._0, bytes)); 
        fn Array(f) => {
            inner_align := program.get_info(f.inner)[].align_bytes;
            range(0, f.len.zext()) { _ |
                @debug_assert_eq(bytes.i.mod(inner_align.zext()), 0);
                @try(zero_padding(program, f.inner, bytes)) return;
            };
        }
        fn Struct(f) => {
            @err_assert(f.layout_done, "ICE: layout not ready") return;
            @err_assert(!f.is_union, "TODO: you can't zero padding of a union because we don't know which varient is active.") return;
            prev := 0;
            size: i64 = program.get_info(ty)[].stride_bytes.zext();
            start := bytes.i;
            for f.fields { t |
                diff := t.byte_offset - prev;
                assert(diff >= 0, "ICE: backwards field offset");
                set_zeros(diff);
                @try(zero_padding(program, t.ty, bytes)) return;
                prev = bytes.i - start;
            };
            set_zeros(size - prev); // eat trailing stride padding
        }
        fn Bool() => {
            b := bytes.bytes[bytes.i];
            @debug_assert(b == 0 || b == 1, "zero_padding a junk bool: %", b);
            bytes.i += 1;
        }
        @default => {
            // no padding
            bytes.i += info.stride_bytes.zext();  
        };
    };
    .Ok
}

to_values :: fn(program: *SelfHosted, $T: Type, v: T) Values #generic = {
    bytes := T.cast_to_bytes(v&);
    program.from_bytes(bytes)
};

fn to_expr(self: *SelfHosted, $T: Type, value: T, loc: Span) FatExpr #generic = {
    value := self.to_values(T, value); 
    (expr = (Value = (bytes = value)), loc = loc, ty = self.get_or_create_type(T), done = true)
}

fn from_bytes(program: *SelfHosted, bytes: []u8) Values = {
    ::if(Values);
    to_value(bytes, program.get_alloc())
}

/////////////
/// Types ///

fn get_type(self: *SelfHosted, ty: Type) *TypeInfo /* const */ #inline = {
    self.types&.nested_index(ty.as_index())
}

fn should_deduplicate(type: *TypeInfo) bool = {
    @if_let(type) fn Struct(it) => return(it.is_tuple);   // :only_intern_tuples
    // Placeholder: because it will get filled in later
    !@is(type, .Tagged, .Enum, .Named, .Placeholder)
}

// TODO: call this maybe_intern_type?
fn intern_type(self: *SelfHosted, info: TypeInfo) Type = {
    ::if(Type);
    if info&.should_deduplicate() {
        get_or_insert :: get_or_insert_pinky_swear_not_to_mutate_the_key;
        self.type_lookup.raw&.get_or_insert(info&, self.type_lookup.alloc, fn(key) => {
            id: Type = from_index(self.types.len);
            key[] = self.types&.push(info);  // make the pointer stable
            self.type_extra&.push(TypeMeta.zeroed()); // is_sized = false
            id
        })[]
    } else {
        id: Type = from_index(self.types.len);
        self.types&.push(info);
        self.type_extra&.push(TypeMeta.zeroed()); // is_sized = false
        id
    }
}

fn save_guessed_name(self: *SelfHosted, type: Type, name: Symbol, loc: Span) void = {
    info := self.type_extra&.nested_index(type.as_index());
    guessed := info.inferred_name&;
    if guessed[] == Flag.SYMBOL_ZERO.ident() { 
        // TODO: also check first_dumb_type_name - last_dumb_type_name range?
        guessed[] = name;
        info.loc = (low = loc.low);
    };
}


// TODO: this needs to not do it when it has generic parameters because it hides information.
// TODO: `self.get_info_is_ready(ty) && ` but is_sized isn't getting set for `@struct(_: rawptr)` ? wgpu bindings 
// the `get_info_is_ready` is very important or you get fucked by FRANCA_TRACY trying to size structs before the fields are ready. 
fn get_guessed_name(self: *SelfHosted, type: Type) ?Symbol = {
    loop {
        info := self.get_type(type);
        nominal := @is(info, .Struct, .Tagged, .Enum, .Named);
        if nominal {
            info := self.type_extra&.nested_index(type.as_index());
            n := info.inferred_name.id();
            min := Flag.first_dumb_type_name.ident().id();
            max := Flag.last_dumb_type_name.ident().id();
            // TODO: probably better is to not stomp the name if its a dumb one? 
            if info.inferred_name != Flag.SYMBOL_ZERO.ident() && (n < min || n > max) {
                s := self.pool.get(info.inferred_name);
                if s.len > 1 { // HACK, unhelpful generics are often T. TODO: track more information about where stuff came from.
                    return(Some = info.inferred_name);
                };
            };
        }; 
        
        @match(info) {
            fn Named(it) => {
                type = it._0;
            }
            @default => return(.None);
        };
    }
}

fn update_placeholder(self: *SelfHosted, placeholder_slot: Type, real_type: Type, name: Symbol) void = {
    idx := placeholder_slot.as_index();
    slot := self.types&.nested_index(idx);
    @debug_assert(slot.is(.Placeholder));
    slot[] = (Named = (real_type, name));
    @debug_assert(self.type_lookup&.get(slot&).is_none());
    
    // just in case.    copy-paste call to truncate but now want to have the list full size ahead of time. 
    i := self.type_extra.len - 1; // we happen to know this can't underflow because there are builtin types.  
    while => i > idx {
        self.type_extra&.nested_index(i)[] = TypeMeta.zeroed(); // is_sized = false
        i -= 1;
    };
}

// these are hardcoded numbers in TypeId constructors
// if you remove any remember to fix later indices! 
// also you need to be very very careful when renumbering them 
// because i assume im allowed to use these common ones 
// as literals without calling intern_type. 
fn init_fixed_types(self: *SelfHosted) void = {
    @debug_assert_eq(self.types.len, 0);
    self.intern_type(.Placeholder);  // UnknownType
    self.intern_type(.void);
    self.opaque_index(.Type);
    self.intern_type(Int = (bit_count = 64, signed = true));
    self.intern_type(.Bool);
    self.intern_type(Named = (from_index(1), Flag.CVariadic.ident()));  // #5
    self.intern_type(.Never);
    self.intern_type(.F64);
    self.opaque_index(.OverloadSet);
    self.opaque_index(.ScopeId);
    self.intern_type(Int = (bit_count = 32, signed = false));  // #10
    self.intern_type(.VoidPtr); // #11
    self.intern_type(.F32);
    self.opaque_index(.FuncId);
    self.opaque_index(.LabelId);
    self.opaque_index(.Symbol);
    
    opaque_index :: fn(self: *SelfHosted, name: Flag) void = {
        //self.intern_type(Struct = (
        //    fields = {
        //        f := Field.list(self.get_alloc());
        //        f&.push(
        //            name = Flag._.ident(),
        //            ty = from_index(10),
        //            nullable_tag = zeroed(*Annotations),
        //            default = zeroed(Var),
        //            byte_offset = 0,
        //        );
        //        f.as_raw()
        //    },
        //    layout_done = true,
        //    is_tuple = false,
        //    is_union = false,
        //    scope = NOSCOPE,
        //));
        // TODO: the struct version would make more sense but then i have to take out 
        //       a few places where i do @as(u32)FuncId, etc. 
        self.intern_type(Enum = (
            fields = empty(),
            raw = from_index(10),
            sequential = false,
        ));
    };
}

fn get_info_is_ready(self: *SelfHosted, ty: Type) bool = {
    ty := self.raw_type(ty);
    old := self.type_extra&.nested_index(ty.as_index());
    old.is_sized 
}

:: as_ref(TypeMeta);
// You can't call log_type in here (like in the debug_asserts) because log_type calls this and you'll get stuck recursing!
fn get_info(self: *SelfHosted, ty_original: Type) *TypeMeta = {
    // there are a lot of Enum / Placeholder -> Named which always have the same repr. 
    // amortise the cost of raw_type(). 
    old_original := self.type_extra&.nested_index(ty_original.as_index());
    if old_original.is_sized {
        return(old_original);
    };
    ty  := self.raw_type(ty_original);
    ::if(*TypeMeta);
    old := if ty != ty_original {
        old := self.type_extra&.nested_index(ty.as_index());
        if old.is_sized {
            old_original[] = old[];
            return(old_original);
        };
        old
    } else {
        old_original
    };
    
    // TODO: when i didnt have some of the trunc impls this had a "tried to call missing function"
    new :: fn(align_bytes: u16, contains_pointers: bool, stride_bytes: u32) TypeMeta = {
        @debug_assert_eq(stride_bytes.mod(align_bytes.zext()), 0);
        (
            loc = (low = 0),
            stride_bytes = stride_bytes,
            align_bytes = align_bytes,
            contains_pointers = contains_pointers,
            is_sized = true,
        )
    };
    
    info := @match(self.get_type(ty)) {
        fn Placeholder() => {
            self.codemap.show_error_line(self.last_loc, true /*TODO*/);
            @panic("Unfinished type %", ty.as_index()); // TODO: err!
        }
        fn Struct(f) => {
            // TODO: shouldn't need to do this here. instead do it in the event loop. 
            //       old sema didn't need this, it just obsessively calledfinish_layout every time it produced a value. 
            self.finish_layout(ty); 
            @debug_assert(f.layout_done, "ICE: layout not done! T%", ty);
            align: u16 = 1;
            pointers := false;
            bytes := 0;

            first := true;
            each f.fields { arg |
                @debug_assert(!first || arg.byte_offset == 0, "ICE: first field should have offset 0");
                first = false;
                info := self.get_info(arg.ty);
                a := self.alignment(arg, info);
                align = align.max(a.trunc());
                @debug_assert(arg.byte_offset != FIELD_LAYOUT_NOT_DONE, "ICE: field offset for % not known", self.pool.get(arg.name));
                @debug_assert(f.is_union || arg.byte_offset >= bytes, "ICE: fields not in order in T% field: % at % is before %", ty, self.pool.get(arg.name), arg.byte_offset, bytes); // TODO: assert not past max_u16 or better have a safety checking intcast
                end := arg.byte_offset + info.stride_bytes.zext();
                if f.is_union {
                    bytes = bytes.max(info.stride_bytes.zext());
                } else {
                    bytes = bytes.max(end);
                };
                pointers = pointers.or(info.contains_pointers);
            };
            
            bytes = align_to(bytes, align.zext());
            @assert_lt(bytes, 1.shift_left(32), "cannot have a type larger than 4GB");
            new(align, pointers, bytes.trunc())
        }
        fn Tagged(f) => {
            bytes: u32 = 0;
            pointers := false;  
            
            @debug_assert(f.cases.len > 0, "TODO: i guess this can just be void");
            each f.cases { f |
                info := self.get_info(f._1);
                bytes = bytes.max(info.stride_bytes);
                pointers = pointers.or(info.contains_pointers);
            };
            // :tag_is_i64
            bytes += 8;
            
            align: u32 = 8;
            extra := bytes.mod(align);
            if extra != 0 {
                bytes += align - extra;
            };
            
            // TODO: currently tag is always i64 so align 8 but should use byte since almost always enough. but you just have to pad it out anyway.
            //       even without that, if i add 16 byte align, need to check the fields too.  :tag_is_i64
            new(8, pointers, bytes)
        }
        fn Never() => new(1, false, 0);
        fn Int(int) => {
            // :SmallTypes
            @switch(int.bit_count) {
                @case(8) => new(1, false, 1);
                @case(16) => new(2, false, 2);
                @case(32) => new(4, false, 4);
                @default => new(8, false, 8);
            }
        }
        fn F32()  => new(4, false, 4);
        fn F64()  => new(8, false, 8);
        fn void() => new(1, false, 0);
        fn Bool() => new(1, false, 1); // :SmallTypes
        fn Ptr(_)     => new(8, true, 8);
        fn VoidPtr()  => new(8, true, 8);
        fn FnPtr(_)   => new(8, true, 8);
        fn Label(_)   => new(4, false, 4);
        fn Fn(_)      => new(4, false, 4);
        fn Array(f) => {
            ::if(TypeMeta);
            if f.len == 0 {
                new(1, false, 0)
            } else {
                info := self.get_info(f.inner);
                bytes: i64 = info.stride_bytes.zext() * f.len.zext();
                @assert_lt(bytes, 1.shift_left(32), "cannot have a type larger than 4GB (trying to make an Array of % elements)", f.len);
                new(
                    info.align_bytes,
                    info.contains_pointers,
                    bytes.trunc(),
                )
            }
        }
        @default => unreachable();
    };
    
    // save if we poked this in before.
    if ty != ty_original {
        info.loc = old_original.loc;
        info.inferred_name = old_original.inferred_name;
        old_original[] = info;
    };
    info.loc = old.loc;
    info.inferred_name = old.inferred_name;
    
    old[] = info;
    old_original
}

fn raw_type(c: *SelfHosted, ty: Type) Type = 
    c.comp().raw_type(ty);

fn finish_layout_deep(self: *SelfHosted, ty: Type) void = {
    @debug_assert_ule(ty.as_index(), self.types.len);
    if(self.finished_layout_deep&.get(ty.as_index()), => return());
    self.finished_layout_deep&.set(ty.as_index()); // do this at the beginning to stop recursion.
    self.finish_layout(ty);
    ty := self.raw_type(ty);
    
    @match(self.get_type(ty)) {
        fn Fn(f) => {
            self.finish_layout_deep(f.arg);
            self.finish_layout_deep(f.ret);
        }
        fn FnPtr(f) => {
            self.finish_layout_deep(f.ty.arg);
            self.finish_layout_deep(f.ty.ret);
        }
        fn Ptr(inner)   => self.finish_layout_deep(inner[]);
        fn Label(inner) => self.finish_layout_deep(inner[]);
        fn Array(f)  => self.finish_layout_deep(f.inner);
        fn Struct(f) => each f.fields { f |
            self.finish_layout_deep(f.ty);
        };
        fn Tagged(f) => each f.cases { f | 
            self.finish_layout_deep(f._1);
        };
        @default => ();
    };
}

fn alignment(self: *SelfHosted, f: *Field, info: *TypeMeta) i64 = {
    if !f.nullable_tag.is_null() {  // redundant. 
        if f.get_tag(Flag.align.ident()) { a | 
            res := self.poll_in_place(i64, => self.eval(a.args&, i64));
            res := self.unwrap_report_error(i64, res);
            return res;
        };
    };
    info.align_bytes.zext()
}

fn finish_layout(self: *SelfHosted, ty: Type) void = {
    ty := self.raw_type(ty);
    info := self.get_type(ty);
    @match(info) {
        fn Array(f)  => self.finish_layout(f.inner);
        fn Tagged(f) => each f.cases { variant |
            self.finish_layout(variant._1);
        };
        fn Struct(f) => {
            if(f.layout_done, => return());
            // :struct_layout_in_type_info
            // Here we mutate layout_done and the byte_offset of the fields, which we know the hash function ignores
            
            bytes := 0;
            each f.fields& { p |
                self.finish_layout(p.ty);
                p.byte_offset = 0;
                if !f.is_union {
                    info := self.get_info(p.ty);
                    a    := self.alignment(p, info);
                    bytes = align_to(bytes, a);
                    p.byte_offset = bytes;
                    
                    // TODO: this must be wrong? surely you dont want to have the array padding out to alignment if its just a nested struct.
                    //       tho its what c does. so i guess i need different reprs. and then size_of vs stride_of become different things.
                    bytes += info.stride_bytes.zext();
                };
            };
    
            f.layout_done = true;
        }
        @default => ();
    };
}

// TODO: can't have these added to the eq overload set to early or it gets stuck
:: {
    fn eq(a: *Field, b: *Field) bool = {
        (a.name& == b.name&) && (a.ty& == b.ty&) && (a.default& == b.default&)
    }
    TypeInfoStruct :: get_variant_type(TypeInfo, TypeInfo.Tag().Struct);
    fn eq(a: *TypeInfoStruct, b: *TypeInfoStruct) bool = {
        (a.is_tuple& == b.is_tuple&) && (a.fields& == b.fields&)
    }
};

// TODO: defend against the same memory being aliased as different types that would want to emit the constant differently? -- Jun 30
AddressSpace :: @struct {
    Tag :: @enum(u8) (Unknown, FuncId, BakedVarId);
    Idx :: @union(
        fid: FuncId, 
        baked: BakedVarId,
    );
    T :: @struct {
        tag: Tag;
        idx: Idx;
    };
    Map :: import("@/lib/collections/page_map.fr")'PageMap(T);
    
    fn insert(self: *Map, addr: rawptr, value: SymIdx) void = {
        self.insert(u8.ptr_from_raw(addr).slice(1), value);
    }
    
    fn insert(self: *Map, vbytes: []u8, value: SymIdx) void = {
        self.insert_at(vbytes, @match(value) {
            fn FuncId(it) => (idx = (fid = it), tag = .FuncId);
            fn BakedVarId(it) => (idx = (baked = it), tag = .BakedVarId);
            fn Var() => todo();
        });
    }
    
    // TODO: replace addr with vbytes and assert if it's too big for an existing bake 
    //       (eventually handle that properly somehow)
    fn get(self: *Map, addr: rawptr) ?Ty(SymIdx, i64) = {
        e, off := self.get_at(addr) || return(.None);
        ::enum(Tag);
        return(Some = (@match(e.tag) {
            fn Unknown() => panic("PageMap.get on valid address but uninit tag");
            fn FuncId() => (FuncId = e.idx.fid);
            fn BakedVarId() => (BakedVarId = e.idx.baked);
        }, off));
    }
};