//! Assorted functions work working with the bytes of comptime values.
//! - Emitting relocatable baked constants for AOT (fn bake_relocatable_value).
//! - Spilitting aggragate values into a list of integers for function calls. 
//! - Dynamically calling (a subset of) c abi functions.
//! - Determining struct size/layout

//////////////////////
/// Bake Constants ///

// This avoids needing to linearly scan all the functions when they try to emit_relocatable_constant of a function pointer. 
// You only need to call it if they created a Values of it, so we might need to emit it as a constant later 
// (like for vtables where you construct the value in memory as with comptime pointers and then we emit relocations). 
// [Jun 30] building the compiler ends up with ~17000 FuncIds created and 21 calls to this function. So thats pretty good. 
// TODO: tho that means we wont find it if someone cheats somehow and gets the pointer in a way i didn't think of. 
//       it relies on them only being created through '!fn_ptr'/const_coerce expressions. 
fn created_jit_fn_ptr_value(self: *SelfHosted, f: FuncId, ptr: i64) void = {
    self.baked.functions&.insert(ptr, f);
}

fn get_baked(c: *SelfHosted, id: BakedVarId) *Ty(rawptr, BakedVar) = {
    c.baked.values&.nested_index(@as(i64) id.id.zext())
}

fn put_baked(c: *SelfHosted, v: BakedVar, jit_ptr: ?i64) BakedVarId = {
    i := c.baked.values.len;
    j := jit_ptr.or(=> 0).rawptr_from_int();
    c.baked.values&.push(@as(Ty(rawptr, BakedVar)) (j, v)); // TODO: this shouldn't need type hint.
    id: BakedVarId = (id = i.trunc());
    if jit_ptr { ptr | 
        c.baked.lookup&.insert(ptr, id); // TODO: check unique? 
    };
    id
}

fn emit_relocatable_constant_body(c: *SelfHosted, bytes: Slice(u8), ty: Type, force_default_handling: bool) Res(Slice(BakedEntry)) = {
    out: List(BakedEntry) = list(c.get_alloc());
    value := bytes.to_value();
    @try(c.emit_relocatable_constant_body(ty, value&, out&, force_default_handling)) return;
    (Ok = out.items())
}

Baked :: @struct(
    values: BucketArray(Ty(rawptr, BakedVar)),
    // TODO: defend against the same memory being aliased as different types that would want to emit the constant differently? -- Jun 30
    // TODO: this doesn't help for small strings because they're stored inline in the Values. 
    // deduplicate values by thier jit address. 
    lookup: HashMap(i64, BakedVarId),
    // avoid iterating all function pointers when you try to have one as a constant. 
    functions: HashMap(i64, FuncId),
    // Some types need special handling (Slice, CStr, etc). This are called at comptime and might call back into the auto dynamic versions to handle thier fields. 
    custom_bake_constant: HashMap(Type, FuncId),
);

BakeHandler :: @FnPtr(self: rawptr) Slice(BakedEntry);

// TODO: be careful about the same string being baked as CStr and Str.
// TODO: i should have constant pointers as a concept in my language.
//       right now i could check if its in the constant data arena for a hint that should catch a few of them.
fn emit_relocatable_constant(c: *SelfHosted, ty: Type, value: *Values) Res(BakedVarId) = {
    to_zero: ReadBytes = (bytes = value.bytes(), i = 0);
    c.zero_padding(ty, to_zero&);  // Needed for reproducible builds!
    
    jit_ptr := value.jit_addr();
    // TODO: this barely does anything! cause like cstr and slice don't go through this function. they call dyn_bake_relocatable_value. -- Jun 30
    //       this is the right idea, they just need to check thier pointers against the cache first. 
    if c.baked.lookup&.get(jit_ptr) { v | 
        // TODO: be careful about aliasing as different types with different custom_bake_constant? -- Jun 30 
        return(Ok = v);
    };
    
    // TODO: is it really that helpful to tell the backend to use zeroinitilizer? could at least have a faster memcmp. 
    (fn() => {
        break :: local_return;
        for value.bytes() { b |
            if(b != 0, => break());
        };
        return(Ok = c.put_baked((Zeros = value.bytes().len()), (Some = jit_ptr)));
    })();
    
    // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
    info := c.get_info(ty);
    if (!info.contains_pointers) {
        v := value.bytes().clone(c.get_alloc()); // TODO: do i have to clone? 
        return(Ok = c.put_baked((Bytes = v.rs()), (Some = jit_ptr)));
    };

    out: List(BakedEntry) = list(c.get_alloc());
    @try(c.emit_relocatable_constant_body(ty, value, out&, false)) return;
    (Ok = c.put_baked((VoidPtrArray = out.rs()), (Some = jit_ptr)))
}
    
// TODO: deduplicate small constant strings. they get stored in Values inline so can't be fixed by baked.lookup
// TODO: make sure baked.lookup actually ever helps. might need to add checks in more places.
fn emit_relocatable_constant_body(
    c: *SelfHosted,
    ty: Type,
    value: *Values,
    out: *List(BakedEntry),
    force_default_handling: bool,
) PRes = {
    info := c.get_info(ty);
    @err_assert(value.len() == info.stride_bytes.zext(), 
        "ICE: Tried to emit constant value of wrong size. Expected % bytes but found %.", 
        @as(i64) info.stride_bytes.zext(), value.len()
    ) return;
    
    // TODO: alignment check? 
    
    // :bake_relocatable_value
    if (!force_default_handling) {
        xx := @try(c.get_custom_bake_handler(ty)) return;
        if xx { f |
            values := f(value.jit_addr().rawptr_from_int());
            out.push_all(values);
            return(.Ok);
        };
    };

    // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
    if (!info.contains_pointers) {
        assert_eq(value.len().mod(8), 0); // TODO. but rust version doesn't handle this either -- Jun 30
        
        bytes := value.bytes();
        ptr := ptr_cast_unchecked(u8, i64, bytes.ptr);
        ints: Slice(i64) = (ptr = ptr, len = bytes.len / 8);
        for ints { i | 
            // TODO: small prims
            out.push((Num = (value = i, ty = .I64)));
        };
        return(.Ok);
    };

    // The rust version skipped to raw type at the top but that's kinda wrong cause there might be an overload for just some intermediate? 
    @match(c.get_type(ty)) {
        (fn FnPtr(_) => {
            ptr := i64.assume_cast(value)[];
            if c.baked.functions&.get(ptr) { fid |
                // assert_eq!(*f_ty, program[f].finished_ty().unwrap()); // TODO?
                out.push((FnPtr = fid));
                return(.Ok);
            };
            
            // This is super slow but its only the error path so its fine. 
            enumerate c.jitted.dispatch { i, check | 
                check := int_from_rawptr(check[]);
                if check == ptr {
                    fid := funcid_from_index(i);
                    return(@err(
                        "function not found for constant\naddr=% fid=% name=% \n(ICE: we should have noticed that we took its address)", 
                        ptr, fid, c.log_name(fid),
                    ));
                };
            };
            
            return(@err(
                "function not found for constant\naddr=% type=%. \nwe can't even find it if we scan all jitted functions. maybe we're confused about its type?",
                ptr, c.log_type(ty),
            ));
        });
        (fn Ptr(inner) => {
            // TODO: allow zero?
            inner_info := c.get_info(inner[]);
            // load the pointer and recurse.
            addr := i64.assume_cast(value)[];
            
            // Eventually i want ?*T to use zero as .None but for now allow null pointers. 
            if addr == 0 {
                out.push((Num = (value = 0, ty = .P64)));
                return(.Ok);
            };
            
            // emit_relocatable_constant only does it by the address of the value but pointers are small so stored inline. 
            if c.baked.lookup&.get(addr) { v | 
                out.push((AddrOf = v));
                return(.Ok);
            };
            
            ptr := u8.ptr_from_int(addr);
            data := slice(ptr, inner_info.stride_bytes.zext());
            inner_value := data.to_value(c.get_alloc()); // TODO: do i have to clone? 
            id := @try(c.emit_relocatable_constant(inner[], inner_value&)) return;
            c.baked.lookup&.insert(addr, id);
            out.push((AddrOf = id));
        });
        (fn Struct(f) => {
            // Just do all the fields.
            each f.fields { f | 
                info := c.get_info(f.ty);
                @assert_eq(@as(i64) info.stride_bytes.zext().mod(8), 0, "TODO: non-8-aligned fields %", c.pool.get(f.name));
                v := value.bytes().slice(f.byte_offset, f.byte_offset + info.stride_bytes.zext());
                v := v.to_value(c.get_alloc()); // TODO: do i have to clone? 
                @try(c.emit_relocatable_constant_body(f.ty, v&, out, false)) return;
            };
        });
        (fn Tagged(f) => {
            tag := i64.unchecked_cast(value)[]; // just read the tag
            assert(f.cases.len >= tag, "invalid constant tagged union");
            name, inner := f.cases.items()[tag];
            case_info := c.get_info(inner);
            out.push((Num = (value = tag, ty = .I64)));
            payload_size: i64 = case_info.stride_bytes.zext();
            b := value.bytes();
            tag_size := 8;
            payload := b.slice(tag_size, tag_size + payload_size).to_value();
            
            padding_size := b.len - payload_size - tag_size;
            @try(c.emit_relocatable_constant_body(inner, payload&, out, false)) return;
            
            padding_slots, m := padding_size.div_mod(8);
            @assert_eq(m, 0, "TODO: non-8-mod enum padding");
            
            // :tagged_prims_hack
            types := c.flat_tuple_types(ty);
            types := types.items().slice_last(padding_slots);
            types := types.unwrap();
            // now types is just padding

            // If this is a smaller varient, pad out the slot.
            for types { p |
                ty := @unwrap(c.prim(p), "not prim") return;
                out.push((Num = (value = 0, ty = ty)));
            };
        });
        (fn Enum(f) => {
            return(c.emit_relocatable_constant_body(f.raw, value, out, force_default_handling));
        });
        (fn Named(f) => {
            return(c.emit_relocatable_constant_body(f._0, value, out, force_default_handling));
        });
        (fn VoidPtr() => {
            ptr := i64.assume_cast(value)[];
            if ptr == 0 {
                // You're allowed to have a constant null pointer (like for global allocator interface instances).
                out.push((Num = (value = 0, ty = .P64)));
                return(.Ok);
            };
            
            if c.baked.functions&.get(ptr) { fid |
                // Maybe you wanted to store a bunch of type erased functions (like i have to for export_ffi). 
                out.push((FnPtr = fid));
                
                func := c.get_function(fid);
                // this will only show if we're spam adding everying to baked.functions while debugging!
                @debug_assert(func.get_flag(.TookPointerValue), "void pointer constant but not marked .TookPointerValue %", func.log(c)); 
                
                return(.Ok);
            };
            
            if c.baked.lookup&.get(ptr) { v | 
                // I'm not sure why you'd get here but I guess its fine.
                out.push((AddrOf = v));
                return(.Ok);
            };
            
            return(@err("You can't have a void pointer as a constant. The compiler can't tell how many bytes to put in the final executable."));
        });
        @default => {
            return(@err("ICE: bad constant"));
        };
    };
    .Ok
}

// :bake_relocatable_value
fn get_custom_bake_handler(c: *SelfHosted, ty: Type) Res(?BakeHandler) = {
    if c.baked.custom_bake_constant&.get(ty) { fid |
        if c.get_fn_callable(fid) { fptr |
            fptr := assume_types_fn(rawptr, Slice(BakedEntry), fptr);
            return(Ok = (Some = fptr));
        };
        res := c.poll_in_place(rawptr) { () Maybe(rawptr) |
            @match(c.get_fn_callable(fid)) {
                fn Some(ptr) => (Ok = ptr);
                fn None() => (Suspend = c.wait_for(Jit = fid));
            }
        };
        fptr := @try(res) return;
        fptr := assume_types_fn(rawptr, Slice(BakedEntry), fptr);
        return(Ok = (Some = fptr));
    };
    (Ok = .None)
}


// TODO: do this lazily and as part of the event loop. 
fn check_for_new_aot_bake_overloads(self: *SelfHosted) PRes = {
    os := self.env.bake_os.expect("cannot aot during bootstrapping.");
    overloads := self.dispatch.overloads&.nested_index(os.as_index());
    prev := overloads.ready.len();
    self.compute_new_overloads(overloads);
    // TODO: this will miss them if someone caused new things to resolve outside this function.
    //       so if you ever call bake_relocatable_value manually. :FUCKED -- Jun 19
    current := overloads.ready.len();
    new := overloads.ready.items().slice(prev, current).clone(temp());
    for new { f |
        continue :: local_return;
        func := self.get_function(f);
        args := func.arg.bindings&;
        if args.len() != 1 {
            continue();
        };
        
        value := self.poll_in_place(void) {()Maybe(void)|
            return :: local_return;
            arg_ty := @check(self.infer_arguments(f)) return;
            ret_ty := @check(self.infer_return(f)) return;
            .Ok
        };
        @try(value) return;
        f_ty := func.finished_ty().expect("bake overload fn type");
        info := self.get_type(f_ty.ret);
        if !info.is(.Struct) {
            continue();
        };
        fields := info.Struct.fields&;
        // TODO: make sure its actually a Slice(BakedEntry)
        if fields.len() != 2 {
            continue();
        };

        first := f_ty.arg; 
        ty := self.unptr_ty(first).or(=> continue());
        prev := self.baked.custom_bake_constant&.insert(ty, f);
        // TODO: show the two locations and the type. :ERROR_QUALITY
        @err_assert(prev.is_none(), "conflicting overload for bake AOT constant") return;
    };
    .Ok
};

fn Res($T: Type) Type = Result(T, *CompileError);

fn jit_addr(v: *Values) i64 = {
    s := v.bytes();
    u8.int_from_ptr(s.ptr)
}

fn bytes(self: *Values) Slice(u8) = {
    @match(self) {
        (fn Small(v) Slice(u8) => (ptr = ptr_cast_unchecked(i64, u8, v._0&), len = v._1.zext()));
        (fn Big(v) Slice(u8) => v.items());
    }
}

fn len(self: *Values) i64 = {
    @match(self) {
        (fn Small(v) i64 => { v._1.zext() });
        (fn Big(v) i64 => { v.len });
    }
}

// fn from_values
fn assume_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    assert_eq(b.len, T.size_of());  // TODO: check alignment
    ptr_cast_unchecked(u8, T, b.ptr)
}

fn unchecked_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    ptr_cast_unchecked(u8, T, b.ptr)
}

fn get_type(c: CompilerRs, ty: Type) *TypeInfo #inline = 
    c[][].get_type(ty)[];

fn get_info(c: CompilerRs, ty: Type) TypeMeta #inline = 
    c[][].get_info(ty)[];

fn to_value(bytes: Slice(u8)) Values = {
    ::if(Values);
    if bytes.len <= 8 {
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or((@as(i64) b.zext()).shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {
        (Big = (cap = bytes.len, ptr = bytes.ptr, len = bytes.len))
    }
}

fn to_value(bytes: Slice(u8), a: Alloc) Values = {
    ::if(Values);
    if bytes.len > 8 {
        new := a.alloc(u8, bytes.len);
        new.copy_from(bytes);
    };
    bytes.to_value()
}

fn log_type(c: CompilerRs, ty: Type) Str = {
    c.cast()[][].log_type(ty)
}

// Mote: this is weak! often you'd rather use immediate_eval_expr.
fn as_const(self: *FatExpr) ?Values = {
    @if_let(self.expr&)
        fn Value(f) => { return(Some = f.bytes); };  // TODO: this should work when the branch returns never (no extra { ..; }) -- Jul 8
    
    .None
}

////////////////////////
/// Splitting Values /// 

// TODO: :const_field_fix
// TODO: if you move the ffi dynamic abi call to this language you can remove the rust version of this function. 
/// Take some opaque bytes and split them into ints. So (u8, u8) becomes a vec of two i64 but u16 becomes just one.
fn deconstruct_values(program: *SelfHosted, ty: Type, bytes: *ReadBytes, out: *List(i64), offsets: ?*List(Ty(Prim, u16))) PRes = {
    info := program.get_info(ty);

    @debug_assert_eq(bytes.i.mod(info.align_bytes.zext()), 0);
    found_len := bytes.bytes.len() - bytes.i;
    @debug_assert(
        found_len >= info.stride_bytes.zext(),
        "deconstruct_values of % wants % bytes but found %: %",
        program.log_type(ty), info.stride_bytes, found_len, {
            bytes := bytes.bytes.slice(bytes.i, bytes.bytes.len);
            out: List(u8) = list(temp());
            for bytes{ b | 
                @fmt(out&, "%, ", @as(i64) b.zext());
            };
            out.items()
        }
    );
    
    single :: fn($T: Type, prim: Prim) void => {
        offset := bytes.i;
        if offsets { offsets |
            offsets.push(@as(Ty(Prim, u16)) (prim, bytes.i.trunc()));
        };
        raw := bytes.read_next(T);
        out.push(@as(i64) raw.int());
    };
    
    @match(program.get_type(ty)) {
        fn Placeholder() => { return(@err("ICE: Unfinished type")); }; // TODO: show the type
        fn Never()       => return(@err("invalid type: Never"));
        fn F64()     => i64.single(.F64);
        fn FnPtr(_)  => i64.single(.P64);
        fn Ptr(_)    => i64.single(.P64);
        fn VoidPtr() => i64.single(.P64);
        fn F32()     => u32.single(.F32);
        fn Fn(_)     => u32.single(.I32);
        fn Label(_)  => u32.single(.I32);
        fn void()    => return(.Ok);
        fn Bool()    => u8.single(.I8);
        fn Tagged(f) => {
            start := bytes.i;
            start_count := out.len;
            i64.single(.I64);
            tag := out.last().unwrap()[];
            @err_assert(tag < f.cases.len, "Invalid tag % in constant", tag) return;
            case := f.cases.index(tag);
            @try(deconstruct_values(program, case._1, bytes, out, offsets)) return;
            padding := (@as(i64) info.stride_bytes.zext()) - (bytes.i - start);
            @err_assert(padding >= 0, "ICE: confused about @tagged size.") return;
            end_count := out.len;
            padding_slots := (@as(i64) info.size_slots.zext()) - (end_count - start_count);
            @err_assert(padding_slots >= 0, "ICE: too many slots????") return;
            
            // :tagged_prims_hack
            // TODO: try to make a test that fails without this. 
            //       ie just replace with 
            //       range(0, padding_slots) { _ |
            //          // TODO: wrong prim
            //          i64.single(.I64);
            //      };
            //      otherwise reduce the copy-paste of the 3 versions of this. 
            types := program.flat_tuple_types(ty);
            types := types.items().slice_last(padding_slots);
            types := types.unwrap();
            // now types is just padding

            // If this is a smaller varient, pad out the slot.
            for types { p |
                ty := @unwrap(program.prim(p), "not prim") return;
                out.push(0);
                if offsets { offsets |
                    // TODO: do we care about the offsets?
                    offsets.push(@as(Ty(Prim, u16)) (ty, bytes.i.trunc()));
                };
            };
            bytes.i += padding;
        }
        fn Enum(f)   => return(deconstruct_values(program, f.raw, bytes, out, offsets));
        fn Named(f)  => return(deconstruct_values(program, f._0, bytes, out, offsets)); // TODO: make Named payload not a tuple or do destructuring here. -- Jul 8
        fn Int(_)    => {
            @switch(program.get_info(ty).stride_bytes()) {
                @case(1) => u8.single(.I8);
                @case(2) => u16.single(.I16);
                @case(4) => u32.single(.I32);
                @case(8) => i64.single(.I64);
                @default fn(n: u16) => return(@err("ICE: bad int stride %", n));
            };
        }
        fn Array(f) => {
            inner_align := program.get_info(f.inner).align_bytes();
            range(0, f.len.zext()) { _ |
                @debug_assert_eq(bytes.i.mod(inner_align.zext()), 0);
                @try(deconstruct_values(program, f.inner, bytes, out, offsets)) return;
            };
        }
        fn Struct(f) => {
            @err_assert(f.layout_done, "ICE: layout not ready") return;
            prev := 0;
            size := program.get_info(ty).stride_bytes();
            start := bytes.i;
            for f.fields { t |
                continue :: local_return;
                if t.kind == .Const {
                    continue();
                };
                assert(prev <= t.byte_offset, "ICE: backwards field offset");
                bytes.i = t.byte_offset;
                @try(deconstruct_values(program, t.ty, bytes, out, offsets)) return;
                prev = t.byte_offset;
            };
            bytes.i = start + size.zext(); // eat trailing stride padding
        }
    };
    .Ok
}

// When binding const arguments you want to split a large value into smaller ones that can be referred to by name.
fn chop_prefix(self: *SelfHosted, prefix: Type, t: *ReadBytes) ?Values #once = {
    info := self.get_info(prefix);
    @debug_assert_eq(0, t.i.mod(info.align_bytes.zext()));
    if t.take(info.stride_bytes.zext()) { bytes | 
        return(Some = bytes.to_value(self.get_alloc()));
    };
    .None
}

// You need this for reproducible builds!
// Without calling this on when baking constants, you can leak comptime addresses in @tagged padding (which happened in get_or_create_type in the compiler).
// TODO: decide if i need to do this for baked arg key hashing. 
fn zero_padding(program: *SelfHosted, ty: Type, bytes: *ReadBytes) PRes = {
    info := program.get_info(ty);

    @debug_assert_eq(bytes.i.mod(info.align_bytes.zext()), 0);
    found_len := bytes.bytes.len() - bytes.i;
    @debug_assert(
        found_len >= info.stride_bytes.zext(),
        "zero_padding of % wants % bytes but found %",
        program.log_type(ty), info.stride_bytes, found_len
    );
    
    set_zeros :: fn(padding: i64) void => {
        range(0, padding) { i |
            bytes.bytes[bytes.i + i] = 0;
        };
        bytes.i += padding;
    };
    
    @match(program.get_type(ty)) {
        fn Tagged(f) => {
            start := bytes.i;
            tag := bytes.read_next(i64);
            @err_assert(tag < f.cases.len, "Invalid tag % in constant", tag) return;
            case := f.cases.index(tag);
            @try(zero_padding(program, case._1, bytes)) return;
            padding := (@as(i64) info.stride_bytes.zext()) - (bytes.i - start);
            @err_assert(padding >= 0, "ICE: confused about @tagged size.") return;
            set_zeros(padding);
        }
        fn Enum(f)   => return(zero_padding(program, f.raw, bytes));
        fn Named(f)  => return(zero_padding(program, f._0, bytes)); 
        fn Array(f) => {
            inner_align := program.get_info(f.inner).align_bytes();
            range(0, f.len.zext()) { _ |
                @debug_assert_eq(bytes.i.mod(inner_align.zext()), 0);
                @try(zero_padding(program, f.inner, bytes)) return;
            };
        }
        fn Struct(f) => {
            @err_assert(f.layout_done, "ICE: layout not ready") return;
            prev := 0;
            size := program.get_info(ty).stride_bytes();
            start := bytes.i;
            for f.fields { t |
                continue :: local_return;
                if t.kind == .Const {
                    continue();
                };
                diff := t.byte_offset - prev;
                assert(diff >= 0, "ICE: backwards field offset");
                set_zeros(diff);
                @try(zero_padding(program, t.ty, bytes)) return;
                prev = bytes.i - start;
            };
            set_zeros(size.zext() - prev); // eat trailing stride padding
        }
        @default => {
            // no padding
            bytes.i += info.stride_bytes.zext();  
        };
    };
    .Ok
}


ReadBytes :: @struct(bytes: [] u8, i: i64);

fn read_next(self: *ReadBytes, $T: Type) T #generic = {
    @debug_assert_eq(self.i.mod(T.size_of()), 0);
    @safety(.Bounds) self.i + T.size_of() - 1 < self.bytes.len();
    
    ptr := self.bytes.ptr.offset(self.i);
    self.i += T.size_of();
    ptr_cast_unchecked(u8, T, ptr)[]
}

fn take(self: *ReadBytes, size: i64) ?[]u8 = {
    if(self.i + size - 1 >= self.bytes.len(), => return(.None));
    ptr := self.bytes.ptr.offset(self.i);
    self.i += size;
    (Some = (ptr = ptr, len = size))
}

////////////////////
/// Dynamic Call ///

fn call_dynamic_values(program: CompilerRs, f_ptr: i64, f_ty: *FnType, args_value: []u8, comp_ctx: bool) Res(Values) = {
    parts: List(i64) = list(temp());
    reader: ReadBytes = (bytes = args_value, i = 0);
    @try(program[][].deconstruct_values(f_ty.arg, reader&, parts&, .None)) return;
    parts: RsVec(i64) = parts.rs();
    program.call_dynamic(f_ptr, f_ty, parts&, comp_ctx)
}

fn call_dynamic(program: CompilerRs, ptr: i64, f_ty: *FnType, args: *RsVec(i64), comp_ctx: bool) Res(Values) = {
    program.call_dynamic_inner(ptr, f_ty, args, comp_ctx)
}

// TODO: this is the only place I still use float_mask.
//       i could get rid of it if i switch to using PrimSig.
//       then the next step is to lazily jit shims so i don't have to implement the calling convention twice. 
// Note: FnType by ptr because i think i do (u32, u32) calling convention wrong. 
fn call_dynamic_inner(program: CompilerRs, ptr: i64, f_ty: *FnType, args: *RsVec(i64), comp_ctx: bool) Res(Values) = {
    @if(TRACE_CALLS) @println("[call dynamic at addr=%] args=%", ptr, args.items());
    
    arg := program.get_info(f_ty.arg);
    ret := program.get_info(f_ty.ret);
    if arg.float_mask == 0 && ret.float_mask == 0 {
    } else {
        @err_assert(ret.size_slots <= 1, "TODO: float call with multiple returns at comptime") return;
    };

    if comp_ctx {
        c := (**SelfHosted).int_from_ptr(program);
        //@println("comp_ctx; %", c);
        args.insert(0, c, temp());
    };
    :: if(?RsVec(u8));
    indirect_ret: ?RsVec(u8) = if ret.size_slots > 2 {
        mem: List(u8) = 0.repeated(@as(i64) ret.stride_bytes.zext(), program.get_alloc());
        mem: RsVec(u8) = mem.rs();
        ret_addr := u8.int_from_ptr(mem.ptr);
        args.insert(0, ret_addr, temp());
        @assert_eq(arg.float_mask, 0);
        (Some = mem)
    } else {
        .None
    };
    @err_assert(args.len() <= 8, "too many args for comptime call") return;
    // not doing this is ub but like... meh.
    // if something interesting happens to be after the args and you call with the wrong signature you could read it.
    // its fine to read garbage memory and put it in the registers you know callee will ignore (if types are right).
    // However, now that unit is zero sized, the vec may have no allocation, so you get a segfault trying to load from it.
    READ_GARBAGE :: false;
    if args.is_empty().or(!READ_GARBAGE) {
        a := args[].assume_owned(temp());
        a&.reserve(8 - args.len());
        args[] = a.rs();
        //while args.len() < 8 {
        //    args.push(0);
        //}
    };
    
    //@println("call with %", args.items());

    :: if(Res(Values));
    result: Res(Values) = if ret.size_slots <= 1 {
        r := if arg.float_mask == 0 && ret.float_mask == 0 {
            arg8ret1(ptr, args.ptr)
        } else {
            // TODO: assert: arg.float_mask.count_ones() == arg.size_slots as u32 && ret.float_mask.count_ones() == ret.size_slots as u32
            //       return(@err("ICE: i dont do mixed int/float registers but backend does"))
            // :FUCKED, but I'm going to replace this with using the normal jit once i port that. 
            arg8ret1_all_floats(ptr, args.ptr)
        };
        assert(ret.stride_bytes <= 8, "big ret");
        (Ok = (Small = (r, ret.stride_bytes.trunc())))
    } else {
        if ret.size_slots == 2 {
            ::Res(Ty(Prim, Prim));
            (f, s) := program[][].prim_pair(f_ty.ret).unwrap();
            r := arg8ret2(ptr, args.ptr);
            // TODO: floats!
            if f.eq(.I64).or(f.eq(.P64)) {
                if s.eq(.I64).or(s.eq(.P64)) {
                    return(Ok = to_values(program[][], Ty(i64, i64), (r.fst, r.snd)));
                };
                if s.eq(.I8) {
                    return(Ok = to_values(program[][], Ty(i64, u8), (r.fst, @as(u8) r.snd.trunc())));
                };
            };
            if f.eq(.I32) {
                if s.eq(.I32) {
                    return(Ok = to_values(program[][], Ty(u32, u32), (@as(u32) r.fst.trunc(), @as(u32) r.snd.trunc())));
                };
            };
            @err("TODO: unhandled prim pair for comptime call")
        } else {
            arg8ret_struct(ptr, args.ptr);
            mem := indirect_ret.unwrap();
            (Ok = (Big = mem))
        }
    };
    @if(TRACE_CALLS) @println("[returned dynamic from addr=%]", ptr);
    result
}

to_values :: fn(program: *SelfHosted, $T: Type, v: T) Values #generic = {
    bytes := T.cast_to_bytes(v&);
    program.from_bytes(bytes)
}

fn from_bytes(program: *SelfHosted, bytes: []u8) Values = {
    ::if(Values);
    if bytes.len() <= 8 {
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or(b.zext().shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {
       (Big = bytes.clone(program.get_alloc()).rs())
    }
}

// loads 8 words from args into x0-x7 then calls fnptr
fn arg8ret1(fnptr: i64, first_of_eight_args: *i64) i64 #asm #aarch64 #c_call = (
    mov(Bits.X64, x16, x0),  // save callee since we need to use this register for args
    mov(Bits.X64, x17, x1),
    ldr_uo(Bits.X64, x0, x17, 0),
    ldr_uo(Bits.X64, x1, x17, 1),
    ldr_uo(Bits.X64, x2, x17, 2),
    ldr_uo(Bits.X64, x3, x17, 3),
    ldr_uo(Bits.X64, x4, x17, 4),
    ldr_uo(Bits.X64, x5, x17, 5),
    ldr_uo(Bits.X64, x6, x17, 6),
    ldr_uo(Bits.X64, x7, x17, 7),
    br(x16, 0), // tail call
);

IntPair :: @struct(fst: i64, snd: i64);
// TODO: copy-paste of arg8ret1
fn arg8ret2(fnptr: i64, first_of_eight_args: *i64) IntPair #asm #aarch64 #c_call = (
    mov(Bits.X64, x16, x0),  // save callee since we need to use this register for args
    mov(Bits.X64, x17, x1),
    ldr_uo(Bits.X64, x0, x17, 0),
    ldr_uo(Bits.X64, x1, x17, 1),
    ldr_uo(Bits.X64, x2, x17, 2),
    ldr_uo(Bits.X64, x3, x17, 3),
    ldr_uo(Bits.X64, x4, x17, 4),
    ldr_uo(Bits.X64, x5, x17, 5),
    ldr_uo(Bits.X64, x6, x17, 6),
    ldr_uo(Bits.X64, x7, x17, 7),
    br(x16, 0), // tail call
);

fn arg8ret_struct(fnptr: i64, first_of_eight_args: *i64) void #asm #aarch64 #c_call = (
    ldr_uo(Bits.X64, x8, x1, 0),  // arm cc has return address in x8 but we want to represent it as first argument. 
    add_im(Bits.X64, x17, x1, 8, 0),
    mov(Bits.X64, x16, x0),  // save callee since we need to use this register for args
    ldr_uo(Bits.X64, x0, x17, 0),
    ldr_uo(Bits.X64, x1, x17, 1),
    ldr_uo(Bits.X64, x2, x17, 2),
    ldr_uo(Bits.X64, x3, x17, 3),
    ldr_uo(Bits.X64, x4, x17, 4),
    ldr_uo(Bits.X64, x5, x17, 5),
    ldr_uo(Bits.X64, x6, x17, 6),
    ldr_uo(Bits.X64, x7, x17, 7),
    br(x16, 0), // tail call
);

// loads 8 words from args into d0-d7 then calls fnptr. the return value in d0 is bit cast to an int in x0 for you.
fn arg8ret1_all_floats(fnptr: i64, first_of_eight_args: *i64) i64 #asm #aarch64 #c_call = (
    sub_im(Bits.X64, sp, sp, 16, 0),
    stp_so(.X64, fp, lr, sp, @as(i7) 0),

    f_ldr_uo(Bits.X64, x0, x1, 0),
    f_ldr_uo(Bits.X64, x1, x1, 1),
    f_ldr_uo(Bits.X64, x2, x1, 2),
    f_ldr_uo(Bits.X64, x3, x1, 3),
    f_ldr_uo(Bits.X64, x4, x1, 4),
    f_ldr_uo(Bits.X64, x5, x1, 5),
    f_ldr_uo(Bits.X64, x6, x1, 6),
    f_ldr_uo(Bits.X64, x7, x1, 7),

    br(x0, 1),
    fmov_from(x0, x0),
    ldp_so(.X64, fp, lr, sp, @as(i7) 0),
    add_im(Bits.X64, sp, sp, 16, 0),
    ret(),
);

// args 1-6: RDI, RSI, RDX, RCX, R8, R9; then stack
// floats: xmm0 - xmm7
fn arg8ret1(fnptr: i64, first_of_eight_args: *i64) i64 #asm #x86 #c_call = simple_x86;
fn arg8ret2(fnptr: i64, first_of_eight_args: *i64) IntPair #asm #x86 #c_call = simple_x86;
fn arg8ret_struct(fnptr: i64, first_of_eight_args: *i64) void #asm #x86 #c_call = simple_x86;
fn arg8ret1_all_floats(fnptr: i64, first_of_eight_args: *i64) i64 #asm #x86 #c_call = """
    movq xmm0, [rsi]
    movq xmm1, [rsi + 8]
    movq xmm2, [rsi + 16]
    movq xmm3, [rsi + 24]
    movq xmm4, [rsi + 32]
    movq xmm5, [rsi + 40]
    movq xmm6, [rsi + 48]
    movq xmm7, [rsi + 56]
    call rdi
    movq rax, xmm0
    ret
""";

simple_x86 :: """
    mov rax, rdi
    mov r11, rsi
    mov rdi, [r11]
    mov rsi, [r11 + 8]
    mov rdx, [r11 + 16]
    mov rcx, [r11 + 24]
    mov r8,  [r11 + 32]
    mov r9,  [r11 + 40]
    push [r11 + 48]
    push [r11 + 56]
    push [r11 + 64] // want to always allow 8 args but we might have a struct return pointer as a fake first arg. 
    call rax
    add sp, 24 // i have to fix the stack pointer // TODO: i don't trust if callee was expecting the args? 
    ret
""";

// TODO: decide if I need to bring back zero_padding. 
//       I used to do it on from_values/to_values because i blindly use the bytes of Values as hashmap keys for deduplicating 
//       functions with the same const arguments. and that gets scary when you can have different byte patterns for the same logical value.
//       but if i really care about that I should do it deeply following pointers too. 
//       for now i don't have any tests that need it so its probably fine for now.   
//       removing it seems not to change the amount of llvm ir emitted for the compiler, so it wasnt doing anything for me yet. 
//       a more robust solution would be to jit the type's overload of hash. 
//       I also use *Field as const args for reflection stuff which is a bit sketchy, but becomes better if i keep all the typeinfos
//       in a bucketarray so they're stable without cloning. thats good for avoiding duplicates if you use typeinfo at runtime too. 
//            -- Jul 13

/////////////
/// Types ///
// TODO: maybe the compiler should have an ast.fr for stuff like this. 

// TODO: should this do raw_type?
fn get_type(self: *SelfHosted, ty: Type) *TypeInfo /* const */ #inline = {
    self.types&.nested_index(ty.as_index())
}

fn should_deduplicate(type: *TypeInfo) bool = {
    @match(type) {
        fn Struct(f) bool => {
            f.is_tuple  // :only_intern_tuples
        }
        fn Tagged(_) => false;
        fn Enum(_)   => false;
        fn Named(_)  => false;
        fn Placeholder() => false; // because it will get filled in later
        fn VoidPtr()  => false; // TODO: hack to match old behaviour and still call intern_type at the beginning
        @default     => true;
    }
}

// TODO: call this maybe_intern_type?
fn intern_type(self: *SelfHosted, info: TypeInfo) Type = {
    ::if(Type);
    if info&.should_deduplicate() {
        key := info&;
        
        // TODO: miscompilation if  you try to do this>>?
        //@assert(!key.is(.Placeholder), "wtf");
        //@println("%", key.tag());
        
        or self.type_lookup&.get(key&) {| 
            id := typeid_from_index(self.types.len);
            stable := self.types&.push(info);
            self.type_lookup&.insert(stable, id);
            self.type_extra&.push(TypeMeta.zeroed()); // is_sized = false
            id
        }
    } else {
        id := typeid_from_index(self.types.len);
        self.types&.push(info);
        self.type_extra&.push(TypeMeta.zeroed()); // is_sized = false
        id
    }
}

fn save_guessed_name(self: *SelfHosted, type: Type, name: Symbol) void = {
    guessed := self.type_extra&.nested_index(type.as_index())[].inferred_name&;
    if guessed[] == Flag.SYMBOL_ZERO.ident() { 
        // TODO: also check first_dumb_type_name - last_dumb_type_name range?
        guessed[] = name;
    };
}

fn update_placeholder(self: *SelfHosted, placeholder_slot: Type, real_type: Type, name: Symbol) void = {
    idx := placeholder_slot.as_index();
    slot := self.types&.nested_index(idx);
    @debug_assert(slot.is(.Placeholder));
    slot[] = (Named = (real_type, name));
    // TODO: I don't understand why you need this, but you definitely do...
    //       cause its always a Named (we just put that there), and we don't deduplcate those,
    //       so who's ever trying to look it up...? 
    //       i dont think the rust code did this????? -- Jul 21
    @debug_assert(self.type_lookup&.get(slot&).is_none());
    self.type_lookup&.insert(slot, placeholder_slot);
    
    // just in case.    copy-paste call to truncate but now want to have the list full size ahead of time. 
    i := self.type_extra.len - 1; // we happen to know this can't underflow because there are builtin types.  
    while => i > idx {
        self.type_extra&.nested_index(i)[] = TypeMeta.zeroed(); // is_sized = false
        i -= 1;
    };
}

// these are hardcoded numbers in TypeId constructors
// if you remove any remember to fix later indices!
fn init_fixed_types(self: *SelfHosted) void = {
    @debug_assert_eq(self.types.len, 0);
    self.intern_type(.Placeholder);  // UnknownType
    self.intern_type(.void);
    self.intern_type(Named = (typeid_from_index(10), Flag.Type.ident()));
    self.intern_type(Int = (bit_count = 64, signed = true));
    self.intern_type(.Bool);
    self.intern_type(.VoidPtr);
    self.intern_type(.Never);
    self.intern_type(.F64);
    self.intern_type(Named = (typeid_from_index(10), Flag.OverloadSet.ident()));
    self.intern_type(Named = (typeid_from_index(10), Flag.ScopeId.ident()));
    self.intern_type(Int = (bit_count = 32, signed = false));
    self.intern_type(.VoidPtr); // TODO: why is this here twice? 
    self.intern_type(.F32);
    self.intern_type(Named = (typeid_from_index(10), Flag.FuncId.ident()));
    self.intern_type(Named = (typeid_from_index(10), Flag.LabelId.ident()));
    self.intern_type(Named = (typeid_from_index(10), Flag.Symbol.ident()));
}

fn get_info_is_ready(self: *SelfHosted, ty: Type) bool = {
    ty := self.raw_type(ty);
    old := self.type_extra&.nested_index(ty.as_index());
    old.is_sized 
}

:: as_ref(TypeMeta);
fn get_info(self: *SelfHosted, ty: Type) *TypeMeta = {
    ty := self.raw_type(ty);
    old := self.type_extra&.nested_index(ty.as_index());
    if old.is_sized {
        return(old);
    };
    
    // :TooManyArgsX64
    // TODO: when i didnt have some of the trunc impls this had a "tried to call missing function"
    new :: fn(size_slots: u16, align_bytes: u16, float_mask: u32, contains_pointers: bool, stride_bytes: u16, pass_by_ref: bool) TypeMeta #inline = {
        @debug_assert_eq(stride_bytes.mod(align_bytes), 0);
        (
            float_mask = float_mask,
            size_slots = size_slots,
            stride_bytes = stride_bytes,
            align_bytes = align_bytes,
            contains_pointers = contains_pointers,
            pass_by_ref = pass_by_ref,
            is_sized = true,
        )
    };
    
    info := @match(self.get_type(ty)) {
        fn Placeholder() => {
            @panic("Unfinished type %", ty.as_index()); // TODO: err!
            new(0, 0, 0, false, 0, false)
        }
        fn Struct(f) => {
            // TODO: shouldn't need to do this here. instead do it in the event loop. 
            //       old sema didn't need this, it just obsessively calledfinish_layout every time it produced a value. 
            self.finish_layout(ty); 
            @debug_assert(f.layout_done, "ICE: layout not done! %", self.log_type(ty));
            size: u16 = 0;
            @debug_assert(f.fields.is_empty().or(=> f.fields[0].byte_offset == 0), "ICE: first field should have offset 0");
            align: u16 = 1;
            mask: u32 = 0; // TODO: this is barely used. can go away if dyn call above uses prims instead. 
            pointers := false;
            bytes := 0;

            each f.fields { arg |
                continue :: local_return;
                if(arg.kind == .Const, => continue());
                info := self.get_info(arg.ty);
                align = align.max(info.align_bytes);
                @debug_assert_eq(arg.byte_offset.mod(info.align_bytes.zext()), 0, "ICE: unaligned field");
                @debug_assert_ge(arg.byte_offset, bytes, "ICE: fields not in order"); // TODO: assert not past max_u16 or better have a safety checking intcast
                end := arg.byte_offset + info.stride_bytes.zext();
                bytes = bytes.max(end);
                size += info.size_slots;
                if size > 16 {
                    // you only actually care about floats for passing in registers so if its bigger than 16, wont matter anyway?
                    // until i do struct c abi properly and tuples mean args in a different way.
                    mask = 0;
                } else {
                    mask = mask.shift_left(info.size_slots.zext());
                    mask = mask.bit_or(info.float_mask);
                };
                pointers = pointers.or(info.contains_pointers);
            };
            
            extra := bytes.mod(align.zext());
            if extra != 0 {
                bytes += align.zext() - extra;
            };

            new(size, align, mask, pointers, bytes.trunc(), size > 2)
        }
        fn Tagged(f) => {
            size: u16 = 0;
            bytes: u16 = 0;
            pointers := false;  
            
            @debug_assert(f.cases.len > 0, "TODO: i guess this can just be void");
            each f.cases { f |
                info := self.get_info(f._1);
                size = size.max(info.size_slots);
                bytes = bytes.max(info.stride_bytes);
                pointers = pointers.or(info.contains_pointers);
            };
            size += 1;  // :tag_is_i64
            bytes += 8;
            
            align: u16 = 8;
            extra := bytes.mod(align);
            if extra != 0 {
                bytes += align - extra;
            };
            
            
            // TODO: currently tag is always i64 so align 8 but should use byte since almost always enough. but you just have to pad it out anyway.
            //       even without that, if i add 16 byte align, need to check the fields too.  :tag_is_i64
            new(size, 8, 0, pointers, bytes, size > 2)
        }
        fn Never() => new(0, 1, 0, false, 0, false);
        fn Int(int) => {
            // :SmallTypes
            @switch(int.bit_count) {
                @case(8) => new(1, 1, 0, false, 1, false);
                @case(16) => new(1, 2, 0, false, 2, false);
                @case(32) => new(1, 4, 0, false, 4, false);
                @default => new(1, 8, 0, false, 8, false);
            }
        }
        // TODO: the float_mask thing is no longer enough information!
        //       you can't tell if its the whole register or just half. tho maybe you never need to know so its fine...
        fn F32()  => new(1, 4, 1, false, 4, false);
        fn F64()    => new(1, 8, 1, false, 8, false);
        fn void() => new(0, 1, 0, false, 0, false);
        fn Bool() => new(1, 1, 0, false, 1, false); // :SmallTypes
        fn Ptr(_)     => new(1, 8, 0,  true, 8, false);
        fn VoidPtr()  => new(1, 8, 0,  true, 8, false);
        fn FnPtr(_)   => new(1, 8, 0,  true, 8, false);
        fn Label(_)   => new(1, 4, 0, false, 4, false);
        fn Fn(_)      => new(1, 4, 0, false, 4, false);
        fn Array(f) => {
            ::if(TypeMeta);
            if f.len == 0 {
                new(0, 1, 0, false, 0, false)
            } else {
                info := self.get_info(f.inner);
                new(
                    info.size_slots * @as(u16) f.len.trunc(),
                    info.align_bytes,
                    0,
                    info.contains_pointers,
                    info.stride_bytes * f.len.trunc(),
                    info.pass_by_ref.or(f.len > 1),
                )
            }
        }
        fn Enum(_) => {
            panic("unreachable: get meta of raw enum");
            new(0, 0, 0, false, 0, false)
        }
        fn Named(_) => {
            panic("unreachable: get meta of raw named");
            new(0, 0, 0, false, 0, false)
        }
    };
    info.inferred_name = old.inferred_name;  // save if we poked this in before. 
    old[] = info;
    old
}

fn raw_type(c: *SelfHosted, ty: Type) Type = {
    loop {
        @match(c.get_type(ty)) {
            fn Named(f) => {
                ty = f._0;
            }
            fn Enum(f) => {
                ty = f.raw;
            }
            @default => {
                return(ty);
            };
        };
    };
    ty
}

fn finish_layout_deep(self: *SelfHosted, ty: Type) void = {
    @debug_assert_le(ty.as_index(), self.types.len);
    if(self.finished_layout_deep&.get(ty.as_index()), => return());
    self.finished_layout_deep&.set(ty.as_index()); // do this at the beginning to stop recursion.
    self.finish_layout(ty);
    ty := self.raw_type(ty);
    
    @match(self.get_type(ty)) {
        fn Fn(f) => {
            self.finish_layout_deep(f.arg);
            self.finish_layout_deep(f.ret);
        }
        fn FnPtr(f) => {
            self.finish_layout_deep(f.ty.arg);
            self.finish_layout_deep(f.ty.ret);
        }
        fn Ptr(inner) => self.finish_layout_deep(inner[]);
        fn Label(inner) => self.finish_layout_deep(inner[]);
        fn Array(f) => self.finish_layout_deep(f.inner);
        fn Struct(f) => {
            each f.fields { f |
                if f.kind != .Const {
                    self.finish_layout_deep(f.ty);
                };
            };
        }
        fn Tagged(f) => {
            each f.cases { f | 
                self.finish_layout_deep(f._1);
            };
        }
        @default => ();
    };
}

fn finish_layout(self: *SelfHosted, ty: Type) void = {
    ty := self.raw_type(ty);
    info := self.get_type(ty);
    @match(info) {
        fn Array(f) => self.finish_layout(f.inner);
        fn Tagged(f) => {
            each f.cases { variant |
                self.finish_layout(variant._1);
            };
        }
        fn Struct(f) => {
            if(f.layout_done, => return());
            f.const_field_count = 0;
            @debug_assert_eq(f.const_field_count, 0, "TODO: handle partial then error if you call this multiple times?");
            // :struct_layout_in_type_info
            // Here we mutate const_field_count, layout_done, and the byte_offset of the fields, which we know the hash function ignores
            
            bytes := 0;
            each f.fields& { p |
                continue :: local_return;
                if p.kind == .Const {
                    f.const_field_count += 1;
                    continue();
                };
                
                self.finish_layout(p.ty);
    
                info := self.get_info(p.ty);
                inv_pad := bytes.mod(info.align_bytes.zext());
                if inv_pad != 0 {
                    bytes += info.align_bytes.zext() - inv_pad;
                };
                p.byte_offset = bytes;
                
    
                // TODO: this must be wrong? surely you dont want to have the array padding out to alignment if its just a nested struct.
                //       tho its what c does. so i guess i need different reprs. and then size_of vs stride_of become different things.
                bytes += info.stride_bytes.zext();
            };
    
            f.layout_done = true;
        }
        @default => ();
    };
}

// TODO: can't have these added to the eq overload set to early or it gets stuck
:: {
    fn eq(a: *Field, b: *Field) bool = {
        (a.name& == b.name&) && (a.ty& == b.ty&) && (a.default& == b.default&) && (a.kind& == b.kind&)
    }
    TypeInfoStruct :: get_variant_type(TypeInfo, TypeInfo.Tag().Struct);
    fn eq(a: *TypeInfoStruct, b: *TypeInfoStruct) bool = {
        (a.is_tuple& == b.is_tuple&) && (a.fields& == b.fields&)
    }
};
