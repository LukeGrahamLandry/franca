//! Assorted functions work working with the bytes of comptime values.
//! - Emitting relocatable baked constants for AOT (fn bake_relocatable_value).
//! - Splitting aggragate values for function calls with partially $const parameters. 
//! - Track the mapping between comptime and runtime address spaces. 
//! - Determining struct size/layout

//////////////////////
/// Bake Constants ///
// See compiler/ast_external.fr/{BakedValue, BakedReloc}

/*
TODO: 
    - remove Qbe.Dat and use a similar (or the same?) sparse representiation. 
        then emit_ir shouldn't need to call bake_iter_legacy anymore. 
    - update the userspace apis to use the new representation. 
        table.emit_relocatable_constant_body, fn bake_relocatable_value(), fn dyn_bake_relocatable_value() #ct,
    - allow the frontend to ask for certain alignment of constant data and to ask for zero init on large 
        areas without actually allocating them at comptime. 
    - correctly handle internal pointers. you should be able to alias an offset into another constant. 
    - :ScaryIfAliasWrongType
        this is scary if you lie about the type so they don't match and would have expected to be baked differently. 
        once we allow internal pointers, the simple case would be if we see a pointer to the first field of a struct 
        and then later see a pointer to the whole struct. somehow we have to go back and fix the old thing to point 
        to the beginning of a larger memory area. for now we're just super cringe and declare that IFNDR   -- Mar 9, 2025 
        or another example would be: the same string being baked as CStr and Str.
    - i should have constant pointers as a concept in my language.
        right now i could put anything i show should be constant 
        (@const_slice, StringPool, TypeInfo, etc) in a special arena. 
        and then check where the pointer is for a hint that should catch a few of them.
    - life gets very confusing if someone mutates the value later after we think it's safe to emit it. 
        right now we're being very careful in bake_translate_legacy to allow a user bake impl that 
        modifies the bytes for AOT but doesn't change the comptime bytes if the same value is used again 
        later (which ArenaAlloc relies on). it would feel more robust if all the user bakes were delayed until 
        the very end of compilation but my general goal is to be more unordered not less so that would be unfortunate. 
*/

fn bake_relocatable_constant(c: *SelfHosted, value: []u8, type: Type) Result(BakedVarId, *CompileError) = {
    to_zero: ReadBytes = (bytes = value, i = 0);
    c.comp().zero_padding(type, to_zero&);  // Needed for reproducible builds!
    
    jit_ptr := u8.int_from_ptr(value.ptr);
    // values <= 8 bytes are stored inline so thier jit_addr is not useful. 
    if value.len() > 8 {
        // TODO: this barely does anything! cause like cstr and slice don't go through this function. they call dyn_bake_relocatable_value. -- Jun 30
        //       this is the right idea, they just need to check thier pointers against the cache first. 
        if c.baked.lookup&.get(jit_ptr) { v | 
            // TODO: be careful about aliasing as different types with different custom_bake_constant? -- Jun 30 
            return(Ok = v);
        };
    };
    
    // TODO: storing jit_ptr in put_baked is sketchy for small values,
    //       but currently it's only used for jit constant access and if it were small you'd have just loaded it directly in emit_bc. -- Nov 3
    
    // TODO: is it really that helpful to tell the backend to use zeroinitilizer? could at least have a faster memcmp. 
    if value.all_zeroes() {
        return(Ok = c.put_baked((template = (Zeroes = value.len())), (Some = jit_ptr)));
    };
    
    // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
    info := c.get_info(type);
    if !info.contains_pointers {
        // If this is actually a top level Value and the is less than 8, 
        // this is very very bad because it would be stored inline and not have a stable address. 
        // BUT we also recurse here when following pointers so we can't deny small values here. 
        return(Ok = c.put_baked((template = (Bytes = value)), (Some = jit_ptr)));
    };
    
    out := BakedReloc.list(c.get_alloc());
    @try(c.bake_relocatable_body(value, type, out&, 0, false)) return;
    return(Ok = c.put_baked((template = (Bytes = value), relocations = out&.items()), (Some = jit_ptr)));
}

// TODO: make sure baked.lookup actually ever helps. might need to add checks in more places.
fn bake_relocatable_body(c: *SelfHosted, value: []u8, type: Type, out: *List(BakedReloc), reloc_off: u32, force_default_handling: bool) Result(void, *CompileError) = {
    info          := c.get_info(type);
    first_word    := if(value.len < 8, => 0 /*don't read OOB*/, => ptr_cast_unchecked(u8, i64, value.ptr)[]);
    only_one_word := value.len == 8;
    @debug_assert_eq(value.len(), info.stride_bytes.zext(), "Tried to emit constant value of wrong size.");
    treat_as_raw_bytes := false 
        // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
        || !info.contains_pointers 
        // We only support 64-bit targets so it contains_pointers and is 8 bytes then it's exactly one pointer. 
        // Eventually i want ?*T to use zero as .None but for now allow null pointers. 
        || (only_one_word && first_word == 0)
    ;
    if(treat_as_raw_bytes, => return(.Ok));
    
    // :ScaryIfAliasWrongType
    // If we've already seen this pointer, don't emit it again, just alias the old one. 
    if only_one_word {
        if c.baked.lookup&.get(first_word) { v | 
            out.push(off = reloc_off, target = (BakedVarId = v));
            return(.Ok);
        };
    
        c.update_pending_took_pointer();  // :SLOW to do this every time 
        if c.baked.functions&.get(first_word) { fid |
            out.push(off = reloc_off, target = (FuncId = fid));
            return(.Ok);
        };
    };
    @debug_assert_ge(value.len, 8, "we only support 64-bit targets so how can it contains_pointers but be small?");
    
    // We don't bother allowing user overloads if there's no pointers in the type.
    // Maybe we should but it seems a bit creepy to encourage you to use a magic hook that runs
    // at a random time during compilation to do something that can only be a spooky action at a distance side effect. idk. 
    // :bake_relocatable_value
    if !force_default_handling {
        xx := @try(c.get_custom_bake_handler(type)) return;
        if xx { f |
            values := f(u8.raw_from_ptr(value.ptr));
            bytes_out := fixed_list(value);
            bake_translate_legacy(values, bytes_out&, out, reloc_off);
            return(.Ok);
        };
    };
    
    @match(c.get_type(type)) {
        fn FnPtr() => {
            // This is super slow but its only the error path so its fine
            if find_function_aa(c, first_word.rawptr_from_int()) { name |
                return(@err("ICE: we should have noticed that we took the address of % but somehow no", name));
            };
            
            return(@err(
                "function not found for constant\naddr=% type=%. we can't even find it if we scan all jitted functions.",
                first_word, c.log_type(type),
            ));
        }
        fn Ptr(inner) => {
            // First time seeing this address. Recurse and bake whatever it points to. 
            inner_info := c.get_info(inner[]);
            ptr := u8.ptr_from_int(first_word);
            data := slice(ptr, inner_info.stride_bytes.zext());
            id := @try(c.bake_relocatable_constant(data, inner[])) return;
            out.push(off = reloc_off, target = (BakedVarId = id));
        }
        fn Struct(f) => {
            @err_assert(!f.is_union, "TODO: union in a constant. but its hard cause what if theres pointers in there but we don't know which is active") return;
            
            // Just do all the fields.
            off := 0;
            each f.fields { f |
                info := c.get_info(f.ty);
                @err_assert(off <= f.byte_offset, "struct fields must be in order (%)", c.log_type(type)) return;
                off = f.byte_offset + info.stride_bytes.zext();
                v  := value.slice(f.byte_offset, off);
                @try(c.bake_relocatable_body(v, f.ty, out, reloc_off + f.byte_offset.trunc(), force_default_handling)) return;
            };
        }
        fn Tagged(f) => {
            tag := first_word; // For now tag is always 8 bytes. this is stupid but requires more thought to change. -- Mar 9, 2025. 
            @err_assert(f.cases.len > tag, "invalid tag % in constant @tagged.", tag) return;
            name, inner := f.cases.items()[tag];
            case_info := c.get_info(inner);
            payload_size: i64 = case_info.stride_bytes.zext();
            b := value;
            tag_size :: 8;
            payload  := value.slice(tag_size, tag_size + payload_size);
            @try(c.bake_relocatable_body(payload, inner, out, reloc_off + tag_size, force_default_handling)) return;
            
            padding := value.rest(tag_size + payload_size);
            if !is_all_zeroes(padding) {
                return(@err("ICE: zero_padding() didn't work on a @tagged constant. This is required for reproducibility."));
            };
        }
        fn Enum(f)  => return(c.bake_relocatable_body(value, f.raw, out, reloc_off, force_default_handling));
        fn Named(f) => return(c.bake_relocatable_body(value, f._0, out, reloc_off, force_default_handling));
        // TODO: explain that it's fine if it's reachable by an alias that knows the type
        // TODO: record it and see if we find an alias that knows the type later? 
        //       that sounds slow but wouldn't happen very often. 
        fn VoidPtr() => return(@err("You can't have a void pointer as a constant. The compiler can't tell how many bytes to put in the final executable."));
        @default => return(@err("ICE: constant that contains_pointers is a type that doesn't make any sense."));
    };
    .Ok
}

fn bake_legacy(self: *SelfHosted, legacy: BakedVar, jit_addr: ?i64) BakedVarId = {
    value: BakedValue = @match(legacy) {
        fn VoidPtrArray(legacy) => {
            bytes := u8.list(self.get_alloc());
            relocs := BakedReloc.list(self.get_alloc());
            bake_translate_legacy(legacy.items(), bytes&, relocs&, 0);
            (template = (Bytes = bytes.items()), relocations = (relocs.items()))
        };
        fn Bytes(bytes) => (template = (Bytes = bytes.items()));
        fn Zeros(len) => (template = (Zeroes = len));
    };
    self.put_baked(value, jit_addr)
}

// This avoids needing to linearly scan all the functions when they try to emit_relocatable_constant of a function pointer. 
// You only need to call it if they created a Values of it, so we might need to emit it as a constant later 
// (like for vtables where you construct the value in memory as with comptime pointers and then we emit relocations). 
// [Jun 30] building the compiler ends up with ~17000 FuncIds created and 21 calls to this function. So thats pretty good. 
// TODO: tho that means we wont find it if someone cheats somehow and gets the pointer in a way i didn't think of. 
//       it relies on them only being created through '!fn_ptr'/const_coerce expressions. 
fn created_jit_fn_ptr_value(self: *SelfHosted, f: FuncId, ptr: i64) void = {
    @debug_assert(ptr != 0, "created_jit_fn_ptr_value cannot be null");
    self.baked.functions&.insert(ptr, f);
}


fn put_baked(c: *SelfHosted, v: BakedValue, jit_ptr: ?i64) BakedVarId = {
    id: BakedVarId = (id = c.baked.values.len.trunc());
    push(c.baked.values&, v); 
    if jit_ptr { ptr | 
        c.baked.lookup&.insert(ptr, id); // TODO: check unique? 
    };
    id
}

fn emit_relocatable_constant_body(c: *SelfHosted, bytes: Slice(u8), ty: Type, force_default_handling: bool) Res(Slice(BakedEntry)) = {
    out := BakedReloc.list(temp());  // doesn't escape!
    // TODO: zero here? 
    @try(c.bake_relocatable_body(bytes, ty, out&, 0, force_default_handling)) return;
    
    value: BakedValue = (template = (Bytes = bytes), relocations = out.items());
    (Ok = bake_collect_legacy(value&, c.get_alloc()))
}

Baked :: @struct(
    values: BucketArray(BakedValue),
    // TODO: defend against the same memory being aliased as different types that would want to emit the constant differently? -- Jun 30
    // TODO: this doesn't help for small strings because they're stored inline in the Values. 
    // deduplicate values by thier jit address. 
    lookup: HashMap(i64, BakedVarId),
    // avoid iterating all function pointers when you try to have one as a constant. 
    functions: HashMap(i64, FuncId),
    // Some types need special handling (Slice, CStr, etc). This are called at comptime and might call back into the auto dynamic versions to handle thier fields. 
    custom_bake_constant: HashMap(Type, FuncId),
);

BakeHandler :: @FnPtr(self: rawptr) Slice(BakedEntry);

fn update_pending_took_pointer(c: *SelfHosted) void = {
    // TODO: have it tell us when pointers are ready. :SLOW
    found := false;
    m := c.comptime_codegen.m;
    unordered_retain c.pending_took_pointer& { it |
        fid, id := it[];
        need_more := true;
        use_symbol(m, id) { s | 
            p := s.jit_addr.int_from_rawptr();
            if p != 0 {
                c.baked.functions&.insert(p, fid);
            };
            p2 := s.shim_addr.int_from_rawptr();
            if p != p2 && p2 != 0 {
                c.baked.functions&.insert(p2, fid);
            };
            need_more = p == 0 || p == p2;
        };
        need_more
    };
}

// :bake_relocatable_value
fn get_custom_bake_handler(c: *SelfHosted, ty: Type) Res(?BakeHandler) = {
    if c.baked.custom_bake_constant&.get(ty) { fid |
        if c.get_fn_callable(fid) { fptr |
            fptr := assume_types_fn(rawptr, Slice(BakedEntry), fptr);
            return(Ok = (Some = fptr));
        };
        res := c.poll_in_place(rawptr) { () Maybe(rawptr) |
            @match(c.get_fn_callable(fid)) {
                fn Some(ptr) => (Ok = ptr);
                fn None() => (Suspend = c.wait_for(Jit = fid));
            }
        };
        fptr := @try(res) return;
        fptr := assume_types_fn(rawptr, Slice(BakedEntry), fptr);
        return(Ok = (Some = fptr));
    };
    (Ok = .None)
}

// TODO: do this lazily and as part of the event loop. 
fn check_for_new_aot_bake_overloads(self: *SelfHosted) PRes = {
    os := self.env.bake_os.expect("cannot aot during bootstrapping.");
    
    overloads := self.dispatch.overloads&.nested_index(os.as_index());
    prev := overloads.ready.len();
    self.compute_new_overloads(overloads);
    // TODO: this will miss them if someone caused new things to resolve outside this function.
    //       so if you ever call bake_relocatable_value manually. :FUCKED -- Jun 19
    current := overloads.ready.len();
    new := overloads.ready.items().slice(prev, current).clone(temp());
    for new { f |
        continue :: local_return;
        func := self.get_function(f);
        args := func.arg.bindings&;
        if args.len() != 1 {
            continue();
        };
        
        value := self.poll_in_place(void) {()Maybe(void)|
            return :: local_return;
            arg_ty := @check(self.infer_arguments(f)) return;
            ret_ty := @check(self.infer_return(f)) return;
            .Ok
        };
        @try(value) return;
        f_ty := func.finished_ty().expect("bake overload fn type");
        info := self.get_type(f_ty.ret);
        if !info.is(.Struct) {
            continue();
        };
        fields := info.Struct.fields&;
        // TODO: make sure its actually a Slice(BakedEntry) :FUCKED this will be so confusing someday 
        if fields.len() != 2 {
            continue();
        };

        first := f_ty.arg; 
        ty := self.unptr_ty(first).or(=> continue());
        prev := self.baked.custom_bake_constant&.insert(ty, f);
        // TODO: show the two locations and the type. :ERROR_QUALITY
        @err_assert(prev.is_none(), "conflicting overload for bake AOT constant") return;
    };
    .Ok
};

fn Res($T: Type) Type = Result(T, *CompileError);

fn unchecked_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    ptr_cast_unchecked(u8, T, b.ptr)
}

fn to_value(bytes: Slice(u8)) Values = {
    ::if(Values);
    if bytes.len <= 8 {
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or((@as(i64) b.zext()).shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {
        (Big = (cap = bytes.len, ptr = bytes.ptr, len = bytes.len))
    }
}

fn to_value(bytes: Slice(u8), a: Alloc) Values = {
    ::if(Values);
    if bytes.len > 8 {
        new := a.alloc(u8, bytes.len);
        new.copy_from(bytes);
    };
    bytes.to_value()
}

////////////////////////
/// Splitting Values /// 

// When binding const arguments you want to split a large value into smaller ones that can be referred to by name.
fn chop_prefix(self: *SelfHosted, prefix: Type, t: *ReadBytes) ?Values #once = {
    info := self.get_info(prefix);
    @debug_assert_eq(0, t.i.mod(info.align_bytes.zext()));
    if t.take(info.stride_bytes.zext()) { bytes | 
        return(Some = bytes.to_value(self.get_alloc()));
    };
    .None
}

// You need this for reproducible builds!
// Without calling this on when baking constants, you can leak comptime addresses in @tagged padding (which happened in get_or_create_type in the compiler).
// TODO: decide if i need to do this for baked arg key hashing. 
fn zero_padding(program: CompCtx, ty: Type, bytes: *ReadBytes) PRes = {
    info := program.get_info(ty);

    @debug_assert_eq(bytes.i.mod(info.align_bytes.zext()), 0);
    found_len := bytes.bytes.len() - bytes.i;
    @debug_assert(
        found_len >= info.stride_bytes.zext(),
        "zero_padding of % wants % bytes but found %",
        program.log(ty), info.stride_bytes, found_len
    );
    
    set_zeros :: fn(padding: i64) void => {
        range(0, padding) { i |
            bytes.bytes[bytes.i + i] = 0;
        };
        bytes.i += padding;
    };
    
    @match(program.get_type(ty)) {
        fn Tagged(f) => {
            start := bytes.i;
            tag := bytes.read_next(i64);
            @err_assert(tag < f.cases.len, "Invalid tag % in constant", tag) return;
            case := f.cases.index(tag);
            @try(zero_padding(program, case._1, bytes)) return;
            padding := (@as(i64) info.stride_bytes.zext()) - (bytes.i - start);
            @err_assert(padding >= 0, "ICE: confused about @tagged size.") return;
            set_zeros(padding);
        }
        fn Enum(f)   => return(zero_padding(program, f.raw, bytes));
        fn Named(f)  => return(zero_padding(program, f._0, bytes)); 
        fn Array(f) => {
            inner_align := program.get_info(f.inner).align_bytes();
            range(0, f.len.zext()) { _ |
                @debug_assert_eq(bytes.i.mod(inner_align.zext()), 0);
                @try(zero_padding(program, f.inner, bytes)) return;
            };
        }
        fn Struct(f) => {
            @err_assert(f.layout_done, "ICE: layout not ready") return;
            @err_assert(!f.is_union, "TODO: you can't zero padding of a union because we don't know which varient is active.") return;
            prev := 0;
            size: i64 = program.get_info(ty)[].stride_bytes.zext();
            start := bytes.i;
            for f.fields { t |
                diff := t.byte_offset - prev;
                assert(diff >= 0, "ICE: backwards field offset");
                set_zeros(diff);
                @try(zero_padding(program, t.ty, bytes)) return;
                prev = bytes.i - start;
            };
            set_zeros(size - prev); // eat trailing stride padding
        }
        @default => {
            // no padding
            bytes.i += info.stride_bytes.zext();  
        };
    };
    .Ok
}

to_values :: fn(program: *SelfHosted, $T: Type, v: T) Values #generic = {
    bytes := T.cast_to_bytes(v&);
    program.from_bytes(bytes)
};

fn to_expr(self: *SelfHosted, $T: Type, value: T, loc: Span) FatExpr #generic = {
    value := self.to_values(T, value); 
    (expr = (Value = (bytes = value, coerced = false)), loc = loc, ty = self.get_or_create_type(T), done = true)
}

fn from_bytes(program: *SelfHosted, bytes: []u8) Values = {
    ::if(Values);
    if bytes.len() <= 8 {
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or(b.zext().shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {
       (Big = bytes.clone(program.get_alloc()).as_raw())
    }
}


/////////////
/// Types ///

// TODO: should this do raw_type?
fn get_type(self: *SelfHosted, ty: Type) *TypeInfo /* const */ #inline = {
    self.types&.nested_index(ty.as_index())
}

fn should_deduplicate(type: *TypeInfo) bool = {
    @if_let(type) fn Struct(it) => return(it.is_tuple);   // :only_intern_tuples
    // Placeholder: because it will get filled in later
    // VoidPtr: TODO: hack to match old behaviour and still call intern_type at the beginning
    !@is(type, .Tagged, .Enum, .Named, .Placeholder, .VoidPtr)
}

// TODO: call this maybe_intern_type?
fn intern_type(self: *SelfHosted, info: TypeInfo) Type = {
    ::if(Type);
    if info&.should_deduplicate() {
        key := info&;

        or self.type_lookup&.get(key&) {
            id: Type = from_index(self.types.len);
            stable := self.types&.push(info);
            self.type_lookup&.insert(stable, id);
            self.type_extra&.push(TypeMeta.zeroed()); // is_sized = false
            id
        }
    } else {
        id: Type = from_index(self.types.len);
        self.types&.push(info);
        self.type_extra&.push(TypeMeta.zeroed()); // is_sized = false
        id
    }
}

fn save_guessed_name(self: *SelfHosted, type: Type, name: Symbol) void = {
    guessed := self.type_extra&.nested_index(type.as_index())[].inferred_name&;
    if guessed[] == Flag.SYMBOL_ZERO.ident() { 
        // TODO: also check first_dumb_type_name - last_dumb_type_name range?
        guessed[] = name;
    };
}

fn update_placeholder(self: *SelfHosted, placeholder_slot: Type, real_type: Type, name: Symbol) void = {
    idx := placeholder_slot.as_index();
    slot := self.types&.nested_index(idx);
    @debug_assert(slot.is(.Placeholder));
    slot[] = (Named = (real_type, name));
    @debug_assert(self.type_lookup&.get(slot&).is_none());
    
    // just in case.    copy-paste call to truncate but now want to have the list full size ahead of time. 
    i := self.type_extra.len - 1; // we happen to know this can't underflow because there are builtin types.  
    while => i > idx {
        self.type_extra&.nested_index(i)[] = TypeMeta.zeroed(); // is_sized = false
        i -= 1;
    };
}

// these are hardcoded numbers in TypeId constructors
// if you remove any remember to fix later indices!
fn init_fixed_types(self: *SelfHosted) void = {
    @debug_assert_eq(self.types.len, 0);
    self.intern_type(.Placeholder);  // UnknownType
    self.intern_type(.void);
    self.intern_type(Named = (from_index(10), Flag.Type.ident()));
    self.intern_type(Int = (bit_count = 64, signed = true));
    self.intern_type(.Bool);
    self.intern_type(.VoidPtr);
    self.intern_type(.Never);
    self.intern_type(.F64);
    self.intern_type(Named = (from_index(10), Flag.OverloadSet.ident()));
    self.intern_type(Named = (from_index(10), Flag.ScopeId.ident()));
    self.intern_type(Int = (bit_count = 32, signed = false));
    self.intern_type(.VoidPtr); // TODO: why is this here twice? 
    self.intern_type(.F32);
    self.intern_type(Named = (from_index(10), Flag.FuncId.ident()));
    self.intern_type(Named = (from_index(10), Flag.LabelId.ident()));
    self.intern_type(Named = (from_index(10), Flag.Symbol.ident()));
}

fn get_info_is_ready(self: *SelfHosted, ty: Type) bool = {
    ty := self.raw_type(ty);
    old := self.type_extra&.nested_index(ty.as_index());
    old.is_sized 
}

:: as_ref(TypeMeta);
fn get_info(self: *SelfHosted, ty: Type) *TypeMeta = {
    ty := self.raw_type(ty);
    old := self.type_extra&.nested_index(ty.as_index());
    if old.is_sized {
        return(old);
    };
    
    // TODO: when i didnt have some of the trunc impls this had a "tried to call missing function"
    new :: fn(size_slots: u16, align_bytes: u16, contains_pointers: bool, stride_bytes: u32) TypeMeta = {
        @debug_assert_eq(stride_bytes.mod(align_bytes.zext()), 0);
        (
            stride_bytes = stride_bytes,
            size_slots = size_slots,
            align_bytes = align_bytes,
            contains_pointers = contains_pointers,
            is_sized = true,
        )
    };
    
    info := @match(self.get_type(ty)) {
        fn Placeholder() => {
            @panic("Unfinished type %", ty.as_index()); // TODO: err!
            new(0, 0, false, 0)
        }
        fn Struct(f) => {
            // TODO: shouldn't need to do this here. instead do it in the event loop. 
            //       old sema didn't need this, it just obsessively calledfinish_layout every time it produced a value. 
            self.finish_layout(ty); 
            @debug_assert(f.layout_done, "ICE: layout not done! %", self.log_type(ty));
            size: u16 = 0;
            align: u16 = 1;
            pointers := false;
            bytes := 0;

            first := true;
            each f.fields { arg |
                @debug_assert(!first || arg.byte_offset == 0, "ICE: first field should have offset 0");
                first = false;
                
                info := self.get_info(arg.ty);
                align = align.max(info.align_bytes);
                @debug_assert_eq(arg.byte_offset.mod(info.align_bytes.zext()), 0, "ICE: unaligned field");
                @debug_assert(f.is_union || arg.byte_offset >= bytes, "ICE: fields not in order in %", self.log_type(ty)); // TODO: assert not past max_u16 or better have a safety checking intcast
                end := arg.byte_offset + info.stride_bytes.zext();
                if f.is_union {
                    size = size.max(info.size_slots);
                    bytes = bytes.max(info.stride_bytes.zext());
                } else {
                    size += info.size_slots;
                    bytes = bytes.max(end);
                };
                pointers = pointers.or(info.contains_pointers);
            };
            
            extra := bytes.mod(align.zext());
            if extra != 0 {
                bytes += align.zext() - extra;
            };
            
            @assert_lt(bytes, 1.shift_left(32), "cannot have a type larger than 4GB");
            new(size, align, pointers, bytes.trunc())
        }
        fn Tagged(f) => {
            size: u16 = 0;
            bytes: u32 = 0;
            pointers := false;  
            
            @debug_assert(f.cases.len > 0, "TODO: i guess this can just be void");
            each f.cases { f |
                info := self.get_info(f._1);
                size = size.max(info.size_slots);
                bytes = bytes.max(info.stride_bytes);
                pointers = pointers.or(info.contains_pointers);
            };
            size += 1;  // :tag_is_i64
            bytes += 8;
            
            align: u32 = 8;
            extra := bytes.mod(align);
            if extra != 0 {
                bytes += align - extra;
            };
            
            // TODO: currently tag is always i64 so align 8 but should use byte since almost always enough. but you just have to pad it out anyway.
            //       even without that, if i add 16 byte align, need to check the fields too.  :tag_is_i64
            new(size, 8, pointers, bytes)
        }
        fn Never() => new(0, 1, false, 0);
        fn Int(int) => {
            // :SmallTypes
            @switch(int.bit_count) {
                @case(8) => new(1, 1, false, 1);
                @case(16) => new(1, 2, false, 2);
                @case(32) => new(1, 4, false, 4);
                @default => new(1, 8, false, 8);
            }
        }
        fn F32()  => new(1, 4, false, 4);
        fn F64()  => new(1, 8, false, 8);
        fn void() => new(0, 1, false, 0);
        fn Bool() => new(1, 1, false, 1); // :SmallTypes
        fn Ptr(_)     => new(1, 8, true, 8);
        fn VoidPtr()  => new(1, 8, true, 8);
        fn FnPtr(_)   => new(1, 8, true, 8);
        fn Label(_)   => new(1, 4, false, 4);
        fn Fn(_)      => new(1, 4, false, 4);
        fn Array(f) => {
            ::if(TypeMeta);
            if f.len == 0 {
                new(0, 1, false, 0)
            } else {
                info := self.get_info(f.inner);
                bytes: i64 = info.stride_bytes.zext() * f.len.zext();
                @assert_lt(bytes, 1.shift_left(32), "cannot have a type larger than 4GB (trying to make an Array of % elements)", f.len);
                new(
                    info.size_slots * @as(u16) f.len.trunc(),
                    info.align_bytes,
                    info.contains_pointers,
                    bytes.trunc(),
                )
            }
        }
        @default => unreachable();
    };
    info.inferred_name = old.inferred_name;  // save if we poked this in before. 
    old[] = info;
    old
}

fn raw_type(c: *SelfHosted, ty: Type) Type = 
    c.comp().raw_type(ty);

fn finish_layout_deep(self: *SelfHosted, ty: Type) void = {
    @debug_assert_le(ty.as_index(), self.types.len);
    if(self.finished_layout_deep&.get(ty.as_index()), => return());
    self.finished_layout_deep&.set(ty.as_index()); // do this at the beginning to stop recursion.
    self.finish_layout(ty);
    ty := self.raw_type(ty);
    
    @match(self.get_type(ty)) {
        fn Fn(f) => {
            self.finish_layout_deep(f.arg);
            self.finish_layout_deep(f.ret);
        }
        fn FnPtr(f) => {
            self.finish_layout_deep(f.ty.arg);
            self.finish_layout_deep(f.ty.ret);
        }
        fn Ptr(inner)   => self.finish_layout_deep(inner[]);
        fn Label(inner) => self.finish_layout_deep(inner[]);
        fn Array(f)  => self.finish_layout_deep(f.inner);
        fn Struct(f) => each f.fields { f |
            self.finish_layout_deep(f.ty);
        };
        fn Tagged(f) => each f.cases { f | 
            self.finish_layout_deep(f._1);
        };
        @default => ();
    };
}

fn finish_layout(self: *SelfHosted, ty: Type) void = {
    ty := self.raw_type(ty);
    info := self.get_type(ty);
    @match(info) {
        fn Array(f)  => self.finish_layout(f.inner);
        fn Tagged(f) => each f.cases { variant |
            self.finish_layout(variant._1);
        };
        fn Struct(f) => {
            if(f.layout_done, => return());
            // :struct_layout_in_type_info
            // Here we mutate layout_done and the byte_offset of the fields, which we know the hash function ignores
            
            bytes := 0;
            each f.fields& { p |
                self.finish_layout(p.ty);
                p.byte_offset = 0;
                if !f.is_union {
                    info := self.get_info(p.ty);
                    inv_pad := bytes.mod(info.align_bytes.zext());
                    if inv_pad != 0 {
                        bytes += info.align_bytes.zext() - inv_pad;
                    };
                    p.byte_offset = bytes;
                    
                    // TODO: this must be wrong? surely you dont want to have the array padding out to alignment if its just a nested struct.
                    //       tho its what c does. so i guess i need different reprs. and then size_of vs stride_of become different things.
                    bytes += info.stride_bytes.zext();
                };
            };
    
            f.layout_done = true;
        }
        @default => ();
    };
}

// TODO: can't have these added to the eq overload set to early or it gets stuck
:: {
    fn eq(a: *Field, b: *Field) bool = {
        (a.name& == b.name&) && (a.ty& == b.ty&) && (a.default& == b.default&)
    }
    TypeInfoStruct :: get_variant_type(TypeInfo, TypeInfo.Tag().Struct);
    fn eq(a: *TypeInfoStruct, b: *TypeInfoStruct) bool = {
        (a.is_tuple& == b.is_tuple&) && (a.fields& == b.fields&)
    }
};
