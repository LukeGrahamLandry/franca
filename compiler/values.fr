//! Assorted functions work working with the bytes of comptime values.
//! - Emitting relocatable baked constants for AOT (fn bake_relocatable_value).

//////////////////////
/// Bake Constants ///

// This avoids needing to linearly scan all the functions when they try to emit_relocatable_constant of a function pointer. 
// You only need to call it if they created a Values of it, so we might need to emit it as a constant later 
// (like for vtables where you construct the value in memory as with comptime pointers and then we emit relocations). 
// [Jun 30] building the compiler ends up with ~17000 FuncIds created and 21 calls to this function. So thats pretty good. 
// TODO: tho that means we wont find it if someone cheats somehow and gets the pointer in a way i didn't think of. 
//       it relies on them only being created through '!fn_ptr'/const_coerce expressions. 
fn created_jit_fn_ptr_value(self: *SelfHosted, f: FuncId, ptr: i64) void #compiler = {
    self.baked.functions&.insert(ptr, f);
}

// TODO: call this from rust. 
fn save_bake_callback(self: *SelfHosted, ty: Type, f: BakeHandler) PRes #compiler = {
    prev := self.baked.custom_bake_constant&.insert(ty, f);
    if prev.is_some() {|
        // TODO: show the two locations and the type. :ERROR_QUALITY
        msg :: "conflicting overload for bake AOT constant";
        return (Err = (span = self.last_loc, msg = msg));
    };
    .Ok
}

fn get_baked(c: *SelfHosted, id: BakedVarId) *Ty(rawptr, BakedVar) #compiler = {
    c.baked.values&.nested_index(@as(i64) id.id.zext())
}

fn put_baked(c: *SelfHosted, v: BakedVar, jit_ptr: ?i64) BakedVarId = {
    i := c.baked.values.len;
    j := jit_ptr.or(=> 0).rawptr_from_int();
    c.baked.values&.push(@as(Ty(rawptr, BakedVar)) (j, v)); // TODO: this shouldn't need type hint.
    id: BakedVarId = (id = i.trunc());
    if jit_ptr { ptr | 
        c.baked.lookup&.insert(ptr, id); // TODO: check unique? 
    };
    id
}

fn emit_relocatable_constant_body(c: CompilerRs, bytes: Slice(u8), ty: Type, force_default_handling: bool) Res(Slice(BakedEntry)) = {
    out: List(BakedEntry) = list(c.get_alloc());
    value := bytes.to_value();
    @try(c.emit_relocatable_constant_body(ty, value&, out&, force_default_handling)) return;
    (Ok = out.items())
}

Baked :: @struct(
    values: BucketArray(Ty(rawptr, BakedVar)),
    // TODO: defend against the same memory being aliased as different types that would want to emit the constant differently? -- Jun 30
    // TODO: this doesn't help for small strings because they're stored inline in the Values. 
    // deduplicate values by thier jit address. 
    lookup: HashMap(i64, BakedVarId),
    // avoid iterating all function pointers when you try to have one as a constant. 
    functions: HashMap(i64, FuncId),
    // Some types need special handling (Slice, CStr, etc). This are called at comptime and might call back into the auto dynamic versions to handle thier fields. 
    custom_bake_constant: HashMap(Type, BakeHandler),
);
BakeHandler :: @FnPtr(self: rawptr) Slice(BakedEntry);

// TODO: be careful about the same string being baked as CStr and Str.
// TODO: i should have constant pointers as a concept in my language.
//       right now i could check if its in the constant data arena for a hint that should catch a few of them.
fn emit_relocatable_constant(c: CompilerRs, ty: Type, value: *Values) Res(BakedVarId) = {
    jit_ptr := value.jit_addr();
    // TODO: this barely does anything! cause like cstr and slice don't go through this function. they call dyn_bake_relocatable_value. -- Jun 30
    //       this is the right idea, they just need to check thier pointers against the cache first. 
    if c.baked.lookup&.get(jit_ptr) { v | 
        // TODO: be careful about aliasing as different types with different custom_bake_constant? -- Jun 30 
        return(Ok = v);
    };
    
    // TODO: is it really that helpful to tell the backend to use zeroinitilizer? could at least have a faster memcmp. 
    (fn() => {
        break :: local_return;
        for value.bytes() { b |
            if(b != 0, => break());
        };
        return(Ok = c[][].put_baked((Zeros = value.bytes().len()), (Some = jit_ptr)));
    })();
    
    // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
    info := c.get_info(ty);
    if (!info.contains_pointers) {|
        v := value.bytes().clone(c.get_alloc()); // TODO: do i have to clone? 
        return(Ok = c[][].put_baked((Bytes = v.rs()), (Some = jit_ptr)));
    };

    out: List(BakedEntry) = list(c.get_alloc());
    @try(c.emit_relocatable_constant_body(ty, value, out&, false)) return;
    (Ok = c[][].put_baked((VoidPtrArray = out.rs()), (Some = jit_ptr)))
}

// TODO: deduplicate small constant strings. they get stored in Values inline so can't be fixed by baked.lookup
// TODO: make sure baked.lookup actually ever helps. might need to add checks in more places.
fn emit_relocatable_constant_body(
    c: CompilerRs,
    ty: Type,
    value: *Values,
    out: *List(BakedEntry),
    force_default_handling: bool,
) PRes = {
    info := c.get_info(ty);
    if value.len() != info.stride_bytes.zext() {|
        msg := @format("ICE: Tried to emit constant value of wrong size. Expected % bytes but found %.", @as(i64) info.stride_bytes.zext(), value.len()) temp();
        return(Err = (span = c.last_loc, msg = msg.items()));
    };
    // TODO: alignment check? 
    
    // :bake_relocatable_value
    if (!force_default_handling) {|
        if c.baked.custom_bake_constant&.get(ty) { f |
            values := f(value.jit_addr().rawptr_from_int());
            out.push_all(values);
            return(.Ok);
        };
    };

    // Eventually we'll recurse to something with no pointers. ie Str -> [u8; n]
    if (!info.contains_pointers) {|
        assert_eq(value.len().mod(8), 0); // TODO. but rust version doesn't handle this either -- Jun 30
        
        bytes := value.bytes();
        ptr := ptr_cast_unchecked(u8, i64, bytes.ptr);
        ints: Slice(i64) = (ptr = ptr, len = bytes.len / 8);
        for ints { i | 
            // TODO: small prims
            out.push((Num = (i, .I64)));
        };
        return(.Ok);
    };

    // The rust version skipped to raw type at the top but that's kinda wrong cause there might be an overload for just some intermediate? 
    @match(c.get_type(ty)) {
        (fn FnPtr(_) => {
            ptr := i64.assume_cast(value)[];
            if c.baked.functions&.get(ptr) { fid |
                // assert_eq!(*f_ty, program[f].finished_ty().unwrap()); // TODO?
                out.push((FnPtr = fid));
                return (.Ok);
            };
            msg :: "function not found for constant. (might be ICE created_jit_fn_ptr_value)";
            return (Err = (span = c.last_loc, msg = msg));
        });
        (fn Ptr(inner) => {
            // TODO: allow zero?
            inner_info := c.get_info(inner[]);
            // load the pointer and recurse.
            addr := i64.assume_cast(value)[];
            
            // Eventually i want ?*T to use zero as .None but for now allow null pointers. 
            if addr == 0 {|
                out.push((Num = (0, .P64)));
                return(.Ok);
            };
            
            // emit_relocatable_constant only does it by the address of the value but pointers are small so stored inline. 
            if c.baked.lookup&.get(addr) { v | 
                out.push((AddrOf = v));
                return(.Ok);
            };
            
            ptr := u8.ptr_from_int(addr);
            data := slice(ptr, inner_info.stride_bytes.zext());
            inner_value := data.to_value(); // TODO: do i have to clone? 
            id := @try(c.emit_relocatable_constant(inner[], inner_value&)) return;
            c.baked.lookup&.insert(addr, id);
            out.push((AddrOf = id));
        });
        (fn Struct(f) => {
            // Just do all the fields.
            each f.fields { f | 
                info := c.get_info(f.ty);
                @assert_eq(info.stride_bytes.zext().mod(8), 0, "TODO: non-8-aligned fields");
                v := value.bytes().slice(f.byte_offset, f.byte_offset + info.stride_bytes.zext());
                v := v.to_value(); // TODO: do i have to clone? 
                @try(c.emit_relocatable_constant_body(f.ty, v&, out, false)) return;
            };
        });
        (fn Tagged(f) => {
            tag := i64.unchecked_cast(value)[]; // just read the tag
            assert(f.cases.len >= tag, "invalid constant tagged union");
            (name, inner) := f.cases.items()[tag];
            case_info := c.get_info(inner);
            out.push((Num = (tag, .I64)));
            payload_size: i64 = case_info.stride_bytes.zext();
            if payload_size > 0 {|
                b := value.bytes();
                tag_size := 8;
                payload := b.slice(tag_size, tag_size + payload_size).to_value();
                
                //for payload&.bytes() { b |
                //    print(@as(i64) b.zext());
                //    print(", ");
                //};
                //println("]");
                
                @try(c.emit_relocatable_constant_body(inner, payload&, out, false)) return;
            };
        });
        (fn Enum(f) => {
            return(c.emit_relocatable_constant_body(f.raw, value, out, force_default_handling));
        });
        (fn Named(f) => {
            return(c.emit_relocatable_constant_body(f._0, value, out, force_default_handling));
        });
        (fn VoidPtr() => {
            ptr := i64.assume_cast(value)[];
            if ptr == 0 {|
                // You're allowed to have a constant null pointer (like for global allocator interface instances).
                out.push((Num = (0, .P64)));
                return(.Ok);
            };
            
            if c.baked.functions&.get(ptr) { fid |
                // Maybe you wanted to store a bunch of type erased functions (like i have to for export_ffi). 
                out.push((FnPtr = fid));
                return(.Ok);
            };
            
            if c.baked.lookup&.get(ptr) { v | 
                // I'm not sure why you'd get here but I guess its fine.
                out.push((AddrOf = v));
                return(.Ok);
            };
            
            msg :: "You can't have a void pointer as a constant. The compiler can't tell how many bytes to put in the final executable.";
            return (Err = (span = c.last_loc, msg = msg));
        });
        @default => {
            return (Err = (span = c.last_loc, msg = "ICE: bad constant"));
        };
    };
    .Ok
}

fn Res($T: Type) Type = Result(T, ParseErr);

fn jit_addr(v: *Values) i64 = {
    s := v.bytes();
    u8.int_from_ptr(s.ptr)
}

fn bytes(self: *Values) Slice(u8) = {
    @match(self) {
        (fn Small(v) Slice(u8) => (ptr = ptr_cast_unchecked(i64, u8, v._0&), len = v._1.zext()));
        (fn Big(v) Slice(u8) => v.items());
    }
}

fn len(self: *Values) i64 = {
    @match(self) {
        (fn Small(v) i64 => { v._1.zext() });
        (fn Big(v) i64 => { v.len });
    }
}

// fn from_values
fn assume_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    assert_eq(b.len, T.size_of());  // TODO: check alignment
    ptr_cast_unchecked(u8, T, b.ptr)
}

fn unchecked_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    ptr_cast_unchecked(u8, T, b.ptr)
}

fn get_type(c: CompilerRs, ty: Type) *TypeInfo = {
    {c.vtable.get_type}(c.cast(), ty)
}

fn get_info(c: CompilerRs, ty: Type) TypeMeta = {
    {c.vtable.get_type_meta}(c.cast(), ty)
}

fn to_value(bytes: Slice(u8)) Values = {
    ::if(Values);
    if bytes.len <= 8 {|
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or((@as(i64) b.zext()).shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {|
        (Big = (cap = bytes.len, ptr = bytes.ptr, len = bytes.len))
    }
}

fn to_value(bytes: Slice(u8), a: Alloc) Values = {
    ::if(Values);
    if bytes.len > 8 {|
        new := a.alloc(u8, bytes.len);
        new.copy_from(bytes);
    };
    bytes.to_value()
}

fn log_type(c: CompilerRs, ty: Type) Str = {
    {c.vtable.log_type}(c.cast(), ty)
}

// Mote: this is weak! often you'd rather use immediate_eval_expr.
fn as_const(self: *FatExpr) ?Values = {
    @if_let(self.expr&)
        fn Value(f) => { return(Some = f.bytes); };  // TODO: this should work when the branch returns never (no extra { ..; }) -- Jul 8
    
    .None
}

////////////////////////
/// Splitting Values /// 

// TODO: :const_field_fix
// TODO: if you move the ffi dynamic abi call to this language you can remove the rust version of this function. 
/// Take some opaque bytes and split them into ints. So (u8, u8) becomes a vec of two i64 but u16 becomes just one.
fn deconstruct_values(program: CompilerRs, ty: Type, bytes: *ReadBytes, out: *List(i64), offsets: ?*List(Ty(Prim, u16))) PRes = {
    info := program.get_info(ty);

    @debug_assert_eq(bytes.i.mod(info.align_bytes.zext()), 0);
    found_len := bytes.bytes.len() - bytes.i;
    @err_assert(
        found_len >= info.stride_bytes.zext(),
        "deconstruct_values of % wants % bytes but found %",
        program.log_type(ty), info.stride_bytes, found_len, // TODO: show the value
    ) return;
    
    single :: fn($T: Type, prim: Prim) void => {
        offset := bytes.i;
        if offsets { offsets |
            offsets.push(@as(Ty(Prim, u16)) (prim, bytes.i.trunc()));
        };
        raw := bytes.read_next(T);
        out.push(@as(i64) raw.int());
    };
    
    @match(program.get_type(ty)) {
        fn Placeholder() => { return(@error("ICE: Unfinished type")); }; // TODO: show the type
        fn Never()       => return(@error("invalid type: Never"));
        fn F64()     => i64.single(.F64);
        fn FnPtr(_)  => i64.single(.P64);
        fn Ptr(_)    => i64.single(.P64);
        fn VoidPtr() => i64.single(.P64);
        fn F32()     => u32.single(.F32);
        fn Fn(_)     => u32.single(.I32);
        fn Label(_)  => u32.single(.I32);
        fn void()    => return(.Ok);
        fn Bool()    => u8.single(.I8);
        fn Tagged()  => return(@error("tagged %", program.log_type(ty)));
        fn Enum(f)   => return(deconstruct_values(program, f.raw, bytes, out, offsets));
        fn Named(f)  => return(deconstruct_values(program, f._0, bytes, out, offsets)); // TODO: make Named payload not a tuple or do destructuring here. -- Jul 8
        fn Int(_)    => {
            @switch(program.get_info(ty).stride_bytes()) {
                @case(1) => u8.single(.I8);
                @case(2) => u16.single(.I16);
                @case(4) => u32.single(.I32);
                @case(8) => i64.single(.I64);
                @default fn(n: u16) => return(@error("ICE: bad int stride %", n));
            };
        }
        fn Array(f) => {
            inner_align := program.get_info(f.inner).align_bytes();
            range(0, f.len.zext()) { _ |
                @debug_assert_eq(bytes.i.mod(inner_align.zext()), 0);
                @try(deconstruct_values(program, f.inner, bytes, out, offsets)) return;
            };
        }
        fn Struct(f) => {
            @err_assert(f.layout_done, "ICE: layout not ready") return;
            prev := 0;
            size := program.get_info(ty).stride_bytes();
            start := bytes.i;
            for f.fields { t |
                continue :: local_return;
                if t.kind == .Const {|
                    continue();
                };
                assert(prev <= t.byte_offset, "ICE: backwards field offset");
                bytes.i = t.byte_offset;
                @try(deconstruct_values(program, t.ty, bytes, out, offsets)) return;
                prev = t.byte_offset;
            };
            bytes.i = start + size.zext(); // eat trailing stride padding
        }
    };
    .Ok
}

ReadBytes :: @struct(bytes: [] u8, i: i64);

fn read_next(self: *ReadBytes, $T: Type) T #generic = {
    @debug_assert_eq(self.i.mod(T.size_of()), 0);
    @safety(.Bounds) self.i + T.size_of() - 1 < self.bytes.len();
    
    ptr := self.bytes.ptr.offset(self.i);
    self.i += T.size_of();
    ptr_cast_unchecked(u8, T, ptr)[]
}

////////////////////
/// Dynamic Call ///

fn call_dynamic_values(program: CompilerRs, f_ptr: i64, f_ty: *FnType, args_value: []u8, comp_ctx: bool) Res(Values) #compiler = {
    parts: List(i64) = list(temp());
    reader: ReadBytes = (bytes = args_value, i = 0);
    @try(program.deconstruct_values(f_ty.arg, reader&, parts&, .None)) return;
    parts := parts.rs();
    program.call_dynamic(f_ptr, f_ty, parts&, comp_ctx)
}

// TODO: this is the only place I still use float_mask.
//       i could get rid of it if i switch to using PrimSig.
//       then the next step is to lazily jit shims so i don't have to implement the calling convention twice. 
// Note: FnType by ptr because i think i do (u32, u32) calling convention wrong. 
fn call_dynamic(program: CompilerRs, ptr: i64, f_ty: *FnType, args: *RsVec(i64), comp_ctx: bool) Res(Values) #compiler = {
    @if(TRACE_CALLS) @println("[call dynamic at addr=%]", ptr);
    
    arg := program.get_info(f_ty.arg);
    ret := program.get_info(f_ty.ret);
    F :: @FnPtr(f: i64, a: *i64) i64;
    ::if(F);
    bounce: F = if arg.float_mask == 0 && ret.float_mask == 0 {|
        arg8ret1
    } else {|
        // TODO: assert: arg.float_mask.count_ones() == arg.size_slots as u32 && ret.float_mask.count_ones() == ret.size_slots as u32
        //       return(@error("ICE: i dont do mixed int/float registers but backend does"))
        // :FUCKED, but I'm going to replace this with using the normal jit once i port that. 
        @err_assert(ret.size_slots <= 1, "TODO: float call with multiple returns at comptime") return;
        arg8ret1_all_floats
    };

    if comp_ctx {|
        c := (**SelfHosted).int_from_ptr(program);
        args.insert(0, c, temp());
    };
    :: if(?RsVec(u8));
    indirect_ret: ?RsVec(u8) = if ret.size_slots > 2 {|
        mem: List(u8) = 0.repeated(@as(i64) ret.stride_bytes.zext(), program.get_alloc());
        mem: RsVec(u8) = mem.rs();
        ret_addr := u8.int_from_ptr(mem.ptr);
        args.insert(0, ret_addr, temp());
        @assert_eq(arg.float_mask, 0);
        (Some = mem)
    } else {|
        .None
    };
    @err_assert(args.len() <= 8, "too many args for comptime call") return;
    // not doing this is ub but like... meh.
    // if something interesting happens to be after the args and you call with the wrong signature you could read it.
    // its fine to read garbage memory and put it in the registers you know callee will ignore (if types are right).
    // However, now that unit is zero sized, the vec may have no allocation, so you get a segfault trying to load from it.
    READ_GARBAGE :: false;
    if args.is_empty().or(!READ_GARBAGE) {|
        a := args[].assume_owned(temp());
        a&.reserve(8 - args.len());
        args[] = a.rs();
        //while args.len() < 8 {
        //    args.push(0);
        //}
    };

    // since we're going to be reading 8 things regardless, it has to be at least vaguely a real pointer.  
    if args.cap == 0 {|
        // TEMP TEMP TEMP. relies on rust leaking everything!!!!! :FUCKED
        // no reason im just being lazy
        mega_hack := 0;
        args.ptr = mega_hack&;
    };
    
    :: if(Res(Values));
    if ret.size_slots <= 1 {|
        r := bounce(ptr, args.ptr);
        assert(ret.stride_bytes <= 8, "big ret");
        (Ok = (Small = (r, ret.stride_bytes.trunc())))
    } else {|
        if ret.size_slots == 2 {|
            ::Res(Ty(Prim, Prim));
            (f, s) := program.prim_pair(f_ty.ret).unwrap();
            r := arg8ret2(ptr, args.ptr);
            // TODO: floats!
            if f.eq(.I64).or(f.eq(.P64)) {|
                if s.eq(.I64).or(s.eq(.P64)) {|
                    return(Ok = to_values(program[][], Ty(i64, i64), (r.fst, r.snd)));
                };
                if s.eq(.I8) {|
                    return(Ok = to_values(program[][], Ty(i64, u8), (r.fst, @as(u8) r.snd.trunc())));
                };
            };
            if f.eq(.I32) {|
                if s.eq(.I32) {|
                    return(Ok = to_values(program[][], Ty(u32, u32), (@as(u32) r.fst.trunc(), @as(u32) r.snd.trunc())));
                };
            };
            @error("TODO: unhandled prim pair for comptime call")
        } else {|
            arg8ret_struct(ptr, args.ptr);
            mem := indirect_ret.unwrap();
            (Ok = (Big = mem))
        }
    }
}

to_values :: fn(program: *SelfHosted, $T: Type, v: T) Values #generic = {
    bytes := T.cast_to_bytes(v&);
    program.from_bytes(bytes)
}

fn from_bytes(program: *SelfHosted, bytes: []u8) Values = {
    ::if(Values);
    if bytes.len() <= 8 {|
        v := 0;
        shift := 0;
        for bytes { b |
            v = v.bit_or(b.zext().shift_left(shift));
            shift += 8;
        };
        (Small = (v, bytes.len.trunc()))
    } else {|
       (Big = bytes.clone(program.get_alloc()).rs())
    }
}

// loads 8 words from args into x0-x7 then calls fnptr
arg8ret1 :: fn(fnptr: i64, first_of_eight_args: *i64) i64 #asm #aarch64 #c_call = (
    mov(Bits.X64, x16, x0),  // save callee since we need to use this register for args
    mov(Bits.X64, x17, x1),
    ldr_uo(Bits.X64, x0, x17, 0),
    ldr_uo(Bits.X64, x1, x17, 1),
    ldr_uo(Bits.X64, x2, x17, 2),
    ldr_uo(Bits.X64, x3, x17, 3),
    ldr_uo(Bits.X64, x4, x17, 4),
    ldr_uo(Bits.X64, x5, x17, 5),
    ldr_uo(Bits.X64, x6, x17, 6),
    ldr_uo(Bits.X64, x7, x17, 7),
    br(x16, 0), // tail call
);

IntPair :: @struct(fst: i64, snd: i64);
// TODO: copy-paste of arg8ret1
arg8ret2 :: fn(fnptr: i64, first_of_eight_args: *i64) IntPair #asm #aarch64 #c_call = (
    mov(Bits.X64, x16, x0),  // save callee since we need to use this register for args
    mov(Bits.X64, x17, x1),
    ldr_uo(Bits.X64, x0, x17, 0),
    ldr_uo(Bits.X64, x1, x17, 1),
    ldr_uo(Bits.X64, x2, x17, 2),
    ldr_uo(Bits.X64, x3, x17, 3),
    ldr_uo(Bits.X64, x4, x17, 4),
    ldr_uo(Bits.X64, x5, x17, 5),
    ldr_uo(Bits.X64, x6, x17, 6),
    ldr_uo(Bits.X64, x7, x17, 7),
    br(x16, 0), // tail call
);

arg8ret_struct :: fn(fnptr: i64, first_of_eight_args: *i64) void #asm #aarch64 #c_call = (
    ldr_uo(Bits.X64, x8, x1, 0),  // arm cc has return address in x8 but we want to represent it as first argument. 
    add_im(Bits.X64, x1, x1, 8, 0),
    mov(Bits.X64, x16, x0),  // save callee since we need to use this register for args
    mov(Bits.X64, x17, x1),
    ldr_uo(Bits.X64, x0, x17, 0),
    ldr_uo(Bits.X64, x1, x17, 1),
    ldr_uo(Bits.X64, x2, x17, 2),
    ldr_uo(Bits.X64, x3, x17, 3),
    ldr_uo(Bits.X64, x4, x17, 4),
    ldr_uo(Bits.X64, x5, x17, 5),
    ldr_uo(Bits.X64, x6, x17, 6),
    ldr_uo(Bits.X64, x7, x17, 7),
    br(x16, 0), // tail call
);

// loads 8 words from args into d0-d7 then calls fnptr. the return value in d0 is bit cast to an int in x0 for you.
arg8ret1_all_floats :: fn(fnptr: i64, first_of_eight_args: *i64) i64 #asm #aarch64 #c_call = (
    sub_im(Bits.X64, sp, sp, 16, 0),
    stp_so(.X64, fp, lr, sp, @as(i7) 0),

    f_ldr_uo(Bits.X64, x0, x1, 0),
    f_ldr_uo(Bits.X64, x1, x1, 1),
    f_ldr_uo(Bits.X64, x2, x1, 2),
    f_ldr_uo(Bits.X64, x3, x1, 3),
    f_ldr_uo(Bits.X64, x4, x1, 4),
    f_ldr_uo(Bits.X64, x5, x1, 5),
    f_ldr_uo(Bits.X64, x6, x1, 6),
    f_ldr_uo(Bits.X64, x7, x1, 7),

    br(x0, 1),
    fmov_from(x0, x0),
    ldp_so(.X64, fp, lr, sp, @as(i7) 0),
    add_im(Bits.X64, sp, sp, 16, 0),
    ret(),
);


// TODO: decide if I need to bring back zero_padding. 
//       I used to do it on from_values/to_values because i blindly use the bytes of Values as hashmap keys for deduplicating 
//       functions with the same const arguments. and that gets scary when you can have different byte patterns for the same logical value.
//       but if i really care about that I should do it deeply following pointers too. 
//       for now i don't have any tests that need it so its probably fine for now.   
//       removing it seems not to change the amount of llvm ir emitted for the compiler, so it wasnt doing anything for me yet. 
//       a more robust solution would be to jit the type's overload of hash. 
//       I also use *Field as const args for reflection stuff which is a bit sketchy, but becomes better if i keep all the typeinfos
//       in a bucketarray so they're stable without cloning. thats good for avoiding duplicates if you use typeinfo at runtime too. 
//            -- Jul 13
