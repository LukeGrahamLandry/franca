:: assert(enum_count(FnFlag) < 64, "not enough bits for FnFlag");

fn make_unit_expr(self: *SelfHosted, loc: Span) *FatExpr = {
    self.box(synthetic_ty((Value = (bytes = unit_value, coerced = false)), loc, void))
}

fn index(comp: *SelfHosted, f: FuncId) *Func = {
    comp.functions&.nested_index(f.as_index())
}

fn ptr_ty(self: *SelfHosted, ty: Type) Type = {
    self.intern_type(Ptr = ty)
}

fn unptr_ty(c: *SelfHosted, ty: Type) ?Type = {
    @match(c.get_type(ty)) {
        fn Ptr(inner) => (Some = inner[]);
        @default => .None; // TODO: do i need to raw_type? 
    }
}

// TODO: skip through named and unique as well.
fn ptr_depth(self: *SelfHosted, ptr_ty: Type) i64 = {
    ptr_ty = self.raw_type(ptr_ty);
    d := 0;
    while => self.unptr_ty(ptr_ty) { inner |
        d += 1;
        ptr_ty = self.raw_type(inner);
    };
    d
}

fn func_type(self: *SelfHosted, id: FuncId) ?Type = {
    func := self.get_function(id);
    ty := func.finished_ty().or(=> return(.None));
    (Some = self.intern_type(Fn = ty))
}

fn append_impl(body: *FuncImpl, a: Alloc, new: FuncImpl) void = {
    @match(body) {
        fn Empty() => {
            body[] = new;
        }
        fn Merged(it) => {
            if new&.is(.Merged) {
                it.push_all(new.Merged.items(), a);
            } else {
                it.push(new, a);
            }
        }
        @default => {
            l := a.alloc(FuncImpl, 2);
            l[0] = body[];
            l[1] = new;
            
            body[] = (Merged = (ptr = l.ptr, len = 2, cap = 2));
        };
    }
} 

fn find_or_create_impl(body: *FuncImpl, a: Alloc, $p: FuncImpl.Tag()) *get_variant_type(FuncImpl, p) #generic = {
    if find_impl(body, p) { it |
        return(it);
    };
    
    make :: fn() => {
        new := FuncImpl.zeroed();
        new&.tag_ptr()[] = p;
        new
    };
    
    @match(body) {
        fn Empty() => {
            body[] = make();
            get_variant_ptr(FuncImpl, body, p) // TODO: there's no typecheck if you just return body here :FUCKED
        }
        fn Merged(it) => {
            it.push(make(), a);
            get_variant_ptr(FuncImpl, it[it.len - 1]&, p)
        }
        @default => {
            l := a.alloc(FuncImpl, 2);
            l[0] = body[];
            l[1] = make();
            
            body[] = (Merged = (ptr = l.ptr, len = 2, cap = 2));
            get_variant_ptr(FuncImpl, l[1]&, p)
        };
    }
} 


:: tagged(FuncImpl);
fn jitted_aarch64(self: *FuncImpl) ?[]u32 = {
    if choose_impl(self, @slice(FuncImpl.Tag.JittedAarch64)) { it |
        return(it.JittedAarch64);
    };
    .None
}

fn ptr_type(self: *SelfHosted, inner: Type) Type = {
    self.intern_type(Ptr = inner)
}

// This converts a const type in the compiler's binary to a type id in the target program.
// It can only be done for some core interned types (not unique ones like structs and enums).
fn get_or_create_type(self: *SelfHosted, $T: Type) Type = {
    @if(@run T == UnknownType || T == LabelId || T == FuncId || T == Symbol || T == ScopeId || T == OverloadSet || T == Type) {
        return(T);
    };
    info :: T.get_type_info_ref();
    @inline_match(info) {
        fn Int() => self.intern_type(info[]);
        // Some types have hardcoded ids. 
        // they can also be used directly but doing it through this function might make it less painful to decouple them later.
        fn Bool() => bool;
        fn void() => void;
        fn VoidPtr() => rawptr;
        fn Never() => Never;
        fn F32() => f32;
        fn F64() => f64;
        @default => panic("Tried to convert unsupported type from const to runtime");
    }
}

fn specific(self: ResultType) ?Type #inline = {
    ::if(?Type);
    @match(self) {
        fn Specific(t) => if(t == UnknownType, => .None, => (Some = t));
        @default => .None;
    }
}

fn set(self: *FatExpr, value: Values, ty: Type) void = {
    @debug_assert(!ty.is_unknown());
    self.expr = (Value = (bytes = value, coerced = false));
    self.ty = ty;
    self.done = true;
}

fn synthetic_ty(expr: Expr, loc: Span, ty: Type) FatExpr = {
    (expr = expr, loc = loc, ty = ty, done = false)
}

fn synthetic(expr: Expr, loc: Span) FatExpr = {
    (expr = expr, loc = loc, ty = UnknownType, done = false)
}

fn clone(self: *Values, a: Alloc) Values = {
    @match(self) {
        fn Small(_) => self[];
        fn Big(f) => (Big = f.items().clone(a).as_raw());
    }
}

fn tuple_of(self: *SelfHosted, types: []Type) Type = 
    self.comp().tuple_of(types);

fn error(self: *SelfHosted, arg: CompileError) *CompileError = {
    mem := self.get_alloc().alloc(CompileError, 1);
    mem.ptr[] = arg;
    mem.ptr
}

:: CBox(FatExpr);
:: CBox(List(FatExpr));
:: CBox(CompileError);
fn CBox($T: Type) void = {
    fn box(self: *SelfHosted, e: T) *T = {
        out := self.get_alloc().alloc(T, 1);
        out.ptr[] = e;
        out.ptr
    }
}

fn check(v: FatExpr, ret: FatExpr) FatExpr #macro = @{
    v := @[v];
    // This compiles faster than using @if twice. -- Sep 17
    @match(v&) {
        fn Ok(v) => v[];
        fn Err(e) => @[ret](Err = e[]);
        fn Suspend(s) => @[ret](Suspend = s[]) 
    }
}

fn want(self: *LazyType) ResultType = {
    @match(self) {
        fn Finished(t) => (Specific = t[]);
        fn Returning(t) => (Returning = t[]);
        @default => .None;
    }
}

fn ty(self: *LazyType) ?Type #inline = {
    @match(self) {
        fn Finished(t) => (Some = t[]);
        @default => .None;
    }
}

fn want(self: ?Type) ResultType #inline = {
    @match(self) {
        fn Some(t) => (Specific = t);
        fn None() => .None;
    }
}

fn want(self: Type) ResultType #inline = {
    if(self.is_unknown(), => return(.None));
    (Specific = self)
}

fn unwrap(self: *LazyType) Type = {
    // TODO: compiler segfaults if you write this with @match!! -- Jul 9 :FUCKED
    assert(self.is(.Finished), "type not ready");
    self.Finished
}

// TODO: add i128 to the language so i can represent all u64/i64 easily. 
// TODO: cope with sign extension. maybe just switch over sizes and load a casted pointer like the old version did. 
fn int_value(value: *Values, int: IntTypeInfo) Res(i64) = {
    len := value.len();
    if len != 1 && len != 2 && len != 4 && len != 8 {
        return(@err("bad int size %", len));
    };
    @debug_assert(value.is(.Small));
    i := if int.signed {
        // TODO: are negative i8/i16/i32 stored sign extended? 
        value.Small._0
    } else {
        // TODO: cope with u64
        value.Small._0
    };
    min, max := int.range();
    @err_assert(i <= max && i >= min, "tried to coerce but value out of bounds for FROM type!?!?!!?") return;
    (Ok = i)
}

::if(Ty(i64, i64));
fn range(int: IntTypeInfo) Ty(i64, i64) = {
    if int.signed {
        // TODO: UB on i1. 
        max := 1.shift_left(int.bit_count - 1) - 1;
        (-max, max) /// TODO: is this right? 
    } else {
        // TODO: this is wrong. you want max u64, it would be nice if i had i128 so could represent all numbers. 
        max :=  1.shift_left(int.bit_count) - 1;
        // TODO: segfault if i put 2^63 here. :fucked
        // 4611686018427387904 is a number i can put here and not die,
        // but it seems that im relying on this being wrong to pick the right overload somewhere. 
        max := if(int.bit_count == 64, => 4611686018427387904, => max);
        (0, max)
    }
}

fn tuple_types(self: *SelfHosted, ty: Type) ?[]Type = 
    self.comp().tuple_types(ty);

fn arg_types(self: *SelfHosted, ty: Type) []Type = 
    self.comp().arg_types(ty);

// this is only used for FnType.arity right now 
// and thats kinda hacky in the first place so this should probably go away. 
// and we should just be more strict about representing fn foo(a: i64, b: i64) void; vs fn foo(ab: Ty(i64, i64)) void; are those conflicting overloads? 
// -- Nov 18
fn len_arg_types(self: *SelfHosted, ty: Type) i64 = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            if f.is_tuple {
                @debug_assert(f.scope == NOSCOPE, "tuples don't have constant fields.");
                // if that changes, we're not sure what the caller is expecting here. 
                // but its also kinda dumb because function args are tuples and functions can have constant arguments. 
                return(f.fields.len);
            };
            1
        };
        // :hacky_type_compression
        fn Array(it) => 
            if(it.len.zext() > max_homogeneous_tuple, => it.len.zext(), => 1);
        @default => 1;
    }
}

fn remove_named(self: *Pattern, arg_name: Var, a: Alloc) void = {
    start := self.bindings.len();
    self.bindings&.ordered_retain(fn(b) => !(b.name&.is(.Var) && b.name.Var == arg_name));
    @debug_assert_ne(start, self.bindings.len());
    if self.bindings.is_empty() {
        // TODO: make it not annoying to call if_empty_add_unit
        self.bindings&.push((
            name = .None,
            ty = (Finished = void),
            nullable_tag = zeroed(*Annotations),
            default = binding_missing_default(zeroed Span),
            kind = .Var,
        ), a);
    };
}

fn tuple_types_arr(self: *SelfHosted, requested: ResultType, len: i64) Result([]ResultType, *CompileError) = {
    :: List(ResultType);
    (Ok = @match(requested) {
        fn Specific(ty) => {
            ::?[]Type;
            types := or self.tuple_types(ty) {
                return(@err("Type Error: found tuple but requested non-tuple %", self.log_type(ty)))
            };
            out: List(ResultType) = list(types.len, temp());
            for types { ty | 
                out&.push(Specific = ty);
            };
            out.items()
        }
        fn Tuple(types) => {
            types
        }
        @default => ResultType.None.repeated(len, temp()).items();
    })
}

fn unique_const(self: *SelfHosted, name: Symbol) Var #once = {
    // TODO: expose a function for this!
    self.scopes.dup_var(
        kind = .Const,
        name = name,
        id = 0,
        scope = from_index(0),
        block = 0,
    )
}

fn from_index(idx: i64) ScopeId = {
    val: u32 = idx.trunc();
    ptr_cast_unchecked(From = u32, To = ScopeId, ptr = val&)[]
}

:: {
    AutoClone(get_variant_type(Expr, Expr.Tag().Value));
    AutoClone(get_variant_type(Expr, Expr.Tag().Switch));
    AutoClone(get_variant_type(Expr, Expr.Tag().Call));
    AutoClone(get_variant_type(Expr, Expr.Tag().Block));
    AutoClone(get_variant_type(Expr, Expr.Tag().FieldAccess));
    AutoClone(get_variant_type(Expr, Expr.Tag().PrefixMacro));
    AutoClone(get_variant_type(Expr, Expr.Tag().PtrOffset));
    AutoClone(get_variant_type(Expr, Expr.Tag().If));
    AutoClone(get_variant_type(Expr, Expr.Tag().As));
    AutoClone(Expr);
    AutoClone(FatExpr);
    AutoClone(Stmt);
    AutoClone(FatStmt);
    AutoClone(Binding);
    AutoClone(LazyType);
    AutoClone(Values);
    AutoClone(Func);
    AutoClone(Annotation);
    AutoClone(FuncImpl);
    AutoClone(*FatExpr);
    AutoClone(*Func);
    AutoClone(?FatExpr);
    AutoClone(Ty(i64, FatExpr));
    AutoClone(Pattern);
    AutoClone(get_variant_type(Stmt, Stmt.Tag()._DeclNamed));
    AutoClone(get_variant_type(Stmt, Stmt.Tag()._DeclVar));
    AutoClone(get_variant_type(Stmt, Stmt.Tag().DeclVarPattern));
    AutoClone(get_variant_type(Stmt, Stmt.Tag().Set));
    AutoClone(get_variant_type(Expr, Expr.Tag().FrcImport));
    
    fn deep_clone(nullable_tag: **Annotations, a: Alloc) *Annotations = {
        ::ptr_utils(Annotations);
        if(nullable_tag[].is_null(), => return(nullable_tag[]));
        out := a.box(Annotations);
        out[] = init(a, nullable_tag.len);
        out.len = nullable_tag.len;
        enumerate nullable_tag[] { i, tag |
            out[][i] = deep_clone(tag, a);
        };
        out
    }
    fn deep_clone(b: **Binding, a: Alloc) *Binding = {
        out := a.box(Binding);
        out[] = deep_clone(b[], a);
        out
    }
    
    AutoHash(MemoKey, TrivialHasher);
    AutoEq(MemoKey);
    fn hash(h: *TrivialHasher, s: *FuncId) void = {
        i := s[].as_index();
        h.hash(i&);
    }
    fn hash(h: *TrivialHasher, s: *OverloadSet) void = {
        i := s[].as_index();
        h.hash(i&);
    }
    fn eq(a: OverloadSet, b: OverloadSet) bool #redirect(Ty(FuncId, FuncId), bool);
    DerefEq(OverloadSet);
    
    // TODO: this isn't a deep clone but it's probably fine since you won't be specializing a c function? idk -- Apr 13, 2025
    fn deep_clone(self: *PrecompiledIr, a: Alloc) PrecompiledIr = 
        self[];
    
};

MarkNotDone :: @struct();
:: WalkAst(MarkNotDone, void);

fn handle_expr(self: *MarkNotDone, expr: *FatExpr) Result(DoMore, void) #once = {
    expr.done = false;
    (Ok = DoMore.Continue)
}

fn handle_stmt(self: *MarkNotDone, stmt: *FatStmt) Result(DoMore, void) #once = {
    stmt.done = false;
    (Ok = .Continue)
}
fn handle_func(self: *MarkNotDone, func: *Func) Result(DoMore, void) #once = (Ok = .Continue);
fn handle_type(self: *MarkNotDone, ty: *LazyType) Result(DoMore, void) #once = (Ok = .Continue);
fn handle_pattern(self: *MarkNotDone, p: *Pattern) Result(DoMore, void) #once = (Ok = .Continue);

// this currently works but probably only loosly overlaps with being correct because its nap time. 
fn adjust_int_length(value: *Values, int: *IntTypeInfo) void = {
    @debug_assert(value.is(.Small), "ICE: big value for int");
    byte_count := value.Small._1&;
    value := value.Small._0&;
    want_byte_count: u8 = ((int.bit_count + 7) / 8).trunc();
    if want_byte_count * 8 != int.bit_count.trunc() {
        // weird ints are stored dumby currently 
        want_byte_count = 8;
    };
    i := byte_count[];
    while => byte_count[] > want_byte_count {
        byte_count[] -= 1;
        //if int.signed {
        //    debug_assert_eq!(*value & (255 << i), 255);
        //} else {
        //    debug_assert_eq!(*value & (255 << i), 0);
        //}
        i -= 1;
    };
    i := byte_count[];
    while => byte_count[] < want_byte_count {
        byte_count[] += 1;
        i += 1;
        //if int.signed {
        //    debug_assert_eq!(*value & (255 << i), 255);
        //} else {
        //    debug_assert_eq!(*value & (255 << i), 0);
        //}
    };
}

fn is_const(self: *FatExpr) bool = {
    @match(self.expr&) {
        fn Tuple(parts) => {
            each parts { part |
                if(!part.is_const(), => return(false));
            };
            true
        }
        // TODO: a test that breaks without these extra ones or remove it. 
        fn StructLiteralP(b) => {
            each b.bindings { b |
                if b.get_default() { part |
                    if(!part.is_const(), => return(false));
                };
            };
            true
        }
        fn GetVar(n)   => n.kind == .Const;
        fn As(f)       => f.value.is_const();
        fn Cast(value) => value[].is_const();
        @default => @is(self.expr&, .Closure, .Builtin, .ConstEval, .FnPtr, .FromBitLiteral, .ContextualField, .String, .Value);
    }
}

fn get_build_options(c: *SelfHosted) *BuildOptions = 
    BuildOptions.ptr_from_raw(c.env.build_options);

empty_struct_literal :: fn(loc: Span) FatExpr = 
    (expr = (StructLiteralP = (bindings = empty(), loc = loc)), loc = loc, ty = UnknownType, done = false);


// :slow also ugly
// TODO: for function args it would make sense to allow `_` multiple times to discard. 
//       but then i should make that a special identifier you can't actually use, i guess thats what Name.None should be. 
fn require_unique_fields(self: *SelfHosted, pattern: *Pattern) Res(void) = {
    fields := pattern.bindings.items();
    // I use this for function args too and that allows missing name for single arg. 
    if(fields.len <= 1, => return(.Ok));
    
    enumerate fields { i, a |
        name := a.ident().or(=> return(@err("field need name")));
        each fields.slice(i + 1, fields.len) { b |
            if name == b.ident().or(=> return(@err("field need name"))) {
                return(@err("Pattern used name '%' twice", self.pool.get(name)));
            };
        };
    };
    .Ok
}

fn comp(self: *SelfHosted) CompCtx = 
    self.vtable.with(self.legacy_indirection);

HasTags :: fn(T: Type) bool = has_field(Deref(T), @symbol nullable_tag);

fn tags(b: ~T) []Annotation #where(HasTags) = {
    ::ptr_utils(@type b.nullable_tag[]);
    @if(b.nullable_tag.is_null(), empty(), b.nullable_tag.items())
}

fn get_tag(b: ~T, name: Symbol) ?*Annotation #where(HasTags) = {
    each b.tags() { ann |
        if(ann.name == name, => return(Some = ann));
    };
    .None
}

fn has_tag(b: ~T, name: Symbol) bool #where(HasTags) =
    b.get_tag(name).is_some();
