CompilerDispatch :: @struct(    
    const_var_in_progress: DynamicBitSet, // TODO: just store this with the expression?
    function_in_progress: DynamicBitSet, // TODO: just store this with the expression?
    overloads: BucketArray(OverloadSetData),
    enclosing_function: ?FuncId,  // for adding runtime callees. 
    const_bound_memo: HashMap(MemoKey, FuncId),
    yield_count: i64 = 0,
    return_labels: BucketArray(FuncId), // [LabelId]
    next_task_id: i64 = 0,
    root_tasks: List(*Task),
    lit_fn_count: i64 = 0,
);

Task :: @rec @struct(
    action: Action,
    waiting_for: ?*Task,
    parent: ?*Task,
    id: i64, // TODO: remove
    done: bool = false,
    trace: ?TraceCtx,
);

TaskId :: @struct(id: i64);

ExprAttempt :: @struct(expr: *FatExpr, requested: ResultType);
VarAttempt :: @struct(name: Var, requested: ResultType);
Action :: @rec @tagged(
    CompileBody: FuncId,  // After this, we can generate bytecode.
    Jit: FuncId, // After this, it's safe to call a function pointer. 
    EvalConstant: VarAttempt,  // find_const. 
    ResolveOverload: OverloadAttempt,
    FinishRecType: @struct(hole: Type, name: Var, value: *FatExpr),
    // Used for function callees, you can't call a function until ~all~ are ready.
    All: List(*Task),
);

CountedTask :: @struct(task: *Task, steps: i64);

OverloadAttempt :: @struct(
    os: OverloadSet, 
    call: ExprAttempt, 
    callsite: ?FuncId,
    last_ready_count: i64,
    options: List(FuncId),
    state: i64 = 0,
    needed_coerce: bool = false,
    arg_index: i64 = 0,
);

fn init(a: Alloc) CompilerDispatch = {
    temp := a.alloc(List(ExprAttempt), 1);
    temp.ptr[] = list(a);
    (
        const_var_in_progress = empty(a), 
        function_in_progress = empty(a), 
        overloads = init(10, a),
        enclosing_function = .None,
        const_bound_memo = init(a),
        return_labels = init(10, a),
        root_tasks = list(10, a),
    )
}

poll_in_place :: fn(self: *SelfHosted, $T: Type, $f: @Fn() Maybe(T)) Res(T) #generic = {
    loop {
        value := f();
        @match(value&) {
            fn Ok(it)  => return(Ok = it[]);
            fn Err(it) => return(Err = it[]);
            fn Suspend(new_dep) => {
                t := self.new_task(new_dep[][], .None, .None);
                @try(self.poll_until_finished(t)) return;
            };
        };
    }
};

fn new_task(self: *SelfHosted, action: Action, waiting_for: ?*Task, parent: ?*Task) *Task = {
    new_task := self.get_alloc().alloc(Task, 1);
    new_task := new_task.ptr;
    new_task[] = (action = action, waiting_for = waiting_for, parent = parent, id = self.dispatch.next_task_id, trace = .None);
    self.dispatch.next_task_id += 1;
    new_task
}

fn poll_until_finished(self: *SelfHosted, task: *Task) Result(void, *CompileError) = {
    root_len := self.dispatch.root_tasks.len;
    self.dispatch.root_tasks&.push(task);
    top_task := task;
    the_mark := __temp_alloc.mark();
    @debug_assert(task.parent.is_none(), "cannot poll a task with a parent. recursion bad.");
    out := Result(void, *CompileError).zeroed();
    loop {
        __temp_alloc.reset_retaining_capacity(the_mark);
        if self.poll_loop_body(task&, out&) {
            return(out);
        };
    }
}

// TODO: we never set waiting_for! decide if we want that or remove it. 

// this is a dumb way to factor this but i want to call it from examples/compiler_gui too. 
fn poll_loop_body(self: *SelfHosted, task_in: **Task, out: *Result(void, *CompileError)) bool = {
    task := task_in[];
    continue :: fn() Never => {
        task_in[] = task;
        return(false)
    };
    if task.waiting_for { next | 
        task = next;
        continue();
    };
    
    finished :: fn() void => {
        @debug_assert(task.waiting_for.is_none(), "made itself wait??");
        task.done = true;
        if task.parent { next | 
            task = next;
            continue();
        };
        
        //@debug_assert(top_task.done, "we think we're done but no");  // TOOD: pass in
        self.dispatch.root_tasks&.pop();
        //@debug_assert_eq(root_len, self.dispatch.root_tasks.len, "book keeping?"); // TOOD: pass in
        
        out[] = .Ok;
        return(true);
    };
    
    if task.done {
        finished();
    };
    
    @match(task.action&) {
        fn All(tasks) => {
            while => tasks.len != 0 {
                xx := tasks.items().last();
                next := xx.expect("none so has last")[];
                if next.done {
                    tasks.pop().expect("pop last");
                } else {
                    next.parent = (Some = task); // TODO: make this already be done 
                    task = next;
                    continue();
                };
            };
            finished();
        }
        @default => ();
    };
    
    
    if task.trace.is_none() {
        task.trace = (Some = zone_begin(.Task));
        @if(ENABLE_TRACY) {
            out := u8.list(temp());  // it seems to make a copy so this is fine
            log(task.action&, self, out&);
            out&.push(0);
            ___tracy_emit_zone_name(task.trace.Some, out.items());
        };
    };
    res := self.exec_task(task);
    if !res&.is(.Suspend) {
        zone := task.trace.unwrap();
        zone_end(zone);
        task.trace = .None;
    };
    
    @match(res) {
        fn Err(e) => {
            self.dispatch.root_tasks&.pop();
            //@debug_assert_eq(root_len, self.dispatch.root_tasks.len, "book keeping?");  // TODO: pass in
            out[] = (Err = e);
            return(true);
        }
        fn Ok() => finished();
        fn Suspend(next) => {
            @debug_assert(task.waiting_for.is_none(), "already waiting??");
            t := self.new_task(next[], .None, (Some = task));
            task = t;
            continue();
        }
    };
    unreachable()
}

::tagged(Action);
fn exec_task(self: *SelfHosted, task: *Task) Maybe(void) = {
    if task.waiting_for { dep |  
        // TODO: this should be unreachable now. 
        @debug_assert(!task.done, "task done but still has dependencies");
        return(Suspend = dep.action&);
    };
    if task.done {
        return(.Ok);
    };
    self.dispatch.enclosing_function = .None;
    
    @match(task.action&) {
        fn EvalConstant(v) => {
            @log_event("[TRY TASK] %: %%%", task.action&.tag(), self.pool.get(v.name.name), "%", v.name.id) self;
            name := v.name;
            var  := self.scopes.get_constant(name);
            var  := var.expect("var should always be known");
            @check(self.handle_declare_constant(name, var._1&, var._0&)) return;
            self.dispatch.const_var_in_progress&.unset(name.id.zext());
        }
        fn CompileBody(fid) => {
            fid := fid[];
            @log_event("[TRY TASK] %: % %", task.action&.tag(), fid, self.log_name(fid)) self;
            func := self.get_function(fid);
            if !func.get_flag(.EnsuredCompiled) {
                zone := zone_begin(.SemaFunction); // TODO: defer
                @if(ENABLE_TRACY) {
                    real_name := self.pool.get(func.name);
                    ___tracy_emit_zone_name(zone, real_name);
                };
                res := self.handle_compile_func_body(fid);
                zone_end(zone);
                @check(res) return;
            };
            self.dispatch.function_in_progress&.unset(fid.as_index());
        }
        fn Jit(fid) => {
            fid := fid[];
            @log_event("[TRY TASK] %: % %", task.action&.tag(), fid, self.log_name(fid)) self;
            func := self.get_function(fid);
            //self.codemap.show_error_line(func.loc);
            //@log_event("%", func.log(self)) self;
            
            if !func.get_flag(.EnsuredCompiled) {
                frontend_mutual := self.dispatch.function_in_progress&.get(fid.as_index());
                //@err_assert(!frontend_mutual, "ICE: unhandled mutual recursion!") return;
                self.dispatch.function_in_progress&.set(fid.as_index());
                return(Suspend = self.wait_for(CompileBody = fid));
            };
            
            // when possible, you want to emit_bc for callees first because they might get deduplicated. 
            waiting: List(*Task) = list(self.get_alloc());
            for func.callees { callee |
                @log_event("% calls %", fid.as_index(), callee.as_index()) self;
                callee_func := self.get_function(callee);
                if !callee_func.get_flag(.AsmDone) {
                    if is_mutually_recursive(task, callee) {
                        //@println("mutual recursion %", callee);
                        // TODO: add to mutual_callees and remove from callees. 
                        // TODO: make sure all mutual_callees are compiled somewhere. but rn this only decides mutual if we see that we're already working on it.
                        //       (easy direct cases are added in sema tho)
                    } else {
                        t := self.new_task((Jit = callee), .None, .None);
                        waiting&.push(t);
                    };
                } else {
                    func := self.get_function(callee);
                    // TODO: ...
                    //@debug_assert(func.get_flag(.EnsuredCompiled), "callee should be compiled!");
                };
            };
            if waiting.len != 0 {
                @log_event("sleep on %/% callees", waiting.len, func.callees.len) self;
                return(Suspend = self.wait_for(All = waiting));
            };
            
            arg_ty := @check(self.infer_arguments(fid)) return;
            ret_ty := @check(self.infer_return(fid)) return;
            
            // TODO: assert found? but :AssumeDone
            found := @check(self.fill_in_jit_pointer(fid, func.body&)) return;
            
            fn fill_in_jit_pointer(self: *SelfHosted, fid: FuncId, body: *FuncImpl) Maybe(bool) = {
                func := self.get_function(fid);
                if(func.get_flag(.AsmDone), => return(Ok = true));
                arg_ty := func.finished_arg.expect("fill arg");
                ret_ty := func.finished_ret.expect("fill ret");
                if body.is(.Normal).or(body.is(.Intrinsic)) {
                    @log_event("codegen % %", fid, self.log_name(fid)) self;
                    @err_assert(!arg_ty.is_unknown() && !ret_ty.is_unknown(), "unknown fn types!!") return;
                    @try(self.emit_bc_and_jit(fid)) return;
                    // TODO: not true for rediurects????
                    //@debug_assert(self.bc.jitted&.get_fn(fid).is_some(), "we just emitted but no pointer?? %", func.log(self));
                    
                    @log_event("done codegen % %", fid, self.log_name(fid)) self;
                    if body.is(.Normal).or(body.is(.Intrinsic)) {
                        return(Ok = true);
                    };  // else it became a redirect
                };
                @match(body) {
                    fn Redirect(target) => {
                        ptr := or self.get_fn_old(target[]) {
                            return(Suspend = self.wait_for(Jit = target[]))
                        };
                        self.put_jitted_function(fid, ptr);
                        if self.get_function(target[]).get_flag(.RealAsmDone) {
                            func.set_flag(.RealAsmDone);
                        };
                    };
                    // TODO: do this before too? 
                    fn JittedAarch64(insts) => {
                        if self.env.comptime_arch != .aarch64 {
                            return(Ok = false);
                        };
                        insts: []u8 = (ptr = ptr_cast_unchecked(u32, u8, insts.ptr), len = insts.len * 4);
                        opts := self.get_build_options();
                        addr := {opts.comptime_jit_vptr.asm_bytes}(opts.comptime_jit_data, self.comp(), fid, insts);
                        self.put_jitted_function(fid, addr);
                        func.set_flag(.RealAsmDone);
                    }
                    fn X86AsmBytes(insts) => {
                        if self.env.comptime_arch != .x86_64 {
                            return(Ok = false);
                        };
                        opts := self.get_build_options();
                        addr := {opts.comptime_jit_vptr.asm_bytes}(opts.comptime_jit_data, self.comp(), fid, insts.items());
                        self.put_jitted_function(fid, addr);
                        func.set_flag(.RealAsmDone);
                    }
                    fn Merged(parts) => {
                        ok := false;
                        each parts { option |
                            new := @check(self.fill_in_jit_pointer(fid, option)) return;
                            ok = ok && new;
                        };
                        return(Ok = ok);
                    }
                    // TODO: if they take a function pointer deal with making sure it gets mapped back to the dispatch fid not an impl fid. 
                    fn TargetOsSplit(it) => {
                        if it.os == self.env.comptime_os {
                            ptr := or self.get_fn_old(it.fid) {
                                return(Suspend = self.wait_for(Jit = it.fid))
                            };
                            self.put_jitted_function(fid, ptr);
                        };
                    }
                    fn Bytecode(body) => {
                        body := FnBody.ptr_from_int(body[]);
                        opts := self.get_build_options();
                        assert(THERE_IS_ONLY_ONE_BACKEND, "a");
                        addr := self.do_self_jit(FnBody.raw_from_ptr(body), fid);
                        self.put_jitted_function(fid, addr);
                        func.set_flag(.RealAsmDone);
                        //self.bc.jitted&.bump_dirty();
                    }
                    @default => return(Ok = false);
                };
                (Ok = true)
            };
            
            // created function pointers are always added as mutual_callees to be more chill about loops. 
            for func.mutual_callees { callee |
                @log_event("% mutual calls %", fid.as_index(), callee.as_index()) self;
                callee_func := self.get_function(callee);
                if !callee_func.get_flag(.AsmDone) {
                    t := self.new_task((Jit = callee), .None, .None);
                    waiting&.push(t);
                } else {
                    @debug_assert(callee_func.get_flag(.EnsuredCompiled), "callee should be compiled");
                };
            };
            if waiting.len != 0 {
                @log_event("sleep on %/% mutual callees", waiting.len, func.mutual_callees.len) self;
                return(Suspend = self.wait_for(All = waiting));
            };
            // :AssumeDone
            // TODO: we wont have a callable pointer if you're cross compiling and this function is in an #target_os block.
            //       should make a shim that gives a useful message if you try to call it at comptime by mistake. 
            func.set_flag(.AsmDone);

        }
        fn ResolveOverload(f) => {
            self.dispatch.enclosing_function = f.callsite;  // TODO: make test that fails without this.
            overloads := self.dispatch.overloads&.nested_index(f.os.as_index());
            @log_event("[TRY TASK] %: % %", task.action&.tag(), f.os, self.pool.get(overloads.name)) self;
            @debug_assert(f.call.expr.expr&.is(.Call), "overload resolve must be call");
            call := f.call.expr.expr.Call;
            zone := zone_begin(.SemaOverloads); // TODO: defer
            @if(ENABLE_TRACY) {
                real_name := self.pool.get(overloads.name);
                ___tracy_emit_zone_name(zone, real_name);
            };
            res := self.resolve_in_overload_set_new(call.arg, f);
            zone_end(zone);
            fid := @check(res) return;
            f_value := self.to_values(FuncId, fid);
            //call.f.set(f_value, fid); // THIS TYPE CHECKS! :FUCKED
            call.f.set(f_value, self.get_or_create_type(FuncId));
        }
        fn FinishRecType(it) => {
            @check(self.handle_finish_rec_type(it.hole, it.name, it.value)) return;
        }
        fn All(tasks) => {
            panic("exec all");
        } 
    };
    .Ok
}

fn is_mutually_recursive(task: *Task, callee: FuncId) bool = {
    //while => task.waiting_for.is_some() {
    //    task = task.waiting_for.unwrap();
    //    @if_let(task.action) fn Jit(other) => {
    //        if other == callee {
    //            return(true);
    //        };
    //    };
    //};
    while => task.parent.is_some() {
        task = task.parent.expect("some so unwrap");
        @if_let(task.action) fn Jit(other) => {
            if other == callee {
                return(true);
            };
        };
    };
    false
}

fn Maybe($T: Type) Type = {
    Self :: @tagged(
        Ok: T,
        Err: *CompileError,
        Suspend: *Action,
    );
    
    fn is_err(self: Self) bool = self&.is(.Err);
    fn unwrap(self: Self) T = {
        if self&.is(.Err) {
            @panic("Unwrapped Maybe Err %", self.Err.msg);
        };
        ::tagged(Action);
        @assert(self&.is(.Ok), "unwrapped suspending Maybe %", self.Suspend.tag());
        self.Ok
    }
    
    fn unwrap_err(self: Self) *CompileError = {
        if self&.is(.Err) {
            return(self.Err);
        };
        panic("unwrap_err not err")
    }
    
    ::tagged(Self);
    Self
}

fn wait_for(s: *SelfHosted, t: Action) *Action = {
    @log_event("[YIELD] %", t&.tag()) s;
    s.dispatch.yield_count += 1;
    mem := s.get_alloc().alloc(Action, 1);
    mem.ptr[] = t;
    mem.ptr
}

////////////////////
/// Comptime Jit ///
////////////////////

// TODO: factor out the dispatch table from the aarch64 backend. 
put_jitted_function :: fn(self: *SelfHosted, f: FuncId, addr: rawptr) void = {
    bc := self.todo_get_bc();
    bc.jitted&.extend_blanks(f);
    bc.jitted.dispatch_table[f.as_index()] = addr;
    func := self.get_function(f);
    func.set_flag(.AsmDone);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY {
        self.created_jit_fn_ptr_value(f, addr.int_from_rawptr());
    };
};

fn todo_get_bc(self: *SelfHosted) *BcBackend = {
    assert(THERE_IS_ONLY_ONE_BACKEND, "TODO: abstract more stuff");
    BcBackend.ptr_from_raw(self.get_build_options()[].comptime_jit_data)
}

// If you're about to actually call the pointer, we need to mark the memory as executable.
fn get_fn_callable(self: *SelfHosted, fid: FuncId) ?rawptr = {
    opts := self.get_build_options();
    {opts.comptime_jit_vptr.get_jitted}(opts.comptime_jit_data, self.comp(), fid, true)
}

fn get_fn_old(self: *SelfHosted, f: FuncId) ?rawptr = {
    opts := self.get_build_options();
    {opts.comptime_jit_vptr.get_jitted}(opts.comptime_jit_data, self.comp(), f, false)
}

fn create_jit_shim(self: *SelfHosted, fid: FuncId) Res(void) = {
    opts := self.get_build_options();
    if(!opts.do_jit_shims, => return(.Ok));
    func := self.get_function(fid);
    if func.get_flag(.AlreadyHasShim) || func.get_flag(.RealAsmDone) { 
        return(.Ok);
    };
    
    h    := report_called_uncompiled_or_just_fix_the_problem;
    body := @try({opts.comptime_jit_vptr.jit_shim}(opts.comptime_jit_data, self.comp(), fid, h)) return;
    addr := self.do_self_jit(body, fid);
    self.put_jitted_function(fid, addr);
    @debug_assert(!func.get_flag(.RealAsmDone), "we didn't actually compile it why do we think we did??"); 
    func.set_flag(.AlreadyHasShim);
    
    .Ok
}

fn emit_bc_and_jit(program: *SelfHosted, f: FuncId) PRes #once = {
    opts := program.get_build_options();
    when := ExecStyle.Jit;
    func := program.get_function(f);
    @debug_assert(!func.get_flag(.RealAsmDone), "double compile");
    @debug_assert(program.get_fn_old(f).is_none(), "ICE: not .AsmDone but have pointer");
    
    mark := __temp_alloc.mark();
    if func.get_flag(.LogAst) {
        @println("[#log_ast %] %", f, func.log(program));
        program.codemap.show_error_line(func.loc);
    };
    body := @try({opts.comptime_jit_vptr.emit_bc}(opts.comptime_jit_data, program.comp(), f, when)) return;
    
    // We might have noticed that this function was a duplicate while generating bytecode.
    // So it was Normal before but now its a Redirect. 
    @if_let(func.body&) fn Redirect(target) => {
        if program.get_fn_old(target[]) { old |
            program.put_jitted_function(f, old);
        };
        
        __temp_alloc.reset_retaining_capacity(mark);
        return(.Ok);
    };
    
    zone := zone_begin(.EmitJit); // TODO: defer
    @if(ENABLE_TRACY) {
        real_name := program.pool.get(func.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    
    addr := program.do_self_jit(body, f);
    program.put_jitted_function(f, addr);
    func.set_flag(.RealAsmDone);
    
    zone_end(zone);
    __temp_alloc.reset_retaining_capacity(mark);
    .Ok
}

fn do_self_jit(program: *SelfHosted, body: rawptr, f: FuncId) rawptr = {
    bc := FnBody.ptr_from_raw(body);
    if bc.blocks[0].insts[0]&.is(.Ret0) {
        do_nothing_and_do_it_fast: rawptr : fn() void = ();
        return(do_nothing_and_do_it_fast);
    };
    opts := program.get_build_options();
    addr, forward_calls := or ({opts.comptime_jit_vptr.jit_asm}(opts.comptime_jit_data, program.comp(), f, body, program.env.comptime_arch)) { _ |
        panic("TODO: jit")
    };
    // TODO: this changes behaviour for programs that would have worked anyway.
    //       later functions that try to forward call this will jump directly to the shim instead of using the table. -- Oct 16 
    for forward_calls { f |
        program.create_jit_shim(f);
    };
    addr
}

fn emit_bc(c: *SelfHosted, f: FuncId, when: ExecStyle) Res(rawptr) = {
    if when == .Aot {
        // :bake_relocatable_value
        @try(c.check_for_new_aot_bake_overloads()) return; 
    };
    func := c.get_function(f);
    if func.get_flag(.LogAst) {
        @println("[#log_ast %] %", f, func.log(c));
        c.codemap.show_error_line(func.loc);
    };
    
    zone := zone_begin(.EmitBc); // TODO: defer
    @if(ENABLE_TRACY) {
        real_name := c.pool.get(func.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    
    opts := c.get_build_options();
    res := {opts.comptime_jit_vptr.emit_bc}(opts.comptime_jit_data, c.comp(), f, when);
    
    zone_end(zone);
    res
}
