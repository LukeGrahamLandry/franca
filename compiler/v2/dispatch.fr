CompilerDispatch :: @struct(    
    const_var_in_progress: DynamicBitSet, // TODO: just store this with the expression?
    function_in_progress: DynamicBitSet, // TODO: just store this with the expression?
    overloads: BucketArray(OverloadSetData),
    enclosing_function: ?FuncId,  // for adding runtime callees. 
    const_bound_memo: HashMap(MemoKey, FuncId),
    yield_count: i64 = 0,
    return_labels: BucketArray(FuncId), // [LabelId]
    next_task_id: i64 = 0,
    root_tasks: List(*Task),
    lit_fn_count: i64 = 0,
);

Task :: @rec @struct(
    action: Action,
    waiting_for: ?*Task,
    parent: ?*Task,
    id: i64, // TODO: remove
    done: bool = false,
    trace: ?TraceCtx,
);

TaskId :: @struct(id: i64);

ExprAttempt :: @struct(expr: *FatExpr, requested: ResultType);
VarAttempt :: @struct(name: Var, requested: ResultType);
Action :: @rec @tagged(
    CompileBody: FuncId,  // After this, we can generate bytecode.
    Jit: FuncId, // After this, it's safe to call a function pointer. 
    EvalConstant: VarAttempt,  // find_const. 
    ResolveOverload: OverloadAttempt,
    FinishRecType: @struct(hole: Type, name: Var, value: *FatExpr),
    // Used for function callees, you can't call a function until ~all~ are ready.
    All: List(*Task),
);

CountedTask :: @struct(task: *Task, steps: i64);

OverloadAttempt :: @struct(
    os: OverloadSet, 
    call: ExprAttempt, 
    callsite: ?FuncId,
    last_ready_count: i64,
    options: List(FuncId),
    state: i64 = 0,
    needed_coerce: bool = false,
    arg_index: i64 = 0,
);

fn init(a: Alloc) CompilerDispatch = {
    temp := a.alloc(List(ExprAttempt), 1);
    temp.ptr[] = list(a);
    (
        const_var_in_progress = empty(a), 
        function_in_progress = empty(a), 
        overloads = init(10, a),
        enclosing_function = .None,
        const_bound_memo = init(a),
        return_labels = init(10, a),
        root_tasks = list(10, a),
    )
}

poll_in_place :: fn(self: *SelfHosted, $T: Type, $f: @Fn() Maybe(T)) Res(T) #generic = {
    loop {
        value := f();
        @match(value&) {
            fn Ok(it)  => return(Ok = it[]);
            fn Err(it) => return(Err = it[]);
            fn Suspend(new_dep) => {
                t := self.new_task(new_dep[][], .None, .None);
                @try(self.poll_until_finished(t)) return;
            };
        };
    }
};

fn new_task(self: *SelfHosted, action: Action, waiting_for: ?*Task, parent: ?*Task) *Task = {
    new_task := self.get_alloc().alloc(Task, 1);
    new_task := new_task.ptr;
    new_task[] = (action = action, waiting_for = waiting_for, parent = parent, id = self.dispatch.next_task_id, trace = .None);
    self.dispatch.next_task_id += 1;
    new_task
}

fn poll_until_finished(self: *SelfHosted, task: *Task) Result(void, *CompileError) = {
    root_len := self.dispatch.root_tasks.len;
    self.dispatch.root_tasks&.push(task);
    top_task := task;
    the_mark := mark_temporary_storage();
    @debug_assert(task.parent.is_none(), "cannot poll a task with a parent. recursion bad.");
    out := Result(void, *CompileError).zeroed();
    loop {
        reset_temporary_storage(the_mark);
        if self.poll_loop_body(task&, out&) {
            return(out);
        };
    }
}

// TODO: we never set waiting_for! decide if we want that or remove it. 

// this is a dumb way to factor this but i want to call it from examples/compiler_gui too. 
fn poll_loop_body(self: *SelfHosted, task_in: **Task, out: *Result(void, *CompileError)) bool = {
    task := task_in[];
    continue :: fn() Never => {
        task_in[] = task;
        return(false)
    };
    if task.waiting_for { next | 
        task = next;
        continue();
    };
    
    finished :: fn() void => {
        @debug_assert(task.waiting_for.is_none(), "made itself wait??");
        task.done = true;
        if task.parent { next | 
            task = next;
            continue();
        };
        
        //@debug_assert(top_task.done, "we think we're done but no");  // TOOD: pass in
        self.dispatch.root_tasks&.pop();
        //@debug_assert_eq(root_len, self.dispatch.root_tasks.len, "book keeping?"); // TOOD: pass in
        
        out[] = .Ok;
        return(true);
    };
    
    if task.done {
        finished();
    };
    
    @match(task.action&) {
        fn All(tasks) => {
            while => tasks.len != 0 {
                xx := tasks.items().last();
                next := xx.expect("none so has last")[];
                if next.done {
                    tasks.pop().expect("pop last");
                } else {
                    next.parent = (Some = task); // TODO: make this already be done 
                    task = next;
                    continue();
                };
            };
            finished();
        }
        @default => ();
    };
    
    
    if task.trace.is_none() {
        task.trace = (Some = zone_begin(.Task));
        @if(ENABLE_TRACY) {
            out := u8.list(temp());  // it seems to make a copy so this is fine
            log(task.action&, self, out&);
            out&.push(0);
            ___tracy_emit_zone_name(task.trace.Some, out.items());
        };
    };
    res := self.exec_task(task);
    if !res&.is(.Suspend) {
        zone := task.trace.unwrap();
        zone_end(zone);
        task.trace = .None;
    };
    
    @match(res) {
        fn Err(e) => {
            self.dispatch.root_tasks&.pop();
            //@debug_assert_eq(root_len, self.dispatch.root_tasks.len, "book keeping?");  // TODO: pass in
            out[] = (Err = e);
            return(true);
        }
        fn Ok() => finished();
        fn Suspend(next) => {
            @debug_assert(task.waiting_for.is_none(), "already waiting??");
            t := self.new_task(next[], .None, (Some = task));
            task = t;
            continue();
        }
    };
    unreachable()
}

::tagged(Action);
fn exec_task(self: *SelfHosted, task: *Task) Maybe(void) = {
    if task.waiting_for { dep |  
        // TODO: this should be unreachable now. 
        @debug_assert(!task.done, "task done but still has dependencies");
        return(Suspend = dep.action&);
    };
    if task.done {
        return(.Ok);
    };
    self.dispatch.enclosing_function = .None;
    
    @match(task.action&) {
        fn EvalConstant(v) => {
            @log_event("[TRY TASK] %: %%%", task.action&.tag(), self.pool.get(v.name.name), "%", v.name.id) self;
            name := v.name;
            var  := self.scopes.get_constant(name);
            var  := var.expect("var should always be known");
            @check(self.handle_declare_constant(name, var._1&, var._0&)) return;
            self.dispatch.const_var_in_progress&.unset(name.id.zext());
        }
        fn CompileBody(fid) => {
            fid := fid[];
            @log_event("[TRY TASK] %: % %", task.action&.tag(), fid, self.log_name(fid)) self;
            func := self.get_function(fid);
            if !func.get_flag(.EnsuredCompiled) {
                zone := zone_begin(.SemaFunction); // TODO: defer
                @if(ENABLE_TRACY) {
                    real_name := self.pool.get(func.name);
                    ___tracy_emit_zone_name(zone, real_name);
                };
                res := self.handle_compile_func_body(fid);
                zone_end(zone);
                @check(res) return;
            };
            self.dispatch.function_in_progress&.unset(fid.as_index());
        }
        fn Jit(fid) => {
            fid := fid[];
            @log_event("[TRY TASK] %: % %", task.action&.tag(), fid, self.log_name(fid)) self;
            func := self.get_function(fid);
            //self.codemap.show_error_line(func.loc);
            //@log_event("%", func.log(self)) self;
            
            if !func.get_flag(.EnsuredCompiled) {
                frontend_mutual := self.dispatch.function_in_progress&.get(fid.as_index());
                //@err_assert(!frontend_mutual, "ICE: unhandled mutual recursion!") return;
                self.dispatch.function_in_progress&.set(fid.as_index());
                return(Suspend = self.wait_for(CompileBody = fid));
            };
            
            // when possible, you want to emit_bc for callees first because they might get deduplicated. 
            waiting: List(*Task) = list(self.get_alloc());
            for func.callees { callee |
                @log_event("% calls %", fid.as_index(), callee.as_index()) self;
                callee_func := self.get_function(callee);
                if !callee_func.get_flag(.AsmDone) {
                    if is_mutually_recursive(task, callee) {
                        //@println("mutual recursion %", callee);
                        // TODO: add to mutual_callees and remove from callees. 
                        // TODO: make sure all mutual_callees are compiled somewhere. but rn this only decides mutual if we see that we're already working on it.
                        //       (easy direct cases are added in sema tho)
                    } else {
                        t := self.new_task((Jit = callee), .None, .None);
                        waiting&.push(t);
                    };
                } else {
                    func := self.get_function(callee);
                    @debug_assert(callee_func.get_flag(.EnsuredCompiled) || callee_func.get_flag(.AlreadyHasShim), "direct callee should be compiled. % calls %.", self.log_name(fid), self.log_name(callee));
                };
            };
            if waiting.len != 0 {
                @log_event("sleep on %/% callees", waiting.len, func.callees.len) self;
                return(Suspend = self.wait_for(All = waiting));
            };
            
            arg_ty := @check(self.infer_arguments(fid)) return;
            ret_ty := @check(self.infer_return(fid)) return;
            
            arg_ty := func.finished_arg.expect("fill arg");
            ret_ty := func.finished_ret.expect("fill ret");
            @err_assert(!arg_ty.is_unknown() && !ret_ty.is_unknown(), "unknown fn types!!") return;
            
            //@if(TRACE_SHIMS) @println("dispatch:jit F%: %", fid.as_index(), self.log_name(fid));
            TTT :: FuncImpl.Tag();
            normal_or_intrinsic :: @const_slice(TTT.Normal, TTT.Intrinsic); // ,i T.NewIntrinsic); we don't use emit_ir for comptime yet. 
            
            func := self.get_function(fid);
            if !func.get_flag(.RealAsmDone) {
                shallow_jit_func(self, fid);
            };
           
            // created function pointers are always added as mutual_callees to be more chill about loops. 
            for func.mutual_callees { callee |
                @log_event("% mutual calls %", fid.as_index(), callee.as_index()) self;
                callee_func := self.get_function(callee);
                if !callee_func.get_flag(.AsmDone) {
                    t := self.new_task((Jit = callee), .None, .None);
                    waiting&.push(t);
                } else {
                    // TODO: why does only the x64 version get here with a shim and not EnsuredCompiled?
                    @debug_assert(callee_func.get_flag(.EnsuredCompiled) || callee_func.get_flag(.AlreadyHasShim), "mutual callee / fnptr should be compiled. % calls %", self.log_name(fid), self.log_name(callee));
                };
            };
            if waiting.len != 0 {
                @log_event("sleep on %/% mutual callees", waiting.len, func.mutual_callees.len) self;
                return(Suspend = self.wait_for(All = waiting));
            };
            // :AssumeDone
            // TODO: we wont have a callable pointer if you're cross compiling and this function is in an #target_os block.
            //       should make a shim that gives a useful message if you try to call it at comptime by mistake. 
            func.set_flag(.AsmDone);

        }
        fn ResolveOverload(f) => {
            self.dispatch.enclosing_function = f.callsite;  // TODO: make test that fails without this.
            overloads := self.dispatch.overloads&.nested_index(f.os.as_index());
            @log_event("[TRY TASK] %: % %", task.action&.tag(), f.os, self.pool.get(overloads.name)) self;
            @debug_assert(f.call.expr.expr&.is(.Call), "overload resolve must be call");
            call := f.call.expr.expr.Call;
            zone := zone_begin(.SemaOverloads); // TODO: defer
            @if(ENABLE_TRACY) {
                real_name := self.pool.get(overloads.name);
                ___tracy_emit_zone_name(zone, real_name);
            };
            res := self.resolve_in_overload_set_new(call.arg, f);
            zone_end(zone);
            fid := @check(res) return;
            f_value := self.to_values(FuncId, fid);
            //call.f.set(f_value, fid); // THIS TYPE CHECKS! :FUCKED
            call.f.set(f_value, self.get_or_create_type(FuncId));
        }
        fn FinishRecType(it) => {
            @check(self.handle_finish_rec_type(it.hole, it.name, it.value)) return;
        }
        fn All(tasks) => {
            panic("exec all");
        } 
    };
    .Ok
}

fn is_mutually_recursive(task: *Task, callee: FuncId) bool = {
    //while => task.waiting_for.is_some() {
    //    task = task.waiting_for.unwrap();
    //    @if_let(task.action) fn Jit(other) => {
    //        if other == callee {
    //            return(true);
    //        };
    //    };
    //};
    while => task.parent.is_some() {
        task = task.parent.expect("some so unwrap");
        @if_let(task.action) fn Jit(other) => {
            if other == callee {
                return(true);
            };
        };
    };
    false
}

fn Maybe($T: Type) Type = {
    Self :: @tagged(
        Ok: T,
        Err: *CompileError,
        Suspend: *Action,
    );
    
    fn is_err(self: Self) bool = self&.is(.Err);
    fn unwrap(self: Self) T = {
        if self&.is(.Err) {
            @panic("Unwrapped Maybe Err %", self.Err.msg);
        };
        ::tagged(Action);
        @assert(self&.is(.Ok), "unwrapped suspending Maybe %", self.Suspend.tag());
        self.Ok
    }
    
    fn unwrap_err(self: Self) *CompileError = {
        if self&.is(.Err) {
            return(self.Err);
        };
        panic("unwrap_err not err")
    }
    
    ::tagged(Self);
    Self
}

fn wait_for(s: *SelfHosted, t: Action) *Action = {
    @log_event("[YIELD] %", t&.tag()) s;
    s.dispatch.yield_count += 1;
    mem := s.get_alloc().alloc(Action, 1);
    mem.ptr[] = t;
    mem.ptr
}

////////////////////
/// Comptime Jit ///
////////////////////

put_jitted_import :: fn(self: *SelfHosted, f: FuncId, addr: rawptr) void = {
    func := self.get_function(f);
    if func.get_flag(.TookPointerValue) || ENABLE_TRACY || TRACE_CALLS {
        self.created_jit_fn_ptr_value(f, addr.int_from_rawptr());
    };
};

fn comptime_jit_symbol_id(self: *SelfHosted, fid: FuncId) u32 = {
    func := self.get_function(fid);
    if !func.has_comptime_jit_symbol_id {
        m := self.comptime_codegen.m;
        func.comptime_jit_symbol_id = m.intern(self.comp().fmt_fn_name(fid));
        func.has_comptime_jit_symbol_id = true;
    };
    func.comptime_jit_symbol_id
}

fn create_jit_shim(self: *SelfHosted, fid: FuncId) Res(void) = {
    opts := self.get_build_options();
    if(!opts.do_jit_shims, => return(.Ok));
    func := self.get_function(fid);
    if func.get_flag(.AlreadyHasShim) || func.get_flag(.RealAsmDone) { 
        return(.Ok);
    };
    
    // Need to know types to do ABI stuff.
    res := self.poll_in_place(void) {
        r :: local_return;
        @check(self.infer_arguments(fid)) r;
        @check(self.infer_return(fid)) r;
        .Ok
    };
    @try(res) return;
    
    h := report_called_uncompiled_or_just_fix_the_problem;
    create_jit_shim(self.comp(), self.comptime_codegen, fid, h);
    @debug_assert(!func.get_flag(.RealAsmDone), "we didn't actually compile it why do we think we did??"); 
    func.set_flag(.AlreadyHasShim);
    
    .Ok
}
