// the compiler of theseus 

// TODO: resolve lazily because some things don't care, 
//       like @enum(a, b, c) just treats the expressions as identifiers. 
//       we have to keep the scope info around forever anyway, if it was done here, 
//       we wouldn't traverse the tree twice. but then even more redundant work when cloning for generics? 

CVariadicType :: import("@/lib/variadic.fr").CVariadic;

ResultType :: @rec @tagged(
    Specific: Type,
    Returning: Type,
    Tuple: []ResultType,  // temporary storage! don't hold across yield points. 
    None
);

OverloadSetData :: @struct(
    ready: RawList(FuncId),
    name: Symbol,
    pending: RawList(FuncId),
    inline_cache: Ty(OverloadKey, FuncId),
    table   := zeroed RawHashMap(OverloadKey, FuncId),
);

fn compile_stmt(self: *SelfHosted, stmt: *FatStmt) Maybe(Ty(bool, bool)) #once = {
    if stmt.done {
        return(Ok = (true, false));
    };
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Noop() => (Ok = (true, false));
        fn Eval(expr) => {
            @check(self.compile_expr(expr, .None)) return;
            res := (expr.done, expr.ty.is_never());
            if expr.expr&.is(.Value) && expr.ty == self.get_or_create_type(void) {
                stmt.stmt = .Noop;
            };
            if expr.ty == LabelId || self.get_type(expr.ty).is(.Label) {  // :get_or_create_type
                return(@err("Implicitly discarding a value of type LabelId is illegal.\nYou probably meant to call it: `return;` -> `return();`\nIf you meant to ignore it: `return;` -> `_ := return;`"));
            };
            (Ok = res)
        }
        fn Decl(f) => {
            @assert(f.kind != .Const, "hit unhoisted constant %", self.comp().log(stmt));
            @assert(f.name&.is(.Var), "hit unresolved var %", self.comp().log(stmt));
            @check(self.decl_var(f.name.Var, f.ty&, f.default&)) return;
            (Ok = (f.default.done, false))
        }
        fn Set(f) => {
            if f.place.expr&.is(.GetVar) {
                @check(self.compile_get_var(f.place&, .None, false)) return;
                @check(self.compile_expr(f.value&, f.place.ty.want())) return;
                if !self.ask_can_assign(f.place.ty, f.value.ty) {
                    //return(@err("tried to set % <- % (%)", self.log_type(f.place.ty), self.log_type(f.value.ty), f.value&.log(self)));
                    return(Err = self.error(TypeMismatch = (span = Span.zeroed(), wanted = f.place.ty, found = f.value.ty)));
                };
                return(Ok = (f.value.done, false));
            };
            // TODO: PrefixMacro is sketchy but makes []->.index work.
            if (@is(f.place.expr&, .PrefixMacro, .FieldAccess, .Deref, .Block)) {
                @check(self.compile_place_expr(f.place&, .None, true)) return;
                @check(self.compile_expr(f.value&, f.place.ty.want())) return;
                if !self.ask_can_assign(f.place.ty, f.value.ty) {
                    //return(@err("tried to set % <- % (%)", self.log_type(f.place.ty), self.log_type(f.value.ty), f.value&.log(self)));
                    return(Err = self.error(TypeMismatch = (span = Span.zeroed(), wanted = f.place.ty, found = f.value.ty)));
                };
                // TODO: self.type_check_arg(f.place.ty, f.value.ty, "reassign var")?; /:type_check
                return(Ok = (f.value.done && f.place.done, false));
            };
            @if_let(f.place.expr&) fn GetNamed(n) => {
                return(@err("Tried to assign to undeclared variable: %", self.pool.get(n[])));
            };
            @err("Illegal place expression: %", tag)
        }
        fn DeclVarPattern(f) => self.decl_var_pattern(stmt);
        @default => @panic("TODO: unhandled compile_stmt type % %", stmt.stmt&.tag(), stmt.log(self));
    }
}

fn decl_var_pattern(self: *SelfHosted, stmt: *FatStmt) Maybe(Ty(bool, bool)) #once = {
    @debug_assert(stmt.stmt&.is(.DeclVarPattern));
    bindings, value := (stmt.stmt.DeclVarPattern.binding.bindings&, stmt.stmt.DeclVarPattern.value&);
    
    if bindings.len == 1 {
        b := bindings[0]&;
        if(b.kind == .Const, => return(@err("unreachable? destructure single arg must be closure but const args should already be handled")));
        if b.var() { name |
            // TODO: could reduce the ast node but it doesn't matter. 
            @check(self.decl_var(name, b.ty&, value)) return;
            return(Ok = (value.done, false));
        };
        // match branch that doesn't specify an argument gets here. (because parser puts in fake Binding with name=None).
        // just to be safe, still keep the expression in case it somehow happens in a different situation and has side effects.
        stmt.stmt = (Eval = value[]);
        @check(self.compile_expr(stmt.stmt.Eval&, .None)) return;
        return(Ok = (stmt.stmt.Eval.done, false));
    };
    
    @err_assert(bindings.len == 0 || !value.is_raw_unit(), "tried to destructure void literal to multiple bindings") return;
    
    // So its not a tuple, we don't know how to destructure it (emit_bc does), but we can still type check it. 
    // Even if it was a tuple, all we'd do differently is pass in the ResultTypes seperatly, but now we can just do that for any expression. 
    // If i was inspired this could grow a speical case for tuples like the old one had to avoid allocating the lists,
    // but temp() is cheap so its probably fine.  -- Aug 1
    
    result_types: List(ResultType) = list(bindings.len, temp());
    each bindings { b | 
        if(b.kind == .Const, => return(@err("TODO: destructure with constants")));
        request := ResultType.None; 
        if @check(self.infer_type(b.ty&)) return { known |
            request = (Specific = known);
        };
        result_types&.push(request);
    };
    
    @check(self.compile_expr(value, (Tuple = result_types.items()))) return; 
    types := or self.tuple_types(value.ty) {
        return(@err("destructure expected tuple type"))
    };
    if(types.len != bindings.len, => return(@err("destructuring arity mismatch expected % but found %", bindings.len, types.len)));
    
    enumerate bindings { i, b |
        if b.var() { name | 
            prev := self.scopes.get_var_type(name); 
            self.scopes.put_var_type(name, types[i]);
            self.scopes.get_var_type(name).unwrap()[].took_address = true; // TODO: don't do this
            @debug_assert(prev.is_none() || { t := prev.unwrap(); t.get_type() == types[i] });
        };
        // An inlined closure might have had a polymorphic parameter, but emit_bc expects known types.
        if b.ty&.is(.Infer) {
            b.ty = (Finished = types[i]);
        };
    };
    
    // TODO: :mark_stmt_done 

    (Ok = (value.done, false))
}

::opt_map(Type, Type);
fn compile_place_expr(self: *SelfHosted, place: *FatExpr, requested: ResultType, want_deref: bool) Maybe(void) = {
    loc := place.loc;
    self.last_loc = place.loc;
    @match(place.expr&) {
        fn Block(f) => {
            @check(self.ensure_resolved(f)) return;
            // TODO: the old version didn't have to do this... 
            if f.body.is_empty() && f.ret_label.is_none() {
                place[] = f.result[]; // Not required but makes debugging easier cause there's less stuff.
                return(self.compile_place_expr(place, requested, want_deref));
            };
            return(@err("a block cannot be a place expression (for spite reasons, not technical ones)"));
        }
        fn Cast(arg) => {
            return(self.compile_place_expr(arg[], requested, want_deref));
        }
        fn GetVar(var) => {
            info := self.scopes.get_var_type(var[]) || { 
                return(@err("var must be declared: %", var.log(self)))
            };
            info.took_address = true;
            ptr_ty := self.ptr_type(info.get_type());

            place.done = true;
            place[] = synthetic_ty((Addr = self.box(place[])), loc, ptr_ty);
            place.done = true;
            if want_deref {
                @check(self.deref_one(place)) return;
                place.done = true;
            };
        }
        fn FieldAccess(f) => {
            // TODO: could lookup field and pass down requested
            // Note: compile_expr may have already walked the container because it has to check if its an enum/scope.
            @check(self.compile_place_expr(f.container, .None, false)) return;
            done := f.container.done;
            
            path := list(*Field, temp()); 
            if !self.find_field_use(f.container.ty, f.name, path&) {
                inner, _ := self.deref_depth(f.container.ty);
                return(Err = self.error(InvalidField = (span = Span.zeroed(), container = inner, name = f.name)));
            };
            if path.len > 0 {
                @check(self.deref_one(f.container)) return;  // TODO: dumb
                xxx := path.items();
                each_rev xxx { it |
                    e := self.box(f.container[]);  // TODO: compiler bug if you inline this !!
                    f.container.expr = (FieldAccess = (container = e, name = it.name, no_dot_call = false));
                    f.container.ty = it.ty;
                };
                @check(self.compile_place_expr(f.container, .None, false)) return;
            };

            name := f.name;
            bytes, field_val_ty, depth := @try(self.field_access_get_type(f.container.ty, name)) return;
            @err_assert(depth >= 1, "PlaceExpr of FieldAccess should be ptr") return;
            range(0, depth - 1) { _ |
                @check(self.deref_one(f.container)) return;
            };
            field_ptr_ty := self.ptr_type(field_val_ty);
            @debug_assert(!f.container.expr&.is(.GetVar), "ICE: place expr can't be direct var access");
            e := self.box(f.container[]);
            // HACK 
            // don't want to use Cast because graphics/shaders wants to know the field name after compiling. 
            //if bytes == 0 {
            //    place.expr = (Cast = e);
            //} else {
                place.expr = (PtrOffset = (
                    ptr = e,
                    bytes = bytes,
                    name = name,
                ));
            //};
            place.done = done;
            if want_deref {
                place.ty = field_ptr_ty;
                // Now we have the offset-ed ptr, add back the deref
                @check(self.deref_one(place)) return;
                place.done = done;
            } else {
                place.ty = field_ptr_ty;
            };
        }
        fn Deref(_) => {
            return(self.compile_deref_place(place, requested, want_deref));
        }
        fn GetNamed(n) => return(@err("Undeclared Identifier: %!", self.pool.get(n[])));
        fn PrefixMacro(_) => {
            // TODO: this is sketchy but makes []->.index work.
            //       need to think about how requested/want_deref are handled
            @check(self.compile_expr(place, requested)) return;
            return(self.compile_place_expr(place, requested, want_deref));
        }
        @default => {
            if !place.ty.is_unknown() {
                // TODO: pass in if we're currently trying to access a field so we can give a better error message if its on an int or something?
                return(@err("Place expression of type % expected pointer dereference.\n%", self.log_type(place.ty), place.log(self)));
            };
            return(@err("TODO: other `place=e;` % %", place.log(self), self.log_type(place.ty)));
        };
    };
    .Ok
}

// TODO: require_unique_field_names should check #use as well
// TODO: loops when pointers
fn find_field_use(self: *SelfHosted, base_type: Type, name: Symbol, path: *List(*Field)) bool = {
    type, _ := self.deref_depth(base_type);
    info := self.get_type(type);
    @match(info) {
        fn Tagged(f) => f.cases.contains(fn(it) => it._0 == name);
        fn Struct(f) => {
            each f.fields { it | 
                if it.name == name {
                    return(true);
                }
            };
            each f.fields { it | 
                if it.has_tag(Flag.use.ident()) {
                    if self.find_field_use(it.ty, name, path) {
                        path.push(it);
                        return(true);
                    };
                }
            };
            false
        }
        @default => false;
    }
}

fn compile_deref_place(self: *SelfHosted, place: *FatExpr, requested: ResultType, want_deref: bool) Maybe(void) = {
    @debug_assert(place.expr&.is(.Deref));
    arg := place.expr.Deref;
    // When you see a !deref, treat the expression as a pointer value.
    if arg.expr&.is(.GetVar) && !want_deref && arg.expr.GetVar.kind != .Const {
        @check(self.compile_get_var(arg, requested, true)) return; // :?
        @debug_assert(arg.expr&.is(.Deref), "compile_get_var should make &[] not %", self.comp().log(arg));
    };
    req := requested.specific().map(fn(r) => self.ptr_type(r));
    @check(self.compile_expr(arg, req.want())) return;
    if want_deref {
        place.ty = or self.unptr_ty(arg.ty) {
            return(@err("tried to deref non-pointer"))
        };
        place.done = arg.done;
    } else {
        place[] = arg[];
    };
    .Ok
}

fn deref_depth(self: *SelfHosted, type: Type) Ty(Type, i64) = { 
    depth := 0;    
    inner := self.raw_type(type);
    while => self.unptr_ty(inner) { next_inner | 
        inner = self.raw_type(next_inner);
        depth += 1;
    };
    (inner, depth)
}

// :PlaceExpr
fn field_access_get_type(self: *SelfHosted, container_input_type: Type, name: Symbol) Res(Ty(i64, Type, i64)) #once = {  // (off, field_type, ptr_depth)
    // Pointers never have fields, so the thing behind the pointer, shouldn't be a pointer.
    // This lets you write `self: *Self; self.name` instead of `self: *Self; self[].name`.
    inner, depth := self.deref_depth(container_input_type);
    self.finish_layout(inner);
    
    @match(self.get_type(inner)) {
        fn Struct(f) => each f.fields { f | 
            if(f.name == name, => return(Ok = (f.byte_offset, f.ty, depth)));
        };
        fn Tagged(f) => each f.cases { f |
            if f._0 == name {
                tag_offset := 8;
                return(Ok = (tag_offset, f._1, depth));
            };
        };
        @default => ();
    };
    (Err = self.error(InvalidField = (span = Span.zeroed(), container = inner, name = name)))
}

// TODO: have a different version of @check for something that can error but not yield? 
fn deref_one(self: *SelfHosted, ptr: *FatExpr) Maybe(void) = {
    raw   := self.raw_type(ptr.ty);  // TODO: why are we going through enums...? -- Jul 30
    inner := or self.unptr_ty(raw) {
        return(@err("expected ptr for deref_one"))
    };
    
    @match(ptr.expr&) {
        fn Addr(arg) => {
            // this allows auto deref to work on let ptr vars.
            if arg.expr&.is(.GetVar) {
                if self.scopes.get_var_type(arg.expr.GetVar) { it |
                    @debug_assert(self.scopes.get_var_data(arg.expr.GetVar)[].took_address, "deref but not took_address");
                } else {
                    @debug_assert(false, "deref_one missing variable?");
                };
                // raw var expr is not allowed, we always refer to variables through thier address.
                ptr[] = synthetic_ty((Deref = self.box(ptr[])), ptr.loc, inner);
            } else {
                // Avoid reduntant (whatever)&[].
                ptr[] = arg[][];
                if ptr.ty.is_unknown() {
                    ptr.ty = inner; // TODO: this shouldn't happen
                };
            };
        }
        @default => {
            ptr[] = synthetic_ty((Deref = self.box(ptr[])), ptr.loc, inner);
        };
    };
    .Ok
}

fn decl_var(self: *SelfHosted, name: Var, ty: *LazyType, value: *FatExpr) Maybe(void) = {
    // TODO: this would make sense but i don't actually need it i guess since i store variable types globally.
    //@debug_assert(self.dispatch.enclosing_function.is_some(), "ICE: runtime vars must have an enclosing function");
    no_type := ty.is(.Infer);
    self.last_loc = value.loc;
    want := ResultType.None;
    xxx := @check(self.infer_type(ty)) return;
    if xxx { known |
        want = (Specific = known);
    };
    @check(self.compile_expr(value, want)) return;
    final_ty := value.ty;
    @debug_assert(!final_ty.is_unknown(), "if we didn't yield, we should know the type %", self.comp().log(value));
    if no_type {
        // Since there was no type annotation, we don't need a type check. Whatever we got is the type of this variable now. 
        ty[] = (Finished = final_ty);
        self.finish_layout_deep(final_ty);
    } else {
        expected_ty := ty.unwrap();
        // TODO: :delay_layout
        //       Instead of doing this now, add it as an Action in the dispatch loop.
        //       We don't actually care about the field offsets yet, we just need them later for emitting bytecode. 
        //       This was one of the changes that inspired the sema rewrite.  -- Jul 25
        self.finish_layout_deep(expected_ty);
        self.finish_layout_deep(final_ty);
        @check(self.can_assign(expected_ty, final_ty)) return;
        // TODO: panic("TODO: self.type_check_arg(value, ty, \"var decl\")?;");
    };

    prev := self.scopes.get_var_type(name); 
    self.scopes.put_var_type(name, final_ty); // TODO: this returns is_new, i want to just assert that. we shouldn't be compiling more than once -- Jul 25
    // TODO: prev should always be none?? but its not a constant and seems to always be the same so its probablby not a super huge deal? -- Apr 23
    //       maybe its just cause im not zeroing the stmt and end up compiling multiple times. -- Apr 25
    @debug_assert(prev.is_none() || { t := prev.unwrap(); t.get_type() == final_ty });
    @err_assert(name.name != Flag.return.ident(), "naming a variable 'return' is not the best idea") return;
    @err_assert(name.name != Flag.local_return.ident(), "naming a variable 'local_return' is not the best idea") return;
    // TODO: :mark_stmt_done
    //       this function is called multiple times (if the containing block needs to yield). need to have a done flag on the fatstmt
    //       but thats easier to change once all the stuff is written in one language -- Jul 25
    .Ok
}

fn ensure_resolved_sign(self: *SelfHosted, fid: FuncId) Maybe(void) #inline = {
    func := self.get_function(fid);
    if(func.get_flag(.ResolvedSign), => return(.Ok));
    inners :: fn(self: *SelfHosted, func: *Func) Maybe(void) = {
        @try(self.resolve_sign(func)) return;
        @try(self.require_unique_fields(func.arg&)) return;
        func.set_flag(.ResolvedSign);
        .Ok
    };
    inners(self, func)
}

fn ensure_resolved_body(self: *SelfHosted, fid: FuncId) Res(void) #inline = {
    func := self.get_function(fid);
    if(func.get_flag(.ResolvedBody), => return(.Ok));
    innerb :: fn(self: *SelfHosted, func: *Func) Res(void) = {
        @try(self.resolve_sign(func)) return;
        @try(self.require_unique_fields(func.arg&)) return;
        func.set_flag(.ResolvedSign);
        @try(self.resolve_body(func)) return;
        func.set_flag(.ResolvedBody);
        .Ok
    };
    innerb(self, func)
}

fn handle_compile_func_body(self: *SelfHosted, fid: FuncId) Maybe(void) #once = {
    func := self.get_function(fid);
    if(func.get_flag(.EnsuredCompiled), => return(.Ok));
    assert(!func.get_flag(.MayHaveAquiredCaptures), "closures need to be specialized");
    assert(!func.get_flag(.AnyConstArgs), "const args need to be specialized");
    
    @debug_assert(self.dispatch.function_in_progress&.get(fid.as_index()), "expected to do work on in progress function %", fid);
    @try(self.ensure_resolved_body(fid)) return;
    
    // Before we can do the body, we really need to know the argument types, 
    // and it would be nice to know the return type too but that's less important. 
    @check(self.infer_arguments(fid)) return;
    
    // you need to only do this once so args are unique but we might yield below. 
    // you can't yield in this block! MadeVarsForRuntimeArgs needs to be atomic!
    // TODO: alternativly, if i trusted myself, you could just say its fine when the arg is already there, 
    //       because surely we put it there ourselves last time around. 
    //       but for now i think this is a valuable sanity check that renumbering went well.  -- Jul 30
    if !func.get_flag(.MadeVarsForRuntimeArgs) {
        each func.arg.bindings { b | 
            // TODO: probably want to change this so you can do as much compiling as possible before expanding templates.
            @debug_assert(b.kind != .Const, "ICE: Tried to emit before binding const args.");
            @if_let(b.name) fn Var(name) => {
                @debug_assert(b.kind == name.kind);
                is_new := self.scopes.put_var_type(name, b.ty&.unwrap());
                if(!is_new, => return(@err("overwrite arg? %", name&.log(self))));
            };
        };
        func.set_flag(.MadeVarsForRuntimeArgs);
    };
    ok := false;
    
    check_body := true;
    if func.get_flag(.BodyIsSpecial) {
        check_body = @check(self.emit_special_body(fid)) return;
        ok = true;
    };
    
    @if(check_body)
    @if_let(func.body&) fn Normal(expr) => {
        ok = true;
        old_func := self.dispatch.enclosing_function;
        self.dispatch.enclosing_function = (Some = fid);
        
        if !func.ret&.is(.Infer) {
            @check(self.infer_return(fid)) return;
        };
            
        if func.return_var { return_var |
            // TODO: this means you cant early return from non-block functions but the error message will be useless -- Jul 9 
            @if_let(expr.expr&) fn Block(f) => {
                @check(self.ensure_resolved(f)) return;
                if f.ret_label.is_none() {|  // we might have already tried to compile this function. 
                    ret: LabelId = from_index(self.dispatch.return_labels.len);
                    self.dispatch.return_labels&.push(fid);
                    label_ty := self.get_or_create_type(LabelId);
                    if func.finished_ret { ret_ty | 
                        label_ty = self.intern_type(Label = ret_ty);
                    };
                    val := self.to_values(LabelId, ret);
                    @try(self.save_const_values(return_var, val, label_ty, func.loc)) return;
                    f.ret_label = (Some = ret);
                };
            };
        };
        
        @check(self.compile_expr(expr, func.finished_ret.want())) return;
        //@debug_assert(self.dispatch.enclosing_function.is_some() && self.dispatch.enclosing_function.unwrap() == fid, "lost enclosing");
        
        if func.ret&.is(.Infer) {
            func.finished_ret = (Some = expr.ty);
        } else {
            wanted := @check(self.infer_return(fid)) return;
            if expr.is_const() {
                @check(self.coerce_const_expr(expr, (Specific = wanted), false)) return;
            };
            self.last_loc = expr.loc;
            @check(self.can_assign(wanted, expr.ty)) return;
        };
        self.dispatch.enclosing_function = old_func;
    };
    if !ok {
        self.last_loc = func.loc;
        return(@err(
            "non special function '%' must have body not %. (or ICE: double compiled function?)", 
            self.pool.get(func.name),
            func.body&.tag(),
        ));
    };
    
    func.set_flag(.EnsuredCompiled);
    .Ok
}

fn get_runtime_string_type(self: *SelfHosted) Type = {
    inner_u8  := self.get_or_create_type(u8);
    inner_str := self.poll_in_place(Type, => self.create_slice_type(inner_u8, Span.zeroed()));
    inner_str := self.unwrap_report_error(Type, inner_str);
    inner_str
}

fn emit_special_body(self: *SelfHosted, fid: FuncId) Maybe(bool) = {
    @check(self.infer_return(fid)) return;
    func := self.get_function(fid);
    if func.get_flag(.EnsuredCompiled) {
        return(Ok = false);
    };
    good := false;
    fn eval_str(self: *SelfHosted, e: *FatExpr) Maybe(Symbol) = {
        str := self.get_runtime_string_type();
        ir := @check(self.immediate_eval_expr(e, str)) return;
        ir := Str.assume_cast(ir&)[];
        (Ok = self.pool.insert_owned(ir)) 
    }
    
    did_import := false;
    is_weak_import := false;
    find_import :: fn(lib_name: Symbol) void => {
        did_import = true;
        @try(self.create_import_body(func, fid, lib_name, is_weak_import)) return;
    };
    
    check_body := true;
    each func.annotations { tag | 
        @switch(tag.name) {
            @case(Flag.libc.ident()) => {
                done :: local_return;
                good = true;
                func.set_flag(.NoContext);
                
                find_import(Flag.libc.ident());
            };
            @case(Flag.import.ident()) => {
                // :io_driver
                lib_name := @match(tag.non_void_arg()) {
                    fn Some(lib_name) => @check(self.eval_str(lib_name)) return;
                    fn None() => Flag.SYMBOL_ZERO.ident();
                };
                
                find_import(lib_name);
                
                good = true;
            };
            @case(Flag.unsafe_noop_cast.ident()) => {
                good = true;
            };
            @case(Flag.comptime_addr.ident()) => {
                good = true;
                ::as_ref(FatExpr);
                value := @unwrap(tag.non_void_arg(), "#comptime_addr requires arg") return;
                ptr := @check(self.eval(value, i64)) return;
                @err_assert(func.body&.is(.Empty), "#comptime_addr function must not have a body") return;
                // only the comptime field matters here. emit_ir checks that aot code never tries to call something marked .ComptimeOnly
                func.body = (DynamicImport = (comptime = ptr, name = func.name, lib = Flag.comptime_addr.ident(), weak = false));
                ptr := ptr.rawptr_from_int();
                self.put_jitted_import(fid, ptr);
                func.set_flag(.YesContext);
                func.set_flag(.ComptimeOnly);
            };
            @case(Flag.intrinsic.ident()) => {
                good = true;
                value := @unwrap(tag.non_void_arg(), "#intrinsic requires arg") return;
                intrinsic_ty := @unwrap(self.env.intrinsic_type, "used #intrinsic during boot") return;
                @err_assert(func.body&.is(.Empty), "#intrinsic function must not have a body") return;
                op := @check(self.immediate_eval_expr(value, intrinsic_ty)) return;
                op := i64.assume_cast(op&)[];
                func.set_flag(.Intrinsic);
                func.body = (Intrinsic = op);
                if op != INTRINSIC_GET_ENV {
                    func.set_flag(.NoContext);
                } else { 
                    func.set_flag(.YesContext);
                };
            };
            // TODO: im afraid this is slow :( its especially dumb because i do it for a billion int_from_ptr instantiations that do nothing
            @case(Flag.ir.ident()) => {
                good = true;
                value := @unwrap(tag.non_void_arg(), "#ir requires arg") return;
                args  := value.items();
                ::if(Ty(Qbe.O, Qbe.Cls));
                o, k := if args.len != 2 {
                    env := self.comp().get_comptime_env();
                    @err_assert(env.ir_op_cls_types != UnknownType, "expected #ir(.op, .cls) or to have called register_hacky_franca_ir_types()") return;
                    payload := @check(self.immediate_eval_expr(value, env.ir_op_cls_types)) return;
                    Ty(Qbe.O, Qbe.Cls).assume_cast(payload&)[]
                } else {
                    op_name  := @unwrap(args[0]&.ident(), "#ir(X, _) expected symbol") return;
                    cls_name := @unwrap(args[1]&.ident(), "#ir(_, X) expected symbol") return;
                    o := @unwrap(Qbe.O.from_name(self.pool.get(op_name)), "#ir invalid op name") return;
                    k := @unwrap(Qbe.Cls.from_name(self.pool.get(cls_name)), "#ir invalid op name") return;
                    (o, k)
                };
                @err_assert(k.raw() < 4 && o.raw() < ::Qbe.O.enum_count(), "invalid argument in #ir") return;
                func.set_flag(.Intrinsic);
                
                if import("@/backend/opt/fold.fr")'can_fold(o) {
                    func.set_flag(.TryConstantFold);
                };
                @err_assert(func.body&.is(.Empty), "#ir function must not have a body") return;
                func.body = (NewIntrinsic = (ir_op  = @as(i32) o, ir_cls = @as(i32) k));
            };
            @case(Flag.redirect.ident()) => {
                good = true;
                value := @unwrap(tag.non_void_arg(), "#redirect requires arg") return;
                RedirectType :: @struct(arg: Type, ret: Type, os: OverloadSet);
                payload := self.tuple_of(@slice(Type, Type, OverloadSet));
                payload := @check(self.immediate_eval_expr(value, payload)) return;
                payload := RedirectType.assume_cast(payload&)[];
                f_ty: FnType = (arg = payload.arg, ret = payload.ret, unary = self.comp().is_unary(payload.arg));
                target := @check(self.resolve_by_type(payload.os, f_ty, func.loc)) return;
                func.body = (Redirect = target);
                func.callees&.push(target, self.get_alloc());
            };
            @case(Flag.weak.ident()) => {
                @err_assert(!did_import, "#weak must come before #import/#libc, sorry") return;
                is_weak_import = true;
            };
            @default => ();
        };
    };
    @err_assert(good, "Function has no implementation %", func.log(self)) return;
    (Ok = check_body)
}

// adds DynamicImport to func.impl
fn create_import_body(self: *SelfHosted, func: *Func, fid: FuncId, lib_name: Symbol, weak: bool) CRes(void) = {
    @err_assert(func.body&.is(.Empty), "#import function must not have a body") return;
    func.set_flag(.BodyIsSpecial);
    info := self.dyn_import(func.name, lib_name, weak);
    if info.comptime != 0 {
        self.put_jitted_import(fid, rawptr_from_int info.comptime);
    } else {
        @try(self.create_jit_shim(fid)) return;
    };
    func.body = (DynamicImport = info);
    .Ok
}

fn dyn_import(self: *SelfHosted, name: Symbol, lib_name: Symbol, weak: bool) DynamicImport = {
    handle2 := self.comptime_libraries&.get(lib_name);
    is_libc := lib_name == Flag.libc.ident(); 
    name_s := self.pool.get(name);
    {
    lib_name := lib_name;
    while => handle2 { handle |
        if handle.get(name_s) { addr |
            return(name = name, lib = lib_name, comptime = addr.int_from_rawptr(), weak = weak);
        };
        handle2 = .None;
        // :MultiDylibLibc HACK
        if is_libc && self.comptime_codegen.m.goal.os == .linux { 
            next_name := @tfmt("%*", self.pool.get(lib_name));
            if self.pool.lookup&.get(next_name) { s |
                lib_name = s;
                handle2 = self.comptime_libraries&.get(lib_name);
            };
        };
    };
    };
    // :AssumeDones
    // TODO: how to catch if it actually doesn't exist or they typo-ed the #import string.
    (name = name, lib = lib_name, comptime = 0, weak = weak)
}

::tagged(ResultType);
fn compile_expr(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    if expr.done {
        return(.Ok);
    };
    @check(self.compile_expr_inner(expr, requested)) return;
    @if_let(requested) fn Specific(type) => {
        if expr.expr&.is(.Value) {
            if type != expr.ty {
                @check(self.coerce_const_expr(expr, requested, false)) return;
            };
        };
    };
    @debug_assert(!expr.ty.is_unknown(), "not typed");
    .Ok
}

fn compile_expr_inner(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    if expr.done {
        @debug_assert(expr.ty != UnknownType, "ICE: done but unknown type %", self.comp().log(expr));
        return(.Ok);
    };
    todo_dont_lose := self.dispatch.enclosing_function;
    old_loc := self.last_loc;
    self.last_loc = expr.loc;
    
    @match(expr.expr&) {
        fn Poison(placeholder) => {
            msg := "";
            if placeholder[] == .InProgressMacro {
                msg = "if this is while resolving a type definition you could try @rec";
            };
            return(@err("Poison expression %. \n%", placeholder, msg));
        }
        fn Value(f) => {
            if(expr.ty == UnknownType, => return(@err("ICE: Value expression must have known type:\n %", log(f.bytes&, self, UnknownType))));
            if !f.coerced {
                @if_let(requested) fn Specific(ty) => {
                    if expr.ty != ty {
                        @check(self.coerce_const_expr(expr, requested, false)) return;
                    };
                };
            };
            expr.done = true;
        }
        fn Call() => {
            @check(self.compile_call(expr, requested)) return;
        }
        fn Block(f) => {
            @check(self.ensure_resolved(f)) return;  // i think this obviously has to go before hoist_constants
            if !f.hoisted_constants {
                @check(self.hoist_constants(f.body.items())) return;
                f.hoisted_constants = true;
            };
            done := true;
            dead := false;
            all_nop := true;
            each f.body { stmt | 
                if dead { 
                    stmt.stmt = .Noop;
                } else {
                    new_done, new_dead := @check(self.compile_stmt(stmt)) return;
                    dead = new_dead;
                    stmt.done = true; // TODO: you probably want to check if inner expressions are done but this seems fine so far. 
                    done = new_done && done;
                };
                all_nop = all_nop && stmt.stmt&.is(.Noop);
            };
            if dead {
                f.result.expr = .Unreachable;
            };
                
            if requested&.is(.None) && !expr.ty.is_unknown() && !expr.ty.is_never() {
                requested = (Specific = expr.ty);
            };
            @check(self.compile_expr(f.result, requested)) return;
            
            expr.ty = f.result.ty;
            
            // HACK. for when you inline something with an early return that ends in a never. :early_return_fallthrough_never ::block_never_unify_early_return_type
            // TODO: seperate out a `unify` operation and use it everywhere you update expr.ty? 
            //       Switch and If already have this so its starting to feel a bit dumb.  -- Aug 19
            if f.result.ty.is_never() {
                if requested.specific() { ty |
                    expr.ty = ty;
                };
            };
            
            expr.done = f.result.done && done;
            // this should help imm_eval not actually call things that are just a value (tho for now it still jits them -- Sep 8)
            if all_nop && f.ret_label.is_none().or(f.result.expr&.is(.Value)) {
                expr[] = f.result[]; // Not required but makes debugging easier cause there's less stuff.
            };
        }
        fn Tuple(parts) => {
            requested_types := @try(self.tuple_types_arr(requested, parts.len)) return;
            types: List(Type) = list(parts.len, temp()); 
            
            // TODO: handle spread to single element correctly. ie. x: Array(i64, 2) = (0, ..1);
            // if lengths don't match, handle spread syntax or error out
            if requested_types.len != parts.len {
                @check(self.expand_spread(parts, requested_types)) return;
            };
            
            done := true;
            enumerate parts { i, part | 
                if !part.done {
                    @check(self.compile_expr(part, requested_types[i])) return;
                };
                types&.push(part.ty);
                done = done && part.done;
            };
            expr.done = done;
            
            if expr.ty.is_unknown() {
                expr.ty = self.tuple_of(types.items());
                self.finish_layout_deep(expr.ty);
            };  // TODO: else :type_check
            
            // homogeneous tuple literals to coerce to arrays
            @if_let(requested) fn Specific(ty) => if ty != expr.ty {
                @if_let(self.get_type(ty)) fn Array(it) => if parts.len == it.len.zext() {
                    expr.ty = ty;
                    // TODO: do i need to re-typecheck each part or did passing it in as requested handle that?
                };
            };
        }
        fn PrefixMacro() => {
            @check(self.compile_prefix_macro(expr)) return;
            @check(self.compile_expr(expr, requested)) return;
        }
        fn GetVar(var) => return(self.compile_get_var(expr, requested, false));
        fn GetNamed(n) => return(@err("Undeclared Identifier: %.", self.pool.get(n[])));
        fn String(i) => {
            // This cannot be a Value node because Str and CStr are not builtins so the parser is unable to create it.
            // But also auto casting a random comptime known Str to a CStr would be sketchy because you don't know if it was mutable. 
            
            // Auto-cast to null terminated string. 
            @if_let(requested) fn Specific(want) => {
                env := self.env;
                if env.c_str_type != UnknownType && env.c_str_type == want {
                    str := self.to_values(CStr, self.pool.get_c_str(i[]));
                    expr.set(str, env.c_str_type);
                    return(.Ok);
                };
            };
            
            str_type := self.get_runtime_string_type();
            str := self.to_values(Str, self.pool.get(i[]));
            expr.set(str, str_type);
        }
        fn ConstEval(inner) => {
            // :PushConstFnCtx 
            // We need to track that any callees of this expression are not runtime callees!
            old_func := self.dispatch.enclosing_function;
            self.dispatch.enclosing_function = .None;
            @check(self.compile_expr(inner[], requested)) return;
            self.dispatch.enclosing_function = old_func;
            // TODO: its a bit silly that i have to specifiy the type since the first thing it does is compile it
            value := @check(self.immediate_eval_expr(inner[], inner.ty)) return;
            expr.set(value, inner.ty);
        }
        fn Deref(inner) => @check(self.compile_deref_place(expr, requested, true)) return;
        fn Addr(inner) => {
            // Note: the old version had special handling for GetVar here to avoid loops but this seems fine... - Jul 30
            @check(self.compile_place_expr(inner[], requested, false)) return;
            expr[] = inner[][];
        }
        fn PtrOffset(f) => {
            expr.done = f.ptr.done;
        }
        fn GetParsed(index) => {
            eprintln("ICE: if you get here i think it means you dropped an error from resolve and we hit it later?");
            e := @try(self.parser.finish_pending(index[])) return;
            return(@err("ICE: GetParsed is should have been handled by scope.fr. %", e.log(self)));
        }
        fn Closure(func) => {
            fid := self.add_function(func[][]);
            self.set(expr, FuncId, fid);
            func := self.get_function(fid);
            @try(self.update_function_metadata(func, .None)) return;
            //if func.ret&.is(.Infer) {
            //    @if_let(func.body&) fn Normal(body) => {
            //        guessed := self.type_of(body);
            //        @if_let(guessed) fn Ok(type) => {
            //            @println("guessed! %", self.log_type(type));
            //            func.ret = (Finished = type);
            //            func.finished_ret = (Some = type);
            //        };
            //    };
            //};
            
            // this avoids `function arguments must have type annotation (cannot be inferred)` on `=` functions with hint. 
            // you want it so you don't have to type `: *List(u8)` on every #x86_bytes inline asm
            // TODO: unify with inference for '=>' functions?
            if !func.get_flag(.AllowRtCapture) {
                unhelpful :: local_return;
                @if_let(requested) fn Specific(ty) => {
                    f_ty := @match(self.get_type(ty)) {
                        fn Fn(f_ty) => f_ty[];
                        fn FnPtr(f) => f.ty;
                        @default => unhelpful();
                    };
                    types := self.arg_types(f_ty.arg);
                    if(func.arg.bindings.len != types.len, => unhelpful());
                    enumerate func.arg.bindings& { i, arg |
                        if arg.ty&.is(.Infer) {
                            arg.ty = (Finished = types[i]);
                        };
                    };
                    if func.ret&.is(.Infer) {
                        func.ret = (Finished = f_ty.ret);
                        func.finished_ret = (Some = f_ty.ret);
                    };
                };
            };
            
            if func.arg.bindings.len == 1 {
                arg := func.arg.bindings.index(0);
            };
            
            if func.ret&.is(.Infer) {
                // TODO: this change is sketch if we're just exploring to resolve an overload  -- Aug 5
                @match(requested) {
                    fn Returning(t) => {
                        func.ret = (Finished = t);
                    }
                    fn Specific(f_ty) =>{
                        @if_let(self.get_type(f_ty)) fn Fn(f_ty) => {
                            func.ret = (Finished = f_ty.ret);
                        };
                    }
                    @default => ();
                }
            };
            if !requested&.is(.None) {
                @check(self.coerce_const_expr(expr, requested, false)) return;
            };
        }
        // :PlaceExpr
        fn FieldAccess(f) => {
            // TODO: this is unfortunate. it means you prewalk instead of letting placeexpr do the recursion
            //       but need to check if its a value that has special fields first.
            @check(self.compile_expr(f.container, .None)) return;
           
            if f.container.ty == ScopeId { // :get_or_create_type 
                scope := @check(self.eval(f.container, ScopeId)) return; 
                var := @unwrap(find_var_in_scope(self, f.name, scope), "'%' not found in scope", self.pool.get(f.name)) return;
                value, ty := @check(self.find_const(var, requested)) return;
                expr.set(value, ty);
            } else {
                if f.container.ty == self.get_or_create_type(Type) {
                    type := @check(self.eval(f.container, Type)) return; 
                    @check(self.contextual_field(f.name, expr, type)) return;
                    // TODO: should typecheck against result type be here or elsewhere?
                } else {
                    // Otherwise its a normal struct/tagged field.
                    @check(self.compile_place_expr(expr, requested, true)) return;
                };
            };
            @debug_assert(!expr.done || expr.ty != UnknownType, "field access has unknown type");
        }
        fn StructLiteralP() => {
            @check(self.construct_struct_literal(expr, requested)) return;
        }
        fn ContextualField(name) => {
            @err_assert(requested&.is(.Specific), "ContextualField % requires type hint", self.pool.get(name[])) return;
            @check(self.contextual_field(name[], expr, requested.Specific)) return;
        }
        fn Uninitialized() => {
            if expr.ty.is_unknown() {
                @err_assert(requested&.is(.Specific), "Uninitialized requires type hint") return;
                expr.ty = requested.Specific;
            };
            expr.done = true;
        }
        fn Quote(arg) => {
            @check(self.compile_quote(expr, requested)) return;
        }
        fn Slice(arg) => {
            @match(arg.expr&) {
                fn Tuple(parts) => {
                    @err_assert(parts.len <= max_homogeneous_tuple, "TODO: @slice on the stack will miscompile if too large. this is dumb") return;
                    fst := parts.index(0);
                    @check(self.compile_expr(fst, .None)) return;
                    if !fst.ty.is_unknown() {
                        hint: ResultType = (Specific = fst.ty);
                        each parts.items().slice(1, parts.len) { e | 
                            @check(self.compile_expr(e, hint)) return;
                        };
                        @check(self.compile_expr(arg[], .None)) return; // TODO: don't recompile. just poke the type in.
                    } else {
                        @check(self.compile_expr(arg[], .None)) return;
                    };
                };
                @default => {
                    @check(self.compile_expr(arg[], .None)) return;
                };
            };
            types := self.arg_types(arg.ty);
            // TODO: typecheck that all are the same!
            expr.ty = @check(self.create_slice_type(types[0], arg.loc)) return;
            expr.done = arg.done;
        }
        fn As(f) => {
            f := f[];
            type := @check(self.eval(f.type, Type)) return;
            @debug_assert(type != UnknownType, "@as to unknowntype");
            @check(self.compile_expr_inner(f.value, type.want())) return;  // Note: skipping stricter check
            @debug_assert(f.value.ty != UnknownType, "@as from unknowntype");
            if f.value.ty != type {
                want := self.get_type(type);
                have := self.get_type(f.value.ty);
                // coerce_const_expr never allows int->enum because that seems too loose to do implicitly, but we want to allow it for @as. 
                if f.value.expr&.is(.Value) && (@is(want, .Int, .F32, .F64)) && !(@is(have, .Enum)) {
                    @check(self.coerce_const_expr(f.value, (Specific = type), false)) return;
                    expr[] = f.value[];
                    return(self.compile_expr(expr, (Specific = type)));
                };
                fn can_as_cast(self: *SelfHosted, ty: Type) bool = {
                    ty := self.get_type(ty);
                    if(ty.is(.Enum) || ty.is(.Named) || ty.is(.VoidPtr), => return(true));
                    if(!ty.is(.Int), => return(false));
                    b := ty.Int.bit_count;
                    b != 8 && b != 16 && b != 32 && b != 64
                };
                // TODO: should do something more strict than this (ie. check enum bounds? especially if it's constant)
                @err_assert(self.can_as_cast(f.value.ty) || self.can_as_cast(type), "invalid @as cast (%) to (%)", self.log_type(f.value.ty), self.log_type(type)) return;
                expr.ty = type;
                expr.expr = (Cast = f.value);
                expr.done = f.value.done;
            } else {
                expr[] = f.value[];
            };
        }
        fn Cast(arg) => {
            @check(self.compile_expr(arg[], .None)) return;
            expr.done = arg.done;
        }
        fn FnPtr(arg) => {
            @check(self.compile_fn_ptr(expr)) return;
        }
        fn If(f) => {
            @check(self.compile_expr(f.cond, self.get_or_create_type(bool).want())) return;

            // TODO: this mostly can't happen anymore because you use the if function and params dont get forwarded like that. 
            //       should allow promoting things to constants. 
            // If its constant, don't even bother emitting the other branch
            if f.cond.expr&.is(.Value) {
                cond := bool.assume_cast(f.cond.expr.Value.bytes&)[];
                ::if(*FatExpr);
                // Now we fully dont emit the branch
                expr[] = if(cond, => f.if_true, => f.if_false)[];
                // need to force the compile again to keep if constant for nested folding.
                return(self.compile_expr(expr, requested));
            };
            
            @check(self.compile_expr(f.if_true, requested)) return;
            true_ty := f.if_true.ty;
            if requested&.is(.None) && !true_ty.is_never() {
                // This is especially helpful for macros that expand to chained ifs (like @switch)
                requested = (Specific = true_ty);
            };
            @check(self.compile_expr(f.if_false, requested)) return;
            if true_ty.is_never() {
                true_ty = f.if_false.ty;
            } else {
                @check(self.can_assign(true_ty, f.if_false.ty)) return;
            };
            expr.ty = true_ty;
            expr.done = f.cond.done && f.if_true.done && f.if_false.done;
        }
        fn Switch(f) => {
            @check(self.compile_expr(f.value, i64.want())) return;  // :get_or_create_type
            
            if f.value.is_const() {
                value := @check(self.immediate_eval_expr(f.value, i64)) return;  // :get_or_create_type
                @debug_assert(value&.is(.Small), "expr::switch on big");
                inspect := value.Small._0;  // i think it's fine even if size is <8, high bits will be zeroed. 
                each f.cases { it |
                    if it._0 == inspect {
                        expr[] = it._1;
                        return(self.compile_expr(expr, requested));
                    };
                };
                expr[] = f.default[];
                return(self.compile_expr(expr, requested));
            };
            
            unify :: fn(expect: *ResultType, new: Type) void = {
                dont_like_this_type := @match(expect) {
                    fn Specific(inner) => inner[].is_never();
                    fn Returning(_) => false;
                    @default => true;
                };
                if dont_like_this_type && !new.is_never() {
                    expect[] = (Specific = new);
                }
            };
            
            expected_type := requested; 
            @check(self.compile_expr(f.default, expected_type)) return;
            unify(expected_type&, f.default.ty);
            // TODO: ensure the value tags are unique. 
            done := f.default.done && f.value.done;
            each f.cases { it |
                @check(self.compile_expr(it._1&, expected_type)) return;
                unify(expected_type&, it._1.ty);
                done = done && it._1.done;
            };
            ty := @match(expected_type) {
                fn Specific(inner) => inner;
                fn None() => Never;
                @default => return(@err("could not unify switch branch types"));
            };
            expr.ty = ty;
            expr.done = done;
        }
        fn Loop(arg) => {
            @check(self.compile_expr(arg[], .None)) return;
            expr.done = arg.done;
            expr.ty = self.get_or_create_type(Never); 
        }
        fn FromBitLiteral(f) => {
            ty := self.intern_type(Int = (bit_count = f.bit_count, signed = false));
            value := self.to_values(i64, f.value);
            @switch(f.bit_count) {
                @case(8) => {
                    value.Small._1 = 1;
                };
                @case(16) => {
                    value.Small._1 = 2;
                };
                @case(32) => {
                    value.Small._1 = 4;
                };
                @default => ();
            };
            expr.set(value, ty);
        }
        fn Unreachable() => {
            expr.done = true;
            expr.ty = self.get_or_create_type(Never); 
        }
        fn UndeclaredVar(it) => {
            if self.find_var_in_scope(it.name, it.scope) { v |
                // TODO: this is bad because if you ever get here it means you were resolved before all the stuff above you was ready,
                //       so you got lucky that you didn't to anything and we can fix it now,
                //       but if someone had put something in a higher scope and you wanted to bind something shadowing it, you might get the wrong thing. 
                
                expr.expr = (GetVar = v);
                @check(self.compile_expr(expr, requested)) return;
            } else {
                return(@err("%", self.comp().log(expr)));
            };
        }
        // TODO: it's bad that this is a totally different codepath from imported functions 
        //       but also functions are way easier to play tricks with since you can put 
        //       shims inside the call transparently. 
        fn DataSymbol(info) => if info.lib != Flag.SYMBOL_ZERO.ident() {
            if self.comptime_libraries&.get(info.lib) { handle |
                if handle.get(self.pool.get(info.name)) { addr |
                    info.comptime = addr.int_from_rawptr();
                };
            };
        }
        fn CVariadic(args) => {
            each args { it |
                @check(self.compile_expr(it, .None)) return;
            };
            expr.done = true;
            expr.ty = CVariadicType;
        };
        fn FrcImport() => {
            @if(DISABLE_IMPORT_FRC) return(@err("TODO: DISABLE_IMPORT_FRC because scoping when you import the compiler is messed up"));
            @check(self.sema_frc_import(expr)) return;
        }
        @default => @panic("TODO: unhandled node type %: %", expr.expr&.tag(), expr.log(self));
    };
    self.last_loc = old_loc;
    self.dispatch.enclosing_function = todo_dont_lose;
    @debug_assert(!expr.ty.is_unknown(), "[compile_expr] Unknown type for %", expr.log(self));
    .Ok
}

fn expand_spread(self: *SelfHosted, parts: *RawList(FatExpr), requested_types: []ResultType) Maybe(void) = {
    want, found := (requested_types.len, parts.len);
    is_spread := parts[parts.len - 1].expr&.is(.Spread);
    @err_assert(is_spread && want > found && parts.len > 0, "Type Error: tuple arity mismatch. expected % but found % (spread=%)", want, found, is_spread) return;
    extra := want - (found - 1);
    spread_value := parts[found - 1].expr.Spread;
    spread_ty := requested_types[want - 1];
    // TODO: better error message if you try to coerce V:() to something that doesn't have all default fields. 
    @check(self.compile_expr(spread_value, spread_ty)) return;
    // TODO: type check that all the new elements are the same type
    parts.len -= 1;  // AFTER the yield
    
    // TODO: only evaluate spread_value once, this would be super easy in emit_ir but making new variables here is painful.  
    a := self.get_alloc();
    parts.reserve(extra, a);
    range(0, extra) { _ |
        parts.push(self.clone(spread_value), a);
    };
    .Ok
}

fn compile_quote(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    arg := expr.expr.Quote;

    unquote_placeholders := @unwrap(self.env.unquote_placeholders, "quote during boot") return;
    
    // TODO: make env.unquote_placeholders a @FnPtr instead and then this goes away (becomes an automatic jit-shim)
    if self.get_fn_callable(unquote_placeholders).is_none() {
        // note: do this before fucking with the expr because it will yield the first time!
        @check(self.infer_arguments(unquote_placeholders)) return;
        @check(self.infer_return(unquote_placeholders)) return;
        return(Suspend = self.wait_for(Jit = unquote_placeholders));
    };
    
    //walk: MarkNotDone = ();
    //walk&.walk_expr(arg); // TODO: might not need this :SLOW
    
    walk: Unquote = (compiler = self, placeholders = list(self.get_alloc()));
    @try(walk&.walk_expr(arg)) return;
    expr_ty := @unwrap(self.env.fat_expr_type, "used quoted ast during bootstrapping") return;
    value := self.to_values(FatExpr, arg[]);
    expr.set(value, expr_ty);
        
    if !walk.placeholders.is_empty() {
        walk.placeholders&.push(expr[]);
        arg: FatExpr = (expr = (Tuple = walk.placeholders.as_raw()), loc = expr.loc, ty = UnknownType, done = false);
        arg := self.box(arg);
        arg: FatExpr = (expr = (Slice = arg), loc = expr.loc, ty = UnknownType, done = false);
        f := self.to_expr(FuncId, unquote_placeholders, expr.loc); 
        f.done = true;
        expr[] = synthetic_ty((Call = (f = self.box(f), arg = self.box(arg))), expr.loc, expr_ty);
        @check(self.compile_expr(expr, requested)) return;
    };
    .Ok
}

fn compile_fn_ptr(self: *SelfHosted, expr: *FatExpr) Maybe(void) = {
    arg := expr.expr.FnPtr&;
    // TODO: pass through better type hint
    fid := @check(self.eval(arg[], FuncId)) return;
    @check(self.infer_arguments(fid)) return;
    @check(self.infer_return(fid)) return;
    func := self.get_function(fid);
    // TODO: typecheck
    //err!("!fn_ptr expected const fn not {}", self.program.log_type(ty));
    
    @err_assert(!func.get_flag(.AnyConstArgs), "cannot take pointer to function with const args") return;
    
    self.took_pointer_value(fid);
    expr.done = true;
    
    // Instead of self.add_callee(fid)
    if self.dispatch.enclosing_function { current_f |
        current := self.get_function(current_f);
        current.mutual_callees&.add_unique(fid, self.get_alloc());
    };
    
    // TODO: for now you just need to not make a mistake with calling convention
    // The backend still needs to do something with this, so just leave it
    ty := @unwrap(func.finished_ty(), "!fnptr expected known type") return;
    ty := self.intern_type(FnPtr = (ty = ty));
    expr.ty = ty;
    .Ok
}

fn construct_struct_literal(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    pattern := expr.expr.StructLiteralP&;
    if(!requested&.is(.Specific), => return(@err("struct literal requires type hint"))); // TODO: I'll probably want this to be a new type of yield eventually. 
    requested := requested.Specific;
    raw_container_type := self.raw_type(requested);
    bindings := pattern.bindings&;
    
    @match(self.get_type(raw_container_type)) {
        fn Struct(f) => {
            if f.is_union {
                @err_assert(bindings.len == 1, "% is a union, value should have one active varient not %", self.log_type(requested), bindings.len) return;
            };
            
            to_remove := list(i64, temp());
            enumerate bindings { i, b | 
                continue :: local_return;
                name := or b.ident() {
                    return(@err("struct literal requires field names"))
                };
                // Note: I'm not reordering them here because i want to define evaluation order to be the order you write them in not the declaration order. 
                //       (because there might be side effects)
                if find_struct_field(f, name, i) { field |
                    // common case: it was just a normal field
                    // can't compile the value yet since there might be #use later that fill poke fields backwards 
                    b.ty = (Finished = field.ty);
                    continue();
                };
                
                // we didn't find the field but it might be a field of an inner #use struct
                path := list(*Field, temp());
                if !self.find_field_use(raw_container_type, name, path&) {
                    return(Err = self.error(InvalidField = (name = name, container = requested, span = expr.loc)))
                };
                top_field := path[0];
                
                // there might be an incomplete inner pattern to add this binding to
                each bindings { it |
                    check_name := it.ident() || return(@err("struct literal requires field names"));
                    if check_name == top_field.name {
                        // some other field in the outer pattern is an incomplete pattern for the #use struct 
                        @try(self.poke_used_binding(it, b)) return;
                        to_remove&.push(i);
                        continue();
                    };
                };
                
                // this is the first time finding something for this #use struct so start a new inner pattern. 
                // it might still be invalid (if it's a #use on a pointer field), but we'll catch that when we try to compile after this whole loop. 
                new := Binding.list(self.get_alloc()); // TODO: reserve the number of fields this struct has? 
                new&.push(b[]);
                b.default.expr = (StructLiteralP = (bindings = new.as_raw(), loc = b.default.loc));
                b.default.ty = UnknownType;
                b.name = (Ident = top_field.name);
                b.ty = (Finished = top_field.ty);
            };
            for_rev to_remove { i |
                bindings.items().unordered_remove(i);
                bindings.len -= 1;
            };
            
            // since we allow incomplete patterns that get filled in later by a #use field, 
            // can't compile any of the values until we've seen them all. 
            // also it's not ok if you suspend before going through to_remove. 
            done := true;
            enumerate bindings { i, b | 
                value := or b.get_default() {
                    return(@err("struct literal requires field value (use '=' not ':')"))
                };
                ty := b.unwrap_ty();
                @check(self.compile_expr(value, (Specific = ty))) return;
                done = done && value.done;
                self.last_loc = value.loc;
                @check(self.can_assign(ty, value.ty)) return;
            };
            
            // If they're missing some, check for default values.
            if !f.is_union && f.fields.len != bindings.len {
                enumerate f.fields { i, field | 
                    continue :: local_return;
                    
                    // did they provide a value for this field? 
                    if(i < bindings.len && bindings.index(i).ident().unwrap() == field.name, => continue());  // :NotForCorrectness
                    if(bindings.contains(fn(b) => b.ident().unwrap() == field.name), => continue());
                    
                    default := field.get_default() || 
                        return(Err = self.error(MissingField = (span = expr.loc, container = requested, name = field.name)));
                    
                    bindings.push(
                        (
                            name = (Ident = field.name),
                            ty = (Finished = field.ty), 
                            nullable_tag = zeroed(*Annotations),
                            default = synthetic_ty((GetVar = default), pattern.loc, UnknownType),
                            kind = .Var,
                        ),
                        self.get_alloc()
                    );
                    // TODO: would be more clear to manually do the find_const->coerce_const_expr here? this way is less code but less explicit. 
                    @check(self.compile_expr(bindings.index(bindings.len - 1)[].default&, (Specific = field.ty))) return;
                };
                @err_assert(f.fields.len == bindings.len, "ICE: struct field count mismatch but we don't know which is missing") return;
            };

            @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
            expr.done = done;
            expr.ty = requested;
            .Ok
        }
        fn Tagged(f) => {
            @err_assert(bindings.len == 1, "% is a @tagged, value should have one active varient not %", self.log_type(requested), bindings.len) return;
            b := bindings.index(0);
            name := or b.ident() {
                return(@err("struct literal requires field names"))
            };
            each f.cases& { f |
                if f._0 == name {
                    ::as_ref(FatExpr);
                    value := @unwrap(b.get_default(), "struct literal needs value") return;
                    @check(self.compile_expr(value, (Specific = f._1))) return;
                    @check(self.can_assign(f._1, value.ty)) return;
                    expr.done = value.done;
                    expr.ty = requested;
                    return(.Ok);
                };
            };
            (Err = self.error(InvalidField = (name = name, container = requested, span = expr.loc)))
        }
        @default => @err("found struct literal but expected % = %", requested, self.log_type(requested));
    }
}

// adding more weird tree manipulation is kinda ass but it's nice to not make emit_ir deal with #use 
// given @struct(foo: @struct(x = 0, 1 = 1) #use); 
// (foo = (x = 0), y = 1)); -> (foo = (x = 0, y = 1));
fn poke_used_binding(self: *SelfHosted, dest: *Binding, src: *Binding) CRes(void) #once = {
    @match(dest.get_default().unwrap()[].expr&) {
        fn StructLiteralP(it) => {
            it.bindings&.push(src[], self.get_alloc());
            .Ok
        }
        @default => @err("trying to assign #use-d field but the value for inner struct is not a pattern"); // incomprehensible
    }
}

fn contextual_field(self: *SelfHosted, name: Symbol, expr_out: *FatExpr, type: Type) Maybe(void) = {
    @match(self.get_type(type)) {
        fn Enum(f) => {
            each f.fields { f |
                if f._0 == name {
                    ff := f._1&;
                    value := ff.clone(self.get_alloc());
                    expr_out.set(value, type);
                    return(.Ok);
                };
            };
        }
        fn Struct(f) => {
            if !(f.scope == NOSCOPE) {
                if find_var_in_scope(self, name, f.scope) { var |
                    value, ty := @check(self.find_const(var, .None)) return;
                    expr_out.set(value, ty);
                    return(.Ok);
                };
            };
        }
        fn Tagged(f) => {
            enumerate f.cases { i, f | 
                if f._0 == name {
                    @err_assert(f._1 == void, 
                        "contextual field % of tagged union must be unit found %",
                        self.pool.get(name), self.log_type(f._1)
                    ) return;
                    
                    // We could create a StructLiteralP and let the backend deal with it, but we know the answer right now. 
                    // Note that this forces an extra case for :tagged_prims_hack
                    info := self.get_info(type);
                    @debug_assert(info.is_sized, "unsized type for tagged contextual field");
                    size: i64 = info.stride_bytes.zext(); // TODO: allow yield on sizing here. 
                    bytes := 0x00.repeated(size, self.get_alloc());
                    assert(i <= 255, "TODO: giant ass @tagged"); 
                    assert(info.align_bytes <= 8, "TODO: handle large alignment");
                    bytes[0] = i.trunc(); // :endian
                    expr_out.set(bytes.items().to_value(), type);
                    expr_out.ty = type;
                    expr_out.done = true;
                    return(.Ok);
                };
            };
        }
        fn Named(f) => return self.contextual_field(name, expr_out, f._0);
        @default => ();
    };
    
    (Err = self.error(InvalidContextualField = (span = expr_out.loc, type = type, name = name)))
}

fn compile_call_overload_set(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    f := expr.expr.Call.f;
    arg := expr.expr.Call.arg;
    os := @check(self.eval(f, OverloadSet)) return;
    data := self.dispatch.overloads&.nested_index(os.as_index());
    if data.ready.len == 1 && data.pending.len == 0 {
        self.set(f, FuncId, data.ready[0]);
    } else {
        // HACK. experiment. 
        // interesting that this doesnt work. i guess it needs help to know the type? 
        //if Flag.if.ident() == data.name {
            //self.codemap.show_error_line(arg.loc);
        //    if arg.expr&.is(.Tuple) && arg.expr.Tuple.len == 3 {
        //        parts := arg.expr.Tuple&;
        //        tt := synthetic_ty((Call = (f = parts.index(1), arg = self.make_unit_expr(expr.loc))), expr.loc, UnknownType);
        //        ff := synthetic_ty((Call = (f = parts.index(2), arg = self.make_unit_expr(expr.loc))), expr.loc, UnknownType);
        //        expr.expr = (If = (cond = parts.index(0), if_true = self.box(tt), if_false = self.box(ff)));
        //        return(self.compile_expr(expr, requested));
        //    };
        //};
        mem := self.recycle_funcid&.pop() || empty();
        return(Suspend = self.wait_for(ResolveOverload = (
            os = os, 
            call = (expr = expr, requested = requested), 
            callsite = self.dispatch.enclosing_function,
            last_ready_count = 0,
            options = (maybe_uninit = mem, len = 0, gpa = self.get_alloc()),
        )));
    };
    .Ok
}

::if(ResultType);
fn compile_call(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    @debug_assert(expr.expr&.is(.Call));
    f := expr.expr.Call.f;
    arg := expr.expr.Call.arg;
    req_fn: ResultType = @match(requested) {
        fn Specific(t) => (Returning = t);
        @default => .None;
    };
    
    if f.expr&.is(.Closure) && f.expr.Closure.get_flag(.AllowRtCapture) {
        f.expr.Closure.set_flag(.Once);
    };
    
    @check(self.compile_expr(f, req_fn)) return;
    
    if f.ty == self.get_or_create_type(OverloadSet) {
        @check(self.compile_call_overload_set(expr, requested)) return;
    };
    
    if f.ty == self.get_or_create_type(FuncId) {|
        return(self.call_direct(expr, requested));
    };
    
    if f.ty == self.get_or_create_type(LabelId) {
        // We're trying to early return but we didn't know the return type yet when the label was created. 
        // You should only get here for .Generic or .Infer. 
        label := @check(self.eval(f, LabelId)) return;
        fid := self.dispatch.return_labels&.nested_index(label.as_index())[];
        func := self.get_function(fid);
        @match(@check(self.infer_type(func.ret&)) return) {
            fn Some(ty) => {
                @check(self.compile_expr(arg, ty.want())) return;
                @check(self.can_assign(ty, arg.ty)) return;
            }  
            fn None() => {
                @check(self.compile_expr(arg, .None)) return;
                // It was .Infer, but we'll save it incase we have multiple returns.
                // TODO: I think this is wrong because you're allowd to have polymorphic closures but I don't clone the func.  
                func.ret = (Finished = arg.ty);
            }
        };
        f.ty = self.intern_type(Label = arg.ty);
        expr.ty = self.get_or_create_type(Never);
        expr.done = arg.done;
        return(.Ok);
    };
    
    @match(self.get_type(f.ty)) {
        fn FnPtr(it) => {
            // Feels like a pretty reasonable invarient: if we have a function pointer, we need to know exactly what its types are. 
            @check(self.compile_expr(arg, (Specific = it.ty.arg))) return;
            @check(self.can_assign(arg.ty, it.ty.arg)) return; // TODO: correct varience
        
            expr.ty = it.ty.ret;
        }
        fn Fn(it) => return(self.call_direct(expr, requested));
        fn Label(it) => {
            @check(self.compile_expr(arg, (Specific = it[]))) return;
            @check(self.can_assign(it[], arg.ty)) return;
            expr.ty = self.get_or_create_type(Never); 
        }
        @default => return(@err("not callable %", f.log(self)));
    };
    expr.done = f.done && arg.done;
    
    .Ok
}

// TODO: check function ret type against requested
// Note: when we get here, we might not know the type of the function or the argument. 
fn call_direct(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    f_expr := expr.expr.Call.f;
    arg_expr := expr.expr.Call.arg;
    @debug_assert(!f_expr.ty.is_unknown(), "call_direct expected fn expr to be compiled");

    fid := @check(self.immediate_eval_expr(f_expr, f_expr.ty)) return;
    fid := FuncId.assume_cast(fid&)[];
    func := self.get_function(fid);
    
    if func.get_flag(.UnboundGenerics) {
        @try(self.resolve_sign(func)) return;
        new_fid := @check(self.eval_where_from_expression(fid, arg_expr)) return; 
        fid  = @unwrap(new_fid, "called a #where function directly (not through an overload set) but the types did not match") return;
        f_expr[] = self.to_expr(FuncId, fid, f_expr.loc);
        func = self.get_function(fid);
        @debug_assert(!func.get_flag(.UnboundGenerics));
    };
    
    arg_ty := UnknownType;
    
    @try(self.adjust_named_arguments(fid, arg_expr)) return;
    
    if func.get_flag(.AnyConstArgs) {
        @if_let(self.get_type(f_expr.ty)) fn Fn(it) => {
            arg_ty = it.arg;
            @check(self.compile_expr(arg_expr, (Specific = it.arg))) return;
        };
    
        @check(self.ensure_resolved_sign(fid)) return;
        
        // This creates a new function with only runtime args, and updates the arg_expr accordingly. 
        zone := zone_begin(.BakeConstArgs);
        res := self.curry_const_args(fid, f_expr, arg_expr, zone);
        zone_end(zone);
        fid = @check(res) return;
        func = self.get_function(fid);
        // Since we may have just made a new function, make sure we know its types,
        // and then sanity check that our types still match. 
        if arg_expr.expr&.is(.Tuple) {
            arg_expr.ty = UnknownType;
            arg_expr.done = false;
        };
        arg_ty = @check(self.infer_arguments(fid)) return;
        @check(self.compile_expr(arg_expr, arg_ty.want())) return;
        // It's probably an ICE if this fails on something non-#generic. 
        if arg_ty != arg_expr.ty {
            self.last_loc = arg_expr.loc;
            return(@err("arg type mismatch after removing const args"));
        };
    };
    
    if func.get_flag(.UnsafeNoopCast) {
        @check(self.compile_expr(arg_expr, arg_ty.want())) return;
        //@check(self.can_assign(arg_ty, arg_expr.ty)) return; TODO: type check
        @check(self.infer_return(fid)) return;
        ret_ty := func.finished_ret.expect("known ret");
        expr.expr = (Cast = arg_expr);
        expr.ty = ret_ty;
        expr.done = arg_expr.done;
        return(.Ok);
    };
    
    capturing   := func.get_flag(.AllowRtCapture).or(func.get_flag(.MayHaveAquiredCaptures));
    will_inline := capturing || func.get_flag(.Inline);
    deny_inline := func.get_flag(.NoInline);
    if(will_inline && deny_inline, => return(@err("must inline a function marked #noinline")));
    
    allow_fold := self.dispatch.enclosing_function.is_some() && {
        func := self.get_function(self.dispatch.enclosing_function.unwrap());
        !func.get_flag(.SyntheticImmEval)
    };
    
    if will_inline {
        @if(arg_ty == UnknownType)
        @if_let(self.get_type(f_expr.ty)) fn Fn(it) => {
            arg_ty = it.arg;
            @check(self.compile_expr(arg_expr, (Specific = it.arg))) return;
        };
    
        if capturing {
            // We want to allow for polymorphic arguments, but its better if we can infer the annotated ones before duplicating.
            // currently this is just an optimisation.  
            @check(self.infer_arguments_partial(fid)) return;
        };
        
        // If we're just inlining for #inline, compile first so some work on the ast is only done once.
        // note: compile() checks if its ::Inline before actually generating asm so it doesn't waste its time.
        // if its a '=>' function, we can't compile it out of context, and same if it has a const arg of a '=>' function. 
        if !capturing {
            @check(self.compile_body(fid)) return;
        };
        
        // TODO: check that you're calling from the same place as the definition.
        foldable := @check(self.emit_capturing_call(fid, expr, requested)) return;
        if func.get_flag(.TryConstantFold) && allow_fold && foldable {
            // we've already inlined the call so it won't just be a call_dynamic_values, 
            // and we'll have to suspend on compiling a stub to get the value, 
            // so we make a ConstEval to remember that this needs to be folded. 
            // don't need to worry about recursing because the call node is gone. 
            e := self.box(expr[]);
            expr[] = (ty = expr.ty, done = false, expr = (ConstEval = e), loc = expr.loc);
            return(self.compile_expr(expr, requested));
        };
        return(.Ok);
    };
    
    arg_ty = @check(self.infer_arguments(fid)) return;
    @check(self.compile_expr(arg_expr, (Specific = arg_ty))) return;
    if arg_ty != arg_expr.ty {
        @check(self.can_assign(arg_ty, arg_expr.ty)) return;
    };
    
    ret_ty := @check(self.infer_return(fid))    return;
    is_const_context := self.dispatch.enclosing_function.is_none();
    // TODO: its a bit of a hack to check is_const_context? 
    //       it fixes a problem where you try to compile too soon and then think you're already done even if you make a function for it later.
    expr.done = f_expr.done && arg_expr.done && !is_const_context;
    expr.ty = ret_ty;
    
    if f_expr.ty == self.get_or_create_type(FuncId) {
        f_ty: FnType = (arg = arg_ty, ret = ret_ty, unary = func.arg.bindings.len <= 1);
        f_expr.ty = self.intern_type(Fn = f_ty);
    };
    
    if !f_expr.expr&.is(.Value) {
        _ := @check(self.immediate_eval_expr(f_expr, f_expr.ty)) return;
    };
    if func.get_flag(.TryConstantFold) && allow_fold && arg_expr.is_const() {
        value := @check(self.immediate_eval_expr(expr, ret_ty)) return;
        expr.set(value, ret_ty); // TODO: redundant?
        return(.Ok);
    } else {
        // this fixes functions with all const args the reduce to just a value emitting useless calls to like get the number 65 or whatever if you do ascii("A"). 
        if !deny_inline && arg_expr.is_raw_unit() {
            @if_let(func.body&) fn Normal(value) => {
                if value.expr&.is(.Value) {
                    bytes := value.expr.Value.bytes&;
                    expr.set(bytes.clone(self.get_alloc()), ret_ty);
                };
            };
        };
    };
    
    //    :CompileIntrinsicsFirst jun 24, 2025: 
    //    also the intution below is actively harmful because you really want to 
    //    sema called intrinsics before emitting the call to them to allow them to be inlined, 
    //    otherwise the first call won't be inlined which in general is just sad but for 
    //    set_dynamic_context and trace_start is a miscompilation. 
    // 
    // you kinda want to do this but its wrong when its only available for some backends but some have a normal body. 
    // seems dumb to add callees when we know its an intrinsic we're going to emit inline. 
    // but also it's the same speed so nobody cares. 
    //  -- Dec 5                     ^
    //                               ^
    //                               ^
    if !expr.expr&.is(.Value) {  // && !func.get_flag(.Intrinsic) {
        if func.get_flag(.ComptimeOnly) {
            if self.dispatch.enclosing_function { current_f |
                outer := self.get_function(current_f);
                outer.set_flag(.ComptimeOnly);
            };
        };
        self.add_callee(fid);
    };
    
    if func.get_flag(.Once) {
        // TODO: better error message. should show the previous usage. 
        if(func.get_flag(.OnceConsumed), => return(@err("tried to call once function again")));
        func.set_flag(.OnceConsumed);
        assert(expr.done, "ICE: if we applied #once, the expression really needs to be .done");
    };
    
    if self.dispatch.enclosing_function.is_some() {
        expr.done = arg_expr.done;
    };
    
    .Ok
}

fn add_callee(self: *SelfHosted, fid: FuncId) bool = {
    is_const_context := true;
    if self.dispatch.enclosing_function { current_f |
        if(current_f == fid, => return(false)); 
        is_const_context = false;
        new := self.get_function(fid);
        current := self.get_function(current_f);
        mutual := new.callees.items().contains(current_f&);
        if mutual {
            current.mutual_callees&.add_unique(fid, self.get_alloc());
        } else {
            current.callees&.add_unique(fid, self.get_alloc());
        };
    };
    is_const_context
}

fn quick_guess_type(self: *SelfHosted, expr: *FatExpr) ?Type = {
    if !expr.ty.is_unknown() {
        return(Some = expr.ty);
    };
    @match(expr.expr&) {
        fn Block(f) => self.quick_guess_type(f.result);
        @default => .None;
    }
}

// Replace a call expr with the body of the target function.
fn emit_capturing_call(self: *SelfHosted, f: FuncId, expr_out: *FatExpr, requested: ResultType) Maybe(bool) #once = {
    @try(self.ensure_resolved_body(f)) return; // it might not be a closure. it might be an inlined thing.
    @debug_assert(expr_out.expr&.is(.Call));
    arg_expr := expr_out.expr.Call.arg;
    
    // TODO
    //assert!(!self.currently_inlining.contains(&f), "Tried to inline recursive function.");
    //self.currently_inlining.push(f);
    
    func := self.get_function(f);
    if !func.get_flag(.AllowRtCapture) {
        t := @check(self.infer_arguments(f)) return;
    };
    foldable := false;
    if func.finished_arg { arg | 
        @check(self.compile_expr(arg_expr, (Specific = arg))) return;
        @check(self.can_assign(arg, arg_expr.ty)) return;
        foldable = arg_expr.is_const();
    };
    
    // TODO: scary to hold this reference accross doing so much shit. 
    @err_assert(func.body&.is(.Normal), "emit_capturing_call: '=>' function '%' must have body expression.", self.pool.get(func.name)) return;
    aliased_body := func.body.Normal&;
    
    // This is complicated to allow `fn() => .a` to stay polymorphic and have different inferred types at different callsites. 
    // Maybe a more robust version of this would be treating the result type like a const arg and duplicating the function? -- Aug 2
    ret_ty := or_else func.finished_ret {
        if func.ret&.is(.Infer) {
            if requested&.is(.Specific) {
                (Some = requested.Specific)
            } else {
                known := self.quick_guess_type(aliased_body);
                known
            }
        } else {
            known := @check(self.infer_return(f)) return;
            (Some = known)
        }
    };
    // Note: this relies on you never passing a garbage hint when exploring overloads. 
    if !requested&.is(.Specific) {
        if ret_ty { ret_ty |
            requested = (Specific = ret_ty);
        };
    };
    
    label_ty := ret_ty
        .map(fn(ty) => self.intern_type(Label = ty))
        .or(=> LabelId);
    
    // for inlined or things with const args, the body will already have been compiled, but the backend needs to see the whole callgraph.
    for func.callees { callee |
        self.add_callee(callee);
    };
    
    // TODO: this feels like it should never happen (and that was the case for a long time) 
    //       but it gets confused by assert_cond()::report_erased() somehow. 
    //       but since i have jit-shims now, tracking accurately is less of a big deal.   -- May 27, 2025
    // @err_assert(func.mutual_callees.is_empty(), "TODO: you can't really refer to inlined things so how'd you make it recursive?") return;
    
    if self.dispatch.enclosing_function { caller | 
        if func.get_flag(.YesContext) {
            self.get_function(caller).set_flag(.YesContext);
        };
    };
    may_have_early_return := !aliased_body.expr&.is(.Value);
    arg := func.arg&;
    pattern := self.clone(arg);
    
    // TODO: you want to be able to share work (across all the call-sites) compiling parts of the body that don't depend on the captured variables
    old_ret_var := func.return_var.expect("return var");
    new_ret_var := self.scopes.dup_var(old_ret_var);
    ret_label: LabelId = from_index(self.dispatch.return_labels.len);
    self.dispatch.return_labels&.push(f);
    owned_body: FatExpr = if func.get_flag(.Once) {
        // TODO: better error message if they try to call it again. 
        // TODO: decide what #once with const args should mean. 
        temp := aliased_body[];
        aliased_body.expr = (Poison = .OnceUsed);
        aliased_body.loc = expr_out.loc;
        func.set_flag(.OnceConsumed);
        temp
    } else {
        z := zone_begin(.CloneExpr);
        e := self.clone(aliased_body);
        zone_end(z);
        e
    };
    loc := arg_expr.loc;
    // :NotForCorrectness the second case would be sufficient but this is so common (if(_,!,!), loop(!), etc) that it makes me less sad. 
    if arg_expr.is_raw_unit() {  // 
        // TODO: if !may_have_early_return, should be able to just inline the value but it doesnt work!
        //       similarly i cant have ret_label:None so im clearly wrong. its `not callable` in overloading.rs. trying to call the cond of an if??
        //       -- Jun 16
        expr_out.expr = new_block(empty(), self.box(owned_body));
    } else {
        stmts: List(FatStmt) = list(1, self.get_alloc());
        arg_stmt: Stmt = (DeclVarPattern = (binding = pattern, value = arg_expr[]));
        stmts&.push(stmt = arg_stmt, loc = loc);
        expr_out.expr = new_block(stmts.as_raw(), self.box(owned_body));
    };
    b := expr_out.expr.Block&;
    b.ret_label = (Some = ret_label);
    b.block_resolved = true;
    // TODO: self.currently_inlining.retain(|check| *check != f);
    if ret_ty { ret_ty | 
        expr_out.ty = ret_ty;
    };
    
    if may_have_early_return {
        // Note: not renumbering on the function. didn't need to clone it.
        self.renumber_expr(expr_out, (Some = (old_ret_var, new_ret_var)));
        value := self.to_values(LabelId, ret_label);
        @try(self.save_const_values(new_ret_var, value, label_ty, loc)) return;
    };
    
    @check(self.compile_expr(expr_out, requested)) return;
    if ret_ty { ret_ty |
        @check(self.can_assign(ret_ty, expr_out.ty)) return;
        expr_out.ty = ret_ty;
    };
    (Ok = foldable)
}

// TODO: this still has the problem of disallowing const args that require type hint of previous const args. 
fn const_args_key(self: *SelfHosted, original_f: FuncId, arg_expr: *FatExpr) Maybe(Ty(Values, Type)) #once = {
    func := self[original_f]&;
    ::if(Maybe(Ty(Values, Type)));
    if func.arg.bindings.len == 1 {
        // Hack to pass hint for single const arg becuase the compiler does it. 
        if !func.arg.bindings[0].ty&.is(.Infer) {
            @check(self.infer_arguments(original_f)) return;
        };
        
        @check(self.compile_expr(arg_expr, func.finished_arg.want())) return;
        ty := func.finished_arg.or(=> arg_expr.ty);
        value := @check(self.immediate_eval_expr(arg_expr, ty)) return;
        (Ok = (value, ty))
    } else {
        check_len :: fn(len: i64) => {
            if func.arg.bindings.len != len {
                return(@err( "TODO: non-trivial pattern matching for call to %", self.pool.get(func.name)));
            };
        };

        if !arg_expr.expr&.is(.Tuple) {
            // this only happens for invoke_specialized
            @check(self.compile_expr(arg_expr, func.finished_arg.want())) return;
            types := self.tuple_types(arg_expr.ty);
            if types { types |
                check_len(types.len);
                @if_let(arg_expr.expr&) fn Value(f) => {
                    // TODO: this is super dumb but better than what I did before. -- May 3 -- May 24
                    parts: List(FatExpr) = list(types.len, self.get_alloc());
                    reader: ReadBytes = (bytes = f.bytes&.bytes(), i = 0);
                    for types { ty | 
                        reader.i = align_to(reader.i, self.get_info(ty)[].align_bytes.zext()); // TODO: use field offsets
                        taken := or self.chop_prefix(ty, reader&) {| 
                            return(@err("ICE: not enough bytes to destructure value!"))
                        };
                        parts&.push(synthetic_ty((Value = (bytes = taken, coerced = false)), arg_expr.loc, ty));
                    };
                    @debug_assert_eq(reader.bytes.len, reader.i, "ICE: didn't consume all bytes.");
                    arg_expr.expr = (Tuple = parts.as_raw());
                };
            };
        };

        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple but expected % args.\n%", func.arg.bindings.len, arg_expr.log(self))));
        arg_exprs := arg_expr.expr.Tuple;
        check_len(arg_exprs.len);

        all_const_args: List(u8) = list(self.get_alloc());
        i := 0;
        has_seen_first_const := false;
        types := Type.list(temp());
        enumerate func.arg.bindings { i, binding |
            continue :: local_return;
            if(binding.kind != .Const, => continue());
            arg_ty := or binding.ty&.ty() {
                // TODO: this isn't really what you want. 
                //       the problem is you want to make the key before duplicating the function and binding const args,
                //       so its faster after the first instantiation, but to do that you need to compile the const args, 
                //       and they might need to get a type hint (like struct/enum literals), 
                //       but also thier type might depend on previous const args which this doesn't handle. 
                arg_expr := arg_exprs.index(i);
                if has_seen_first_const.or(=> binding.ty&.is(.Infer)) && !arg_expr.expr&.is(.ContextualField) { // hack to pass hint in similar situations to the old one
                    @check(self.compile_expr(arg_expr, .None)) return;
                    arg_exprs[i].ty
                } else {
                    (@check(self.infer_type(binding.ty&)) return).expect("infer type")
                }
            };
            has_seen_first_const = true;
            @debug_assert(!arg_ty.is_unknown(), "const_args_key expected arg expr to be compiled");
            arg := arg_exprs.index(i);
            value := @check(self.immediate_eval_expr(arg, arg_ty)) return;
            
            // TODO: remove? -- Aug 13
            //       You might not need this. I was hoping it would fix deconstruct_values but i think this is only used in the map. 
            self.aligned_append_value(all_const_args&, value, arg_ty);
            types&.push(arg_ty);
        };
        (Ok = (
            all_const_args.items().to_value(), 
            self.tuple_of(types.items())
        ))
    }
}

fn aligned_append_value(self: *SelfHosted, all_values: *List(u8), new_value: Values, arg_ty: Type) void = {
    alignment: i64 = self.get_info(arg_ty)[].align_bytes.zext();
    extra := all_values.len.mod(alignment);
    if extra != 0 {
        padding := alignment - extra;
        range(0, padding) { _ |
            all_values.push(0);
        };
    };
    
    all_values.push_all(new_value&.bytes());
}

fn remove_const_args(self: *SelfHosted, original_f: FuncId, arg_expr: *FatExpr) Maybe(void) = {
    func := self.get_function(original_f);
    if func.arg.bindings.len == 1 {
        arg_expr.set(unit_value, self.get_or_create_type(void));
    } else {
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple")));
        arg_exprs := arg_expr.expr.Tuple&;
        // We need to suspend out before removing anything...
        enumerate func.arg.bindings { i, binding | 
            if binding.ty&.ty() { expected |
                found := arg_exprs[i].ty;
                @check(self.can_assign(expected, found)) return;
            };
        };
        removed := 0;
        enumerate func.arg.bindings { i, binding | 
            if binding.kind == .Const {
                // TODO: this would be better if i was iterating backwards
                arg_exprs.ordered_remove(i - removed);
                removed += 1; // TODO: this sucks
            };
        };
        if arg_exprs.is_empty() {
            // Note: this started being required when I added fn while.
            arg_expr.set(unit_value, void);
        } else {
            if arg_exprs.len() == 1 {
                arg_expr[] = arg_exprs[0];
            } else {
                arg_expr.ty = UnknownType;
                arg_expr.done = false;
            };
        };
    };
    
    .Ok
}

MemoKey :: Ty(FuncId, Values);
// TODO: to allow #generic, this lazyily infers param types on the new function.
//       it would avoid redundant work to do that on the original func for params that don't depend on constants. 
//       that would jsut require compile_expr to be able to yield on a Poison.Argument, and we catch that here and just ignore. 
fn curry_const_args(self: *SelfHosted, original_f: FuncId, f_expr: *FatExpr, arg_expr: *FatExpr, zone: TraceCtx) Maybe(FuncId) #once = {
    value, type := @check(self.const_args_key(original_f, arg_expr)) return;
    key: MemoKey = (original_f, value);
    @if(ENABLE_TRACY) ___tracy_emit_zone_name(zone, self.comp().log(arg_expr));
    
    // this doesn't help. remove it? 
    //to_zero: ReadBytes = (bytes = key._1&.bytes(), i = 0);
    //self.comp().zero_padding(type, to_zero&);
    
    if self.dispatch.const_bound_memo&.get(key&) { new_f |
        func := self.get_function(new_f);
        if func.get_flag(.AnyConstArgs) {
            @check(self.ensure_resolved_sign(new_f)) return;
            @try(self.ensure_resolved_body(new_f)) return;
            
            original := self.get_function(original_f);
            if func.arg.bindings.len == original.arg.bindings.len {
                return(self.curry_const_args_inner(original_f, new_f, f_expr, arg_expr, 0));
            } else {
                removed_count := original.arg.bindings.len - func.arg.bindings.len;
                @debug_assert(removed_count > 0);
                return(self.curry_const_args_inner(original_f, new_f, f_expr, arg_expr, removed_count));
                return(@err("TODO: unfinished const args but started replacing some so don't really know what to do anymore. "));
            };
        };
        @check(self.remove_const_args(original_f, arg_expr)) return;
        self.set(f_expr, FuncId, new_f);
        return(Ok = new_f);
    };

    func := self.get_function(original_f);
    @debug_assert(func.get_flag(.AnyConstArgs), "baking const args but none are there");
    
    // Some part of the argument must be known at comptime.
    new_func := self.clone(func);
    new_func&.set_flag(.DisallowInOverloadSet);
    mapping: RenumberResults = init();
    self.renumber_func(new_func&, mapping&);
    new_fid := self.add_function(new_func);
    // TODO: mark func as in progress somehow so nobody can yield on it. 
    //       tho really i feel like you want to make this whole operation yield-able,
    //       currently you could get into a situation with a lot of redundant work i think.  

    // Note: putting it in super early! this ensures that if you try to call the function while compiling it, 
    //       you don't keep spawning new variations that can never be finished. 
    self.dispatch.const_bound_memo&.insert(key, new_fid);
    
    @check(self.ensure_resolved_sign(new_fid)) return;
    @try(self.ensure_resolved_body(new_fid)) return;
    
    self.curry_const_args_inner(original_f, new_fid, f_expr, arg_expr, 0)
}

fn renumber_func(self: *SelfHosted, new_func: *Func, mapping: *RenumberResults) void #once = {
    renumber: RenumberVars = (scope = self.scopes, mapping = mapping, compiler = self);
    renumber&.walk_func(new_func);
}

fn curry_const_args_inner(self: *SelfHosted, original_f: FuncId, new_fid: FuncId, f_expr: *FatExpr, arg_expr: *FatExpr, initial_removed_count: i64) Maybe(FuncId) = {
    func := self.get_function(new_fid);
    old_func := self.get_function(original_f);
    original_arg_count := old_func.arg.bindings.len();
    ::if(Maybe(FuncId));
    if original_arg_count == 1 {
        assert(initial_removed_count == 0, "TODO: removed_count single");
        binding := func.arg.bindings[0]&;
        @debug_assert_eq(binding.kind, .Const);
        name := or binding.var() {
            return(@err("arg needs name (unreachable?)"))
        };
        // TODO: if you yield here, you spam clone the function. 
        arg_type := or @check(self.infer_type(binding.ty&)) return {
            @err_assert(func.get_flag(.AllowRtCapture), "only closure may have polymorphic args") return;
            @check(self.compile_expr(arg_expr, .None)) return;
            // It was .Infer, save what we learned for when we try to call get_type_for_arg. 
            // TODO: since there's only one argument you could even save this on the original template? 
            binding.ty = (Finished = arg_expr.ty);
            arg_expr.ty
        };
        value := @check(self.immediate_eval_expr(arg_expr, arg_type)) return;
        @check(self.bind_const_arg(new_fid, name, value, arg_expr.ty, arg_expr.loc, binding)) return;

        func.finished_arg = .None;  // TODO: we know its void because we removed the only argument.
        self.set(f_expr, FuncId, new_fid);
        arg_expr.set(unit_value, self.get_or_create_type(void));
        func.unset_flag(.AnyConstArgs);
        func.unset_flag(.Generic);
        (Ok = new_fid)
    } else {
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple")));
        removed_count := 0;
        range(0, original_arg_count) { i |
            continue :: local_return;
            
            b := func.arg.bindings[i - removed_count]&;
            if removed_count < initial_removed_count {
                if b.kind == .Const {
                    removed_count += 1;
                };
                continue();
                
            };
            
            arg_expr := arg_expr.expr.Tuple[i]&;
            if b.kind != .Const {
                if b.ty&.is(.Infer) {
                    // No correctness reason for this requirement but it seems fair to me. -- Aug 2
                    self.last_loc = arg_expr.loc;
                    @err_assert(func.get_flag(.AllowRtCapture), "only closure may have polymorphic args %", func.log(self)) return;
                    @check(self.compile_expr(arg_expr, .None)) return;
                    b.ty = (Finished = arg_expr.ty);
                };
                continue();
            };
            name := or b.var() {
                return(@err("arg needs name (unreachable?)"))
            };
            // TODO: if you yield here, you spam clone the function. 
            
            // We might not be able to know the types expected for each parameter up front (because they can reference previous consts),
            // but once we're binding an argument, we need to be able to get its type. 
            // (TODO: closures are allowed to infer so that's not even true in the long term?)
            xx := @check(self.infer_type(b.ty&)) return;
            arg_type := or xx {
                self.last_loc = func.loc;
                @err_assert(func.get_flag(.AllowRtCapture), "only closure may have polymorphic args") return;
                @check(self.compile_expr(arg_expr, .None)) return;
                // It was .Infer, save what we learned for when we try to call get_type_for_arg. 
                @debug_assert(b.ty&.is(.Infer));
                b.ty = (Finished = arg_expr.ty);
                arg_expr.ty
            };
            value := @check(self.immediate_eval_expr(arg_expr, arg_type)) return;
            
            // bind_const_arg handles adding closure captures.
            // since it needs to do a remap, it gives back the new argument names so we can adjust our bindings acordingly. dont have to deal with it above since there's only one.
            @check(self.bind_const_arg(new_fid, name, value, arg_expr.ty, arg_expr.loc, b)) return;
            // No remove from arg here because bind_const_arg calls remove_named.
            removed_count += 1;
        };
        @debug_assert_ne(new_fid, original_f);

        // We're leaving it to the caller to infer the new type and type-check the runtime arguments. 
        self.set(f_expr, FuncId, new_fid);
        func.unset_flag(.AnyConstArgs);
        func.unset_flag(.Generic);
        @check(self.remove_const_args(original_f, arg_expr)) return;
        // Don't need to explicitly force capturing because bind_const_arg added them if any args were closures.
        (Ok = new_fid)
    }
}

// TODO: save the bake site on the new variable. 
//       and have self.can_assign take an optional context thingy that tells why it wanted that type, 
//       and try to stick the variable in there sometimes? or show it if the type error happens from a baked thing. 
// The argument type is evaluated in the function declaration's scope, the argument value is evaluated in the caller's scope.
fn bind_const_arg(self: *SelfHosted, o_f: FuncId, arg_name: Var, arg_value: Values, arg_ty_found: Type, loc: Span, b: *Binding) Maybe(void) = {
    // I don't want to renumber, so make sure to do the clone before resolving.
    // TODO: reslove captured constants anyway so dont haveto do the chain lookup redundantly on each speciailization. -- Apr 24
    func := self.get_function(o_f);
    @debug_assert(func.get_flag(.ResolvedBody) && func.get_flag(.ResolvedSign));
    arg_ty := @check(self.get_type_for_arg(func.arg&, arg_name)) return;
    
    @check(self.can_assign(arg_ty, arg_ty_found)) return;

    is_function := !(!self.get_type(arg_ty).is(.Fn) && arg_ty != FuncId);
    // TODO: not sure if i actually need this but it seems like i should.
    if is_function {
        arg_func := FuncId.assume_cast(arg_value&)[];
        arg_func_obj := self.get_function(arg_func);
        
        // you want to allow passing a normal function as a const arg without forcing everything to be inlined. 
        may_have_aquired_captures := arg_func_obj.get_flag(.AllowRtCapture) || arg_func_obj.get_flag(.MayHaveAquiredCaptures);
        if may_have_aquired_captures {
            @err_assert(!func.get_flag(.NoInline), "functions with constant lambda arguments are always inlined") return;
            // :ChainedCaptures
            // TODO: HACK: captures aren't tracked properly.
            func.set_flag(.MayHaveAquiredCaptures);  // redundant now that i just mark inline?
            //self[o_f].cc = (Some = .Inline); // just this is enough to fix chained_captures
            //self[arg_func].cc = (Some = .Inline); // but this is needed too for others (perhaps just when there's a longer chain than that simple example).
            arg_func_obj.set_flag(.Inline);
            func.set_flag(.Inline);
            
            if arg_func_obj.get_flag(.YesContext) {
                func.set_flag(.YesContext);
            };
            
            // :NotForCorrectness
            if arg_func_obj.get_flag(.WasLambdaLiteral) {
                // Currently, we only every bind const args for a specific callsite 
                // (and then deduplicate if the same consts are used for another call). 
                // If any @Fn argument was a literal closure expression (ie foo(fn() => ...)), 
                // we know we will never find another call to this specialization 
                // (because that literal closure is a unique value that was never bound to a name). 
                // So we can destroy this specialization when we inline it to the callsite (avoiding a deep_clone()).  -- Mar 15, 2025
                func.set_flag(.Once);
                
                if b.has_tag(Flag.duplicated.ident()) {
                    // remember for later in case they pass it to something NOT marked #duplicated
                    arg_func_obj.set_flag(.AllowDuplicated);  
                };
                
                if !arg_func_obj.get_flag(.AllowDuplicated) {
                    // if this parameter was marked #duplicated (on the callee's declaration), 
                    // it is allowed to be called multiple times in the body. 
                    // otherwise you're promising to only call it once so we can avoid a copy. 
                    arg_func_obj.set_flag(.Once);
                }
            }
        };
        
        // If we're passing a lambda arg, infer its return type based on the param's hint (not just the callsite). 
        //      f :: fn(x: @Fn() @enum(a, b)) void = { x(); } // callsite doesn't give a hint because statement discards value. 
        //      f(=> .a);  // declaration site doesn't give a hint
        @if_let(self.get_type(arg_ty)) fn Fn(f_ty) => {
            arg_func := self.get_function(arg_func);
            if arg_func.ret&.is(.Infer) {
                arg_func.ret = (Finished = f_ty.ret);
                arg_func.finished_ret = (Some = f_ty.ret);
            };
        };
    };
    @try(self.save_const_values(arg_name, arg_value, arg_ty, loc)) return;
    func.arg&.remove_named(arg_name, self.get_alloc());

    // If it was fully resolved before, we can't leave the wrong answer there.
    // But you might want to call bind_const_arg as part of a resolving a generic signeture so its fine if the type isn't fully known yet.
    func.finished_arg = .None;
    .Ok
}

/// It's fine to call this if the type isn't fully resolved yet.
/// We just need to be able to finish infering for the referenced argument.
fn get_type_for_arg(self: *SelfHosted, arg: *Pattern, arg_name: Var) Maybe(Type) #once = {
    each arg.bindings { arg | 
        if arg.var() { name | 
            if name == arg_name {
                ty := or @check(self.infer_type(arg.ty&)) return {
                    return(@err("called get_type_for_arg on .Infer-ed arg type. expected type annotation."))
                };
                return(Ok = ty);
            };
        };
    };
    @err("missing argument %", arg_name&.log(self))
}
    
fn infer_type(self: *SelfHosted, b: *LazyType) Maybe(?Type) #inline = {
    @match(b) {
        fn PendingEval(e) => {
            ty := @check(self.eval(e, Type)) return;
            b[] = (Finished = ty);
            (Ok = (Some = ty))
        }
        fn Finished(ty) => (Ok = (Some = ty[])); // cool, we're done. 
        fn EvilUninit() => panic("ICE: nothing creates this: eviluninit binding"); 
        fn Returning(_) => return(@err("ICE: tried to infer on LazyType.Returning... figure out what to do about that..."));
        fn Infer() => (Ok = .None);
        fn UnboundGeneric() => return(@err("tried to infer on unbound generic placeholder (missing #where annotation?)"));
        fn Generic() => return(@err("ICE: tried to infer on bound generic placeholder"));
    }
}

fn infer_type(self: *SelfHosted, bindings: *Pattern) Maybe([]Type) = {
    types: List(Type) = list(bindings.bindings.len, temp());
    each bindings.bindings { b |
        ty := or @check(self.infer_type(b.ty&)) return {
            // TODO: really thats not what you want for closures tho... 
            return(@err("function arguments must have type annotation (cannot be inferred unless you say #where)"))
        };
        types&.push(ty);
    };
    (Ok = types.items())
}

fn infer_arguments(self: *SelfHosted, fid: FuncId) Maybe(Type) = {
    func := self.get_function(fid);
    if(func.finished_arg, fn(ty) => return(Ok = ty));
    @check(self.ensure_resolved_sign(fid)) return;
    
    types := @check(self.infer_type(func.arg&)) return;
    if types.len > 0 && types[types.len - 1] == CVariadicType {
        func.set_flag(.NoInline);
        func.set_flag(.AvoidJitShim);
        // on amd64 variadic functions need to use rax so we can't use it for the environment pointer
        // so for consistancy, you can't use the environment stuff on any platform. 
        func.set_flag(.NoContext);
    };
    type := self.tuple_of(types);
    func.finished_arg = (Some = type);  // :const_args_are_not_const_in_tuple
    (Ok = type)
}

fn first_par_type(func: *Func) Type #inline = 
    func.arg.bindings&.index(0)[].ty&.ty().unwrap();

fn get_arg_types_non_blocking(func: *Func, args: *List(ResultType)) void #once = {
    args.clear();
    for func.arg.bindings { b | 
        @match(b.ty&.ty()) {
            fn Some(ty) => args.push(Specific = ty);
            fn None()   => args.push(.None);
        };
    };
}

fn infer_arguments_partial(self: *SelfHosted, fid: FuncId) Maybe(ResultType) = {
    func := self.get_function(fid);
    if(func.finished_arg, fn(ty) => return(Ok = (Specific = ty)));
    @check(self.ensure_resolved_sign(fid)) return;
    
    args: List(ResultType) = list(func.arg.bindings.len, temp());
    all_known := true;
    each func.arg.bindings { b |
        @match(@check(self.infer_type(b.ty&)) return) {
            fn Some(known) => args&.push(Specific = known);
            fn None() => {
                args&.push(.None);
                all_known = false;
            }
        };
    };
    if all_known {
        ty := @check(self.infer_arguments(fid)) return;
        return(Ok = (Specific = ty));
    };
    (Ok = (Tuple = args.items()))
}

fn infer_return(self: *SelfHosted, fid: FuncId) Maybe(Type) = {
    func := self.get_function(fid);
    if(func.finished_ret, fn(ty) => return(Ok = ty));
    @check(self.ensure_resolved_sign(fid)) return;
    
    ty := or @check(self.infer_type(func.ret&)) return {
        // Infer is a valid return type. To deal with that, we just compile the body, and that will set the finished_ret for us. 
        @assert(!self.dispatch.function_in_progress&.get(fid.as_index()), "unhanlded mutual recursion. %", self.log_name(fid));
        self.dispatch.function_in_progress&.set(fid.as_index());
        return(Suspend = self.wait_for(CompileBody = fid))
    };
    func.finished_ret = (Some = ty);
    (Ok = ty)
}

fn type_of(self: *SelfHosted, expr: *FatExpr) Maybe(Type) = {
    if(!expr.ty.is_unknown(), => return(Ok = expr.ty));
    self.last_loc = expr.loc;
    @match(expr.expr&) {
        fn Block(f) => self.type_of(f.result);  // TODO: this is sketchy because it might have nonlocal returns but end with Never
        fn Deref(inner) => {
            ptr := @check(self.type_of(inner[])) return;
            (Ok = self.unptr_ty(ptr).or(=> return(@err("deref non ptr"))))
        }
        fn Addr(inner) => {
            ty := @check(self.type_of(inner[])) return;
            (Ok = self.ptr_ty(ty))
        }
        fn As(f) => {
            type := @check(self.eval(f.type, Type)) return;
            (Ok = type)
        }
        fn GetVar(name) => {
            if self.scopes.get_var_type(name[]) { info |
                return(Ok = info.get_type());
            };
            ::if(Type);
            if name.kind == .Const {
                v, type := @check(self.find_const(name[], .None)) return;
                return(Ok = type);
            };
            return(@err("type_of pending var: %", self.pool.get(name.name)))
        }
        fn Closure(f) => {  
            @check(self.compile_expr(expr, .None)) return;
            (Ok = expr.ty)
        }
        fn Quote() => (Ok = self.env.fat_expr_type || return(@err("tried to guess type of @{} before boot")));
        fn FieldAccess(f) => {
            container := @check(self.type_of(f.container)) return;
            if container == ScopeId || container == Type { // :get_or_create_type 
                return(@err("for now we don't guess type for scope access"));
            };
            // TODO: this forces it to know layout which we don't actually care about yet
            // TODO: #use
            _, field_val_ty, _ := @try(self.field_access_get_type(container, f.name)) return;
            (Ok = field_val_ty)
        }
        @default => @err("failed to guess type");
        //@default => @err("failed to guess type %", expr.log(self)); 
    }
}

fn adjust_named_arguments(self: *SelfHosted, fid: FuncId, arg_expr: *FatExpr) Res(void) = {
    @if_let(arg_expr.expr&) fn StructLiteralP(f) => {
        func := self.get_function(fid);
        // If there's one argument, they must be passing a struct, we don't care about that here.
        if func.arg.bindings.len == 1 && f.bindings.len != 1 {
            return(.Ok);
        };
        args: List(FatExpr) = list(func.arg.bindings.len, self.get_alloc());
        enumerate func.arg.bindings { i, want |
            continue :: local_return;
            want_name := @unwrap(want.ident(), "function param must have name") return;
            each f.bindings { found | 
                found_name := @unwrap(found.ident(), "TODO: mixed named and un-named args are not supported yet") return;
                if found_name == want_name {
                    value := @unwrap(found.get_default(), "use '=' not ':' for named arg") return;
                    @err_assert(args.len == i, "TODO: out of order named args are not supported yet") return;
                    args&.push(value[]);
                    continue();
                };
            };
            // but it might be an inline @tagged so this can't be an error
            return(.Ok);
            //return(@err("arg not found for %", self.pool.get(want_name)));
        };
        arg_expr.expr = (Tuple = args.as_raw());
        arg_expr.done = false;
        arg_expr.ty = UnknownType;
    };
    .Ok
}

// TODO: this is very non-linear with number of overloads. :SLOW :SLOW :SLOW
// 
// This is a super fagile adhoc thing that really needs to be rewritten. 
// Preferably with an attempt at formalizing what shapes of code we accept, 
// instead of just "eh you know if it doesnt compile just add some casts maybe". 
// I think the important principle is to never look at the body of the function you're trying to call. 
// Require all the information for resolving the overload to be present in the header. 
//
fn resolve_in_overload_set_new(self: *SelfHosted, arg_expr: *FatExpr, attempt: *OverloadAttempt) Maybe(FuncId) #once = {
    overloads := self.dispatch.overloads&.nested_index(attempt.os.as_index());
    
    // THIS IS GARBAGE 
    // i should just give up and have a `defer` ast node. 
    // it's so close to working tho
    R :: return;
    zone := @uninitialized TraceCtx;
    have_zone := false;
    start :: fn($i: i64, extra: Str) => @if(ENABLE_TRACY) {
        zone = zone_begin(.SemaOverloads); // TODO: defer
        if extra.len == 0 {
            ___tracy_emit_zone_name(zone, ::(@tfmt("%", i)).sym().str());
        } else {
            ___tracy_emit_zone_name(zone, extra);
        };
        have_zone = true;
    };
    end :: fn() => @if(ENABLE_TRACY) {
        zone_end(zone);
        have_zone = false;
    };
    RR :: fn(value: Maybe(FuncId)) Never => {
        if have_zone {
            end();
        };
        R(value)
    };
    return2 :: @if(ENABLE_TRACY, RR, R);
    
    // this + the hash table helps a bit ~2300 -> ~2220, but it's really not enough. 
    if overloads.ready.len > 6 && arg_expr.ty.is_unknown() {
        bail :: local_return;
        args := arg_expr.items();
        types := Type.list(temp());
        each args { a | 
            if is_const(a) {
                bail();
            };
            r := self.type_of(a);
            if r&.is(.Ok) {
                types&.push(r.Ok);
            } else {
                bail();
            }
        };
        arg_expr.ty = self.tuple_of(types.items());
    };
    
    if !arg_expr.ty.is_unknown() {
        req := UnknownType;
        @if_let(attempt.call.requested) fn Specific(t) => {
            req = t;
        };
        key: OverloadKey = (arg = arg_expr.ty, req = req);
        if overloads.inline_cache._0& == key& {
            return2(Ok = overloads.inline_cache._1);
        };
        t := overloads.table&;
        if t.get(key&) { f |
            return2(Ok = f);
        };
    };
    
    self.compute_new_overloads(overloads);
    had_new_overloads := overloads.ready.len > attempt.last_ready_count;
    if had_new_overloads {
        attempt.state = 0;
        new := overloads.ready.items().slice(attempt.last_ready_count, overloads.ready.len);
        attempt.options&.reserve(new.len);
        want_arity := arity(arg_expr);
        for new { option | 
            if self.filter_overload_sync(arg_expr, want_arity, option) {
                @check(self.ensure_resolved_sign(option)) return2; // TODO: dont let this suspend cause it cant
                attempt.options&.push_assume_capacity(option);
            };
        };
        attempt.last_ready_count = overloads.ready.len;
    };
    want_arity := arity(arg_expr);
    ::if([]FatExpr);
    parts := if(arg_expr.expr&.is(.Tuple), => arg_expr.expr.Tuple.items(), => (ptr = arg_expr, len = 1));
    are_we_done_yet();
    
    are_we_done_yet :: fn() => {
        if attempt.options.len == 1 {
            return2(Ok = self.done_resolving_overload(attempt, overloads, arg_expr));
        };
        if attempt.options.len == 0 {
            self.last_loc = arg_expr.loc;
            return2(self.report_missing_overload(parts, overloads, attempt.call.requested));
        };
    };
    
    fn const_coercable(self: *SelfHosted, t: Type) bool #inline =  
        t == FuncId || @is(self.get_type(t), .Int, .Fn, .FnPtr, .F32, .F64);
    
    if !attempt.got_methods {
        first_arg := arg_expr.items().index(0);
        r := self.type_of(first_arg);
        // if we know this argument can coerce, don't do the filtering
        if r&.is(.Ok) && !self.const_coercable(r.Ok) {
            first_type := r.Ok;
            unordered_retain attempt.options& { fid | 
                func := self.get_function(fid[]);
                func.get_flag(.UnboundGenerics) || func.finished_arg.is_none() || func.first_par_type() == first_type
            };
            attempt.prev_index = attempt.options.len;
        };
    };
    
    // TODO: should be able to move the infer_type into the next phase just before you need it but no -- Sep 2
    if attempt.state < 1 {
        SPAM_TRACE :: false;
        @if(!SPAM_TRACE) start(1, "");
        are_we_done_yet();
        // TODO: while is it faster to write the loop like this than with for?
        current_len := attempt.options.len;
        f_i := if(!had_new_overloads, => attempt.prev_index, => attempt.options.len);
        attempt.prev_index = f_i;
        
        while => f_i > 0 {
            attempt.prev_index = f_i;
            f_i -= 1;
            fid := attempt.options[f_i];
            @if(SPAM_TRACE) start(1, @if(ENABLE_TRACY, @tfmt("%", fid), ""));
            func := self.get_function(fid);
            
            if !func.get_flag(.UnboundGenerics) && func.finished_arg.is_none() {
                break :: local_return;
                generic := func.get_flag(.Generic);
                all_known := true;
                each func.arg.bindings& { b |
                    res := @check(self.infer_type(b.ty&)) return2;
                    all_known = all_known && res&.is_some();
                    
                    // parameters after the first $const can't be inferred without binding const args
                    if(generic && b.kind == .Const, => break());
                };
                if all_known {
                    @check(self.infer_arguments(fid)) return2;
                };
            };
            
            if func.get_flag(.UnboundGenerics) {
                accept := @check(self.eval_where_from_expression(fid, arg_expr)) return2;
                if accept { new_fid | 
                    attempt.options[f_i] = new_fid;
                } else {
                    attempt.options&.unordered_remove_discard(f_i);
                };
            };
            @if(SPAM_TRACE) end();
        };
        
        attempt.state = 1;
        attempt.prev_index = attempt.options.len;
        @if(!SPAM_TRACE) end();
    };
    
    if attempt.state < 2 {
        start(2, "");
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            type := self.type_of(arg_expr);
            if !type&.is(.Ok) {
                continue();
            };
            type := type.Ok;
            if type == FuncId {
                continue();
            };
            arg_expr.ty = type;
            
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                if @check(self.infer_type(b.ty&)) return2 { param | 
                    if !self.ask_can_assign(param, type) {
                        // TODO: this makes it slower and means you can't cache it but also is going to be confusingly wrong in some cases
                        //       because you might make changes based on a type hint from an overload that didn't end up getting selected. 
                        //       but also it's very convenient to have this so here we are for now.   -- Dec 23
                        if self.const_coercable(type) && arg_expr.is_const() { // this helps with :type_through_field
                            res := self.ask_coerce_const_expr(arg_expr, param, true);
                            if res&.is(.Ok) && res.Ok {
                                continue();
                            };
                        };
                        
                        reject();
                    };
                };
            };
            are_we_done_yet();
        };
        attempt.state = 2;
        attempt.prev_index = attempt.options.len;
        end();
    };
    
    if attempt.state < 3 {
        start(3, "");
        @if_let(attempt.call.requested) fn Specific(type) => {
            current_len := attempt.options.len;
            f_i := if(!had_new_overloads, => attempt.prev_index, => attempt.options.len);
            while => f_i > 0 {
                continue :: local_return;
                attempt.prev_index = f_i;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if !func.get_flag(.Generic) && func.finished_ret.is_none() {
                    @check(self.infer_return(fid)) return2;
                };
                
                if func.finished_ret { ret | 
                    if !self.ask_can_assign(type, ret) {
                        reject();
                    };
                };
            };
            are_we_done_yet();
        };
        attempt.state = 3;
        end();
    };
    
    if attempt.state < 4 {
        start(4, "");
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            @check(self.compile_expr(arg_expr, .None)) return2;
            type := arg_expr.ty;
            
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                if b.ty&.ty() { param | 
                    if !self.ask_can_assign(param, type) {
                        reject();
                    };
                };
            };
            are_we_done_yet();
        };
        attempt.state = 4;
        end();
    };
    
    if attempt.state < 5 {
        start(5, "");
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            
            arg_fid := if arg_expr.is_const() && (arg_expr.ty.eq(FuncId) || self.get_type(arg_expr.ty).is(.Fn)) {
                @check(self.immediate_eval_expr(arg_expr, arg_expr.ty)) return2
            } else {
                continue()
            };
            arg_fid := FuncId.assume_cast(arg_fid&)[];
            arg_func := self.get_function(arg_fid);
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    if func.arg.bindings.len != 1 {
                        reject();
                    };
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                if b.ty&.ty() { param | 
                    param_info := self.get_type(param);
                    if not(arg_expr.ty.eq(FuncId).or(=> param_info.is(.Fn))) {
                        reject();
                    };
                    
                    @if_let(param_info) fn Fn(ty) => {
                        res := self.check_promote_id_to_func(arg_fid, ty[]);
                        if res&.is(.Err) {
                            reject();
                        };
                        @check(res) return2;
                    };
                };
            };
            are_we_done_yet();
        };
        attempt.state = 5;
        end();
    };
    
    // this is garbage
    {
        start(7, "");
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            @check(self.compile_expr(arg_expr, .None)) return2;
            type := arg_expr.ty;
            
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                if b.ty&.ty() { param | 
                    type := self.get_type(type);
                    param := self.get_type(param);
                    if type.is(.Int) && param.is(.Int) && type.Int.signed != param.Int.signed {
                        reject();
                    };
                };
            };
            are_we_done_yet();
        };
        end();
    };

    self.last_loc = arg_expr.loc;
    out := u8.list(temp());
    @fmt(out&, "% matching options for %\n", attempt.options.len, self.pool.get(overloads.name));
    if attempt.options.len < 6 {
        for attempt.options { opt |
            func := self.get_function(opt);
            self.codemap.fmt_error_line(func.loc, out&, true /*TODO*/);
            @fmt(out&, "- (");
            each func.arg.bindings { b |
                @match(b.ty&.ty()) {
                    fn Some(type) => {
                        @fmt(out&, "%, ", self.log_type(type));
                    }
                    fn None() => {
                        @fmt(out&, "???, ");
                    };
                };
            };
            @fmt(out&, ") -> %\n", self.log_type(func.finished_ret));
        };
    };
    
    @err("%\nTODO: end of loop. still too many options for '%'\n", out.items(), self.pool.get(overloads.name))
}

fn eval_where_from_expression(self: *SelfHosted, fid: FuncId, arg_expr: *FatExpr) Maybe(?FuncId) = {
    args := arg_expr.items();
    func := self.get_function(fid);
    if func.arg.bindings.len != args.len {
        return(Ok = .None);
    };
    types := Type.list(1, temp());
    enumerate func.arg.bindings& { arg_i, b |
        if b.ty&.is(.Generic) {
            type := @check(self.type_of(args.index(arg_i))) return;
            types&.push(type);
        };
    };
    
    types_value := self.from_bytes(interpret_as_bytes(types.items()));
    key: WhereKey = (template = fid, arg = types_value);
    if self.where_memo&.get(key&) { new_fid |
        return(Ok = (Some = new_fid));
    };
    accept := @check(self.evaluate_where_clauses(fid, types.items(), arg_expr.loc)) return;
    if !accept {
        return(Ok = .None);
    };
    new_fid := @check(self.instantiate_where_generic(fid, types.items())) return;
    (Ok = (Some = new_fid))
}

fn evaluate_where_clauses(self: *SelfHosted, fid: FuncId, generic_types: []Type, loc: Span) Maybe(bool) = {
    types_value := self.from_bytes(interpret_as_bytes(generic_types));
    key: WhereKey = (template = fid, arg = types_value);
    if self.where_memo&.get(key&).is_some() {
        return(Ok = true);
    };
    
    func := self.get_function(fid);
    slow_arg_ty := {
        slow := temp().alloc(Type, generic_types.len);
        each slow { s |
            s[] = Type; // :get_or_create_type
        };
        self.tuple_of(slow)
    };
    // Type of the argument to #where, NOT the function itself
    f_ty := self.intern_type(Fn = (arg = slow_arg_ty, ret = bool, unary = generic_types.len <= 1));  // :get_or_create_type
    
    types_arg_expr: FatExpr = (expr = (Value = (bytes = types_value, coerced = false)), done = false, ty = slow_arg_ty, loc = loc);
    each func.annotations& { ann |
        if ann.name == Flag.where.ident() {
            if ann.non_void_arg() { where_expr |
                if !where_expr.is_raw_unit() {
                    // TODO: it needs to know this is in a const context. 
                    _ := self.poll_in_place(void, => self.compile_expr(where_expr, (Specific = f_ty)));
                    condition := @check(self.eval(where_expr, FuncId)) return;
                    
                    // :get_or_create_type
                    // TODO: bad tack memory!
                    accept := @check(self.invoke(condition, types_arg_expr&, bool, loc)) return;
                    accept := bool.assume_cast(accept&)[];
                    if !accept {
                        return(Ok = false);
                    };
                };
            };
        };
    };
    
    // We accepted all #where clauses. 
    (Ok = true)
}

fn instantiate_where_generic(self: *SelfHosted, template: FuncId, generic_types: []Type) Maybe(FuncId) = {
    template_func := self.get_function(template);
    
    new_func := self.clone(template_func);
    new_func&.unset_flag(.UnboundGenerics);
    new_func&.set_flag(.DisallowInOverloadSet);
    new_func.annotations&.unordered_retain(fn(it) => it.name != Flag.where.ident()); // TODO: don't clone it in the first place

    mapping: RenumberResults = init();
    renumber_func(self, new_func&, mapping&);
    
    n := 0;
    each new_func.arg.bindings& { b |
        @if_let(b.ty&) fn Generic(name) => {
            new := name[];
            type := generic_types[n];
            b.ty = (Finished = type);
            //b.ty = (PendingEval = (expr = (GetVar = new), ty = Type, loc = template_func.loc, done = false));
            self.scopes.put_constant(new, self.to_expr(Type, type, template_func.loc), (Finished = Type)); // :get_or_create_type
            n += 1;
        };
    };
    
    @try(self.resolve_body(new_func&)) return;
    new_func&.unset_flag(.Generic);   
    fid := self.add_function(new_func);
    
    types_value := self.from_bytes(interpret_as_bytes(generic_types));
    key: WhereKey = (template = template, arg = types_value);
    self.where_memo&.insert(key, fid).is_some();
    
    @check(self.infer_arguments(fid)) return;
    
    (Ok = fid)
}

fn done_resolving_overload(self: *SelfHosted, attempt: *OverloadAttempt, overloads: *OverloadSetData, arg_expr: *FatExpr) FuncId = {
    fid := attempt.options[0];
    if !arg_expr.ty.is_unknown() {
        req := UnknownType;
        @if_let(attempt.call.requested) fn Specific(t) => {
            req = t;
        };
        key: OverloadKey = (arg = arg_expr.ty, req = req);
        overloads.inline_cache = (key, fid);
        if overloads.ready.len > 6 {
            overloads.table&.insert(key, fid, self.get_alloc()); 
        };
    };
    fid
}

fn check_promote_id_to_func(self: *SelfHosted, fid: FuncId, f_ty: FnType) Maybe(void) = {
    func := self.get_function(fid);
    //partial_args := @check(self.infer_arguments_partial(fid)) return;
    
    // this allows missing type annotations on '=' function args in situations like import_c's parser handlers. 
    // TODO: kinda hack that '=>' ones can't be done here. 
    if !func.get_flag(.AllowRtCapture) && func.finished_arg.is_none() {
        arg_ty := self.arg_types(f_ty.arg);
        @err_assert(arg_ty.len == func.arg.bindings.len, "FuncId promotion arity mismatch") return;
        enumerate func.arg.bindings& { i, b |
            if b.ty&.is(.Infer) {
                b.ty = (Finished = arg_ty[i]);
            };
            // TODO: else typecheck :FUCKED
        };
    };
    
    // this doesn't help. doesn't happen. but its similar to what the old one did in promote_closure
    // if you take out infering from requested in fn Closure, this happens, but then its wrong in the prelude. 
    if func.ret&.is(.Infer) {
        @if_let(func.body&) fn Normal(body) => {
            guessed := self.type_of(body);
            @if_let(guessed) fn Ok(type) => {
                func.ret = (Finished = type);
                func.finished_ret = (Some = type);
            };
        };
    };
    
    ret := @check(self.infer_return(fid)) return;
    if func.finished_ret { ret | 
        @check(self.can_assign(f_ty.ret, ret)) return;
    };
    .Ok
}

fn filter_overload_sync(self: *SelfHosted, arg_expr: *FatExpr, arg_expr_arity: i64, fid: FuncId) bool #once #inline = {
    func := self.get_function(fid);
    
    if func.body&.is(.Empty) && func.annotations.len == 0 {
        // foward declaration if an overload set. 
        return(false);
    };
    
    if func.arg.bindings.len != arg_expr_arity {
        if !arg_expr.expr&.is(.StructLiteralP) {
            return(false);
        };
        // Maybe they're trying to pass named arguments. 
        have_parts := arg_expr.expr.StructLiteralP.bindings&;
        want_parts := func.arg.bindings&;
        if want_parts.len != have_parts.len {| // TODO: allow default args
            return(false);
        };
        
        // :SLOW but you get here so rarely it's fine (ie. once for self-compiling).
        each want_parts { want_b | 
            next :: local_return;
            want_name := want_b.ident().or(=> return(false)); // idk if this ever happens but it sure can't be named argument
            each have_parts { have_b |
                have_name := have_b.ident().or(=> return(false)); // idk if this ever happens but it sure can't be named argument
                if want_name == have_name {
                    next();
                };
            };
            return(false);
        };
    };
    
    true
} 

// TODO: need to check for conflicts somehow but we can't assume we're able to infer all the options here,
//       because there might be cycles or more might be added later. 
//       do i need to keep a list of all the (os, arg_ty -> fn) and check at the end?
// TODO: do merges
fn resolve_in_overload_set(self: *SelfHosted, f_ty: FnType, i: OverloadSet, loc: Span) Maybe(FuncId) = {
    types := self.arg_types(f_ty.arg);
    self.last_loc = loc;
    want_arity := types.len;
    
    overloads := self.dispatch.overloads&.nested_index(i.as_index());
    self.compute_new_overloads(overloads);
    
    interesting: List(FuncId) = list(temp());
    for overloads.ready { opt | 
        continue :: local_return;
        func := self.get_function(opt);
        if func.arg.bindings.len != want_arity {
            continue();
        };
        if func.finished_ret { have | 
            if !self.ask_can_assign(f_ty.ret, have) {
                continue();
            };
        };
        if func.finished_arg { found | 
            if !self.ask_can_assign(f_ty.arg, found) {
                continue();
            };
        };
        interesting&.push(opt);
    };
    
    // TODO: but what if there are more overloads coming later? 
    if interesting.len == 1 {
        return(Ok = interesting[0]);
    };
        
    ::if([]FatExpr);
    blocked: List(*Action) = list(temp());
    still_interesting: List(FuncId) = list(temp());
    for interesting& { opt | 
        continue :: local_return;
        func := self.get_function(opt);         
        @check(self.ensure_resolved_sign(opt)) return;
        
        g := !func.get_flag(.Generic);
        all_known := g;
        enumerate func.arg.bindings { i, b | 
            if g && !b.ty&.is(.Finished) {
                res := @check(self.infer_type(b.ty&)) return;
                all_known = all_known && res&.is_some();
            };
            if b.ty&.ty() { param | 
                if !self.ask_can_assign(param, types[i]) {
                    continue();
                };
            };
        };
        if all_known {
            @check(self.infer_arguments(opt)) return;
        }; 
        still_interesting&.push(opt);
    };
    
    if still_interesting.len == 1 {
        return(Ok = still_interesting[0]);
    };
    temp := still_interesting;
    still_interesting = interesting;
    interesting = temp;
    still_interesting&.clear();

    want := f_ty.ret;
    for interesting& { opt | 
        continue :: local_return;
        func := self.get_function(opt);    
        if func.finished_ret&.is_none() {
            @check(self.infer_return(opt)) return;
        };
        if func.finished_ret { found | 
            if !self.ask_can_assign(want, found) {
                continue();
            };
        };
        still_interesting&.push(opt);
    };

    interesting = still_interesting;
    
    if !blocked.is_empty() {
        return(Suspend = blocked[0]);  // TODO: return all? don't just pick one. 
    };
    
    if interesting.len == 1 {
        return(Ok = interesting[0]);
    };
    
    
    // TODO: don't spam log since we might discard the error.
    @println("ambigous overload for % -> %", self.log_type(f_ty.arg), self.log_type(f_ty.ret));
    self.last_loc = loc;
    @println("% matching options", interesting.len);
    for interesting { opt |
        func := self.get_function(opt);
        @if(DEBUG_SPAM_LOG) self.codemap.show_error_line(func.loc);
        @println("- % -> %", self.log_type(func.finished_arg), self.log_type(func.finished_ret));
    };
    overloads := self.dispatch.overloads&.nested_index(i.as_index());
    @err("TODO: resolve_in_overload_set % %", self.pool.get(overloads.name), interesting.len)
}

fn log_type(self: *SelfHosted, ty: ?Type) Str = {
    @match(ty) {
        fn Some(ty) => self.log_type(ty);
        fn None() => "???";
    }
}

fn compute_new_overloads(self: *SelfHosted, overloads: *OverloadSetData) void = {
    overloads.ready&.push_all(overloads.pending&.items(), self.get_alloc());
    overloads.pending.len = 0;
}

// - you want macros to be able to create new constant declarations in macro expansions and const arg functions.
// - for now constants are always stored globally and restricted visibility is just handled by scope resolution.
// So we delay taking constants until you try to compile the expression that contains them.
// Also, to be order independent, we don't actually evaluate or type-check them yet, that's done lazily the first time they're referenced. 
fn hoist_constants(self: *SelfHosted, body: []FatStmt) Maybe(void) #once = {
    each body { stmt |
        @match(stmt.stmt&) {
            fn DeclFunc(func) => {
                // TODO: aren't the `@check`s below a problem because if they suspend you're totally fucked cause the function's gone but you might not know its overload set or whatever? 
                fid := self.add_function(func[][]);
                
                // just incase someone aliased the pointer
                @debug_assert(!func[].get_flag(.OnceConsumed));
                func[].set_flag(.OnceConsumed);
                stmt.stmt = .Noop;
                
                func := self.get_function(fid);
                overload_out: ?OverloadSet = .None;
                
                // I thought i dont have to add to constants here because we'll find it on the first call when resolving overloads.
                // But it does need to have an empty entry in the overload pool because that allows it to be closed over so later stuff can find it and share if they compile it.
                if func.var_name& { var |
                    // TODO: allow function name to be any expression that resolves to an OverloadSet so you can overload something in a module with dot syntax.
                    // TODO: distinguish between overload sets that you add to and those that you re-export
                    // TODO: assert placeholdervalue
                    
                    i := @check(self.get_or_create_overloads(var.name, var.scope, func.loc)) return;
                    os := self.dispatch.overloads&.nested_index(i.as_index());
                    os.pending&.push(fid, self.get_alloc());
                    overload_out = (Some = i);
                };
                
                @try(self.update_function_metadata(func, overload_out)) return;
            }
            fn Decl(f) => if f.kind == .Const {
                @debug_assert(f.name&.is(.Var), "tried to hoist unresovled decl");
                self.scopes.put_constant(f.name.Var, f.default, f.ty);
                stmt.stmt = .Noop;
            };
            @default => ();
        }
    };
    .Ok
}

fn get_or_create_overloads(self: *SelfHosted, name: Symbol, scope: ScopeId, loc: Span) Maybe(OverloadSet) = {
    // TODO: this mapping doesn't use #use which seems bad 
    var := self.scopes[scope].lookup&.get(name) || 
        @try(self.decl_var(scope, name, .Const, loc, .Unknown)) return;
    ::if(OverloadSet);
    (Ok = if self.is_empty_constant(var) {
        // We're the first to reference this overload set so create it. 
        i: OverloadSet = from_index(self.dispatch.overloads.len);
        self.dispatch.overloads&.push(
            ready = empty(),
            name = var.name,
            pending = empty(),
            // zero is UnknownType which we'll never query so its a fine .None replacement value since i don't have niches yet. 
            inline_cache = Ty(OverloadKey, FuncId).zeroed(),
        );
        os_value := self.to_values(OverloadSet, i);
        @try(self.save_const_values(var, os_value, self.get_or_create_type(OverloadSet), loc)) return;
        i
    } else {
        overloads := @check(self.find_const(var, self.get_or_create_type(OverloadSet).want())) return;
        OverloadSet.assume_cast(overloads._0&)[]
    })
}

fn update_function_metadata(self: *SelfHosted, func: *Func, os: ?OverloadSet) Res(void) = {
    any_const_args :: fn(self: *Func) bool = {
        each self.arg.bindings { b |
            if(b.kind == .Const, => return(true));
        };
        false
    };
    if func.any_const_args() {
        func.set_flag(.AnyConstArgs);
    };

    each func.annotations { tag | 
        @switch(tag.name) {
            @case(Flag.fold.ident())     => func.set_flag(.TryConstantFold);
            @case(Flag.macro.ident())    => {
                func.set_flag(.Macro);
                func.set_flag(.ComptimeOnly);
            };
            @case(Flag.noinline.ident()) => func.set_flag(.NoInline);
            // TODO: generic+unsafe_noop_cast are done in scope for old sema but i want to move them here. -- Jul 30
            @case(Flag.generic.ident())  => func.set_flag(.Generic);
            @case(Flag.log_asm.ident())  => func.set_flag(.LogAsm);
            @case(Flag.log_ir.ident())     => func.set_flag(.LogIr);
            @case(Flag.log_ast.ident())    => func.set_flag(.LogAst);
            @case(Flag.unsafe_noop_cast.ident()) => {
                func.set_flag(.UnsafeNoopCast);
                func.set_flag(.BodyIsSpecial);
            };
            @case(Flag.duplicated.ident()) => func.set_flag(.AllowDuplicated);
            @case(Flag.import.ident())   => {
                func.set_flag(.BodyIsSpecial);
                //func.set_flag(.NoMangle);
            };
            @case(Flag.libc.ident())     => {
                func.set_flag(.BodyIsSpecial);
                //func.set_flag(.NoMangle);
            };
            @case(Flag.redirect.ident()) => {
                @err_assert(func.body&.is(.Empty), "#redirect conflicts with body expression") return;
                os := @unwrap(os, "#redirect function only makes sense as part of an overload set") return;
                os := self.to_values(OverloadSet, os);
                expr := @unwrap(tag.non_void_arg(), "#redirect requires argument") return;
                os := synthetic_ty((Value = (bytes = os, coerced = false)), expr.loc, OverloadSet);
                @err_assert(expr.expr&.is(.Tuple), "#redirect expected tuple") return;
                parts := expr.expr.Tuple&;
                parts.push(os, self.get_alloc());
                func.set_flag(.BodyIsSpecial);
            };
            // TODO: actually im not sure if its better to do this later...
            //       it would be nice to do only one pass over the annotations 
            //       but it would also be nice to do absolutely no work if you never try to call the function. 
            @case(Flag.comptime_addr.ident()) => {
                @err_assert(func.body&.is(.Empty), "#comptime_addr conflicts with body expression") return;
                func.set_flag(.BodyIsSpecial);
                func.set_flag(.Intrinsic);
            };
            @case(Flag.intrinsic.ident()) => {
                func.set_flag(.BodyIsSpecial);
                func.set_flag(.Intrinsic);
            };
            @case(Flag.ir.ident()) => {
                func.set_flag(.BodyIsSpecial);
                func.set_flag(.Intrinsic);
            };
            @case(Flag.inline.ident()) => {
                // TODO: error on conflicting annotations. 
                func.set_flag(.Inline);
            };
            @case(Flag.ct.ident()) => {
                func.set_flag(.ComptimeOnly);
                func.set_flag(.YesContext);
            };
            @case(Flag.avoid_shim.ident()) => func.set_flag(.AvoidJitShim);
            @default => ();
        };
    };
    
    .Ok
}

::if(Maybe(void));
::if_opt(Type, Maybe(void));
fn compile_get_var(self: *SelfHosted, expr: *FatExpr, requested: ResultType, old: bool) Maybe(void) #once = {
    @debug_assert(expr.expr&.is(.GetVar));
    var := expr.expr.GetVar;
    if var.kind == .Const {
        value, ty := @check(self.find_const(var, requested)) return;
        expr.set(value, ty);
        @check(self.coerce_const_expr(expr, requested, false)) return;
        .Ok
    } else {
        if self.scopes.get_var_type(var) { info | 
            type := info.get_type();
            expr.ty = type;
            expr.done = true;
            // Reading a variable. Convert it to `var&[]`. 
            ptr_ty := self.ptr_type(type);
            // :?
            if old {
                info.took_address = true;
                expr[] = synthetic_ty((Addr = self.box(expr[])), expr.loc, ptr_ty);
                expr.done = true;
                // Note: not using deref_one, because don't want to just remove the ref, we want raw variable expressions to not exist. kinda HACK
                expr[] = synthetic_ty((Deref = self.box(expr[])), expr.loc, type);
                expr.done = true;
            };
            return(.Ok);
        };
        // For now runtime vars are always declared in order so we always know thier type.
        // This might change to allow peeking into return-ed expressions when infering closure return type? -- Jul 15
        @err("Unknown type for runtime var %", self.log(var))
    }
}

fn took_pointer_value(self: *SelfHosted, fid: FuncId) void = {
    func := self.get_function(fid);
    if !func.get_flag(.TookPointerValue) {
        m    := self.comptime_codegen.m;
        id   := m.intern(self.comp().fmt_fn_name(fid)); 
        done := false;
        use_symbol(m, id) { s |
            // We might have already compiled the function but didn't know we'd need to remember its address. (ie. #test fn large_struct_ret_return).
            // TODO: make sure im testing the case where is was already jitted so these are not null
            remember :: fn(a: rawptr) => {
                a := a.int_from_rawptr();
                if(a != 0, => self.created_jit_fn_ptr_value(fid, a));
            };
            remember(s.shim_addr);
            remember(s.jit_addr);
            done = !s.jit_addr.is_null();
        };
        // Note: else is not `Suspend = self.wait_for(Jit = fid)` because of cycles, we defer to the backend to deal with finializing it. 
        //       a better system would be to generate a shim right now so we'd always have an address? 
    };
    func.set_flag(.TookPointerValue);
}

// TODO: this is kinda weird. `fn` statements create an overload set or add to an existing one. 
fn is_empty_constant(self: *SelfHosted, name: Var) bool #once = {
    var := self.scopes.get_constant(name);
    var := or(var, => return(true));
    var.expr.expr&.is(.Poison)
}

// Passing in the requested type here feels a bit weird, but I think it will make anon-functions less painful. 
fn find_const(self: *SelfHosted, name: Var, requested: ResultType) Maybe(Ty(Values, Type)) = {
    var := self.scopes.get_constant(name);
    if var& { var | 
        // If we've compiled this before, great.
        //     We can't coerce_constant here because we don't have a unique expression node to stick changes into if needed. 
        //     (need to change the expression to create function pointers because they might not be compiled yet)
        @if_let(var.expr.expr&) fn Value(f) => {
            return(Ok = (f.bytes&.clone(self.get_alloc()), var.expr.ty));
        };
    };
    
    (Suspend = self.wait_for(EvalConstant = (name = name, requested = requested)))
}

fn find_const_non_blocking(self: *SelfHosted, name: Var) ?Ty(Values, Type) #once = {
    var := self.scopes.get_constant(name);
    var := or(var, => return(.None));
    @if_let(var.expr.expr&) fn Value(f) => {
        return(Some = (f.bytes&.clone(self.get_alloc()), var.expr.ty));
    };
    .None
}

fn is_rec_hack(value: *FatExpr) bool = {
    ::AutoEq(?Symbol); ::RefEq(?Symbol);
    value.expr&.is(.PrefixMacro) && value.expr.PrefixMacro.handler.ident() == (Some = Flag.rec.ident())
}

fn handle_declare_constant(self: *SelfHosted, name: Var, ty: *LazyType, value: *FatExpr) Maybe(void) #once = {
    @debug_assert(self.dispatch.enclosing_function.is_none(), "ICE: handle_declare_constant should have no enclosing function");
    
    is_rec := is_rec_hack(value);
    
    // infer the more useful name when you do `foo :: fn() = { ... };`
    @if_let(value.expr) fn Closure(func) => {
        if func.get_flag(.NoName) {
            func.name = name.name;
            func.unset_flag(.NoName);
        };
    };
    
    want := ResultType.None;
    if @check(self.infer_type(ty)) return { known |
        want = (Specific = known);
        if(is_rec, => @err_assert(known == Type, "@rec expected Type") return);
    };
    
    // TODO: HACK that doesn't respect scoping!
    //       eventually it should just notice when a type tries to reference itself and switch to this path without the explicit @rec. 
    if is_rec {
        invocation := value.expr.PrefixMacro&;
        if invocation.arg.is_raw_unit() {
            invocation.arg[] = invocation.target[];
        };
        real_value_box := invocation.arg;
        hole := self.intern_type(.Placeholder);
        self.set(value, Type, hole);
        self.save_guessed_name(hole, name.name, value.loc);
        // This can't just fallthrough and call update_placeholder at the end of this function like the old version did,
        // because we might suspend while compiling and then next time around we'll forget this value is special. 
        // (we'll just think its already done because we'll see the placeholder).
        return(Suspend = self.wait_for(FinishRecType = (hole = hole, name = name, value = real_value_box)));
    };
    
    @check(self.compile_expr(value, want)) return;
    
    if value.ty == UnknownType {
        return(@err("TODO: needed type hint for %", self.pool.get(name.name)));
    };
    if !value.expr&.is(.Value) {
        val := @check(self.immediate_eval_expr(value, value.ty)) return;
        value.set(val, value.ty);
    };
    
    @check(self.coerce_const_expr(value, want, false)) return;
    //@if_let(want) fn Specific(ty) => {
    //    @check(self.can_assign(ty, value.ty)) return;
    //};
    
    self.finish_layout_deep(value.ty);
    
    if value.ty == Type {
        @debug_assert(value.expr&.is(.Value));
        type_value := Type.assume_cast(value.expr.Value.bytes&)[];
        // You're still gonna get a lot of T/Self/RAW which is unhelpful. 
        self.save_guessed_name(type_value, name.name, value.loc);
    };
    
    // TODO: do this if it is a slice as well. 
    //       and really should do it for locals as well if the rhs happens to be constant. 
    @if(false)  // :BringBackDataNames
    if self.get_type(value.ty).is(.Ptr) {
        @debug_assert(value.expr&.is(.Value));
        opts := self.get_build_options();
        if opts.retain_function_names {
            addr := value.expr.Value.bytes.Small._0;
            self.baked.vmem.data_names&.insert(addr, name.name);
        };
    };
    
    .Ok
}

fn eval(self: *SelfHosted, expr: *FatExpr, $T: Type) Maybe(T) #generic = {
    t := self.get_or_create_type(T);
    value := @check(self.immediate_eval_expr(expr, t)) return;
    if value&.len() != size_of(T) {
        return(@err("size mismatch in constant evaluation %\nTODO: the compiler should have caught this before and failed a typecheck.", log(value&, self, t)))
    };
    (Ok = T.assume_cast(value&)[])
}

fn set(self: *SelfHosted, expr: *FatExpr, $T: Type, value: T) void #generic = {
    value := self.to_values(T, value);
    expr.set(value, self.get_or_create_type(T));
}

fn handle_finish_rec_type(self: *SelfHosted, hole: Type, name: Var, value: *FatExpr) Maybe(void) = {
    if false {
        @println("handle_finish_rec_type % % %", name.scope.as_index(), self.pool.get(name.name), hole);
        if self.pool.get(name.name) == "QbeModule" {
            each self.imports { name, value |
                @println("- %: %", self.pool.get(name), value[].as_index());
            };
            println("---");
        };
    };
    result_type := @check(self.eval(value, Type)) return;
    assert(result_type != hole, "ICE: @rec resolved to itself");
    self.update_placeholder(hole, result_type, name.name);
    .Ok
}

fn save_const_values(self: *SelfHosted, name: Var, value: Values, final_ty: Type, loc: Span) CRes(void) = {
    self.save_const(name, (Value = (bytes = value, coerced = false)), final_ty, loc)
}

fn save_const(self: *SelfHosted, name: Var, val_expr: Expr, final_ty: Type, loc: Span) CRes(void) #once = {
    @debug_assert(name.kind == .Const, "tried to save non-constant");
    // TODO: do i have to check if someone is already working on this constant? -- Jul 30
    val := self.scopes.get_var_data(name);
    // TODO: this used to work before new overloading rework. -- Aug 27
    //if val._1&.is(.Finished) {
    //    return(@err("tried to re-save constant %", name&.log(self)));
    //};
    //if !val._0.expr&.is(.Poison) {
    //    return(@err("tried to stomp constant %", name&.log(self)));
    //};
    
    val.expr.expr = val_expr;
    val.expr.ty = final_ty;
    // TODO: don't allocate for this. just use .temporary
    val.set_type(Finished = final_ty);
    //self.scopes.put_var_type(name, final_ty);
    .Ok
}

fn ask_coerce_const_expr(self: *SelfHosted, expr: *FatExpr, want: Type, query: bool) Maybe(bool) = {
    found := expr.ty;
    if found == want {
        return(Ok = true);
    };
    // can't do this because it mightn ot be a value
    //@assert(!expr.expr.Value.coerced, "const mismatch. % vs % but already coerced", self.log_type(found), self.log_type(want));
    // TODO: set coereced? 
    
    want_info := self.get_type(want);
    found_info := self.get_type(found);
    if found == OverloadSet {
        @match(want_info) {
            fn Fn(f_ty) => {
                expr.done = false;  // do this early in case we suspend, we don't want to give up on doing the coersion
                os := @check(self.eval(expr, OverloadSet)) return;
                fid := @check(self.resolve_by_type(os, f_ty[], expr.loc)) return;
                if(query, => return(Ok = true));
                self.set(expr, FuncId, fid); // :HERE
                return(Ok = true);
            }
            fn FnPtr(f) => {
                expr.done = false;  // do this early in case we suspend, we don't want to give up on doing the coersion
                os := @check(self.eval(expr, OverloadSet)) return;
                fid := @check(self.resolve_by_type(os, f.ty, expr.loc)) return;
                if(query, => return(Ok = true));
                
                // no need to typecheck that `fid` matches `want`, we only got it by resolving in the overloadset. 
                self.make_fn_ptr_expr(expr, fid, want);
                @check(self.compile_fn_ptr(expr)) return;
                return(Ok = true);
            }
            @default => ();
        };
    };
    if want == FuncId {
        @if_let(found_info) fn Fn() => {
            if(query, => return(Ok = true));
            expr.ty = FuncId;
            return(Ok = true);
        };
    };
    
    if found == FuncId || found_info.is(.Fn) {
        @match(want_info) {
            fn Fn(f_ty) => {
                expr.done = false;  // do this early in case we suspend, we don't want to give up on doing the coersion
                fid := @check(self.eval(expr, FuncId)) return;
                @check(self.check_promote_id_to_func(fid, f_ty[])) return;
                if(query, => return(Ok = true));
                
                expr.ty = want;
                // TODO: typecheck args
                return(Ok = true);
            }
            fn FnPtr(f_ty) => {
                expr.done = false;
                fid := @check(self.eval(expr, FuncId)) return;
                @check(self.check_promote_id_to_func(fid, f_ty.ty)) return;
                self.make_fn_ptr_expr(expr, fid, UnknownType);  // TODO: why am i saying `want` directly in the overloadset case but not here
                // TODO: typecheck
                @check(self.compile_expr(expr, (Specific = want))) return;
                return(Ok = true);
            }
            fn VoidPtr() => {
                if(query, => return(Ok = true));
                fid := @check(self.eval(expr, FuncId)) return;
                self.make_fn_ptr_expr(expr, fid, rawptr);  // :get_or_create_type
                // TODO: could call compile_expr/compile_fn_expr instead of imm+took
                return(Ok = true);
            }
            @default => ();
        };
    };

    @match(found_info) {
        fn FnPtr(f_ty) => {
            if want == rawptr {
                if(query, => return(Ok = true));
                expr.ty = rawptr;
                return(Ok = true);
            };
        }
        fn void() => {
            while => want_info.is(.Named) {
                want_info = self.get_type(want_info.Named._0);
            };
            @if_let(want_info) fn Struct(f) => {
                each f.fields { f | 
                    if f.get_default().is_none() {
                        // TODO: better error message
                        //return(@err("had non-default fields (or if it's a call: expected function args) (or if its a block expected non-void return)"));
                        return(Ok = false);
                    };
                };
                if(query, => return(Ok = true));
                expr[] = empty_struct_literal(expr.loc);
                expr.ty = want;
                @check(self.construct_struct_literal(expr, (Specific = want))) return;
                return(Ok = true);
            };
        }
        fn Int(have_int) => {
            @match(want_info) {
                fn Int(want_int) => {
                    min, max := want_int[].range();
                    @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
                    value := expr.expr.Value.bytes&;
                    v := @try(value.int_value(have_int[])) return;
                    if v <= max && v >= min {
                        if(query, => return(Ok = true));
                        value.adjust_int_length(want_int);
                        expr.ty = want;
                        return(Ok = true);
                    };
                }
                fn F64() => {
                    max := 1.shift_left(MANTISSA_DIGITS_f64) - 1;
                    min := -max; // TODO: is that true? 
                    @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
                    value := expr.expr.Value.bytes&;
                    i := @try(value.int_value(have_int[])) return;
                    if i <= max && i >= min {
                        if(query, => return(Ok = true));
                        self.set(expr, f64, i.float());
                        return(Ok = true);
                    };
                }
                fn F32() => { // TODO: copy paste
                    max := 1.shift_left(MANTISSA_DIGITS_f32) - 1;
                    min := -max; // TODO: is that true? 
                    @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
                    value := expr.expr.Value.bytes&;
                    i := @try(value.int_value(have_int[])) return;
                    if i <= max && i >= min {
                        if(query, => return(Ok = true));
                        f: f32 = i.float().cast();
                        self.set(expr, f32, f);
                        return(Ok = true);
                    };
                }
                @default => ();
            };
        }
        fn F64() => {
            @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
            value := expr.expr.Value.bytes&;
            vf := f64.assume_cast(value)[];
            @match(want_info) {
                fn Int(want_int) => {
                    v := vf.int();
                    if v.float() == vf {
                        min, max := want_int[].range();
                        if v <= max && v >= min {
                            if(query, => return(Ok = true));
                            value[] = self.to_values(i64, v);
                            value.adjust_int_length(want_int);
                            expr.ty = want;
                            return(Ok = true);
                        };
                    };
                }
                fn F32() => {
                    v: f32 = vf.cast();
                    // if v.cast() == vf {  // TODO: maybe you want this but it seems too strict to be usable
                    // TODO: if vf < MAX_F32 && vf > MIN_F32 {
                    if(query, => return(Ok = true));
                    self.set(expr, f32, v);
                    return(Ok = true);
                }
                @default => ();
            };
        }
        @default => ();
    };
    (Ok = false)
}

fn make_fn_ptr_expr(self: *SelfHosted, expr: *FatExpr, fid: FuncId, want: Type) void = {
    self.took_pointer_value(fid);
    self.set(expr, FuncId, fid);
    expr.done = false;
    e := self.box(expr[]);  // :fucked segfault if you inline this!!!!!! -- Aug 6
    expr.expr = (FnPtr = e);
    expr.ty = want;
    expr.done = false;
}

// TODO: you can never do this to a constant directly in case its aliased once i have const ptrs. 
fn coerce_const_expr(self: *SelfHosted, expr: *FatExpr, req: ResultType, query: bool) Maybe(void) = {
    if req.specific() { want | 
        ok := @check(self.ask_coerce_const_expr(expr, want, query)) return;
        if !ok {
            if self.ask_can_assign(want, expr.ty) {
                return(.Ok);
            };
            return(Err = self.error(CoerceConst = (span = Span.zeroed(), wanted = want, found = expr.ty, expr = expr)));
        };
    };
    .Ok
}

fn can_assign(self: *SelfHosted, want: Type, found: Type) Maybe(void) = {
    ::if(Maybe(void));
    if self.ask_can_assign(want, found) {
        .Ok
    } else {
        // TODO: dont allocate + fmt here beacause overload resolution spams this. 
        (Err = self.error(TypeMismatch = (span = Span.zeroed(), wanted = want, found = found)))
    }
}

fn ask_can_assign(self: *SelfHosted, want: Type, found: Type) bool #inline = 
    want == found || found == Never || ask_can_assign_inner(self, want, found);

fn ask_can_assign_inner(self: *SelfHosted, want: Type, found: Type) bool = {
    // do need this tho sadly. TODO: fix me!
    if(found == UnknownType, => return(true)); // TODO: remove
    
    want_info  := self.get_type(want);
    found_info := self.get_type(found);
    @match(found_info) {
        // Named means @rec which should be transparent
        fn Named(f) => return(self.ask_can_assign(want, f._0));
        fn Fn(f) => {
            if want == FuncId {
                return(true);
            };
        }
        fn Label(f) => {
            if want == LabelId {
                // TODO: typecheck!!
                return(true);
            };
        }
        fn Struct(found_struct) => {
            @if_let(want_info) fn Struct(want_struct) => {
                if found_struct.is_tuple && want_struct.is_tuple && found_struct.fields.len == want_struct.fields.len {
                    range(0, want_struct.fields.len) { i | 
                        w := want_struct.fields[i].ty;
                        f := found_struct.fields[i].ty;
                        if !self.ask_can_assign(w, f) {
                            return(false);
                        };
                    };
                    return(true);
                };
            };
        }
        fn Int(a) => {
            @if_let(want_info) fn Int(b) => {
                if a.bit_count == 64 && b.bit_count != 32 && b.bit_count != 16 && b.bit_count != 8 && b.bit_count != 64 {
                    return(true);
                };
                if b.bit_count == 64 && a.bit_count != 32 && a.bit_count != 16 && a.bit_count != 8 && a.bit_count != 64 {
                    return(true);
                };
            };
        };
        @default => ();
    };
    @match(want_info) {
        // Named means @rec which should be transparent 
        fn Named(f) => return(self.ask_can_assign(f._0, found));
        fn Fn(f) => {
            if found == FuncId {
                // TODO: typecheck!!
                return(true);
            };
        }
        fn Label(f) => {
            if found == LabelId {
                // TODO: typecheck!!
                return(true);
            };
        }
        @default => ();
    };
    
    false
}

fn resolve_by_type(self: *SelfHosted, os: OverloadSet, f_ty: FnType, loc: Span) Maybe(FuncId) = {
    // This is mostly used for macros so its a big win to not spin up a whole operation just to notice that @match still goes to the same place. 
    // saves ~10% of the time Aug 30.
    data := self.dispatch.overloads&.nested_index(os.as_index());
    key: OverloadKey = (arg = f_ty.arg, req = f_ty.ret);
    if data.inline_cache._0& == key& {
        return(Ok = data.inline_cache._1);
    };
    zone := zone_begin(.SemaOverloads); // TODO: defer
    @if(ENABLE_TRACY) {
        real_name := self.pool.get(data.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    res := self.resolve_in_overload_set(f_ty, os, loc);
    zone_end(zone);
    fid := @check(res) return;
    data.inline_cache = (key, fid);
    (Ok = fid)
}

::?FnType;
fn immediate_eval_expr(self: *SelfHosted, expr: *FatExpr, ret_ty: Type) Maybe(Values) = {
    self.last_loc = expr.loc;
    @debug_assert(!ret_ty.is_unknown(), "immediate_eval_expr unknown");
    old_func := self.dispatch.enclosing_function;
    self.dispatch.enclosing_function = .None;
    ::?Values;
    
    @match(expr.expr&) {
        fn Call(f) => {
            // goes from 30000 lit_fn to 7000. saved 55/645 ms.   -- Sep 2
            // this might fold so it should go before a check_quick_eval. 
            @check(self.compile_call(expr, (Specific = ret_ty))) return;
        }
        fn ContextualField(name) => {
            // saves 500
            @check(self.contextual_field(name[], expr, ret_ty)) return;
        }
        fn String(ident) => {
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        fn Closure() => {
            // saves 500
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        fn FromBitLiteral() => {
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        fn PrefixMacro() => {
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        @default => ();
    };
    
    // HACK
    // this makes size match for int literals that get down casted in const_eval_any
    // TODO: this should work on non-int types but it seems to get confused by Never.  -- Nov 12
    //       still doesnt work for everything but need FnPtr for import_c scope experiments -- Feb 3, 2025
    //       :FUCKED sometimes things really really need the coerce but won't get it!
    if expr.expr&.is(.Value) && expr.ty != ret_ty && ((@is(self.get_type(ret_ty).tag(), .Int, .FnPtr)) || expr.ty == OverloadSet) {
        @check(self.coerce_const_expr(expr, (Specific = ret_ty), false)) return;
    };
    
    if self.check_quick_eval(expr, ret_ty) { val |
        self.dispatch.enclosing_function = old_func;
        return(Ok = val);
    };
    
    // Can't just try to compile_expr here!
    // Since were evaluating in const context, any functions that are called in the expression weren't added to anyone's callees. 
    // So we want to say we need to recompile the expression, adding to callees of the lit_fn we're about to make. 

    // If its already a trivial function call, there's nothing else we can do to simplify, 
    // so we have to just yield on the function. 
    @if_let(expr.expr&) fn Call(f) => {
        // TODO: `if self.check_quick_eval(f.arg, f_ty.arg) { arg_value |` 
        //       instead of requiring void so its consistant with the fn_ptr version.
        //       tho also the call_dynamic is kinda sketchy (can only handle simple cases) and 
        //       i could just get rid of it and always generate a function that passes the arguments.    -- Jul 31
        if expr.expr&.is(.Call) && f.arg.is_raw_unit() && f.f.expr&.is(.Value) && self.get_type(f.f.ty).is(.Fn).or(f.f.ty == FuncId) {
            fid := FuncId.assume_cast(f.f.expr.Value.bytes&)[];
            self.dispatch.enclosing_function = old_func;
            func := self.get_function(fid);
            if func.body&.is(.Normal) {
                if func.get_flag(.EnsuredCompiled) {
                    if self.check_quick_eval(func.body.Normal&, ret_ty) { val |
                        self.dispatch.enclosing_function = old_func;
                        return(Ok = val);
                    };
                } else {
                    self.dispatch.function_in_progress&.set(fid.as_index());
                    self.dispatch.enclosing_function = old_func;
                    return(Suspend = self.wait_for(CompileBody = fid));
                };
            };
            return(Suspend = self.wait_for(Jit = fid));
        };
    };
    
    // Different from the version in check_quick_eval because this accepts unfinished ones too. 
    // This is just an optimisation to avoid an intermediate Func on the first use of a given constant. 
    @if_let(expr.expr&) fn GetVar(name) => {
        if name.kind == .Const {
            // Don't need to call find_const here because check_quick_eval already did the easy case. 
            self.dispatch.enclosing_function = old_func;
            return(Suspend = self.wait_for(EvalConstant = (name = name[], requested = (Specific = ret_ty))));
        };
    };
    @if_let(expr.expr&) fn FnPtr(f_expr) => {
        fid := @check(self.eval(f_expr[], FuncId)) return;
        fn_ptr := or self.get_fn_callable(fid) {
            opts := self.get_build_options();
            // TODO: really should be putting this in someone's callees list.
            //       but for jit we fix when you try to call and for aot we notice when you relocate it so its kinda fine... 
            //       --- Sep 21
            @check(self.infer_arguments(fid)) return;
            @check(self.infer_return(fid)) return;
            // TODO: fixth ecases wehre you blindly return .Ok
            @try(self.create_jit_shim(fid)) return;
            self.get_fn_callable(fid).expect("jit shim to be ready")
        };
        
        
        values := self.to_values(rawptr, fn_ptr);
        @debug_assert(values.Small._0 != 0, "Expr:FnPtr cannot return null");
        expr.expr = (Value = (bytes = values, coerced = false));
        expr.done = true;
        return(Ok = values);
    };
    
    // TODO: should probably include coerce_const_expr in check_quick_eval so it nests better. 
    if expr.expr&.is(.Value) {
        ok := @check(self.ask_coerce_const_expr(expr, ret_ty, false)) return;
        if ok {
            if self.check_quick_eval(expr, ret_ty) { val |            
                self.dispatch.enclosing_function = old_func;
                return(Ok = val);
            };
        };
    };
    
    // The expression is too complex to deal with here. 
    // So box it into a function, compile that normally, and then just call into it with no arguments when we come around again.
    // This operation is make_lit_function from the old compiler. 
    // We know we'll yield to compile the new function, and don't want its body to alias the old expr. 
    
    bindings: List(Binding) = list(self.get_alloc());
    bindings&.if_empty_add_unit();
    arg: Pattern = (bindings = bindings.as_raw(), loc = expr.loc);
    def: FnDef = (name = .None, arg = arg, ret = (Finished = ret_ty), tags = list(temp()), loc = expr.loc);
    //def.line = self.pool.insert_borrowed(self.comp().log(expr), self.get_alloc());
    
    // we might have already tried to compile the expression in a different context,
    // but we need to do it again so we're sure to add callees to the newly created function,
    // so we don't try to call something that isn't compiled yet. 
    walk: MarkNotDone = ();
    walk&.walk_expr(expr);
    fake_func := make_func(def, (Some = expr), false, false);
    // We didn't bother setting a scope because we don't need one, the expression will already have been resolved. 
    fake_func&.set_flag(.ResolvedBody);
    fake_func&.set_flag(.ResolvedSign);
    fake_func&.set_flag(.SyntheticImmEval);
    self.dispatch.lit_fn_count += 1;
    fake_func&.set_flag(.ComptimeOnly);
    // Since we know this function has no args, we'll always fold it. 
    // This makes sure that if we start analyzing the expression without noticing that its actually in const-context,
    // we won't add a callee that becomes unnecessary when the function call gets reduced to an inlined value eventually. 
    // This fixes a bunch of .ComptimeOnly callees that the AOT backends choke on.  -- Aug 21
    // The SyntheticImmEval is used to prevent a loop where we try to fold by calling imm_eval on inner nested expressions? maybe?
    // TODO: i cant quite understand why, but you need something like that even if you don't add this extra TryConstFold. 
    // TODO: this introduces a ICE: Tried to call un-compiled function. for the test `floats`
    //       happens at `assert_eq(5.neg(), 5.7.neg().int());` and only if neg is #fold
    fake_func&.set_flag(.TryConstantFold);
    fake_func.finished_arg = (Some = void);
    fake_func.finished_ret = (Some = ret_ty);
   
    fid := self.add_function(fake_func);
    f_expr := self.box(self.to_expr(FuncId, fid, expr.loc));
    expr.expr = (Call = (f = f_expr, arg = self.make_unit_expr(expr.loc)));  // so we try this task again, we get an easy function call.
    expr.done = false;
    self.dispatch.enclosing_function = old_func;
    self.dispatch.function_in_progress&.set(fid.as_index());
    (Suspend = self.wait_for(CompileBody = fid))
}

// TODO: decide if need to set 
fn check_quick_eval(self: *SelfHosted, expr: *FatExpr, ret_ty: Type) ?Values = {
    @match(expr.expr&) {
        fn Value(f) => (Some = f.bytes&.clone(self.get_alloc()));
        fn GetVar(f) => {
            if f.kind == .Const {
                // This is a super common case because you type `arg: i64` a lot. 
                res := self.find_const_non_blocking(f[]);
                if res { f |
                    // TODO: this is wrong, you need to const coerce before doing it and not lose information if there's a type error. 
                    //       but without this you get an infinite loop on #link_rename
                    //       TODO: revisit this, #link_rename doesn't exist anymore but whatever the problem it revealed probably does -- Jun 15, 2025
                    // TODO: make a test that fails if you just do this now!
                    expr.expr = (Value = (bytes = f._0&.clone(self.get_alloc()), coerced = false));
                    expr.ty = f._1;
                    // We can't report a type erorr from here so just let the slow path deal with it. 
                    if(ret_ty == f._1, => return(Some = f._0));
                };
            };
            .None
        }
        fn Block(it) => {
            if it.block_resolved && it.body.is_empty() {
                return(self.check_quick_eval(it.result, ret_ty));
            };
            .None
        }
        fn Tuple(parts) => {
            all: List(u8) = list(self.get_alloc());
            each parts { part |
                // TODO: tuple_types
                val := or self.check_quick_eval(part, part.ty) {
                    return(.None)
                };
                // Required since deconstruct_values (which we use for dyn_call) expects correctly aligned fields. 
                self.aligned_append_value(all&, val, part.ty);
            };
            (Some = (Big = all.as_raw()))
        }
        fn Call(it) => {
            if it.f.expr&.is(.Value) {
                @if_let(self.get_type(it.f.ty)) fn FnPtr(f) => {
                    if self.check_quick_eval(it.arg, f.ty.arg) { arg_value |
                        if it.arg.ty != f.ty.arg {| // TODO
                            return(.None);
                        };
                        f_ptr := i64.assume_cast(it.f.expr.Value.bytes&)[];
                        self.finish_layout_deep(it.f.ty);
                        zone := zone_begin(.CallDynamic);
                        res := self.call_dynamic_values(f_ptr, f.ty, arg_value&.bytes());
                        zone_end(zone);
                        expr.set(res&.clone(self.get_alloc()), f.ty.ret);
                        return(Some = res);
                    };
                };
                
                @if_let(self.get_type(it.f.ty)) fn Fn(f) => {
                    if self.check_quick_eval(it.arg, f.arg) { arg_value |
                        if it.arg.ty != f.arg {| // TODO
                            return(.None);
                        };
                        f_id := FuncId.assume_cast(it.f.expr.Value.bytes&)[];
                        if self.get_fn_callable(f_id) { f_ptr |
                            func := self.get_function(f_id);
                            if(func.get_flag(.AvoidJitShim), => return(.None));
                            self.finish_layout_deep(it.f.ty);
                            res := self.call_dynamic_values(f_ptr.int_from_rawptr(), f[], arg_value&.bytes());
                            expr.set(res&.clone(self.get_alloc()), f.ret);
                        };
                    };
                };
            };
            .None
        }
        fn Deref(it) => {
            if self.check_quick_eval(it[], self.ptr_ty(ret_ty)) { ptr |
                info := self.get_info(ret_ty);
                if info.is_sized {
                    @debug_assert(ptr&.is(.Small), "want inline ptr");
                    bytes: []u8 = (ptr = u8.ptr_from_int(ptr.Small._0), len = info.stride_bytes.zext());
                    return(Some = self.from_bytes(bytes));
                };
            };
            .None
        }
        fn PtrOffset(it) => {
            if self.check_quick_eval(it.ptr, it.ptr.ty) { ptr |
                @debug_assert(ptr&.is(.Small) && ptr.Small._1 == 8);
                ptr.Small._0 += it.bytes;
                expr.expr = (Value = (bytes = ptr, coerced = false));
                return(Some = ptr);
            };
            .None
        }
        @default => .None;
    }
}

fn create_slice_type(self: *SelfHosted, inner: Type, loc: Span) Maybe(Type) = {
    // TODO: this is called 3208 times when self-compiling. Aug 29. 
    f     := @unwrap(self.env.make_slice_t, "slice type not ready!") return;  
    arg   := self.to_expr(Type, inner, loc);
    value := @check(self.invoke(f, self.box(arg), Type, loc)) return;  // :get_or_create_type
    (Ok = Type.assume_cast(value&)[])
}

fn invoke(self: *SelfHosted, callee: FuncId, argument: *FatExpr, return_type: Type, loc: Span) Maybe(Values) = {
    f     := self.to_expr(FuncId, callee, loc);
    s_ty  := synthetic_ty((Call = (f = self.box(f), arg = argument)), loc, Type);
    s_ty  := self.box(s_ty);
    value := self.poll_in_place(Values, => self.immediate_eval_expr(s_ty, return_type));
    value := @try(value) return;  // TODO: miscompilation if you inline this ^ :fucked
    (Ok = value)
} 

///////////////////////
/// Macro Expansion ///

fn compile_prefix_macro(self: *SelfHosted, expr: *FatExpr) Maybe(void) #once = {
    // TODO: Bring back tag checks so i don't have to be paranoid!!
    //       this annoys be enough that im tempted to go back to inlining all of these. 
    //       tho maybe #once is reassuring enough. think about it. -- Jul 22
    @debug_assert(expr.expr&.is(.PrefixMacro)); 
    // TODO: you probably want this but constants don't have it (really they shouldn't be compiled without being wrapped in a function?)
    //@err_assert( self.dispatch.enclosing_function.is_some(), "ice: macro expansion must be in function context") return;
    invocation := expr.expr.PrefixMacro&;
    
    // This allows @a E; instead of @a(E);
    if invocation.arg.is_raw_unit() {
        temp := invocation.arg;
        invocation.arg = invocation.target;
        invocation.target = temp;
    };
    
    fat_expr_type := or self.env.fat_expr_type {
        return(self.early_builtin_prefix_macro(expr))
    };
    pair := self.tuple_of(@slice(fat_expr_type, fat_expr_type));
    
    single := invocation.target.is_raw_unit();
    @check(self.compile_expr(invocation.handler, .None)) return;
    if invocation.handler.ty == OverloadSet {
        os := @check(self.eval(invocation.handler, OverloadSet)) return; 
        ::if(FnType); 
        f_ty: FnType = if single {|  // TODO: wrong! what if they actually passed unit?
            (arg = fat_expr_type, ret = fat_expr_type, unary = true)
        } else {
            (arg = pair, ret = fat_expr_type, unary = false)
        };
        fid := @check(self.resolve_by_type(os, f_ty, invocation.handler.loc)) return;
        self.set(invocation.handler, FuncId, fid);
    };
    
    fid := @check(self.eval(invocation.handler, FuncId)) return; 
    func := self.get_function(fid);
    if !func.get_flag(.Macro) {
        self.codemap.show_error_line(func.loc, true /*TODO*/);
    };
    @err_assert(func.get_flag(.Macro), "Tried to invoke non-macro % (missing #macro or missing overload)", self.pool.get(func.name)) return;
    fn_ptr := or self.get_fn_callable(fid) {| 
        return(Suspend = self.wait_for(Jit = fid))
    };
    
    self.last_loc = expr.loc;
    f_ty := func.finished_ty().expect("known type once compiled");
    invocation := invocation[];
    assert(f_ty.ret == fat_expr_type, "tried to call macro with bad ret type. missing overload? \nresolve_in_overloadset assumes someone later will typecheck so giving a garabge match is fine");
    assert(f_ty.arg.eq(fat_expr_type).or(f_ty.arg == pair), "tried to call macro with bad arg type. missing overload? \nresolve_in_overloadset assumes someone later will typecheck so giving a garabge match is fine");
    expr.expr = (Poison = .InProgressMacro);  // they call me RefCell<FatExpr> because im always refing cells 
    callee := assume_types_fn(Arg = Ty(FatExpr, FatExpr), Ret = FatExpr, ptr = fn_ptr);
    self.bump_dirty_new();
    zone := zone_begin(.CallDynamic);
    expr[] = callee(invocation.arg[], invocation.target[]);  // :call_dynamic_values
    zone_end(zone);
    .Ok
}

// If we're early in bootstrapping and haven't compiled the FatExpr type yet, so some special handling.
fn early_builtin_prefix_macro(self: *SelfHosted, expr: *FatExpr) Maybe(void) = {
    invocation := expr.expr.PrefixMacro&;
    name := @match(invocation.handler.expr&) {
        fn GetVar(v) => v.name;
        fn UndeclaredVar(v) => v.name;
        fn GetNamed(v) => v[];
        @default => return(@err("macro calls must be GetVar while bootstrapping. tried to run something too compilicated too soon: %: %", invocation.handler.expr&.tag(), invocation.handler.log(self)));
    };
    @switch(name) {
        @case(Flag.builtin.ident()) => {
            expr[] = @try(self.builtin_macro(invocation.arg[])) return;
        };
        @case(Flag.struct.ident()) => {
            expr[] = @check(self.struct_macro(invocation.arg)) return;
        };
        @case(Flag.enum.ident()) => {
            expr[] = @check(self.enum_macro(invocation.arg, invocation.target)) return;
        };
        @case(Flag.tagged.ident()) => {
            @err_assert(invocation.target.is_raw_unit(), "@tagged expected single arg") return;
            expr[] = @check(self.tagged_macro(invocation.arg)) return;
        };
        @case(Flag.late.ident()) => {
            expr.set(unit_value, self.get_or_create_type(void));
        };
        @default => {
            return(@err("tried to call non-builtin macro '%' while bootstrapping.", self.pool.get(name)));
        };
    };
    
    .Ok
}

fn builtin_macro(self: *SelfHosted, arg: FatExpr) Res(FatExpr) = {
    name := @unwrap(arg&.ident(), "@builtin arg must be String literal") return;
    value, type := @try(self.builtin_macro(name)) return;
    arg&.set(value, type);
    (Ok = arg)
}

fn builtin_macro(self: *SelfHosted, builtin_name: Symbol) Result(Ty(Values, Type), *CompileError) = {
    builtin_type :: fn($T: Type) void => {
        ptr := self.get_or_create_type(T);
        val := ptr_cast_unchecked(Type, u32, ptr&)[];
        return(Ok = ((Small = (val.zext(), 4)), self.get_or_create_type(Type)));
    };
    @switch(builtin_name) {
        @case(Flag.i64.ident())         => builtin_type(i64);
        @case(Flag.bool.ident())        => builtin_type(bool);
        @case(Flag.OverloadSet.ident()) => builtin_type(OverloadSet);
        @case(Flag.ScopeId.ident())     => builtin_type(ScopeId);
        @case(Flag.FuncId.ident())      => builtin_type(FuncId);
        @case(Flag.LabelId.ident())     => builtin_type(LabelId);
        @case(Flag.Symbol.ident())      => builtin_type(Symbol);
        @case(Flag.rawptr.ident())      => builtin_type(rawptr);
        @case(Flag.Type.ident())        => builtin_type(Type);
        @case(Flag.void.ident())        => builtin_type(void);
        @case(Flag.Never.ident())       => builtin_type(Never);
        @case(Flag.true.ident())        => return(Ok = (((Small = (1, 1)), self.get_or_create_type(bool))));
        @case(Flag.false.ident())       => return(Ok = (((Small = (0, 1)), self.get_or_create_type(bool))));
        @case(Flag.f64.ident())         => builtin_type(f64);
        @case(Flag.f32.ident())         => builtin_type(f32);
        @case(Flag.UnknownType.ident()) => builtin_type(UnknownType);
        @case(Flag.CVariadic.ident())   => builtin_type(CVariadicType);
        @default => {
            return(@err("unknown @builtin '%'.", self.pool.get(builtin_name)));
        };
    };
    unreachable()
}

fn struct_type(self: *SelfHosted, pattern: *Pattern, is_union: bool) Maybe(Type) #once = {
    fields: List(Field) = list(pattern.bindings.len, self.get_alloc());
    scope := NOSCOPE;
    each pattern.bindings { b | 
        if b.kind == .Const {
            if scope == NOSCOPE {
                // TODO: this scope means a totally different thing (nothing), its not where variables in the struct will be looked up
                scope = self.scopes.new_scope(scope_from_index(0), self.get_alloc(), pattern.loc);
            };
            name := b.ident().expect("field name");
            expr := @unwrap(b.get_default(), "constant field % must have a value.", self.pool.get(name)) return;
            v := self.unique_const(name);
            self.scopes.put_constant(v, expr[], b.ty);
            self.scopes[scope].lookup&.insert(name, v, self.get_alloc());
            expr.expr = (GetVar = v);  // Save our work because we might yield on a later field. 
        };
    };
    
    // TODO: you want the above to stay in a seperate loop and create the type before we start evaluating types for runtime fields, 
    //       so those expressions can reference local constants also declared in the struct. 
    
    each pattern.bindings { b | 
        if b.kind != .Const {
            fields&.push(@check(self.make_runtime_field(b)) return);
        };
    };
    
    @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
    (Ok = self.intern_type(Struct = (
        fields = fields.as_raw(),
        layout_done = false,
        is_tuple = false,
        is_union = is_union,
        scope = scope,
    )))
}

fn make_runtime_field(self: *SelfHosted, b: *Binding) Maybe(Field) #inline = {
    name := b.ident().expect("field name");

    ty := or @check(self.infer_type(b.ty&)) return {
        // We need to know the type before we can size the struct. 
        default := @unwrap(b.get_default(), "@struct field without type requires default value.") return;
        @err_assert(!is_rec_hack(default), "cannot use @rec on a non-constant value.") return;
        @check(self.compile_expr(default, .None)) return;
        b.ty = (Finished = default.ty);
        default.ty
    };
    
    ::if_opt(*FatExpr, Var);
    default := if b.get_default() { (expr: *FatExpr) |
        v := self.unique_const(name);
        self.scopes.put_constant(v, expr[], b.ty);
        expr.expr = (GetVar = v);  // Save our work because we might yield on a later field. 
        v
    } else {
        zeroed(Var)
    };
    (Ok = (
        name = name,
        ty = ty,
        nullable_tag = b.nullable_tag,
        default = default,
        byte_offset = FIELD_LAYOUT_NOT_DONE,
        // even if they didn't provide a default expression, this slot will still have location info for the binding
        loc = b.default.loc, 
    ))
}

fn return_macro_type(self: *SelfHosted, expr: *FatExpr, type: Type) Maybe(FatExpr) = {
    self.set(expr, Type, type);
    self.type_extra&.nested_index(type.as_index())[].loc.low = expr.loc.low;
    (Ok = expr[])
}

fn union_macro(self: *SelfHosted, fields: *FatExpr) Maybe(FatExpr) = {
    @err_assert(fields.expr&.is(.StructLiteralP), "expected map literal: (name: Type, ... ) for @union") return;
    ty := @check(self.struct_type(fields.expr.StructLiteralP&, true)) return;
    self.return_macro_type(fields, ty)
}

fn struct_macro(self: *SelfHosted, expr: *FatExpr) Maybe(FatExpr) = {
    msg :: "expected map literal: (name: Type, ... ) for @struct";
    ty := @match(expr.expr&) {
        fn StructLiteralP(it) => {
            @check(self.struct_type(it, false)) return
        }
        fn Value(f) => {
            if expr.ty == self.get_or_create_type(void) {
                self.intern_type(Struct = (
                    fields = empty(),
                    layout_done = false,
                    is_tuple = false,
                    is_union = false,
                    scope = NOSCOPE,
                ))
            } else {
                return(@err(msg))
            }
        }
        fn Block(it) => {
            @err_assert(it.result.is_raw_unit(), "@struct {} block should not have a result expression") return; 
            @check(self.ensure_resolved(it)) return;    
            
            fields := Field.list(self.get_alloc());
            
            // TODO: don't iterate twice 
            if !it.hoisted_constants {
                @check(self.hoist_constants(it.body.items())) return;
            };
            
            each it.body { s | 
                // TODO: super confusing if you suspend here. same problem for the other fn struct_type
                @match(s.stmt&) {
                    fn Decl(b) => fields&.push(@check(self.make_runtime_field(b[])) return);
                    fn Noop()      => ();
                    // TODO: allow ConstEval
                    @default => @err_assert(false, "invalid stmt type in @struct {} block: %", s.stmt&.tag()) return; 
                };
            };
            scope := it.scope;
            self.intern_type(Struct = (
                fields = fields.as_raw(),
                layout_done = false,
                is_tuple = false,
                is_union = false,
                scope = scope, 
            ))
        }
        @default => return(@err(msg));
    };
    
    self.return_macro_type(expr, ty)
}

fn enum_macro(self: *SelfHosted, arg: *FatExpr, target: *FatExpr) Maybe(FatExpr) = {
    type := @check(self.eval(arg, Type)) return;

    F :: Ty(Symbol, Values);
    fields: List(F) = list(self.get_alloc());
    as_int: ?IntTypeInfo = @match(self.get_type(type)) {
        fn Int(it) => (Some = it[]);
        @default => .None;
    };
    sequential := as_int.is_some();
    last := -1;
    @match(target.expr&) {
        fn StructLiteralP(pattern) => {
            each pattern.bindings { b | 
                name := @unwrap(b.ident(), "@enum case requires name") return;
                @err_assert(b.get_default().is_some(), "@enum expected case value") return;
                @err_assert(b.ty&.is(.Infer), "@enum case cannot have type annotation (use '=' instead of ':'") return;

                expr := b.get_default().expect("enum value");
                val  := @check(self.immediate_eval_expr(expr, type)) return;
                if sequential {
                    current := @try(val&.int_value(as_int.expect("int"))) return;
                    if current == last + 1 {
                        last += 1;
                    } else {
                        sequential = false;
                    };
                };
                fields&.push(@as(F) (name, val));
            };
            @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
        }
        fn Tuple(names) => {
            @err_assert(sequential, "@enum on tuple of names must be of int type") return;
            as_int := as_int.unwrap();
            repr := if(as_int.bit_count == 8 || as_int.bit_count == 16 || as_int.bit_count == 32, => as_int.bit_count / 8, => 8);
            //@err_assert(type == i64, "TODO: @enum(other-int-types)(Tuple)") return;
            enumerate names { i, name |
                name := @unwrap(name.ident(), "@enum expected ident") return;
                value := self.to_values(i64, i);
                value.Small._1 = repr.trunc();
                fields&.push(@as(F) (name, value));
            };
        }
        @default => return(@err("@enum expected struct literal or tuple of names"));
    };
    unique_ty := self.intern_type(Enum = (raw = type, fields = fields.as_raw(), sequential = sequential));
    self.return_macro_type(arg, unique_ty)
}

fn tagged_macro(self: *SelfHosted, cases_expr: *FatExpr) Maybe(FatExpr) = {
    @err_assert(cases_expr.expr&.is(.StructLiteralP), "@tagged expected map literal like `(name: Type, ...)`") return;
    pattern := cases_expr.expr.StructLiteralP&;
    
    F :: Ty(Symbol, Values);
    C :: Ty(Symbol, Type);
    tag_fields: List(F) = list(self.get_alloc());
    cases: List(C) = list(self.get_alloc());
    enumerate pattern.bindings { i, b |
        // TODO: allow as default so you can use .Name like you can with void?
        //       then need to store default in TypeInfo::Tagged as well. -- Jul 5
        @err_assert(b.get_default().is_none(), "use ':' not '=' with @tagged") return;
        type := @check(self.infer_type(b.ty&)) return;
        type := or type {
            // @tagged(s: i64, n) is valid and infers n as void.
            b.ty = (Finished = self.get_or_create_type(void));
            b.ty.Finished
        };
        name := @unwrap(b.ident(), "@tagged field requires name") return;
        tag_value := self.to_values(i64, i);
        tag_fields&.push(@as(F) (name, tag_value)); // :tag_enums_are_sequential
        cases&.push(@as(C) (name, type))
    };
    
    @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
    tag_type := self.intern_type(Enum = (raw = self.get_or_create_type(i64), fields = tag_fields.as_raw(), sequential = true));
    tagged_type := self.intern_type(Tagged = (cases = cases.as_raw(), tag = tag_type));
    self.return_macro_type(cases_expr, tagged_type)
}

fn compile_body(self: *SelfHosted, fid: FuncId) Maybe(void) #inline = {
    if self.get_function(fid).get_flag(.EnsuredCompiled) {
        .Ok
    } else {
        @assert(!self.dispatch.function_in_progress&.get(fid.as_index()), "unhanlded mutual recursion %", self.log_name(fid));
        self.dispatch.function_in_progress&.set(fid.as_index());
        (Suspend = self.wait_for(CompileBody = fid))
    }
}

// TODO: track if we're in unquote mode or placeholder mode.
Unquote :: @struct(compiler: *SelfHosted, placeholders: List(FatExpr));

// :UnquotePlaceholders
:: WalkAst(Unquote, *CompileError);

fn handle_expr(self: *Unquote, expr: *FatExpr) Result(DoMore, *CompileError) #once = {
    @match(expr.expr) {
        fn Unquote(arg) => {
            //expr_ty := self.compiler.env.fat_expr_type.expect("used unquote ast while bootstrapping");
            // Note: take <arg> but replace the whole <expr>
            idx := self.placeholders.len;
            self.placeholders&.push(arg[]);
            expr[] = (expr = (Placeholder = idx), loc = expr.loc, ty = UnknownType, done = false);
        }
        fn Placeholder(idx) => {
            @err_assert(idx < self.placeholders.len, "ICE: invalid unquote placeholder index %", idx) return;
            value := self.placeholders.index(idx);
            @err_assert(!value.expr&.is(.Poison), "ICE: missing placeholder for unquote") return;
            // This clone fixes a renumbering problem. :double_use_quote
            expr[] = self.compiler.clone(value);
            value.expr = (Poison = .PlaceholderUsed);
            value.done = false;
        }
        fn Quote() => {
            // TODO: add a simpler test case than the derive thing (which is what discovered this problem).
            // Don't go into nested !quote. This allows having macros expand to other macro calls without stomping eachother.
            // TODO: feels like you might still end up with two going on at once so need to have a monotonic id number for each expansion stored in the !placeholder.
            //       but so far this is good enough.
            return(Ok = .Break);
        }
        @default => ();
    };
    (Ok = .Continue)
}
fn handle_stmt(self: *Unquote, stmt: *FatStmt) Result(DoMore, *CompileError) #once = (Ok = .Continue);
fn handle_func(self: *Unquote, func: *Func)    Result(DoMore, *CompileError) #once = (Ok = .Continue);
fn handle_type(self: *Unquote, ty: *LazyType)  Result(DoMore, *CompileError) #once = (Ok = .Continue);
fn handle_pattern(self: *Unquote, p: *Pattern) Result(DoMore, *CompileError) #once = (Ok = .Continue);

fn ensure_resolved(self: *SelfHosted, block: *get_variant_type(Expr, .Block)) Maybe(void) = {
    if(block.block_resolved, => return(.Ok));
    
    r: ResolveScope = new(self, block.scope, self.last_loc);
    zone := zone_begin(.Scope); // TODO: defer
    res := r&.finish_resolving_block(block);
    zone_end(zone);
    @try(res) return;
    .Ok
}
