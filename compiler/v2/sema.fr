// the compiler of theseus 

ResultType :: @rec @tagged(
    Specific: Type,
    Returning: Type,
    Tuple: []ResultType,  // temporary storage! don't hold across yield points. 
    None
);

OverloadSetData :: @struct(
    ready: RsVec(OverloadOption),
    name: Symbol,
    pending: RsVec(FuncId),
);

OverloadOption :: @struct(
    func: FuncId,
    args: []ResultType,
    ret: ?Type, // For #generic, we might not know without the args
);

fn compile_stmt(self: *SelfHosted, stmt: *FatStmt) Maybe(void) = {
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Noop() => return(.Ok);
        fn Eval(expr) => return(self.compile_expr(expr, .None));
        fn DeclVar(f) => {
            assert(f.name.kind != .Const, "hit unhoisted constant");
            return(self.decl_var(f.name, f.ty&, f.value&))
        }
        fn Set(f) => {
            // TODO: or patterns in @match would be nice here. :lang
            tag := f.place.expr&.tag();
            // TODO: PrefixMacro is sketchy but makes []->.index work.
            if tag.eq(.PrefixMacro).or(tag == .GetVar).or(tag == .FieldAccess).or(tag == .Deref) {|
                @check(self.compile_place_expr(f.place&, .None, true)) return;
                @check(self.compile_expr(f.value&, f.place.ty.want())) return;
                if f.place.ty != f.value.ty {|
                    return(@err("tried to set % to %", self.log_type(f.place.ty), self.log_type(f.value.ty)));
                };
                // TODO: self.type_check_arg(f.place.ty, f.value.ty, "reassign var")?; /:type_check
                return(.Ok);
            };
            return(@err("Illegal place expression: %", tag))
        }
        fn DeclVarPattern(f) => return(self.decl_var_pattern(stmt));
        @default => @panic("TODO: unhandled compile_stmt type %", stmt.stmt&.tag());
    };
    unreachable_hack(Maybe(void))
}

fn decl_var_pattern(self: *SelfHosted, stmt: *FatStmt) Maybe(void) #once = {
    @debug_assert(stmt.stmt&.is(.DeclVarPattern));
    (bindings, value) := (stmt.stmt.DeclVarPattern.binding.bindings&, stmt.stmt.DeclVarPattern.value&);
    if value.is_raw_unit() {|
        // TODO: typecheck the pattern! this was fine a long time ago when this node was only made by closures. 
        stmt.stmt = .Noop;
        return(.Ok);
    };
    
    if bindings.len == 1 {|
        b := bindings[0]&;
        if(b.kind == .Const, => return(@err("unreachable? destructure single arg must be closure but const args should already be handled")));
        if b.var() { name |
            // TODO: could reduce the ast node but it doesn't matter. 
            return(self.decl_var(name, b.ty&, value));
        };
        return(@err("TODO: what situation allows DeclVarPattern with no name?"));
    };
    
    // So its not a tuple, we don't know how to destructure it (emit_bc does), but we can still type check it. 
    // Even if it was a tuple, all we'd do differently is pass in the ResultTypes seperatly, but now we can just do that for any expression. 
    // If i was inspired this could grow a speical case for tuples like the old one had to avoid allocating the lists,
    // but temp() is cheap so its probably fine.  -- Aug 1
    
    result_types: List(ResultType) = list(bindings.len, temp());
    each bindings { b | 
        if(b.kind == .Const, => return(@err("TODO: destructure with constants")));
        request := ResultType.None; 
        if @check(self.infer_type(b.ty&)) return { known |
            request = (Specific = known);
        };
        result_types&.push(request);
    };
    
    @check(self.compile_expr(value, (Tuple = result_types.items()))) return; 
    types := or self.tuple_types(value.ty) {|
        return(@err("destructure expected tuple type"))
    };
    if(types.len != bindings.len, => return(@err("destructuring arity mismatch expected % but found %", bindings.len, types.len)));
    
    enumerate bindings { i, b |
        if b.var() { name | 
            prev := self.scopes.get_var_type(name); 
            self.scopes.put_var_type(name, types[i]);
            @assert(prev.is_none().or(=> prev.unwrap() == types[i]));
        };
    };
    
    // TODO: :mark_stmt_done 
    
    .Ok
}

::opt_map(Type, Type);
fn compile_place_expr(self: *SelfHosted, place: *FatExpr, requested: ResultType, want_deref: bool) Maybe(void) = {
    loc := place.loc;
    @match(place.expr&) {
        fn GetVar(var) => {
            val_ty := self.scopes.get_var_type(var[]);
            val_ty := or val_ty {| 
                return(@err("var must be declared: %", var.log(self.pool)))
            };
            ptr_ty := self.ptr_type(val_ty);

            place[] = synthetic_ty((Addr = self.box(place[])), loc, ptr_ty);
            place.done = true;
            if want_deref {|
                @check(self.deref_one(place)) return;
                place.done = true;
            };
        }
        fn FieldAccess(f) => {
            // TODO: could lookup field and pass down requested
            // Note: compile_expr may have already walked the container because it has to check if its an enum/scope.
            @check(self.compile_place_expr(f.container, .None, false)) return;
            done := f.container.done;

            // :AutoDeref
            {
                raw   := self.raw_type(f.container.ty);
                inner := or self.unptr_ty(raw) {|
                    return(@err("PlaceExpr of FieldAccess should be ptr"))
                };
                inner = self.raw_type(inner);
                // Pointers never have fields, so the thing behind the pointer, shouldn't be a pointer.
                // This lets you write `self: *Self; self.name` instead of  `self: *Self; self[].name`.
                while => self.unptr_ty(inner) { next_inner | 
                    @check(self.deref_one(f.container)) return;
                    inner = self.raw_type(next_inner);
                };
            };
            (bytes, field_val_ty) := @check(self.field_access_expr(f.container, f.name)) return;
            field_ptr_ty := self.ptr_type(field_val_ty);
            @debug_assert(!f.container.expr&.is(.GetVar), "ICE: place expr can't be direct var access");
            e := self.box(f.container[]);
            if bytes == 0 {|
                place.expr = (Cast = e);
            } else {|
                place.expr = (PtrOffset = (
                    ptr = e,
                    bytes = bytes,
                    name = f.name,
                ));
            };
            place.done = done;
            if want_deref {|
                place.ty = field_ptr_ty;
                // Now we have the offset-ed ptr, add back the deref
                @check(self.deref_one(place)) return;
                place.done = done;
            } else {|
                place.ty = field_ptr_ty;
            };
        }
        fn Deref(_) => {
            return(self.compile_deref_place(place, requested, want_deref));
        }
        fn GetNamed(n) => return(@err("Undeclared Identifier: %", self.pool.get(n[])));
        fn PrefixMacro(_) => {
            // TODO: this is sketchy but makes []->.index work.
            //       need to think about how requested/want_deref are handled
            @check(self.compile_expr(place, requested)) return;
            return(self.compile_place_expr(place, requested, want_deref));
        }
        @default => {
            if !place.ty.is_unknown() {|
                // TODO: pass in if we're currently trying to access a field so we can give a better error message if its on an int or something?
                return(@err("Place expression of type % expected pointer dereference.\n%", self.log_type(place.ty), place.log(self.pool)));
            };
            return(@err("TODO: other `place=e;` % %", place.log(self.pool), self.log_type(place.ty)));
        };
    };
    .Ok
}

fn compile_deref_place(self: *SelfHosted, place: *FatExpr, requested: ResultType, want_deref: bool) Maybe(void) = {
    @debug_assert(place.expr&.is(.Deref));
    arg := place.expr.Deref;
    // When you see a !deref, treat the expression as a pointer value.
    req := requested.specific().map(fn(r) => self.ptr_type(r));
    @check(self.compile_expr(arg, req.want())) return;
    if want_deref {|
        place.ty = or self.unptr_ty(arg.ty) {|
            return(@err("tried to deref non-pointer"))
        };
        place.done = arg.done;
    } else {|
        place[] = arg[];
    };
    .Ok
}

// :PlaceExpr
fn field_access_expr(self: *SelfHosted, container_ptr: *FatExpr, name: Symbol) Maybe(Ty(i64, Type)) #once = {
    container_ptr_ty := self.raw_type(container_ptr.ty);
    depth := self.ptr_depth(container_ptr_ty);
    if depth != 1 {|
        return(@err("index expr ptr must be one level of indirection. %", self.log_type(container_ptr_ty)));
    };
    container_ty := self.unptr_ty(container_ptr_ty).expect("ICE: we just checked its a pointer");

    raw_container_ty := self.raw_type(container_ty);
    or self.finish_layout(raw_container_ty) { (e: ParseErr) |
        return(Err = self.box(e))
    };
    @match(self.get_type(raw_container_ty)) {
        fn Struct(f) => {
            @debug_assert(f.layout_done);
            each f.fields { f | 
                if f.name == name {|
                    container_ptr.done = true;
                    return(Ok = (f.byte_offset, f.ty));
                };
            };
            return(@err("unknown name % on %", self.pool.get(name), self.log_type(container_ty)))
        }
        fn Tagged(f) => {
            each f.cases { f |
                if f._0 == name {|
                    return(Ok = (8, f._1));
                };
            };
            return(@err("unknown name % on %", self.pool.get(name), self.log_type(container_ty)))
        }
        @default => {
            return(@err("only structs/enums support field access (name: %) but found %", self.pool.get(name), self.log_type(container_ty)))
        };
    }
}

// TODO: have a different version of @check for something that can error but not yield? 
fn deref_one(self: *SelfHosted, ptr: *FatExpr) Maybe(void) = {
    raw   := self.raw_type(ptr.ty);  // TODO: why are we going through enums...? -- Jul 30
    inner := or self.unptr_ty(raw) {|
        return(@err("expected ptr for deref_one"))
    };
    
    @match(ptr.expr&) {
        fn Addr(arg) => {
            // this allows auto deref to work on let ptr vars.
            if arg.expr&.is(.GetVar) {|
                // raw var expr is not allowed, we always refer to variables through thier address.
                ptr[] = synthetic_ty((Deref = self.box(ptr[])), ptr.loc, inner);
            } else {|
                // Avoid reduntant (whatever)&[].
                ptr[] = arg[][];
                if ptr.ty.is_unknown() {|
                    ptr.ty = inner; // TODO: this shouldn't happen
                };
            };
        }
        @default => {
            ptr[] = synthetic_ty((Deref = self.box(ptr[])), ptr.loc, inner);
        };
    };
    .Ok
}

fn decl_var(self: *SelfHosted, name: Var, ty: *LazyType, value: *FatExpr) Maybe(void) = {
    @println("decl_var %", name&.log(self.pool)); // :debug
    @debug_assert(self.dispatch.enclosing_function.is_some(), "ICE: runtime vars must have an enclosing function");
    no_type := ty.is(.Infer);
    self.last_loc = value.loc;
    if !no_type {|
        @check(self.infer_type(ty)) return;
    };
    @check(self.compile_expr(value, ty.want())) return;
    final_ty := value.ty;
    @debug_assert(!final_ty.is_unknown(), "if we didn't yield, we should know the type");
    if no_type {|
        // Since there was no type annotation, we don't need a type check. Whatever we got is the type of this variable now. 
        ty[] = (Finished = final_ty);
        self.finish_layout_deep(final_ty).unwrap();
    } else {|
        expected_ty := ty.unwrap();
        // TODO: :delay_layout
        //       Instead of doing this now, add it as an Action in the dispatch loop.
        //       We don't actually care about the field offsets yet, we just need them later for emitting bytecode. 
        //       This was one of the changes that inspired the sema rewrite.  -- Jul 25
        self.finish_layout_deep(expected_ty).unwrap();
        self.finish_layout_deep(final_ty).unwrap();
        // TODO: panic("TODO: self.type_check_arg(value, ty, \"var decl\")?;");
    };

    prev := self.scopes.get_var_type(name); 
    self.scopes.put_var_type(name, final_ty); // TODO: this returns is_new, i want to just assert that. we shouldn't be compiling more than once -- Jul 25
    // TODO: prev should always be none?? but its not a constant and seems to always be the same so its probablby not a super huge deal? -- Apr 23
    //       maybe its just cause im not zeroing the stmt and end up compiling multiple times. -- Apr 25
    @assert(prev.is_none().or(=> prev.unwrap() == final_ty));
    
    // TODO: :mark_stmt_done
    //       this function is called multiple times (if the containing block needs to yield). need to have a done flag on the fatstmt
    //       but thats easier to change once all the stuff is written in one language -- Jul 25
    .Ok
}

fn ensure_resolved_sign(self: *SelfHosted, fid: FuncId) Maybe(void) = {
    func := self.get_function(fid);
    if(func.get_flag(.ResolvedSign), => return(.Ok));
    res := self.resolve_sign(func);
    if res&.is_err() {|
        return(Err = self.box(res.Err));
    };
    func.set_flag(.ResolvedSign);
    .Ok
}

fn ensure_resolved_body(self: *SelfHosted, fid: FuncId) Maybe(void) = {
    @check(self.ensure_resolved_sign(fid)) return;
    func := self.get_function(fid);
    if(func.get_flag(.ResolvedBody), => return(.Ok));
    res := self.resolve_body(func);
    if res&.is_err() {|
        return(Err = self.box(res.Err));
    };
    func.set_flag(.ResolvedBody);
    .Ok
}

fn handle_compile_func_body(self: *SelfHosted, fid: FuncId) Maybe(void) #once = {
    @debug_assert(self.dispatch.function_in_progress&.get(fid.as_index()), "expected to do work on in progress function");
    self.ensure_resolved_body(fid);
    func := self.get_function(fid);
    if(func.get_flag(.EnsuredCompiled), => return(.Ok));
    assert(!func.get_flag(.MayHaveAquiredCaptures), "closures need to be specialized");
    assert(!func.get_flag(.AnyConstArgs), "const args need to be specialized");
    
    // Before we can do the body, we really need to know the argument types, 
    // and it would be nice to know the return type too but that's less important. 
    @check(self.infer_arguments(fid)) return;
    
    // you need to only do this once so args are unique but we might yield below. 
    // you can't yield in this block! MadeVarsForRuntimeArgs needs to be atomic!
    // TODO: alternativly, if i trusted myself, you could just say its fine when the arg is already there, 
    //       because surely we put it there ourselves last time around. 
    //       but for now i think this is a valuable sanity check that renumbering went well.  -- Jul 30
    if !func.get_flag(.MadeVarsForRuntimeArgs) {|
        each func.arg.bindings { b | 
            // TODO: probably want to change this so you can do as much compiling as possible before expanding templates.
            @debug_assert(b.kind != .Const, "ICE: Tried to emit before binding const args.");
            @if_let(b.name) fn Var(name) => {
                @debug_assert(b.kind == name.kind);
                is_new := self.scopes.put_var_type(name, b.ty&.unwrap());
                @println("bind %", name&.log(self.pool));
                if(!is_new, => return(@err("overwrite arg? %", name&.log(self.pool))));
            };
        };
        func.set_flag(.MadeVarsForRuntimeArgs) ;
    };
    
    @match(func.body&) {
        fn Normal(expr) => {
            old_func := self.dispatch.enclosing_function;
            self.dispatch.enclosing_function = (Some = fid);
            
            if func.return_var { return_var |
                // TODO: this means you cant early return from non-block functions but the error message will be useless -- Jul 9 
                @if_let(expr.expr&) fn Block(f) => {
                    if f.ret_label.is_none() {|  // we might have already tried to compile this function. 
                        ret: LabelId = from_index(self.dispatch.next_label);
                        self.dispatch.next_label += 1;
                        label_ty := LabelId; // :get_or_create_type
                        if func.finished_ret { ret_ty | 
                            label_ty = self.intern_type(Label = ret_ty);
                        };
                        val := self.to_values(LabelId, ret);
                        @check(self.save_const_values(return_var, val, label_ty, func.loc)) return;
                        f.ret_label = (Some = ret);
                    };
                };
            };

            @check(self.compile_expr(expr, func.finished_ret.want())) return;
            self.dispatch.enclosing_function = old_func;
            if func.ret&.is(.Infer) {|
                func.finished_ret = (Some = expr.ty);
            };
        }
        fn Intrinsic(op) => ();
        fn ComptimeAddr(ptr) => {
            panic("TODO: ComptimeAddr");
        }
        @default => panic("TODO: unhandled function body type");
    };
    @println("body compiled: %", fid.as_index());
    func.set_flag(.EnsuredCompiled);
    .Ok
}

::tagged(ResultType);
fn compile_expr(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    self.last_loc = expr.loc;
    if expr.done {|
        assert(expr.ty != UnknownType, "done but unknown type");
        return(.Ok);
    };
    self.dispatch.active_stack.push(expr = expr, requested = requested);
    
    @match(expr.expr&) {
        fn Poison(placeholder) => {
            return(@err("Poison expression %", placeholder));
        }
        fn Value(f) => {
            if(expr.ty == UnknownType, => return(@err("ICE: Value expression must have known type")));
            if !f.coerced {|
                @if_let(requested) fn Specific(ty) => {
                    if expr.ty != ty {|
                        panic("TODO: coerce const");
                    };
                };
            };
            expr.done = true;
        }
        fn Call() => {
            @check(self.compile_call(expr, requested)) return;
        }
        fn Block(f) => {
            if !f.hoisted_constants {|
                @check(self.hoist_constants(f.body.items())) return;
                f.hoisted_constants = true;
            };
            each f.body { stmt | 
                @check(self.compile_stmt(stmt)) return;
            };
            @check(self.compile_expr(f.result, requested)) return;
            expr.ty = f.result.ty;
        }
        fn Tuple(parts) => {
            requested_types := @try(self.tuple_types(requested, parts.len)) return;
            types: List(Type) = list(parts.len, temp()); 
            enumerate parts { i, expr | 
                if !expr.done {|
                    @check(self.compile_expr(expr, requested_types[i])) return;
                };
                types&.push(expr.ty);
            };
            if expr.ty.is_unknown() {|
                expr.ty = self.tuple_of(types.items());
                self.finish_layout_deep(expr.ty);
            };  // TODO: else :type_check
        }
        fn PrefixMacro() => {
            @check(self.compile_prefix_macro(expr)) return;
        }
        fn GetVar(var) => return(self.compile_get_var(expr, requested));
        fn GetNamed(n) => return(@err("Undeclared Identifier: %", self.pool.get(n[])));
        fn String() => {
            // This is only different from a Value node because the 'Str' is not a builtin so the parser is unable to create it.
            byte := self.get_or_create_type(u8);
            expr.ty = @check(self.create_slice_type(byte)) return;
            panic("TODO: self.set_literal(expr, self.pool.get(i)");
            expr.done = true;
        }
        fn ConstEval(inner) => {
            // :PushConstFnCtx // TODO
            // We need to track that any callees of this expression are not runtime callees!
            // self.wip_stack.push((None, ExecStyle::Jit)); // this is what the old version did
            // but maybe we're trying to track that in the dispatch thing now. 
            // i guess expression tasks need to have a containing function context? 
            @check(self.compile_expr(inner[], requested)) return; // TODO: this is probably wrong! ^ -- Jul 30
            // TODO: its a bit silly that i have to specifiy the type since the first thing it does is compile it
            value := @check(self.immediate_eval_expr(inner[], inner.ty)) return;
            expr.set(value, inner.ty);
        }
        fn Deref(inner) => @check(self.compile_deref_place(expr, requested, true)) return;
        fn Addr(inner) => {
            // Note: the old version had special handling for GetVar here to avoid loops but this seems fine... - Jul 30
            @check(self.compile_place_expr(inner[], requested, false)) return;
            expr[] = inner[][];
        }
        fn PtrOffset(_) => return(@err("ICE: PtrOffset should be done=true"));
        fn GetParsed() => return(@err("ICE: GetParsed is handled by scope.fr"));
        fn Closure(func) => {
            fid := self.add_function(func[][]);
            fid_value := self.to_values(FuncId, fid);
            expr.set(fid_value, FuncId); // :get_or_create_type
            func := self.get_function(fid);
            self.update_function_metadata(func);
            if !requested&.is(.None) {|
                @check(self.coerce_const_expr(expr, requested)) return;
            };
        }
        // :PlaceExpr
        fn FieldAccess(f) => {
            // TODO: this is unfortunate. it means you prewalk instead of letting placeexpr do the recursion
            //       but need to check if its a value that has special fields first.
            @check(self.compile_expr(f.container, .None)) return;

            if f.container.ty == Type {| // :get_or_create_type
                return(@err("FieldAccess contextual field on Type"));
            };
            if f.container.ty == ScopeId {| // :get_or_create_type
                // TODO: this should probably be removed because its the same as a struct with const fields. 
                return(@err("FieldAccess on Scope"));
            };

            // Otherwise its a normal struct/tagged field.
            @check(self.compile_place_expr(expr, requested, true)) return;
        }
        @default => @panic("TODO: unhandled node type %: %", expr.expr&.tag(), expr.log(self.pool));
    };
    self.dispatch.active_stack.pop();
    @debug_assert(!expr.ty.is_unknown(), "Unknown type for %", expr.log(self.pool));
    .Ok
}

::if(ResultType);
fn compile_call(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    @debug_assert(expr.expr&.is(.Call));
    f := expr.expr.Call.f;
    arg := expr.expr.Call.arg;
    req_fn: ResultType = @match(requested) {
        fn Specific(t) => (Returning = t);
        @default => .None;
    };
    
    @check(self.compile_expr(f, req_fn)) return;
    
    if f.ty == OverloadSet {|  // :get_or_create_type
        os := @check(self.immediate_eval_expr(f, OverloadSet)) return;
        os := OverloadSet.assume_cast(os&)[];
        return(Suspend = self.wait_for(ResolveOverload = (os = os, call = (expr = expr, requested = requested))));
    };
    
    if f.ty == FuncId {|  // :get_or_create_type
        return(self.call_direct(expr));
    };
    
    if f.ty == LabelId {|  // :get_or_create_type
        panic("TODO: early return before ret type infered (could try more aggressivly in handle_compile_body)");
    };
    
    @match(self.get_type(f.ty)) {
        fn FnPtr(it) => {
            // Feels like a pretty reasonable invarient: if we have a function pointer, we need to know exactly what its types are. 
            @check(self.compile_expr(arg, (Specific = it.ty.arg))) return;
            expr.ty = it.ty.ret;
        }
        fn Fn(it) => {
            // This happens to be an easy case where we already know the argument type.
            @check(self.compile_expr(arg, (Specific = it.arg))) return;
            return(self.call_direct(expr));
        }
        fn Label(it) => {
            @check(self.compile_expr(arg, (Specific = it[]))) return;
            if arg.ty != it[] {|
                return(@err("early return type mismatch. TODO: better type checking"));
            };
            expr.ty = Never; // :get_or_create_type
            expr.done = arg.done; 
        }
        @default => return(@err("not callable %", f.log(self.pool)));
    };
    
    .Ok
}

// TODO: check function ret type against requested
// Note: when we get here, we might not know the type of the function or the argument. 
fn call_direct(self: *SelfHosted, expr: *FatExpr) Maybe(void) = {
    f_expr := expr.expr.Call.f;
    arg_expr := expr.expr.Call.arg;
    fid := @check(self.immediate_eval_expr(f_expr, f_expr.ty)) return;
    fid := FuncId.assume_cast(fid&)[];
    @println("compile direct call %", fid);
    func := self.get_function(fid);
    
    assert(!func.get_flag(.MayHaveAquiredCaptures) && !func.get_flag(.UnsafeNoopCast)
        && !func.get_flag(.TryConstantFold)
    , "Not Yet Implemented: unhandled complex call");
    
    arg_ty := UnknownType;
    
    // If there's no const args, we know its not generic so we should be able to know the argument types. 
    if !func.get_flag(.AnyConstArgs) {|
        arg_ty = @check(self.infer_arguments(fid)) return;
        @check(self.compile_expr(arg_expr, (Specific = arg_ty))) return;
        if arg_ty != arg_expr.ty {|
            return(@err("arg type mismatch. TODO: better type checking"));
        };
    } else {| // there are const args. 
        ::if(ResultType);
        req_arg: ResultType = if(arg_expr.ty == UnknownType, => .None, => (Specific = arg_expr.ty));
        @check(self.compile_expr(arg_expr, req_arg)) return;
        @check(self.ensure_resolved_sign(fid)) return;
        
        // This creates a new function with only runtime args, and updates the arg_expr accordingly. 
        fid = @check(self.curry_const_args(fid, f_expr, arg_expr)) return;
        func = self.get_function(fid);
        // Since we may have just made a new function, make sure we know its types,
        // and then sanity check that our types still match. 
        arg_ty = @check(self.infer_arguments(fid)) return;
        @check(self.compile_expr(arg_expr, arg_ty.want())) return;
        // It's probably an ICE if this fails on something non-#generic. 
        if arg_ty != arg_expr.ty {|
            return(@err("arg type mismatch after removing const args"));
        };
    };
    
    ret_ty := @check(self.infer_return(fid))    return;
    
    capturing   := func.get_flag(.AllowRtCapture).or(func.get_flag(.MayHaveAquiredCaptures));
    will_inline := capturing.or(func.cc.unwrap() == .Inline);
    if(will_inline && func.get_flag(.NoInline), => return(@err("must inline a function marked #noinline")));
    
    if will_inline {|
        // If we're just inlining for #inline, compile first so some work on the ast is only done once.
        // note: compile() checks if its ::Inline before actually generating asm so it doesn't waste its time.
        // if its a '=>' function, we can't compile it out of context, and same if it has a const arg of a '=>' function. 
        if !capturing && !func.get_flag(.EnsuredCompiled) {|
            self.dispatch.function_in_progress&.set(fid.as_index());
            return(Suspend = self.wait_for(CompileBody = fid));
        };

        // TODO: check that you're calling from the same place as the definition.
        return(self.emit_capturing_call(fid, expr));
    };
    
    is_const_context := self.add_callee(fid);
    // TODO: its a bit of a hack to check is_const_context? 
    //       it fixes a problem where you try to compile too soon and then think you're already done even if you make a function for it later.
    expr.done = f_expr.done && arg_expr.done && !is_const_context;
    expr.ty = ret_ty;
    
    if f_expr.ty == FuncId {|  // :get_or_create_type
        f_expr.ty = self.intern_type(Fn = (arg = arg_ty, ret = ret_ty, arity = func.arg.bindings.len.trunc()));
    };
    
    if func.get_flag(.Once) {|
        // TODO: better error message. should show the previous usage. 
        if(func.get_flag(.OnceConsumed), => return(@err("tried to call once function again")));
        func.set_flag(.OnceConsumed);
        assert(expr.done, "ICE: if we applied #once, the expression really needs to be .done");
    };
    
    .Ok
}

fn add_callee(self: *SelfHosted, fid: FuncId) bool = {
    is_const_context := true;
    if self.dispatch.enclosing_function { current_f |
        is_const_context = false;
        current := self.get_function(current_f);
        current.callees&.add_unique(fid, self.get_alloc());
        self.aarch64&.extend_blanks(fid);
        @println("added % calls %", current_f, fid);
    };
    if is_const_context {|
        @println("skip adding callee for %", fid);
    };
    is_const_context
}

// Replace a call expr with the body of the target function.
fn emit_capturing_call(self: *SelfHosted, f: FuncId, expr_out: *FatExpr) Maybe(void) #once = {
    @check(self.ensure_resolved_body(f)) return; // it might not be a closure. it might be an inlined thing.
    @debug_assert(expr_out.expr&.is(.Call));
    arg_expr := expr_out.expr.Call.arg;
    
    // TODO
    //assert!(!self.currently_inlining.contains(&f), "Tried to inline recursive function.");
    //self.currently_inlining.push(f);

    func := self.get_function(f);
    // TODO: if let this? its for return_var
    ret_ty := or func.finished_ret {|
        return(@err("Unknown ret type for % %", f, self.pool.get(func.name)))
    };
    label_ty := self.intern_type(Label = ret_ty);
    // TODO: mutual callees as well.
    // for inlined or things with const args, the body will already have been compiled, but the backend needs to see the whole callgraph.
    for func.callees { callee |
        self.add_callee(callee);
    };

    if(!func.body&.is(.Normal), => return(@err("'=>' function must have body expression.")));
    aliased_body := func.body.Normal&;
    may_have_early_return := !aliased_body.expr&.is(.Value);
    pattern := func.arg&.deep_clone(self.get_alloc());
    
    // TODO: you want to be able to share work (across all the call-sites) compiling parts of the body that don't depend on the captured variables
    old_ret_var := func.return_var.unwrap();
    new_ret_var := self.scopes.dup_var(old_ret_var);
    ret_label: LabelId = from_index(self.dispatch.next_label);
    self.dispatch.next_label += 1;
    
    owned_body: FatExpr = if func.get_flag(.Once) {|
        // TODO: better error message if they try to call it again. 
        // TODO: decide what #once with const args should mean. 
        temp := aliased_body[];
        aliased_body.expr = (Poison = .OnceUsed);
        temp
    } else {|
        aliased_body.deep_clone(self.get_alloc())
    };
    loc := arg_expr.loc;
    // the second case would be sufficient for correctness but this is so common (if(_,!,!), loop(!), etc) that it makes me less sad. 
    if arg_expr.is_raw_unit() {|
        // TODO: if !may_have_early_return, should be able to just inline the value but it doesnt work!
        //       similarly i cant have ret_label:None so im clearly wrong. its `not callable` in overloading.rs. trying to call the cond of an if??
        //       -- Jun 16
        expr_out.expr = (Block = (body = empty(), result = self.box(owned_body), ret_label = (Some = ret_label), hoisted_constants = false));
    } else {|
        stmts: List(FatStmt) = list(1, self.get_alloc());
        arg_stmt: Stmt = (DeclVarPattern = (binding = pattern, value = arg_expr[]));
        stmts&.push(stmt = arg_stmt, annotations = empty(), loc = loc);
        expr_out.expr = (Block = (body = stmts.rs(), result = self.box(owned_body), ret_label = (Some = ret_label), hoisted_constants = false));
    };
    // TODO: self.currently_inlining.retain(|check| *check != f);
    
    if may_have_early_return {|
        // Note: not renumbering on the function. didn't need to clone it.
        self.renumber_expr(expr_out, (Some = (old_ret_var, new_ret_var)));
        value := self.to_values(LabelId, ret_label);
        @check(self.save_const_values(new_ret_var, value, label_ty, loc)) return;
    };
    
    self.compile_expr(expr_out, (Specific = ret_ty))
}

// TODO: this still has the problem of disallowing const args that require type hint of previous const args. 
fn const_args_key(self: *SelfHosted, original_f: FuncId, arg_expr: *FatExpr) Maybe(Values) #once = {
    func := self[original_f]&;
    ::if(Maybe(Values));
    if func.arg.bindings.len == 1 {|
        @check(self.compile_expr(arg_expr, func.finished_arg.want())) return;
        value := @check(self.immediate_eval_expr(arg_expr, arg_expr.ty)) return;
        (Ok = value)
    } else {|
        check_len :: fn(len: i64) => {
            if func.arg.bindings.len != len {|
                return(@err( "TODO: non-trivial pattern matching for call to %", self.pool.get(func.name)));
            };
        };

        types := self.tuple_types(arg_expr.ty);
        if types { types |
            check_len(types.len);
            @if(false) {
                // The rust version did this and i thought it needed but i just typo-ed the test. 
                // TODO: does this ever happen (function are not being a tuple)? or are we strict about arity now? -- Aug 1
                @if_let(arg_expr.expr&) fn Value(f) => {
                    // TODO: this is super dumb but better than what I did before. -- May 3 -- May 24
                    parts: List(FatExpr) = list(types.len, self.get_alloc());
                    reader: ReadBytes = (bytes = f.bytes&.bytes(), i = 0);
                    for types { ty | 
                        taken := or self.chop_prefix(ty, reader&) {| 
                            return(@err("ICE: not enough bytes to destructure value!"))
                        };
                        parts&.push(synthetic_ty((Value = (bytes = taken, coerced = false)), arg_expr.loc, ty));
                    };
                    @assert_eq(reader.bytes.len, reader.i, "ICE: didn't consume all bytes.");
                    arg_expr.expr = (Tuple = parts.rs());
                };
            };
        };
        
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple but expected % args.\n%", func.arg.bindings.len, arg_expr.log(self.pool))));
        arg_exprs := arg_expr.expr.Tuple;
        check_len(arg_exprs.len);

        all_const_args: List(u8) = list(self.get_alloc());
        i := 0;
        enumerate func.arg.bindings { i, binding |
            continue :: local_return;
            if(binding.kind != .Const, => continue());
            arg_ty := binding.ty&.ty().or(=> arg_exprs[i].ty);
            value := @check(self.immediate_eval_expr(arg_exprs.index(i), arg_ty)) return;
            all_const_args&.push_all(value&.bytes());
        };
        (Ok = all_const_args.items().to_value())
    }
}

fn remove_const_args(self: *SelfHosted, original_f: FuncId, arg_expr: *FatExpr) Maybe(void) = {
    func := self.get_function(original_f);
    if func.arg.bindings.len == 1 {|
        arg_expr.set(unit_value, void);
    } else {|
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple")));
        arg_exprs := arg_expr.expr.Tuple&;
        removed := 0;
        enumerate func.arg.bindings { i, binding | 
            if binding.ty&.ty() { expected |
                found := arg_exprs[i - removed].ty;
                if(found != expected, => return(@err("TODO: better typecheck")));
            };
            // TODO: else!!!
            if binding.kind == .Const {|
                // TODO: this would be better if i was iterating backwards
                arg_exprs.ordered_remove(i - removed);
                removed += 1; // TODO: this sucks
            };
        };
        if arg_exprs.is_empty() {|
            // Note: this started being required when I added fn while.
            arg_expr.set(unit_value, void);
        } else {|
            if arg_exprs.len() == 1 {|
                arg_expr[] = arg_exprs[0];
            } else {|
                arg_expr.ty = UnknownType;
                arg_expr.done = false;
            };
        };
    };
    
    .Ok
}

MemoKey :: Ty(FuncId, Values);
// TODO: to allow #generic, this lazyily infers param types on the new function.
//       it would avoid redundant work to do that on the original func for params that don't depend on constants. 
//       that would jsut require compile_expr to be able to yield on a Poison.Argument, and we catch that here and just ignore. 
fn curry_const_args(self: *SelfHosted, original_f: FuncId, f_expr: *FatExpr, arg_expr: *FatExpr) Maybe(FuncId) #once = {
    key: MemoKey = (original_f, @check(self.const_args_key(original_f, arg_expr)) return);
    if self.dispatch.const_bound_memo&.get(key&) { new_f |
        @println("reuse baked const args % -> %", original_f, new_f);
        @check(self.remove_const_args(original_f, arg_expr)) return;
        fid_value := self.to_values(FuncId, new_f);
        f_expr.set(fid_value, FuncId);
        return(Ok = new_f);
    };

    func := self.get_function(original_f);
    @debug_assert(func.get_flag(.AnyConstArgs), "baking const args but none are there");
    @debug_assert_eq(func.get_flag(.AllowRtCapture), func.get_flag(.ResolvedBody));
    // Some part of the argument must be known at comptime.
    new_func := func.deep_clone(self.get_alloc());
    res := self.maybe_renumber_and_dup_scope(new_func&);
    if(res&.is(.Err), => return(Err = self.box(res.Err)));
    new_fid := self.add_function(new_func);
    @println("bake const args % -> %", original_f, new_fid);
    @check(self.ensure_resolved_sign(new_fid)) return;
    @check(self.ensure_resolved_body(new_fid)) return;
    func := self.get_function(new_fid);
    // TODO: mark func as in progress somehow so nobody can yield on it. 
    //       tho really i feel like you want to make this whole operation yield-able,
    //       currently you could get into a situation with a lot of redundant work i think.  

    ::if(Maybe(FuncId));
    if func.arg.bindings.len == 1 {|
        binding := func.arg.bindings[0]&;
        @debug_assert_eq(binding.kind, .Const);
        name := or binding.var() {|
            return(@err("arg needs name (unreachable?)"))
        };
        // TODO: if you yield here, you spam clone the function. 
        arg_type := or @check(self.infer_type(binding.ty&)) return {|
            return(@err("params require type annotation"))
        };
        value := @check(self.immediate_eval_expr(arg_expr, arg_type)) return;
        @check(self.bind_const_arg(new_fid, name, value, arg_expr.ty, arg_expr.loc)) return;

        func.finished_arg = .None;  // TODO: we know its void because we removed the only argument.
        fid_value := self.to_values(FuncId, new_fid);
        f_expr.set(fid_value, FuncId);
        arg_expr.set(unit_value, void);
        self.dispatch.const_bound_memo&.insert(key, new_fid);
        func.unset_flag(.AnyConstArgs);
        func.unset_flag(.Generic);
        (Ok = new_fid)
    } else {|
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple")));
        removed_count := 0;
        range(0, func.arg.bindings.len) { i |
            continue :: local_return;
            @println("%: check %/%", i, i-removed_count, func.arg.bindings.len);
            b := func.arg.bindings[i - removed_count]&;
            arg_expr := arg_expr.expr.Tuple[i]&;
            if b.kind != .Const {|
                continue();
            };
            name := or b.var() {|
                return(@err("arg needs name (unreachable?)"))
            };
            @println("bake arg %: %", i, name&.log(self.pool));
            // TODO: if you yield here, you spam clone the function. 
            
            // We might not be able to know the types expected for each parameter up front (because they can reference previous consts),
            // but once we're binding an argument, we need to be able to get its type. 
            // (TODO: closures are allowed to infer so that's not even true in the long term?)
            arg_type := or @check(self.infer_type(b.ty&)) return {|
                return(@err("params require type annotation"))
            };
            value := @check(self.immediate_eval_expr(arg_expr, arg_type)) return;
            
            // bind_const_arg handles adding closure captures.
            // since it needs to do a remap, it gives back the new argument names so we can adjust our bindings acordingly. dont have to deal with it above since there's only one.
            @check(self.bind_const_arg(new_fid, name, value, arg_type, arg_expr.loc)) return;
            // No remove from arg here because bind_const_arg calls remove_named.
            removed_count += 1;
        };
        @debug_assert_ne(new_fid, original_f);

        // We're leaving it to the caller to infer the new type and type-check the runtime arguments. 
        fid_value := self.to_values(FuncId, new_fid);
        f_expr.set(fid_value, FuncId);
        func.unset_flag(.AnyConstArgs);
        func.unset_flag(.Generic);
        @check(self.remove_const_args(original_f, arg_expr)) return;
        self.dispatch.const_bound_memo&.insert(key, new_fid);
        // Don't need to explicitly force capturing because bind_const_arg added them if any args were closures.
        (Ok = new_fid)
    }
}

// The argument type is evaluated in the function declaration's scope, the argument value is evaluated in the caller's scope.
fn bind_const_arg(self: *SelfHosted, o_f: FuncId, arg_name: Var, arg_value: Values, arg_ty_found: Type, loc: Span) Maybe(void) = {
    // I don't want to renumber, so make sure to do the clone before resolving.
    // TODO: reslove captured constants anyway so dont haveto do the chain lookup redundantly on each speciailization. -- Apr 24
    @println("Bind $% = %", arg_name&.log(self.pool), arg_value&);
    func := self.get_function(o_f);
    @debug_assert(func.get_flag(.ResolvedBody) && func.get_flag(.ResolvedSign));
    arg_ty := @check(self.get_type_for_arg(func.arg&, arg_name)) return;
    
    assert(arg_ty_found == arg_ty, "TODO: better type-check");
    //self.type_check_arg(arg_ty_found, arg_ty, "bind arg")?;

    assert(!self.get_type(arg_ty).is(.Fn) && arg_ty != FuncId, "TODO: $fn");
    // TODO: not sure if i actually need this but it seems like i should.
    //if matches!(self.program[arg_ty], TypeInfo::Fn(_)) {
    //    assert!(
    //        !self.program[o_f].has_tag(Flag::NoInline),
    //        "functions with constant lambda arguments are always inlined"
    //    );
    //    let arg_func = arg_value.unwrap_func_id();
    //    // :ChainedCaptures
    //    // TODO: HACK: captures aren't tracked properly.
    //    self.program[o_f].set_flag(FnFlag::MayHaveAquiredCaptures, true);
    //    self.program[o_f].set_cc(CallConv::Inline)?; // just this is enough to fix chained_captures
    //    self.program[arg_func].set_cc(CallConv::Inline)?; // but this is needed too for others (perhaps just when there's a longer chain than that simple example).
    //}
    @check(self.save_const_values(arg_name, arg_value, arg_ty, loc)) return;
    func.arg&.remove_named(arg_name, self.get_alloc());

    known_type := func.finished_arg.is_some();
    func.finished_arg = .None;
    // If it was fully resolved before, we can't leave the wrong answer there.
    // But you might want to call bind_const_arg as part of a resolving a generic signeture so its fine if the type isn't fully known yet.
    if known_type {|
        //panic("TODO: reinfer?");
        //self.infer_types(o_f)?;
    };
    .Ok
}

/// It's fine to call this if the type isn't fully resolved yet.
/// We just need to be able to finish infering for the referenced argument.
fn get_type_for_arg(self: *SelfHosted, arg: *Pattern, arg_name: Var) Maybe(Type) #once = {
    each arg.bindings { arg | 
        if arg.var() { name | 
            if name == arg_name {|
                ty := or @check(self.infer_type(arg.ty&)) return {|
                    return(@err("called get_type_for_arg on .Infer-ed arg type. expected type annotation."))
                };
                return(Ok = ty);
            };
        };
    };
    @err("missing argument %", arg_name&.log(self.pool))
}
    
fn infer_type(self: *SelfHosted, b: *LazyType) Maybe(?Type) = {
    @match(b) {
        fn PendingEval(e) => {
            ty := @check(self.immediate_eval_expr(e, Type)) return; // :get_or_create_type
            ty := Type.assume_cast(ty&)[]; 
            b[] = (Finished = ty);
            return(Ok = (Some = ty))
        }
        fn Finished(ty) => return(Ok = (Some = ty[])); // cool, we're done. 
        fn EvilUninit() => panic("ICE: nothing creates this: eviluninit binding"); 
        fn Returning(_) => return(@err("ICE: tried to infer on LazyType.Returning... figure out what to do about that..."));
        fn Infer() => return(Ok = .None);
    }
}

fn infer_arguments(self: *SelfHosted, fid: FuncId) Maybe(Type) = {
    func := self.get_function(fid);
    if(func.finished_arg, fn(ty) => return(Ok = ty));
    @check(self.ensure_resolved_sign(fid)) return;
    
    types: List(Type) = list(func.arg.bindings.len, temp());
    each func.arg.bindings { b |
        ty := or @check(self.infer_type(b.ty&)) return {|
            // TODO: really thats not what you want for closures tho... 
            return(@err("function arguments must have type annotation (cannot be inferred)"))
        };
        types&.push(ty);
    };
    ty := self.tuple_of(types.items());
    func.finished_arg = (Some = ty);
    (Ok = ty)
}

fn infer_return(self: *SelfHosted, fid: FuncId) Maybe(Type) = {
    func := self.get_function(fid);
    if(func.finished_ret, fn(ty) => return(Ok = ty));
    @check(self.ensure_resolved_sign(fid)) return;
    
    ty := or @check(self.infer_type(func.ret&)) return {|
        // Infer is a valid return type. To deal with that, we just compile the body, and that will set the finished_ret for us. 
        self.dispatch.function_in_progress&.set(fid.as_index());
        return(Suspend = self.wait_for(CompileBody = fid))
    };
    func.finished_ret = (Some = ty);
    (Ok = ty)
}

fn arity(self: *SelfHosted, expr: *FatExpr) i64 = {
    @match(expr.expr&) {
        fn Tuple(parts) => parts.len;
        @default => 1;
    }
} 

fn resolve_in_overload_set(self: *SelfHosted, arg_expr: *FatExpr, requested_ret: ResultType, i: OverloadSet) Maybe(FuncId) = {
    want_arity := self.arity(arg_expr);
    if want_arity == 1 {|
        // There's no way we can get a hint from partial args (since theres only one), so yield early if needed.
        @check(self.compile_expr(arg_expr, .None)) return;
    };
    
    overloads := self.dispatch.overloads&.nested_index(i.as_index());
    self.compute_new_overloads(overloads);
    interesting: List(OverloadOption) = list(temp());
    each overloads.ready { opt | 
        continue :: local_return;
        @if_let(requested_ret) fn Specific(want) => {
            if opt.ret { have | 
                // TODO: never
                if have != want {|
                    continue();
                };
            };
        };
        
        if opt.args.len != want_arity {|
            continue();
        };
        
        interesting&.push(opt[]);
    };
    
    // TODO: but what if there are more overloads coming later? 
    if interesting.len == 1 {|
        @println("chose overload %", interesting[0].func);
        return(Ok = interesting[0].func);
    };
    
    if want_arity == 1 {|
        @check(self.compile_expr(arg_expr, .None)) return;
    };
    
    ::if([]FatExpr);
    blocked: List(*Action) = list(temp());
    parts := if(arg_expr.expr&.is(.Tuple), => arg_expr.expr.Tuple.items(), => (ptr = arg_expr, len = 1));
    still_interesting: List(OverloadOption) = list(temp());
    each interesting& { opt | 
        continue :: local_return;
        func := self.get_function(opt.func);           
        if !func.get_flag(.AnyConstArgs) {|
            //println("danger! infer arguments");
            //@check(self.infer_arguments(opt.func)) return; // TODO: really if this yields, you want to keep looking because maybe we don't care about it that much
            res := self.infer_arguments(opt.func);
            if res&.is(.Suspend) {|
               blocked&.push(res.Suspend); 
               continue(); // we don't like it, we skip it.
            };
        };
        
        enumerate func.arg.bindings { i, b | 
            arg_expr := parts.index(i);
            if b.ty&.ty() { param | 
                @check(self.compile_expr(arg_expr, .None)) return;
                if arg_expr.ty != param {|
                    continue();
                };
            };
        };
        still_interesting&.push(opt[]);
    };
    if still_interesting.len == 1 {|
        @println("chose overload %", still_interesting[0].func);
        return(Ok = still_interesting[0].func);
    };
    
    temp := still_interesting;
    still_interesting = interesting;
    interesting = temp;
    still_interesting&.clear();
    
    @if_let(requested_ret) fn Specific(want) => {
        each interesting& { opt | 
            continue :: local_return;
            func := self.get_function(opt.func);           
            if !func.get_flag(.AnyConstArgs) {|
                //@check(self.infer_return(opt.func)) return;   
                self.infer_return(opt.func);
                res := self.infer_return(opt.func);
                if res&.is(.Suspend) {|
                    blocked&.push(res.Suspend); 
                    continue(); // we don't like it, we skip it.
                };
            };
            
            if func.finished_ret { found | 
                if found != want {|
                    continue();
                };
            };
            still_interesting&.push(opt[]);
        };
    };
    
    if still_interesting.len == 1 {|
        @println("chose overload %", still_interesting[0].func);
        return(Ok = still_interesting[0].func);
    };
    
    if !blocked.is_empty() {|
        @println("block len = %", blocked.len);
        return(Suspend = blocked[0]);  // TODO: return all? don't just pick one. 
    };
    return(@err("TODO: resolve_in_overload_set %", interesting.len));
    Maybe(FuncId).unreachable_hack()
}

fn compute_new_overloads(self: *SelfHosted, overloads: *OverloadSetData) void = {
    while => !overloads.pending.is_empty() {|
        pending := overloads.pending.items().clone(temp());
        overloads.pending.len = 0;
        for pending { fid | 
            func := self.get_function(fid);
            args: List(ResultType) = list(func.arg.bindings.len, self.get_alloc());
            func.get_arg_types_non_blocking(args&);
            overloads.ready&.push((func = fid, args = args.items(), ret = func.finished_ret), self.get_alloc());
        };
    };
}

fn get_arg_types_non_blocking(func: *Func, args: *List(ResultType)) void = {
    args.clear();
    for func.arg.bindings { b | 
        @match(b.ty&.ty()) {
            fn Some(ty) => args.push(Specific = ty);
            fn None()   => args.push(.None);
        };
    };
}

fn compile_prefix_macro(self: *SelfHosted, expr: *FatExpr) Maybe(void) #once = {
    // TODO: Bring back tag checks so i don't have to be paranoid!!
    //       this annoys be enough that im tempted to go back to inlining all of these. 
    //       tho maybe #once is reassuring enough. think about it. -- Jul 22
    @debug_assert(expr.expr&.is(.PrefixMacro)); 
    invocation := expr.expr.PrefixMacro&;
    fat_expr_type := or self.env.fat_expr_type {|
        return(self.early_builtin_prefix_macro(expr))
    };
    
    panic("TODO: call user macros");
    .Ok
}

// If we're early in bootstrapping and haven't compiled the FatExpr type yet, so some special handling.
fn early_builtin_prefix_macro(self: *SelfHosted, expr: *FatExpr) Maybe(void) #once = {
    invocation := expr.expr.PrefixMacro&;
    if !invocation.handler.expr&.is(.GetVar) {|
        return(@err("macro calls must be GetVar while bootstrapping. tried to run something too compilicated too soon: %", invocation.handler.log(self.pool)));
    };
    
    name := invocation.handler.expr.GetVar.name;
    @switch(name) {
        @case(Flag.builtin.ident()) => {
            if !invocation.arg.expr&.is(.String) {|
                return(@err("@builtin arg must be String literal"));
            };
            builtin_name := invocation.arg.expr.String;
            @println("called builtin %", self.pool.get(builtin_name));
            builtin_type :: fn(T: Type) void => {
                ptr := T;  // :get_or_create_type
                val := ptr_cast_unchecked(Type, u32, ptr&)[];
                expr.set((Small = (val.zext(), 4)), Type);  // :get_or_create_type
            };
            // These could call :get_or_create_type but it doesn't matter for now because these are hardcoded
            @switch(builtin_name) {
                @case(Flag.i64.ident())         => builtin_type(i64);
                @case(Flag.bool.ident())        => builtin_type(bool);
                @case(Flag.OverloadSet.ident()) => builtin_type(OverloadSet);
                @case(Flag.Scope.ident())       => builtin_type(Scope);
                @case(Flag.FuncId.ident())      => builtin_type(FuncId);
                @case(Flag.LabelId.ident())     => builtin_type(LabelId);
                @case(Flag.Symbol.ident())      => builtin_type(Symbol);
                @case(Flag.Type.ident())        => builtin_type(Type);
                @case(Flag.void.ident())        => builtin_type(void);
                @case(Flag.true.ident())        => expr.set((Small = (1, 1)), bool);
                @case(Flag.false.ident())       => expr.set((Small = (0, 1)), bool);
                @case(Flag.compiler_debug_assert_eq_i64.ident()) => {
                    my_assert_eq: rawptr : fn(a: i64, b: i64) i64 = {
                        @println("[my_assert_eq] % vs %", a, b);
                        assert_eq(a, b);
                        a
                    };
                    ty := self.tuple_of(@slice(i64, i64));
                    ty := self.intern_type(FnPtr = (ty = (arg = ty, ret = i64, arity = 2), cc = .CCallReg));
                    expr.set((Small = (my_assert_eq.int_from_rawptr(), 8)), ty);
                };
                @default => {
                    return(@err("unknown @builtin '%'.", self.pool.get(builtin_name)));
                };
            };
        };
        @default => {
            return(@err("tried to call non-builtin macro '%' calls while bootstrapping.", self.pool.get(name)));
        };
    };
    
    .Ok
}

// - you want macros to be able to create new constant declarations in macro expansions and const arg functions.
// - for now constants are always stored globally and restricted visibility is just handled by scope resolution.
// So we delay taking constants until you try to compile the expression that contains them.
// Also, to be order independent, we don't actually evaluate or type-check them yet, that's done lazily the first time they're referenced. 
// TODO: the old compiler did #when here, but I think its better to delay until we know we care and can hope more stuff is ready. 
fn hoist_constants(self: *SelfHosted, body: []FatStmt) Maybe(void) #once = {
    each body { stmt |
        @match(stmt.stmt&) {
            fn DeclFunc(func) => {
                @debug_assert(func[].get_flag(.NotEvilUninit));
                fid := self.add_function(func[][]);
                stmt.stmt = .Noop;
                func := self.get_function(fid);
                overload_out: ?OverloadSet = .None;
                
                // I thought i dont have to add to constants here because we'll find it on the first call when resolving overloads.
                // But it does need to have an empty entry in the overload pool because that allows it to be closed over so later stuff can find it and share if they compile it.
                if func.var_name& { var |
                    // TODO: allow function name to be any expression that resolves to an OverloadSet so you can overload something in a module with dot syntax.
                    // TODO: distinguish between overload sets that you add to and those that you re-export
                    @debug_assert(!func.get_flag(.ResolvedSign));
                    @debug_assert(!func.get_flag(.ResolvedBody));
                    // TODO: assert placeholdervalue
                    if self.is_empty_constant(var[]) {|
                        // We're the first to reference this overload set so create it. 
                        i: OverloadSet = from_index(self.dispatch.overloads.len);
                        self.dispatch.overloads&.push(
                            ready = empty(),
                            name = var.name,
                            pending = fid.single(self.get_alloc()).rs(),
                        );
                        os_value := self.to_values(OverloadSet, i);
                        @check(self.save_const_values(var[], os_value, OverloadSet, func.loc)) return;  // :get_or_create_type
                        @println("create os % (%) for %", i.as_index(), var.log(self.pool), fid);
                        overload_out = (Some = i);
                    } else {|
                        overloads := @check(self.find_const(var[], OverloadSet.want())) return;  // :get_or_create_type
                        i := OverloadSet.assume_cast(overloads._0&)[];
                        os := self.dispatch.overloads&.nested_index(i.as_index());
                        os.pending&.push(fid, self.get_alloc());
                        @println("add to os % (%) for %", i.as_index(), var.log(self.pool), fid);
                        overload_out = (Some = i);
                    };
                };
                
                self.update_function_metadata(func);
            }
            fn DeclVar(f) => if f.name.kind == .Const {|
                self.scopes.put_constant(f.name, f.value, f.ty);
                stmt.stmt = .Noop;
            };
            @default => ();
        }
    };
    .Ok
}

fn update_function_metadata(self: *SelfHosted, func: *Func) void = {
    any_const_args :: fn(self: *Func) bool = {
        each self.arg.bindings { b |
            if(b.kind == .Const, => return(true));
        };
        false
    };
    if func.any_const_args() {|
        func.set_flag(.AnyConstArgs);
    };

    each func.annotations { tag | 
        @switch(tag.name) {
            @case(Flag.fold.ident())     => func.set_flag(.TryConstantFold);
            @case(Flag.noinline.ident()) => func.set_flag(.NoInline);
            // TODO: generic+unsafe_noop_cast are done in scope for old sema but i want to move them here. -- Jul 30
            @case(Flag.generic.ident())  => func.set_flag(.Generic);
            @case(Flag.unsafe_noop_cast.ident()) => func.set_flag(.UnsafeNoopCast);
            @case(Flag.redirect.ident()) => {
                // TODO: pass in ?overloadsetid and do this stuff -- Jul 30
                // // dman it bnoarow chekcser
                //let os = self.as_literal(out.1.unwrap(), loc)?;
                //if let Some(types) = self.program[id].get_tag_mut(Flag::Redirect) {
                //    let Expr::Tuple(parts) = &mut types.args.as_mut().unwrap().expr else {
                //        err!("bad",)
                //    };
                //    parts.push(os);
                //}
            };
            // TODO: actually im not sure if its better to do this later...
            //       it would be nice to do only one pass over the annotations 
            //       but it would also be nice to do absolutely no work if you never try to call the function. 
            @case(Flag.comptime_addr.ident()) => {
                panic("TODO: hoist comptime addr into body right now?");
            };
            @case(Flag.intrinsic.ident()) => {
                panic("TODO: hoist intrinsic into body right now?");
            };
            @case(Flag.inline.ident()) => {
                // TODO: error on conflicting annotations. 
                func.cc = (Some = .Inline);
            };
            @case(Flag.ct.ident()) => {
                // TODO: error on conflicting annotations. 
                func.cc = (Some = .CCallRegCt);
            };
            @default => ();
        };
    };
    
    if func.cc.is_none() {|
        func.cc = (Some = .CCallReg);
    };
}

::if(Maybe(void));
::if_opt(Type, Maybe(void));
fn compile_get_var(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    @debug_assert(expr.expr&.is(.GetVar));
    var := expr.expr.GetVar;
    if var.kind == .Const {|
        (value, ty) := @check(self.find_const(var, requested)) return;
        expr.set(value, ty);
        @check(self.coerce_const_expr(expr, requested)) return;
        .Ok
    } else {|
        if self.scopes.get_var_type(var) { ty | 
            expr.ty = ty;
            expr.done = true;
            // Reading a variable. Convert it to `var&[]` so compiling it checks for smaller loads (u8, etc).
            ptr_ty := self.ptr_type(ty);
            expr[] = synthetic_ty((Addr = self.box(expr[])), expr.loc, ptr_ty);
            expr.done = true;
            // Note: not using deref_one, because don't want to just remove the ref, we want raw variable expressions to not exist. kinda HACK
            expr[] = synthetic_ty((Deref = self.box(expr[])), expr.loc, ty);
            expr.done = true;
            .Ok
        } else {|
            // For now runtime vars are always declared in order so we always know thier type.
            // This might change to allow peeking into return-ed expressions when infering closure return type? -- Jul 15
            @err("Unknown type for runtime var %", self.pool.get(var.name))
        }
    }
}

// TODO: this is kinda weird. `fn` statements create an overload set or add to an existing one. 
fn is_empty_constant(self: *SelfHosted, name: Var) bool #once = {
    var := self.scopes.get_constant(name);
    var := or(var, => return(true));
    var._0.expr&.is(.Poison)
}

// Passing in the requested type here feels a bit weird, but I think it will make anon-functions less painful. 
fn find_const(self: *SelfHosted, name: Var, requested: ResultType) Maybe(Ty(Values, Type)) = {
    // If someone else is already trying to compile this, we don't want to fight over it. 
    if self.dispatch.const_var_in_progress&.get(name.id.zext()) {|
        return(Suspend = self.wait_for(EvalConstant = (name = name, requested = requested)));
    };
    
    var := self.scopes.get_constant(name);
    if var& { var | 
        // If we've compiled this before, great.
        //     We can't coerce_constant here because we don't have a unique expression node to stick changes into if needed. 
        //     (need to change the expression to create function pointers because they might not be compiled yet)
        @if_let(var._0.expr&) fn Value(f) => {
            return(Ok = (f.bytes&.clone(self.get_alloc()), var._0.ty));
        };
    };
    
    self.dispatch.const_var_in_progress&.set(name.id.zext());
    
    // TODO: -- Jul 21
    // its sad to always yield here because _most_ of the time you could just do it now and it would be fine.
    // so an easy optimisation might be just trying now, if it wasn't already const_var_in_progress so we know nobody else is working on it. 
    // but since any _could_ yield, its a bug if we can't compile with _everything_ yielding,
    // so at least keep a flag to toggle this behaviour? 
    
    (Suspend = self.wait_for(EvalConstant = (name = name, requested = requested)))
}

fn find_const_non_blocking(self: *SelfHosted, name: Var) ?Ty(Values, Type) #once = {
    // If someone else is already trying to compile this, we don't want to fight over it. 
    if self.dispatch.const_var_in_progress&.get(name.id.zext()) {|
        return(.None);
    };
    var := self.scopes.get_constant(name);
    var := or(var, => return(.None));
    @if_let(var._0.expr&) fn Value(f) => {
        return(Some = (f.bytes&.clone(self.get_alloc()), var._0.ty));
    };
    .None
}

fn handle_declare_constant(self: *SelfHosted, name: Var, ty: *LazyType, value: *FatExpr) Maybe(void) #once = {
    @debug_assert(self.dispatch.enclosing_function.is_none(), "ICE: handle_declare_constant should have no enclosing function");
    
    // TODO: do the @rec and type checking stuff from the rust version. 
    @check(self.compile_expr(value, .None)) return;
    if value.ty == UnknownType {|
        return(@err("TODO: needed type hint for %", self.pool.get(name.name)));
    };
    if !value.expr&.is(.Value) {|
        val := @check(self.immediate_eval_expr(value, value.ty)) return;
        value.set(val, value.ty);
    };
    .Ok
}

fn save_const_values(self: *SelfHosted, name: Var, value: Values, final_ty: Type, loc: Span) Maybe(void) = {
    self.save_const(name, (Value = (bytes = value, coerced = false)), final_ty, loc)
}

fn save_const(self: *SelfHosted, name: Var, val_expr: Expr, final_ty: Type, loc: Span) Maybe(void) #once = {
    // TODO: do i have to check if someone is already working on this constant? -- Jul 30
    val := self.scopes.get_constant(name);
    val := or val {|
        // dup_var of return label in emit_capturing_call gets here.
        // eventually should remove the ? so its smaller and just do this there instead. 
        ptr := self.scopes.constants&.nested_index(name.id.zext());
        ptr[] = (Some = ((expr = (Poison = .Unknown), loc = loc, ty = UnknownType, done = false), .Infer));
        ptr.as_ref().unwrap()
    };
    
    if val._1&.is(.Finished) {|
        return(@err("tried to re-save constant %", name&.log(self.pool)));
    };
    if !val._0.expr&.is(.Poison) {|
        return(@err("tried to stomp constant %", name&.log(self.pool)));
    };
    
    val._0.expr = val_expr;
    val._0.ty = final_ty;
    val._1 = (Finished = final_ty);
    .Ok
}

fn coerce_const_expr(self: *SelfHosted, expr: *FatExpr, req: ResultType) Maybe(void) = {
    @debug_assert(expr.expr&.is(.Value), "tried to coerce non-value expr");
    
    if req.specific() { req | 
        if expr.ty != req {|
            @assert(!expr.expr.Value.coerced, "const mismatch. % vs % but already coerced", self.log_type(expr.ty), self.log_type(req));
            
            expr.expr.Value.coerced = true;
            expr.ty = req;
            panic("TODO: coerce constant (might be an actual type error)");
        };
    };
    .Ok
}

::?FnType;
fn immediate_eval_expr(self: *SelfHosted, expr: *FatExpr, ret_ty: Type) Maybe(Values) = {
    @println("imm_eval %", expr.log(self.pool));
    old_func := self.dispatch.enclosing_function;
    self.dispatch.enclosing_function = .None;
    ::?Values;
    if self.check_quick_eval(expr, ret_ty) { val |
        self.dispatch.enclosing_function = old_func;
        return(Ok = val);
    };
    
    // Can't just try to compile_expr here!
    // Since were evaluating in const context, any functions that are called in the expression weren't added to anyone's callees. 
    // So we want to say we need to recompile the expression, adding to callees of the lit_fn we're about to make. 

    // If its already a trivial function call, there's nothing else we can do to simplify, 
    // so we have to just yield on the function. 
    @if_let(expr.expr&) fn Call(f) => {
        // TODO: `if self.check_quick_eval(f.arg, f_ty.arg) { arg_value |` 
        //       instead of requiring void so its consistant with the fn_ptr version.
        //       tho also the call_dynamic is kinda sketchy (can only handle simple cases) and 
        //       i could just get rid of it and always generate a function that passes the arguments.    -- Jul 31
        if f.arg.is_raw_unit() && f.f.expr&.is(.Value) && self.get_type(f.f.ty).is(.Fn).or(f.f.ty == FuncId) {|
            fid := FuncId.assume_cast(f.f.expr.Value.bytes&)[];
            if self.aarch64&.get_fn(fid) { fn_ptr | 
                hack := self&;
                args: RsVec(i64) = empty();
                func := self.get_function(fid);
                @debug_assert(!func.get_flag(.AnyConstArgs));
                f_ty := func.finished_ty().expect("known type once compiled");
                @debug_assert(func.cc.unwrap() == .CCallReg);
                self.aarch64&.bump_dirty();
                @println("imm_eval direct % %", fid.as_index(), self.pool.get(func.name));
                // SAFETY: we're passing comp_ctx=false so it won't try to use the pointer as a real CompilerRs.
                result := hack&.call_dynamic(fn_ptr.int_from_rawptr(), f_ty&, args&, false);
                self.dispatch.enclosing_function = old_func;
                if result&.is(.Err) {|
                    return(Err = self.box(result.Err));
                };
                return(Ok = result.Ok);
            };
            self.dispatch.enclosing_function = old_func;
            return(Suspend = self.wait_for(Jit = fid));
        };
    };
    
    // The expression is too complex to deal with here. 
    // So box it into a function, compile that normally, and then just call into it with no arguments when we come around again.
    // This operation is make_lit_function from the old compiler. 
    // We know we'll yield to compile the new function, and don't want its body to alias the old expr. 
    
    bindings: List(Binding) = list(self.get_alloc());
    bindings&.if_empty_add_unit();
    arg: Pattern = (bindings = bindings.rs(), loc = expr.loc);
    def: FnDef = (name = .None, arg = arg, ret = (Finished = ret_ty), tags = list(temp()), loc = expr.loc);
    // HACK? it feels like this would be too shallow... and i don't think the old compiler had to do this... but i can't yet make a test that this breaks.
    expr.done = false; 
    fake_func := make_func(def, (Some = expr[]), false);
    // We didn't bother setting a scope because we don't need one, the expression will already have been resolved. 
    fake_func&.set_flag(.ResolvedBody);
    fake_func&.set_flag(.ResolvedSign);
    fake_func.finished_arg = (Some = void);
    fake_func.finished_ret = (Some = ret_ty);
    fake_func.cc = (Some = .CCallReg);    
    fid := self.add_function(fake_func);
    @println("imm eval wait for lit %: %", fid, expr.log(self.pool));
    f_value := self.to_values(FuncId, fid);
    f_e: Expr = (Value = (bytes = f_value, coerced = false));
    f_expr := self.box(synthetic_ty(f_e, expr.loc, FuncId));
    unit_expr := self.box(synthetic_ty((Value = (bytes = unit_value, coerced = false)), expr.loc, void));
    expr.expr = (Call = (f = f_expr, arg = unit_expr));  // so we try this task again, we get an easy function call.
    expr.done = false;
    self.dispatch.enclosing_function = old_func;
    (Suspend = self.wait_for(Jit = fid))
}

fn check_quick_eval(self: *SelfHosted, expr: *FatExpr, ret_ty: Type) ?Values = {
    @match(expr.expr&) {
        fn Value(f) => (Some = f.bytes&.clone(self.get_alloc()));
        fn GetVar(f) => {
            if f.kind == .Const {|
                // This is a super common case because you type `arg: i64` a lot. 
                res := self.find_const_non_blocking(f[]);
                if res { f |
                    // We can't report a type erorr from here so just let the slow path deal with it. 
                    if(ret_ty == f._1, => return(Some = f._0));
                };
            };
            .None
        }
        fn Block(it) => {
            if it.body.is_empty() {|
                return(self.check_quick_eval(it.result, ret_ty));
            };
            .None
        }
        fn Tuple(parts) => {
            all: List(u8) = list(self.get_alloc());
            each parts { part |
                // TODO: tuple_types
                val := or self.check_quick_eval(part, part.ty) {|
                    return(.None)
                };
                all&.push_all(val&.bytes());
            };
            (Some = (Big = all.rs()))
        }
        fn Call(it) => {
            if it.f.expr&.is(.Value) {|
                @if_let(self.get_type(it.f.ty)) fn FnPtr(f) => {
                    if self.check_quick_eval(it.arg, f.ty.arg) { arg_value |
                        f_ptr := i64.assume_cast(it.f.expr.Value.bytes&)[];
                        // SAFETY: we're passing comp_ctx=false so it won't try to use the pointer as a real CompilerRs.
                        evil_hack := self;
                        evil_hack := self&;
                        self.finish_layout_deep(it.f.ty);
                        res := evil_hack&.call_dynamic_values(f_ptr, f.ty&, arg_value&.bytes(), false);
                        @match(res) {
                            fn Ok(res) => {
                                expr.set(res&.clone(self.get_alloc()), f.ty.ret);
                                return(Some = res);
                            }
                            fn Err(e) => {
                                @println("ERROR: %", e.msg);   
                            } // TODO: don't just swollow errors
                        }
                    };
                };
            };
            .None
        }
        @default => .None;
    }
}

fn create_slice_type(self: *SelfHosted, inner: Type) Maybe(Type) = {
    panic("TODO: create_slice_type")
}
