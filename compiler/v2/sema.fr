// the compiler of theseus 

// TODO: resolve lazily because some things don't care, 
//       like @enum(a, b, c) just treats the expressions as identifiers. 
//       we have to keep the scope info around forever anyway, if it was done here, 
//       we wouldn't traverse the tree twice. but then even more redundant work when cloning for generics? 

ResultType :: @rec @tagged(
    Specific: Type,
    Returning: Type,
    Tuple: []ResultType,  // temporary storage! don't hold across yield points. 
    None
);

OverloadSetData :: @struct(
    ready: RawList(FuncId),
    name: Symbol,
    pending: RawList(FuncId),
    inline_cache: Ty(OverloadKey, FuncId),
    table: ?HashMap(OverloadKey, FuncId) = .None,
);

fn compile_stmt(self: *SelfHosted, stmt: *FatStmt) Maybe(Ty(bool, bool)) #once = {
    if stmt.done {
        return(Ok = (true, false));
    };
    self.last_loc = stmt.loc;
    @match(stmt.stmt&) {
        fn Noop() => (Ok = (true, false));
        fn Eval(expr) => {
            @check(self.compile_expr(expr, .None)) return;
            res := (expr.done, expr.ty.is_never());
            if expr.expr&.is(.Value) && expr.ty == self.get_or_create_type(void) {
                stmt.stmt = .Noop;
            };
            (Ok = res)
        }
        fn Decl(f) => {
            @assert(f.kind != .Const, "hit unhoisted constant %", self.comp().log(stmt));
            @assert(f.name&.is(.Var), "hit unresolved var %", self.comp().log(stmt));
            @check(self.decl_var(f.name.Var, f.ty&, f.default&)) return;
            (Ok = (f.default.done, false))
        }
        fn Set(f) => {
            if f.place.expr&.is(.GetVar) {
                @check(self.compile_get_var(f.place&, .None, false)) return;
                @check(self.compile_expr(f.value&, f.place.ty.want())) return;
                if !self.ask_can_assign(f.place.ty, f.value.ty) {
                    //return(@err("tried to set % <- % (%)", self.log_type(f.place.ty), self.log_type(f.value.ty), f.value&.log(self)));
                    return(Err = self.error(TypeMismatch = (span = Span.zeroed(), wanted = f.place.ty, found = f.value.ty)));
                };
                return(Ok = (f.value.done, false));
            };
            // TODO: PrefixMacro is sketchy but makes []->.index work.
            if (@is(f.place.expr&, .PrefixMacro, .FieldAccess, .Deref, .Block)) {
                @check(self.compile_place_expr(f.place&, .None, true)) return;
                @check(self.compile_expr(f.value&, f.place.ty.want())) return;
                if !self.ask_can_assign(f.place.ty, f.value.ty) {
                    //return(@err("tried to set % <- % (%)", self.log_type(f.place.ty), self.log_type(f.value.ty), f.value&.log(self)));
                    return(Err = self.error(TypeMismatch = (span = Span.zeroed(), wanted = f.place.ty, found = f.value.ty)));
                };
                // TODO: self.type_check_arg(f.place.ty, f.value.ty, "reassign var")?; /:type_check
                return(Ok = (f.value.done && f.place.done, false));
            };
            @if_let(f.place.expr&) fn GetNamed(n) => {
                return(@err("Tried to assign to undeclared variable: %", self.pool.get(n[])));
            };
            @err("Illegal place expression: %", tag)
        }
        fn DeclVarPattern(f) => self.decl_var_pattern(stmt);
        @default => @panic("TODO: unhandled compile_stmt type % %", stmt.stmt&.tag(), stmt.log(self));
    }
}

fn decl_var_pattern(self: *SelfHosted, stmt: *FatStmt) Maybe(Ty(bool, bool)) #once = {
    @debug_assert(stmt.stmt&.is(.DeclVarPattern));
    bindings, value := (stmt.stmt.DeclVarPattern.binding.bindings&, stmt.stmt.DeclVarPattern.value&);
    if value.is_raw_unit() {
        // TODO: typecheck the pattern! this was fine a long time ago when this node was only made by closures. 
        stmt.stmt = .Noop;
        return(Ok = (true, false));
    };
    
    if bindings.len == 1 {
        b := bindings[0]&;
        if(b.kind == .Const, => return(@err("unreachable? destructure single arg must be closure but const args should already be handled")));
        if b.var() { name |
            // TODO: could reduce the ast node but it doesn't matter. 
            @check(self.decl_var(name, b.ty&, value)) return;
            return(Ok = (value.done, false));
        };
        // match branch that doesn't specify an argument gets here. (because parser puts in fake Binding with name=None).
        // just to be safe, still keep the expression in case it somehow happens in a different situation and has side effects.
        stmt.stmt = (Eval = value[]);
        //@println("% %", FatExpr.int_from_ptr(stmt.stmt.Eval&), stmt.stmt.Eval.expr&.tag());
        //self.codemap.show_error_line(stmt.loc);
        //println(stmt.stmt.Eval&.log(self));
        @check(self.compile_expr(stmt.stmt.Eval&, .None)) return;
        return(Ok = (stmt.stmt.Eval.done, false));
    };
    
    // So its not a tuple, we don't know how to destructure it (emit_bc does), but we can still type check it. 
    // Even if it was a tuple, all we'd do differently is pass in the ResultTypes seperatly, but now we can just do that for any expression. 
    // If i was inspired this could grow a speical case for tuples like the old one had to avoid allocating the lists,
    // but temp() is cheap so its probably fine.  -- Aug 1
    
    result_types: List(ResultType) = list(bindings.len, temp());
    each bindings { b | 
        if(b.kind == .Const, => return(@err("TODO: destructure with constants")));
        request := ResultType.None; 
        if @check(self.infer_type(b.ty&)) return { known |
            request = (Specific = known);
        };
        result_types&.push(request);
    };
    
    @check(self.compile_expr(value, (Tuple = result_types.items()))) return; 
    types := or self.tuple_types(value.ty) {
        return(@err("destructure expected tuple type"))
    };
    if(types.len != bindings.len, => return(@err("destructuring arity mismatch expected % but found %", bindings.len, types.len)));
    
    enumerate bindings { i, b |
        if b.var() { name | 
            prev := self.scopes.get_var_type(name); 
            self.scopes.put_var_type(name, types[i]);
            self.scopes.get_var_type(name).unwrap()[].took_address = true; // TODO: don't do this
            @debug_assert(prev.is_none() || { t := prev.unwrap(); t.type == types[i] });
        };
        // An inlined closure might have had a polymorphic parameter, but emit_bc expects known types.
        if b.ty&.is(.Infer) {
            b.ty = (Finished = types[i]);
        };
    };
    
    // TODO: :mark_stmt_done 

    (Ok = (value.done, false))
}

::opt_map(Type, Type);
fn compile_place_expr(self: *SelfHosted, place: *FatExpr, requested: ResultType, want_deref: bool) Maybe(void) = {
    loc := place.loc;
    self.last_loc = place.loc;
    @match(place.expr&) {
        fn Block(f) => {
            @check(self.ensure_resolved(f)) return;
            // TODO: the old version didn't have to do this... 
            if f.body.is_empty() && f.ret_label.is_none() {
                place[] = f.result[]; // Not required but makes debugging easier cause there's less stuff.
                return(self.compile_place_expr(place, requested, want_deref));
            };
            return(@err("a block cannot be a place expression (for spite reasons, not technical ones)"));
        }
        fn Cast(arg) => {
            return(self.compile_place_expr(arg[], requested, want_deref));
        }
        fn GetVar(var) => {
            info := self.scopes.get_var_type(var[]);
            info := or info {| 
                return(@err("var must be declared: %", var.log(self)))
            };
            info.took_address = true;
            ptr_ty := self.ptr_type(info.type);

            place.done = true;
            place[] = synthetic_ty((Addr = self.box(place[])), loc, ptr_ty);
            place.done = true;
            if want_deref {
                @check(self.deref_one(place)) return;
                place.done = true;
            };
        }
        fn FieldAccess(f) => {
            // TODO: could lookup field and pass down requested
            // Note: compile_expr may have already walked the container because it has to check if its an enum/scope.
            @check(self.compile_place_expr(f.container, .None, false)) return;
            done := f.container.done;

            // :AutoDeref
            {
                raw   := self.raw_type(f.container.ty);
                inner := or self.unptr_ty(raw) {
                    return(@err("PlaceExpr of FieldAccess should be ptr"))
                };
                inner = self.raw_type(inner);
                // Pointers never have fields, so the thing behind the pointer, shouldn't be a pointer.
                // This lets you write `self: *Self; self.name` instead of  `self: *Self; self[].name`.
                while => self.unptr_ty(inner) { next_inner | 
                    @check(self.deref_one(f.container)) return;
                    inner = self.raw_type(next_inner);
                };
            };
            name := f.name;
            bytes, field_val_ty := @check(self.field_access_expr(f.container, name)) return;
            field_ptr_ty := self.ptr_type(field_val_ty);
            @debug_assert(!f.container.expr&.is(.GetVar), "ICE: place expr can't be direct var access");
            e := self.box(f.container[]);
            // don't want to use Cast because graphics/src/shaders wants to know the field name after compiling. 
            //if bytes == 0 {
            //    place.expr = (Cast = e);
            //} else {
                place.expr = (PtrOffset = (
                    ptr = e,
                    bytes = bytes,
                    name = name,
                ));
            //};
            place.done = done;
            if want_deref {
                place.ty = field_ptr_ty;
                // Now we have the offset-ed ptr, add back the deref
                @check(self.deref_one(place)) return;
                place.done = done;
            } else {
                place.ty = field_ptr_ty;
            };
        }
        fn Deref(_) => {
            return(self.compile_deref_place(place, requested, want_deref));
        }
        fn GetNamed(n) => return(@err("Undeclared Identifier: %!", self.pool.get(n[])));
        fn PrefixMacro(_) => {
            // TODO: this is sketchy but makes []->.index work.
            //       need to think about how requested/want_deref are handled
            @check(self.compile_expr(place, requested)) return;
            return(self.compile_place_expr(place, requested, want_deref));
        }
        @default => {
            if !place.ty.is_unknown() {
                // TODO: pass in if we're currently trying to access a field so we can give a better error message if its on an int or something?
                return(@err("Place expression of type % expected pointer dereference.\n%", self.log_type(place.ty), place.log(self)));
            };
            return(@err("TODO: other `place=e;` % %", place.log(self), self.log_type(place.ty)));
        };
    };
    .Ok
}

fn compile_deref_place(self: *SelfHosted, place: *FatExpr, requested: ResultType, want_deref: bool) Maybe(void) = {
    @debug_assert(place.expr&.is(.Deref));
    arg := place.expr.Deref;
    // When you see a !deref, treat the expression as a pointer value.
    if arg.expr&.is(.GetVar) && !want_deref && arg.expr.GetVar.kind != .Const {
        @check(self.compile_get_var(arg, requested, true)) return; // :?
        @debug_assert(arg.expr&.is(.Deref), "compile_get_var should make &[] not %", self.comp().log(arg));
    };
    req := requested.specific().map(fn(r) => self.ptr_type(r));
    @check(self.compile_expr(arg, req.want())) return;
    if want_deref {
        place.ty = or self.unptr_ty(arg.ty) {
            return(@err("tried to deref non-pointer"))
        };
        place.done = arg.done;
    } else {
        place[] = arg[];
    };
    .Ok
}

// :PlaceExpr
fn field_access_expr(self: *SelfHosted, container_ptr: *FatExpr, name: Symbol) Maybe(Ty(i64, Type)) #once = {
    container_ptr_ty := self.raw_type(container_ptr.ty);
    depth := self.ptr_depth(container_ptr_ty);
    if depth != 1 {
        return(@err("index expr ptr must be one level of indirection. %", self.log_type(container_ptr_ty)));
    };
    container_ty := self.unptr_ty(container_ptr_ty).expect("ICE: we just checked its a pointer");

    raw_container_ty := self.raw_type(container_ty);
    self.finish_layout(raw_container_ty);
    @match(self.get_type(raw_container_ty)) {
        fn Struct(f) => {
            @debug_assert(f.layout_done);
            each f.fields { f | 
                if f.name == name {
                    return(Ok = (f.byte_offset, f.ty));
                };
            };
            return(Err = self.error(InvalidField = (span = Span.zeroed(), container = container_ty, name = name)))
        }
        fn Tagged(f) => {
            each f.cases { f |
                if f._0 == name {
                    return(Ok = (8, f._1));
                };
            };
            return(Err = self.error(InvalidField = (span = Span.zeroed(), container = container_ty, name = name)))
        }
        @default => {
            return(Err = self.error(InvalidField = (span = Span.zeroed(), container = container_ty, name = name)))
        };
    }
}

// TODO: have a different version of @check for something that can error but not yield? 
fn deref_one(self: *SelfHosted, ptr: *FatExpr) Maybe(void) = {
    raw   := self.raw_type(ptr.ty);  // TODO: why are we going through enums...? -- Jul 30
    inner := or self.unptr_ty(raw) {
        return(@err("expected ptr for deref_one"))
    };
    
    @match(ptr.expr&) {
        fn Addr(arg) => {
            // this allows auto deref to work on let ptr vars.
            if arg.expr&.is(.GetVar) {
                if self.scopes.get_var_type(arg.expr.GetVar) { it |
                    @debug_assert(it.took_address, "deref but not took_address");
                } else {
                    @debug_assert(false, "deref_one missing variable?");
                };
                // raw var expr is not allowed, we always refer to variables through thier address.
                ptr[] = synthetic_ty((Deref = self.box(ptr[])), ptr.loc, inner);
            } else {
                // Avoid reduntant (whatever)&[].
                ptr[] = arg[][];
                if ptr.ty.is_unknown() {
                    ptr.ty = inner; // TODO: this shouldn't happen
                };
            };
        }
        @default => {
            ptr[] = synthetic_ty((Deref = self.box(ptr[])), ptr.loc, inner);
        };
    };
    .Ok
}

fn decl_var(self: *SelfHosted, name: Var, ty: *LazyType, value: *FatExpr) Maybe(void) = {
    @log_event("decl_var %", name&.log(self)) self;
    // TODO: this would make sense but i don't actually need it i guess since i store variable types globally.
    //@debug_assert(self.dispatch.enclosing_function.is_some(), "ICE: runtime vars must have an enclosing function");
    no_type := ty.is(.Infer);
    self.last_loc = value.loc;
    want := ResultType.None;
    xxx := @check(self.infer_type(ty)) return;
    if xxx { known |
        want = (Specific = known);
    };
    @check(self.compile_expr(value, want)) return;
    final_ty := value.ty;
    @debug_assert(!final_ty.is_unknown(), "if we didn't yield, we should know the type %", self.comp().log(value));
    if no_type {
        // Since there was no type annotation, we don't need a type check. Whatever we got is the type of this variable now. 
        ty[] = (Finished = final_ty);
        self.finish_layout_deep(final_ty);
    } else {
        expected_ty := ty.unwrap();
        // TODO: :delay_layout
        //       Instead of doing this now, add it as an Action in the dispatch loop.
        //       We don't actually care about the field offsets yet, we just need them later for emitting bytecode. 
        //       This was one of the changes that inspired the sema rewrite.  -- Jul 25
        self.finish_layout_deep(expected_ty);
        self.finish_layout_deep(final_ty);
        @check(self.can_assign(expected_ty, final_ty)) return;
        // TODO: panic("TODO: self.type_check_arg(value, ty, \"var decl\")?;");
    };

    prev := self.scopes.get_var_type(name); 
    self.scopes.put_var_type(name, final_ty); // TODO: this returns is_new, i want to just assert that. we shouldn't be compiling more than once -- Jul 25
    // TODO: prev should always be none?? but its not a constant and seems to always be the same so its probablby not a super huge deal? -- Apr 23
    //       maybe its just cause im not zeroing the stmt and end up compiling multiple times. -- Apr 25
    @debug_assert(prev.is_none() || { t := prev.unwrap(); t.type == final_ty });
    @err_assert(name.name != Flag.return.ident(), "naming a variable 'return' is not the best idea") return;
    @err_assert(name.name != Flag.local_return.ident(), "naming a variable 'local_return' is not the best idea") return;
    // TODO: :mark_stmt_done
    //       this function is called multiple times (if the containing block needs to yield). need to have a done flag on the fatstmt
    //       but thats easier to change once all the stuff is written in one language -- Jul 25
    .Ok
}

fn ensure_resolved_sign(self: *SelfHosted, fid: FuncId) Maybe(void) #inline = {
    func := self.get_function(fid);
    if(func.get_flag(.ResolvedSign), => return(.Ok));
    inners :: fn(self: *SelfHosted, func: *Func) Maybe(void) = {
        @try(self.resolve_sign(func)) return;
        @try(self.require_unique_fields(func.arg&)) return;
        func.set_flag(.ResolvedSign);
        .Ok
    };
    inners(self, func)
}

fn ensure_resolved_body(self: *SelfHosted, fid: FuncId) Res(void) #inline = {
    func := self.get_function(fid);
    if(func.get_flag(.ResolvedBody), => return(.Ok));
    innerb :: fn(self: *SelfHosted, func: *Func) Res(void) = {
        @try(self.resolve_sign(func)) return;
        @try(self.require_unique_fields(func.arg&)) return;
        func.set_flag(.ResolvedSign);
        @try(self.resolve_body(func)) return;
        func.set_flag(.ResolvedBody);
        .Ok
    };
    innerb(self, func)
}

fn handle_compile_func_body(self: *SelfHosted, fid: FuncId) Maybe(void) #once = {
    func := self.get_function(fid);
    if(func.get_flag(.EnsuredCompiled), => return(.Ok));
    assert(!func.get_flag(.MayHaveAquiredCaptures), "closures need to be specialized");
    assert(!func.get_flag(.AnyConstArgs), "const args need to be specialized");
    
    
    if func.get_flag(.TargetSplit) {
        return(self.handle_target_split(func));
    };
    
    @debug_assert(self.dispatch.function_in_progress&.get(fid.as_index()), "expected to do work on in progress function %", fid);
    @try(self.ensure_resolved_body(fid)) return;
    
    // Before we can do the body, we really need to know the argument types, 
    // and it would be nice to know the return type too but that's less important. 
    @check(self.infer_arguments(fid)) return;
    
    // you need to only do this once so args are unique but we might yield below. 
    // you can't yield in this block! MadeVarsForRuntimeArgs needs to be atomic!
    // TODO: alternativly, if i trusted myself, you could just say its fine when the arg is already there, 
    //       because surely we put it there ourselves last time around. 
    //       but for now i think this is a valuable sanity check that renumbering went well.  -- Jul 30
    if !func.get_flag(.MadeVarsForRuntimeArgs) {
        each func.arg.bindings { b | 
            // TODO: probably want to change this so you can do as much compiling as possible before expanding templates.
            @debug_assert(b.kind != .Const, "ICE: Tried to emit before binding const args.");
            @if_let(b.name) fn Var(name) => {
                @debug_assert(b.kind == name.kind);
                is_new := self.scopes.put_var_type(name, b.ty&.unwrap());
                @log_event("bind param %", name&.log(self)) self;
                if(!is_new, => return(@err("overwrite arg? %", name&.log(self))));
            };
        };
        func.set_flag(.MadeVarsForRuntimeArgs);
    };
    ok := false;
    
    check_body := true;
    if func.get_flag(.BodyIsSpecial) {
        check_body = @check(self.emit_special_body(fid)) return;
        ok = true;
    };
    
    @if(check_body)
    if find_impl(func.body&, .Normal) { expr |
        ok = true;
        old_func := self.dispatch.enclosing_function;
        self.dispatch.enclosing_function = (Some = fid);
        
        if !func.ret&.is(.Infer) {
            @check(self.infer_return(fid)) return;
        };
            
        if func.return_var { return_var |
            // TODO: this means you cant early return from non-block functions but the error message will be useless -- Jul 9 
            @if_let(expr.expr&) fn Block(f) => {
                @check(self.ensure_resolved(f)) return;
                if f.ret_label.is_none() {|  // we might have already tried to compile this function. 
                    ret: LabelId = from_index(self.dispatch.return_labels.len);
                    self.dispatch.return_labels&.push(fid);
                    label_ty := self.get_or_create_type(LabelId);
                    if func.finished_ret { ret_ty | 
                        label_ty = self.intern_type(Label = ret_ty);
                    };
                    val := self.to_values(LabelId, ret);
                    @check(self.save_const_values(return_var, val, label_ty, func.loc)) return;
                    f.ret_label = (Some = ret);
                };
            };
        };
        
        @check(self.compile_expr(expr, func.finished_ret.want())) return;
        //@debug_assert(self.dispatch.enclosing_function.is_some() && self.dispatch.enclosing_function.unwrap() == fid, "lost enclosing");
        
        if func.ret&.is(.Infer) {
            func.finished_ret = (Some = expr.ty);
        } else {
            wanted := @check(self.infer_return(fid)) return;
            if expr.is_const() {
                @check(self.coerce_const_expr(expr, (Specific = wanted), false)) return;
            };
            self.last_loc = expr.loc;
            @check(self.can_assign(wanted, expr.ty)) return;
        };
        self.dispatch.enclosing_function = old_func;
    };
    if !ok {
        self.last_loc = func.loc;
        return(@err(
            "non special function '%' must have body not %. (or ICE: double compiled function?)", 
            self.pool.get(func.name),
            func.body&.tag(),
        ));
    };
    
    @log_event("body compiled: % %", fid.as_index(), self.log_name(fid)) self;
    func.set_flag(.EnsuredCompiled);
    .Ok
}

fn handle_target_split(self: *SelfHosted, func: *Func) Maybe(void) #once = {
    ::AutoEq(?CallConv); ::RefEq(?CallConv);
    @err_assert(func.cc != (Some = .Inline), "#target_os functions cannot be #inline.\nThis is a limitation of the way we handle comptime execution + cross compilation (which affects you even when not cross compiling).") return;
    
    os_ty := @unwrap(self.env.os_type, "#target_os before bootstrap") return;
    if func.body&.is(.Normal) {
        //@debug_assert(func.return_var.is_none(), "should be before resolve");
        // TODO: typecheck now? 
        merged: List(FuncImpl) = list(self.get_alloc());
        for_enum(Os) { os |
            new_func := func.deep_clone(self.get_alloc()); // :SLOW redundantly clones dead code if you immediatly @match on the arg
            @if(!LAZY_SCOPE) {
                mapping: RenumberResults = init();
                renumber: RenumberVars = (scope = self.scopes, mapping = mapping&, compiler = self);
                renumber&.walk_func(new_func&);
            };
            new_func&.unset_flag(.TargetSplit); // very important to not loop!
            factory := self.box(new_func.body.Normal);
            os_value := self.to_values(Os, os);
            os_value := synthetic_ty((Value = (bytes = os_value, coerced = false)), factory.loc, os_ty);
            os_value := self.box(os_value);
            body_expr := synthetic((Call = (f = factory, arg = os_value)), factory.loc);
            body_expr := self.box(body_expr);
            // TODO: garbage HACK about how i represent return labels
            //       you need a block for the outer function's `return` to bind to,
            //       otherwise you don't get a return_var and the error message is useless 
            //       `Compile Error: Poison expression Label`
            body_expr := synthetic(new_block(empty(), body_expr), factory.loc);
            new_func.body.Normal = body_expr;
            new_fid := self.add_function(new_func);
            merged&.push(TargetOsSplit = (os = os, fid = new_fid));
        };
        func.body = (Merged = merged.as_raw());
    };
    @debug_assert(func.body&.is(.Merged), "ICE: lost #target_os impls???");
    each func.body.Merged { impl |
        @err_assert(impl.is(.TargetOsSplit), "TODO: support do_merges + .TargetOsSplit?") return;
        fid := impl.TargetOsSplit.fid;
        if self.comp().want_target_os(impl.TargetOsSplit.os) {
            @check(self.compile_body(fid)) return;
        };
    };
    func.set_flag(.EnsuredCompiled);
    .Ok
}

fn want_target_arch(c: CompCtx, it: Arch) bool = {
    flag := c.get_build_options()[].target_arch_bitset;
    flag.bit_and(1.shift_left(it.raw())) != 0
}

fn want_target_os(c: CompCtx, it: Os) bool = {
    flag := c.get_build_options()[].target_os_bitset;
    flag.bit_and(1.shift_left(it.raw())) != 0
}

fn emit_special_body(self: *SelfHosted, fid: FuncId) Maybe(bool) = {
    @check(self.infer_return(fid)) return;
    func := self.get_function(fid);
    if func.get_flag(.EnsuredCompiled) {
        return(Ok = false);
    };
    good := false;
    fn eval_str(self: *SelfHosted, e: *FatExpr) Maybe(Symbol) = {
        inner_u8 := self.get_or_create_type(u8);
        str := @check(self.create_slice_type(inner_u8, e.loc)) return;
        ir := @check(self.immediate_eval_expr(e, str)) return;
        ir := Str.assume_cast(ir&)[];
        (Ok = self.pool.insert_owned(ir)) 
    }
    
    found_comptime_import :: fn(addr: rawptr) => {
        self.put_jitted_import(fid, addr);
        impls: List(FuncImpl) = list(2, self.get_alloc());
        impls&.push(ComptimeAddr = addr.int_from_rawptr());
        impls&.push(DynamicImport = func.name);
        
        //func.set_flag(.NoMangle);
        func.body = (Merged = impls.as_raw());
    };
       
    find_import :: fn(lib_name: Symbol) bool => {
        ret :: local_return;
        handle := self.comptime_libraries&.get(lib_name);
        if handle { handle |
            name := self.get_link_name(func);
            addr := handle.dlsym(name);
            if !addr.is_null() {
                found_comptime_import(addr);
                ret(true);
            };
        };
        // :AssumeDones
        // TODO: how to catch if it actually doesn't exist or they typo-ed the #import string.
        func.body = (DynamicImport = func.name);
        @eprintln("TODO: maybe missing import %", self.log_name(fid));
        @try(self.create_jit_shim(fid)) return;
        false
    };
    
    check_body := true;
    each func.annotations { tag | 
        @switch(tag.name) {
            @case(Flag.libc.ident()) => {
                done :: local_return;
                func.cc = (Some = .CCallReg);
                good = true;
                func.set_flag(.NoContext);
                
                if !find_import(Flag.libc.ident()) {
                    if get_smuggled_libc_function(self.env.comptime_arch, func.name) { addr |
                        found_comptime_import(addr);
                    };
                    // TODO: else fall back to the driver with resolve_comptime_import?
                };
            };
            @case(Flag.import.ident()) => {
                // :io_driver
                lib_name := @match(tag.non_void_arg()) {
                    fn Some(lib_name) => @check(self.eval_str(lib_name)) return;
                    fn None() => self.pool.insert_owned("");
                };
                
                if !find_import(lib_name) {
                    self.resolve_comptime_import(fid, func, lib_name);
                };
                
                good = true;
            };
            @case(Flag.unsafe_noop_cast.ident()) => {
                good = true;
            };
            @case(Flag.comptime_addr.ident()) => {
                good = true;
                ::as_ref(FatExpr);
                value := @unwrap(tag.non_void_arg(), "#comptime_addr requires arg") return;
                ptr := @check(self.eval(value, i64)) return;
                func.body = (ComptimeAddr = ptr);
                ptr := ptr.rawptr_from_int();
                self.put_jitted_import(fid, ptr);
                func.set_flag(.YesContext);
            };
            @case(Flag.intrinsic.ident()) => {
                good = true;
                value := @unwrap(tag.non_void_arg(), "#intrinsic requires arg") return;
                intrinsic_ty := @unwrap(self.env.intrinsic_type, "used #intrinsic during boot") return;
                op := @check(self.immediate_eval_expr(value, intrinsic_ty)) return;
                op := Intrinsic.assume_cast(op&)[];
                func.set_flag(.Intrinsic);
                impl := find_or_create_impl(func.body&, self.get_alloc(), .Intrinsic);
                impl.bc = op;
                // TODO: redundant with emit_bc
                if op != .GetContext {
                    func.set_flag(.NoContext);
                } else { 
                    func.set_flag(.YesContext);
                };
            };
            // TODO: im afraid this is slow :( its especially dumb because i do it for a billion int_from_ptr instantiations that do nothing
            @case(Flag.ir.ident()) => {
                good = true;
                value := @unwrap(tag.non_void_arg(), "#ir requires arg") return;
                args  := value.items();
                ::if(Ty(Qbe.O, Qbe.Cls));
                o, k := if args.len != 2 {
                    env := self.comp().get_comptime_env();
                    @err_assert(env.ir_op_cls_types != UnknownType, "expected #ir(.op, .cls) or to have called register_hacky_franca_ir_types()") return;
                    payload := @check(self.immediate_eval_expr(value, env.ir_op_cls_types)) return;
                    Ty(Qbe.O, Qbe.Cls).assume_cast(payload&)[]
                } else {
                    op_name  := @unwrap(args[0]&.ident(), "#ir(X, _) expected symbol") return;
                    cls_name := @unwrap(args[1]&.ident(), "#ir(_, X) expected symbol") return;
                    o := @unwrap(Qbe.O.from_name(self.pool.get(op_name)), "#ir invalid op name") return;
                    k := @unwrap(Qbe.Cls.from_name(self.pool.get(cls_name)), "#ir invalid op name") return;
                    (o, k)
                };
                @err_assert(k.raw() < 4 && o.raw() < ::Qbe.O.enum_count(), "invalid argument in #ir") return;
                func.set_flag(.Intrinsic);
                i := find_or_create_impl(func.body&, self.get_alloc(), .NewIntrinsic);
                i.ir_op  = @as(i32) o;
                i.ir_cls = @as(i32) k;
            };
            @case(Flag.redirect.ident()) => {
                good = true;
                value := @unwrap(tag.non_void_arg(), "#redirect requires arg") return;
                RedirectType :: @struct(arg: Type, ret: Type, os: OverloadSet);
                payload := self.tuple_of(@slice(Type, Type, OverloadSet));
                payload := @check(self.immediate_eval_expr(value, payload)) return;
                payload := RedirectType.assume_cast(payload&)[];
                args := self.arg_types(payload.arg);
                f_ty: FnType = (arg = payload.arg, ret = payload.ret, arity = args.len.trunc());
                target := @check(self.resolve_by_type(payload.os, f_ty, func.loc)) return;
                func.body = (Redirect = target);
            };
            // TODO: remove all the different string based ones and have #asm("llvm") #asm("qbe") FuncImpl::AsmStr(Backend:Symbol, Source:Symbol)
            //       so you can have driver programs add new backends without modifying the core compiler. 
            //       then move backends/qbe.fr to be an external thing in examples.   :io_driver
            @case(Flag.asm.ident()) => {
                // - tuple of string literals -> llvm-ir
                // - tuple of 32-bit int literals -> aarch64 asm ops
                // - anything else, comptime eval expecting Slice(u32) -> aarch64 asm ops
                @debug_assert(func.body&.is(.Normal), "we already checked that #asm has body");
                @debug_assert(!func.get_flag(.AnyConstArgs), "#asm const args should already have been bound");
                check_body = false;
                
                asm := func.body.Normal&;
                
                each func.annotations { tag | 
                    continue :: local_return;
                    @switch(tag.name) {
                        @case(Flag.aarch64.ident()) => {
                            @err_assert(!good, "found multiple arches for #asm") return;
                            if !self.comp().want_target_arch(.aarch64) {
                                func.body = .Empty;
                                good = true; 
                                continue();
                            };
                            // TODO: :PushConstFnCtx
                            // TODO: you can't just compile here because then trying to imm_eval hits a not read asm func i think because of ^ callees.
                            //       it recurses and has to emit other asm first but they don't get put in dispatch,
                            //       becuase they don't have a thing in the result stack to do callees first.
                            ::if(RawList(u32));
                            ops: RawList(u32) = if asm.expr&.is(.Tuple) {
                                parts := asm.expr.Tuple&;
                                ops: List(u32) = list(parts.len, self.get_alloc());
                                u32_ty := self.intern_type(Int = (bit_count = 32, signed = false));
                                each parts { int |
                                    i := @check(self.immediate_eval_expr(int, u32_ty)) return;
                                    i := u32.assume_cast(i&)[];
                                    ops&.push(i);
                                };
                                ops.as_raw()
                            } else {
                                return(@err("TODO: untested #asm case."))
                            };
                        
                            func.body = (JittedAarch64 = ops);
                            good = true; 
                            
                            // note: you want to do this before do_merges because flags aren't preserved. 
                            if func.get_flag(.LogAsm) {
                                self.codemap.show_error_line(func.loc);
                                // TODO: go through driver program? :driver_io
                                //@if(USE_LLVM_DIS) print_llvm_mc_dis(ops.items());
                            }; 
                        };
                        // TODO: an erorr message here gets swollowed because you're probably in the type_of shit. this is really confusing. need to do beter
                        // TODO: if its a string literal just take it
                        // TODO: check if they tried to give you something from the stack
                        @case(Flag.llvm.ident()) => {
                            @err_assert(!good, "found multiple arches for #asm") return;
                            ir := @check(self.eval_str(asm)) return;
                            func.body = (LlvmIr = ir);
                            good = true;
                        };
                        @case(Flag.qbe.ident()) => {
                            @err_assert(!good, "found multiple arches for #asm") return;
                            ir := @check(self.eval_str(asm)) return;
                            func.body = (QbeIr = ir);
                            good = true;
                        };
                        @case(Flag.c.ident()) => {
                            @err_assert(!good, "found multiple arches for #asm") return;
                            ir := @check(self.eval_str(asm)) return;
                            func.body = (CSource = ir);
                            good = true;
                        };
                        @case(Flag.x86.ident()) => {
                            @err_assert(!good, "found multiple arches for #asm") return;
                            if !self.comp().want_target_arch(.x86_64) {
                                func.body = .Empty;
                                good = true; 
                                continue();
                            };
                            ir := @check(self.eval_str(asm)) return;
                            func.body = (X86AsmText = ir);
                            good = true;
                        };
                        @case(Flag.x86_bytes.ident()) => {
                            @err_assert(!good, "found multiple arches for #asm") return;
                            if !self.comp().want_target_arch(.x86_64) {
                                func.body = .Empty;
                                good = true; 
                                continue();
                            };
                            // TODO: typecheck
                            type := @unwrap(self.env.push_x86_func, "cannot use #asm before bootstrap") return;
                            fid := @check(self.immediate_eval_expr(asm, type)) return;
                            fid := FuncId.assume_cast(fid&)[];
                            fn_ptr := or self.get_fn_callable(fid) {| 
                                return(Suspend = self.wait_for(Jit = fid))
                            };
    
                            ir: List(u8) = list(self.get_alloc());
                            callee := assume_types_fn(Arg = *List(u8), Ret = void, ptr = fn_ptr);
                            zone := zone_begin(.CallDynamic);
                            callee(ir&);  // :call_dynamic_values
                            zone_end(zone);
                            
                            func.body = (X86AsmBytes = ir.as_raw());
                            good = true;
                            
                            // note: you want to do this before do_merges because flags aren't preserved. 
                            if func.get_flag(.LogAsm) {
                                self.codemap.show_error_line(func.loc);
                                show_hex_bytes(ir.items());
                                // TODO: go through driver program? :driver_io
                                //@if(USE_LLVM_DIS) print_llvm_mc_dis(ir.items());
                            }; 
                        };
                        @case(Flag.bc.ident()) => {
                            return(@err("TODO: deal with #bc. need to make drivers more powerful? this has to work to prove my point."));
                            @err_assert(!good, "found multiple arches for #asm") return;
                            type := @unwrap(self.env.inline_bc_fn_type, "#bc needs you to call `::enable_inline_bc();` first for now. (or tried to use before bootstrap)") return;
                            callee := @check(self.immediate_eval_expr(asm, type)) return;
                            callee := FuncId.assume_cast(callee&)[];
                            fn_ptr := or self.get_fn_callable(callee) { 
                                return(Suspend = self.wait_for(Jit = callee))
                            };
                            
                            bc := self.todo_get_bc();
                            body := @try(self.comp().empty_fn_body(bc, fid, .Aot)) return;
                            callee := assume_types_fn(Arg = *FnBody, Ret = void, ptr = fn_ptr);
                            zone := zone_begin(.CallDynamic);
                            callee(body);  // :call_dynamic_values
                            zone_end(zone);
                            
                            if func.get_flag(.LogBc) {
                                out: List(u8) = list(temp());
                                self.comp().log_bc(func, body, out&);
                                println(out.items());
                            };
                            
                            func.body = (Bytecode = FnBody.int_from_ptr(body));
                            good = true;
                        };
                        @default => ();
                    };
                };
                @err_assert(good, "!asm require arch tag") return;
            };
            // :link_rename
            @case(Flag.link_rename.ident()) => {
                // TODO: for now we only support this for special body (like imports), you should be allowed to do it for exports too tho. 
                // :SLOW
                old := self.dispatch.enclosing_function;
                self.dispatch.enclosing_function = .None;
                link_rename_arg := self.env.link_rename_arg.expect("no #link_rename during bootstrap");
                link_rename_arg_ptr := self.ptr_ty(link_rename_arg);
                renamer_func_ty := self.intern_type(FnPtr = (ty = (arg = link_rename_arg_ptr, ret = void, arity = 1), cc = .CCallReg));
                value := @unwrap(tag.non_void_arg(), "#link_rename requires arg") return;
                renamer_func_ptr := @check(self.immediate_eval_expr(value, renamer_func_ty)) return;
                renamer_func_ptr := i64.assume_cast(renamer_func_ptr&)[];
                func.nullable_link_rename_func = renamer_func_ptr;
                self.dispatch.enclosing_function = old;
            };
            @default => ();
        };
    };
    @err_assert(good, "Function has no implementation %", func.log(self)) return;
    (Ok = check_body)
}

// :link_rename
// TODO: allow #libc before #link_rename
// TODO: the backend needs to call this again if cross compiling. 
//       maybe we should add jit vs aot to LinkRenameArg 
fn get_link_name(self: *SelfHosted, func: *Func) CStr = {
    out: List(u8) = list(temp());
    ::if(CStr);
    if func.nullable_link_rename_func != 0 {
        arg: LinkRenameArg = (
            target = (arch = self.env.comptime_arch, os = self.env.comptime_os),
            out = out&,
            old_name = self.pool.get(func.name),
        );
        addr := rawptr_from_int(func.nullable_link_rename_func);
        addr := assume_types_fn(*LinkRenameArg, void, addr);
        addr(arg&);
        out&.push(0);
        (ptr = out.maybe_uninit.ptr)
    } else {
        self.pool.get_c_str(func.name)
    }
}

// This was for statically linking the compiler and running on blink where you can't just dlopen a libc for a jitted program. 
fn get_smuggled_libc_function(arch: Arch, name: Symbol) ?rawptr = {
    @switch(name) {
        @case(Flag.mmap.ident()) => (Some = @as(rawptr) @as(@FnPtr(addr: rawptr, len: i64, prot: i64, flags: i64, fd: Fd, offset: i64) rawptr) mmap);
        @case(Flag.abort.ident()) => (Some = @as(rawptr) @as(@FnPtr() Never) abort);
        @case(Flag.write.ident()) => (Some = @as(rawptr) @as(@FnPtr(fd: Fd, buf: *u8, size: i64) i64Result) write);
        @case(Flag.read.ident()) => (Some = @as(rawptr) @as(@FnPtr(fd: Fd, buf: *u8, size: i64) i64Result) read);
        @case(Flag.munmap.ident()) => (Some = @as(rawptr) @as(@FnPtr(addr: rawptr, len: i64) voidResult) munmap);
        @case(Flag.open.ident()) => (Some = @as(rawptr) @as(@FnPtr(path: CStr, flags: i64) FdResult) open);
        @case(Flag.close.ident()) => (Some = @as(rawptr) @as(@FnPtr(fd: Fd) voidResult) close);
        @case(Flag.mprotect.ident()) => (Some = @as(rawptr) @as(@FnPtr(addr: rawptr, len: i64, prot: i64) voidResult) mprotect);
        @case(Flag.__clear_cache.ident()) => (Some = @as(rawptr) @as(@FnPtr(beg: rawptr, beg: rawptr) void) clear_instruction_cache);
        // same symbol, doesnt matter which os i say.  
        @case(Flag.uname.ident()) => (Some = @as(rawptr) @as(@FnPtr(buf: *LinuxLibc.Utsname) voidResult) uname); 
        @case(Flag.clock_gettime.ident()) => (Some = @as(rawptr) @as(@FnPtr(clock_id: i64, time_spec: *TimeSpec) void) clock_gettime);
        // TODO: get rid of these once they're not in the compiler anyway.
        @case(Flag.malloc.ident()) => (Some = @as(rawptr) @as(@FnPtr(size: usize) rawptr) malloc);
        @case(Flag.free.ident()) => (Some = @as(rawptr) @as(@FnPtr(ptr: rawptr) void) free);
        @default => .None; 
    }
}

::tagged(ResultType);
fn compile_expr(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    if expr.done {
        return(.Ok);
    };
    @check(self.compile_expr_inner(expr, requested)) return;
    @if_let(requested) fn Specific(type) => {
        if expr.expr&.is(.Value) {
            if type != expr.ty {
                @check(self.coerce_const_expr(expr, requested, false)) return;
            };
        };
            //if !self.ask_can_assign(type, expr.ty) {
            //    @println("% vs %", self.log_type(type), self.log_type(expr.ty));
            //    @println("%", expr.log(self));
            //};
        //};
    };
    @debug_assert(!expr.ty.is_unknown(), "not typed");
    .Ok
}

fn compile_expr_inner(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    if expr.done {
        assert(expr.ty != UnknownType, "done but unknown type");
        return(.Ok);
    };
    todo_dont_lose := self.dispatch.enclosing_function;
    old_loc := self.last_loc;
    self.last_loc = expr.loc;
    //@log_event("compile_expr % %", expr.log(self), self.dispatch.enclosing_function&.tag()) self;
    
    @match(expr.expr&) {
        fn Poison(placeholder) => {
            msg := "";
            if placeholder[] == .InProgressMacro {
                msg = "if this is while resolving a type definition you could try @rec";
            };
            return(@err("Poison expression %. \n%", placeholder, msg));
        }
        fn Value(f) => {
            if(expr.ty == UnknownType, => return(@err("ICE: Value expression must have known type:\n %", log(f.bytes&, self, UnknownType))));
            if !f.coerced {
                @if_let(requested) fn Specific(ty) => {
                    if expr.ty != ty {
                        @check(self.coerce_const_expr(expr, requested, false)) return;
                    };
                };
            };
            expr.done = true;
        }
        fn Call() => {
            @check(self.compile_call(expr, requested)) return;
        }
        fn Block(f) => {
            @check(self.ensure_resolved(f)) return;  // i think this obviously has to go before hoist_constants
            if !f.hoisted_constants {
                @check(self.hoist_constants(f.body.items())) return;
                f.hoisted_constants = true;
            };
            done := true;
            dead := false;
            all_nop := true;
            each f.body { stmt | 
                // TODO: maybe this is dumb. should give warning? 
                //       but it makes it faster at compiling -boot-only because i do that by inserting `return()`
                //       at the ftop of functions and this lets that just delete the unused code without checking it. 
                if dead { 
                    stmt.stmt = .Noop;
                } else {
                    new_done, new_dead := @check(self.compile_stmt(stmt)) return;
                    dead = new_dead;
                    stmt.done = true; // TODO: you probably want to check if inner expressions are done but this seems fine so far. 
                    done = new_done && done;
                };
                all_nop = all_nop && stmt.stmt&.is(.Noop);
            };
            if dead {
                f.result.expr = .Unreachable;
            };
                
            if requested&.is(.None) && !expr.ty.is_unknown() && !expr.ty.is_never() {
                requested = (Specific = expr.ty);
            };
            @check(self.compile_expr(f.result, requested)) return;
            
            expr.ty = f.result.ty;
            
            // HACK. for when you inline something with an early return that ends in a never. :early_return_fallthrough_never ::block_never_unify_early_return_type
            // TODO: seperate out a `unify` operation and use it everywhere you update expr.ty? 
            //       Switch and If already have this so its starting to feel a bit dumb.  -- Aug 19
            if f.result.ty.is_never() {
                if requested.specific() { ty |
                    expr.ty = ty;
                };
            };
            
            expr.done = f.result.done && done;
            // this should help imm_eval not actually call things that are just a value (tho for now it still jits them -- Sep 8)
            if all_nop && f.ret_label.is_none().or(f.result.expr&.is(.Value)) {
                expr[] = f.result[]; // Not required but makes debugging easier cause there's less stuff.
            };
        }
        fn Tuple(parts) => {
            requested_types := @try(self.tuple_types(requested, parts.len)) return;
            types: List(Type) = list(parts.len, temp()); 
            done := true;
            enumerate parts { i, part | 
                if !part.done {
                    @check(self.compile_expr(part, requested_types[i])) return;
                };
                types&.push(part.ty);
                done = done && part.done;
            };
            expr.done = done;
            
            if expr.ty.is_unknown() {
                expr.ty = self.tuple_of(types.items());
                self.finish_layout_deep(expr.ty);
            };  // TODO: else :type_check
        }
        fn PrefixMacro() => {
            @check(self.compile_prefix_macro(expr)) return;
            @check(self.compile_expr(expr, requested)) return;
        }
        fn GetVar(var) => return(self.compile_get_var(expr, requested, false));
        fn GetNamed(n) => return(@err("Undeclared Identifier: %.", self.pool.get(n[])));
        fn String(i) => {
            // This cannot be a Value node because Str and CStr are not builtins so the parser is unable to create it.
            // But also auto casting a random comptime known Str to a CStr would be sketchy because you don't know if it was mutable. 
            
            // Auto-cast to null terminated string. 
            @if_let(requested) fn Specific(want) => {
                env := self.env;
                if env.c_str_type != UnknownType && env.c_str_type == want {
                    str := self.to_values(CStr, self.pool.get_c_str(i[]));
                    expr.set(str, env.c_str_type);
                    return(.Ok);
                };
            };
            
            byte_type := self.get_or_create_type(u8);
            str_type := @check(self.create_slice_type(byte_type, expr.loc)) return;
            str := self.to_values(Str, self.pool.get(i[]));
            expr.set(str, str_type);
        }
        fn ConstEval(inner) => {
            // :PushConstFnCtx 
            // We need to track that any callees of this expression are not runtime callees!
            old_func := self.dispatch.enclosing_function;
            self.dispatch.enclosing_function = .None;
            @check(self.compile_expr(inner[], requested)) return;
            self.dispatch.enclosing_function = old_func;
            // TODO: its a bit silly that i have to specifiy the type since the first thing it does is compile it
            value := @check(self.immediate_eval_expr(inner[], inner.ty)) return;
            expr.set(value, inner.ty);
        }
        fn Deref(inner) => @check(self.compile_deref_place(expr, requested, true)) return;
        fn Addr(inner) => {
            // Note: the old version had special handling for GetVar here to avoid loops but this seems fine... - Jul 30
            @check(self.compile_place_expr(inner[], requested, false)) return;
            expr[] = inner[][];
        }
        fn PtrOffset(f) => {
            expr.done = f.ptr.done;
        }
        fn GetParsed(index) => {
            eprintln("ICE: if you get here i think it means you dropped an error from resolve and we hit it later?");
            e := @try(self.parser.finish_pending(index[])) return;
            return(@err("ICE: GetParsed is should have been handled by scope.fr. %", e&.log(self)));
        }
        fn Closure(func) => {
            fid := self.add_function(func[][]);
            self.set(expr, FuncId, fid);
            func := self.get_function(fid);
            @try(self.update_function_metadata(func, .None)) return;
            //if func.ret&.is(.Infer) {
            //    @if_let(func.body&) fn Normal(body) => {
            //        guessed := self.type_of(body);
            //        @if_let(guessed) fn Ok(type) => {
            //            @println("guessed! %", self.log_type(type));
            //            func.ret = (Finished = type);
            //            func.finished_ret = (Some = type);
            //        };
            //    };
            //};
            
            // this avoids `function arguments must have type annotation (cannot be inferred)` on `=` functions with hint. 
            // you want it so you don't have to type `: *List(u8)` on every #x86_bytes inline asm
            // TODO: unify with inference for '=>' functions?
            if !func.get_flag(.AllowRtCapture) {
                unhelpful :: local_return;
                @if_let(requested) fn Specific(ty) => {
                    f_ty := @match(self.get_type(ty)) {
                        fn Fn(f_ty) => f_ty[];
                        fn FnPtr(f) => f.ty;
                        @default => unhelpful();
                    };
                    types := self.arg_types(f_ty.arg);
                    if(func.arg.bindings.len != types.len, => unhelpful());
                    enumerate func.arg.bindings& { i, arg |
                        if arg.ty&.is(.Infer) {
                            arg.ty = (Finished = types[i]);
                        };
                    };
                    if func.ret&.is(.Infer) {
                        func.ret = (Finished = f_ty.ret);
                        func.finished_ret = (Some = f_ty.ret);
                    };
                };
            };
            
            if func.arg.bindings.len == 1 {
                arg := func.arg.bindings.index(0);
            };
            
            if func.ret&.is(.Infer) {
                // TODO: this change is sketch if we're just exploring to resolve an overload  -- Aug 5
                @match(requested) {
                    fn Returning(t) => {
                        func.ret = (Finished = t);
                    }
                    fn Specific(f_ty) =>{
                        @if_let(self.get_type(f_ty)) fn Fn(f_ty) => {
                            @log_event("% infer closure return from requested. %", fid, self.log_type(f_ty.ret)) self;
                            func.ret = (Finished = f_ty.ret);
                        };
                    }
                    @default => ();
                }
            };
            if !requested&.is(.None) {
                @check(self.coerce_const_expr(expr, requested, false)) return;
            };
        }
        // :PlaceExpr
        fn FieldAccess(f) => {
            // TODO: this is unfortunate. it means you prewalk instead of letting placeexpr do the recursion
            //       but need to check if its a value that has special fields first.
            @check(self.compile_expr(f.container, .None)) return;
            
            
            if f.container.ty == ScopeId { // :get_or_create_type 
                scope := @check(self.eval(f.container, ScopeId)) return; 
                var := @unwrap(find_var_in_scope(self, f.name, scope), "not found in scope") return;
                value, ty := @check(self.find_const(var, requested)) return;
                expr.set(value, ty);
            } else {
                if f.container.ty == self.get_or_create_type(Type) {
                    type := @check(self.eval(f.container, Type)) return; 
                    @check(self.contextual_field(f.name, expr, type)) return;
                    // TODO: should typecheck against result type be here or elsewhere?
                } else {
                    // Otherwise its a normal struct/tagged field.
                    @check(self.compile_place_expr(expr, requested, true)) return;
                };
            };
        }
        fn StructLiteralP() => {
            @check(self.construct_struct_literal(expr, requested)) return;
        }
        fn ContextualField(name) => {
            @err_assert(requested&.is(.Specific), "ContextualField % requires type hint", self.pool.get(name[])) return;
            @check(self.contextual_field(name[], expr, requested.Specific)) return;
        }
        fn Uninitialized() => {
            if expr.ty.is_unknown() {
                @err_assert(requested&.is(.Specific), "Uninitialized requires type hint") return;
                expr.ty = requested.Specific;
            };
            expr.done = true;
        }
        fn Quote(arg) => {
            @check(self.compile_quote(expr, requested)) return;
        }
        fn Slice(arg) => {
            @match(arg.expr&) {
                fn Tuple(parts) => {
                    @err_assert(parts.len <= max_homogeneous_tuple, "TODO: @slice on the stack will miscompile if too large. this is dumb") return;
                    fst := parts.index(0);
                    @check(self.compile_expr(fst, .None)) return;
                    if !fst.ty.is_unknown() {
                        hint: ResultType = (Specific = fst.ty);
                        each parts.items().slice(1, parts.len) { e | 
                            @check(self.compile_expr(e, hint)) return;
                        };
                        @check(self.compile_expr(arg[], .None)) return; // TODO: don't recompile. just poke the type in.
                    } else {
                        @check(self.compile_expr(arg[], .None)) return;
                    };
                };
                @default => {
                    @check(self.compile_expr(arg[], .None)) return;
                };
            };
            types := self.arg_types(arg.ty);
            // TODO: typecheck that all are the same!
            expr.ty = @check(self.create_slice_type(types[0], arg.loc)) return;
            expr.done = arg.done;
        }
        fn As(f) => {
            type := @check(self.eval(f.type, Type)) return;
            @check(self.compile_expr_inner(f.value, type.want())) return;  // Note: skipping stricter check
            if f.value.ty != type {
                fn can_as_cast(self: *SelfHosted, ty: Type) bool = {
                    ty := self.get_type(ty);
                    if(ty.is(.Enum) || ty.is(.Named) || ty.is(.VoidPtr), => return(true));
                    if(!ty.is(.Int), => return(false));
                    b := ty.Int.bit_count;
                    b != 8 && b != 16 && b != 32 && b != 64
                };
                // TODO: should do something more strict than this.
                @err_assert(self.can_as_cast(f.value.ty) || self.can_as_cast(type), "invalid @as cast (%) to (%)", self.log_type(f.value.ty), self.log_type(type)) return;
                expr.ty = type;
                expr.expr = (Cast = f.value);
                expr.done = f.value.done;
            } else {
                expr[] = f.value[];
            };
        }
        fn Cast(arg) => {
            @check(self.compile_expr(arg[], .None)) return;
            expr.done = arg.done;
        }
        fn FnPtr(arg) => {
            @check(self.compile_fn_ptr(expr)) return;
        }
        fn If(f) => {
            @check(self.compile_expr(f.cond, self.get_or_create_type(bool).want())) return;

            // TODO: this mostly can't happen anymore because you use the if function and params dont get forwarded like that. 
            //       should allow promoting things to constants. 
            // If its constant, don't even bother emitting the other branch
            if f.cond.expr&.is(.Value) {
                cond := bool.assume_cast(f.cond.expr.Value.bytes&)[];
                ::if(*FatExpr);
                // Now we fully dont emit the branch
                expr[] = if(cond, => f.if_true, => f.if_false)[];
                // need to force the compile again to keep if constant for nested folding.
                return(self.compile_expr(expr, requested));
            };
            
            @check(self.compile_expr(f.if_true, requested)) return;
            true_ty := f.if_true.ty;
            if requested&.is(.None) && !true_ty.is_never() {
                // This is especially helpful for macros that expand to chained ifs (like @switch)
                requested = (Specific = true_ty);
            };
            @check(self.compile_expr(f.if_false, requested)) return;
            if true_ty.is_never() {
                true_ty = f.if_false.ty;
            } else {
                @check(self.can_assign(true_ty, f.if_false.ty)) return;
            };
            expr.ty = true_ty;
            expr.done = f.cond.done && f.if_true.done && f.if_false.done;
        }
        fn Switch(f) => {
            @check(self.compile_expr(f.value, i64.want())) return;
            unify :: fn(expect: *ResultType, new: Type) void = {
                dont_like_this_type := @match(expect) {
                    fn Specific(inner) => inner[].is_never();
                    fn Returning(_) => false;
                    @default => true;
                };
                if dont_like_this_type && !new.is_never() {
                    expect[] = (Specific = new);
                }
            };
            expected_type := requested; 
            @check(self.compile_expr(f.default, expected_type)) return;
            unify(expected_type&, f.default.ty);
            // TODO: ensure the value tags are unique. 
            done := f.default.done && f.value.done;
            each f.cases { it |
                @check(self.compile_expr(it._1&, expected_type)) return;
                unify(expected_type&, it._1.ty);
                done = done && it._1.done;
            };
            ty := @match(expected_type) {
                fn Specific(inner) => inner;
                fn None() => Never;
                @default => return(@err("could not unify switch branch types"));
            };
            expr.ty = ty;
            expr.done = done;
        }
        fn Loop(arg) => {
            @check(self.compile_expr(arg[], .None)) return;
            expr.done = arg.done;
            expr.ty = self.get_or_create_type(Never); 
        }
        fn FromBitLiteral(f) => {
            ty := self.intern_type(Int = (bit_count = f.bit_count, signed = false));
            value := self.to_values(i64, f.value);
            @switch(f.bit_count) {
                @case(8) => {
                    value.Small._1 = 1;
                };
                @case(16) => {
                    value.Small._1 = 2;
                };
                @case(32) => {
                    value.Small._1 = 4;
                };
                @default => ();
            };
            expr.set(value, ty);
        }
        fn Unreachable() => {
            expr.done = true;
            expr.ty = self.get_or_create_type(Never); 
        }
        fn UndeclaredVar(it) => {
            if self.find_var_in_scope(it.name, it.scope) { v |
                // TODO: this is bad because if you ever get here it means you were resolved before all the stuff above you was ready,
                //       so you got lucky that you didn't to anything and we can fix it now,
                //       but if someone had put something in a higher scope and you wanted to bind something shadowing it, you might get the wrong thing. 
                
                expr.expr = (GetVar = v);
                @check(self.compile_expr(expr, requested)) return;
            } else {
                return(@err("%", self.comp().log(expr)));
            };
        }
        @default => @panic("TODO: unhandled node type %: %", expr.expr&.tag(), expr.log(self));
    };
    self.last_loc = old_loc;
    self.dispatch.enclosing_function = todo_dont_lose;
    @debug_assert(!expr.ty.is_unknown(), "[compile_expr] Unknown type for %", expr.log(self));
    .Ok
}

fn compile_quote(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    arg := expr.expr.Quote;

    unquote_placeholders := @unwrap(self.env.unquote_placeholders, "quote during boot") return;
    
    // note: do this before fucking with the expr because it will yield the first time!
    @check(self.infer_arguments(unquote_placeholders)) return;
    @check(self.infer_return(unquote_placeholders)) return;
    if self.get_fn_callable(unquote_placeholders).is_none() {
        return(Suspend = self.wait_for(Jit = unquote_placeholders));
    };
    
    //walk: MarkNotDone = ();
    //walk&.walk_expr(arg); // TODO: might not need this :SLOW
    
    walk: Unquote = (compiler = self, placeholders = list(self.get_alloc()));
    @try(walk&.walk_expr(arg)) return;
    expr_ty := @unwrap(self.env.fat_expr_type, "used quoted ast during bootstrapping") return;
    value := self.to_values(FatExpr, arg[]);
    expr.set(value, expr_ty);
    @log_event("quote with % placeholders", walk.placeholders.len) self;
        
    if !walk.placeholders.is_empty() {
        walk.placeholders&.push(expr[]);
        arg: FatExpr = (expr = (Tuple = walk.placeholders.as_raw()), loc = expr.loc, ty = UnknownType, done = false);
        arg := self.box(arg);
        arg: FatExpr = (expr = (Slice = arg), loc = expr.loc, ty = UnknownType, done = false);
        f := self.to_expr(FuncId, unquote_placeholders, expr.loc); 
        f.done = true;
        expr[] = synthetic_ty((Call = (f = self.box(f), arg = self.box(arg))), expr.loc, expr_ty);
        @check(self.compile_expr(expr, requested)) return;
    };
    .Ok
}

fn compile_fn_ptr(self: *SelfHosted, expr: *FatExpr) Maybe(void) = {
    arg := expr.expr.FnPtr&;
    // TODO: pass through better type hint
    fid := @check(self.eval(arg[], FuncId)) return;
    @check(self.infer_arguments(fid)) return;
    @check(self.infer_return(fid)) return;
    func := self.get_function(fid);
    // TODO: typecheck
    //err!("!fn_ptr expected const fn not {}", self.program.log_type(ty));
    
    @err_assert(!func.get_flag(.AnyConstArgs), "cannot take pointer to function with const args") return;
    
    self.took_pointer_value(fid);
    
    // Instead of self.add_callee(fid)
    if self.dispatch.enclosing_function { current_f |
        current := self.get_function(current_f);
        current.mutual_callees&.add_unique(fid, self.get_alloc());
        @log_event("added % mutual calls %", current_f, fid) self;
        record_function_call(c = self, caller = current_f, callee = fid);
    };
    
    // TODO: for now you just need to not make a mistake with calling convention
    // The backend still needs to do something with this, so just leave it
    ty := @unwrap(func.finished_ty(), "!fnptr expected known type") return;
    cc := @unwrap(func.cc, "unknown calling convention") return;
    ty := self.intern_type(FnPtr = (ty = ty, cc = cc));
    expr.ty = ty;
    .Ok
}

fn construct_struct_literal(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    pattern := expr.expr.StructLiteralP&;
    if(!requested&.is(.Specific), => return(@err("struct literal requires type hint"))); // TODO: I'll probably want this to be a new type of yield eventually. 
    requested := requested.Specific;
    raw_container_type := self.raw_type(requested);
    bindings := pattern.bindings&;
    
    @match(self.get_type(raw_container_type)) {
        fn Struct(f) => {
            if f.is_union {
                @err_assert(bindings.len == 1, "% is a union, value should have one active varient not %", self.log_type(requested), bindings.len) return;
            };
            
            done := true;
            enumerate bindings { i, b | 
                name := or b.ident() {
                    return(@err("struct literal requires field names"))
                };
                // Note: I'm not reordering them here because i want to define evaluation order to be the order you write them in not the declaration order. 
                //       (because there might be side effects)
                field := or find_struct_field(f, name, i) {
                    return(@err("Tried to assign unknown field %", self.pool.get(name)))
                };
                value := or b.get_default() {
                    return(@err("struct literal requires field value (use '=' not ':')"))
                };
                @check(self.compile_expr(value, (Specific = field.ty))) return;
                done = done && value.done;
                self.last_loc = pattern.loc;
                @check(self.can_assign(field.ty, value.ty)) return;
            };
            
            // If they're missing some, check for default values.
            if !f.is_union && f.fields.len != bindings.len {
                enumerate f.fields { i, field | 
                    continue :: local_return;
                    each bindings { b |   // :SLOW
                        if b.ident() { n | 
                            if(n == field.name, => continue());
                        };
                    };
                    
                    default := @unwrap(field.get_default(), "Missing required field %", self.pool.get(field.name)) return;
                    value, ty := @check(self.find_const(default, field.ty.want())) return;
                    expr := synthetic_ty((Value = (bytes = value, coerced = false)), pattern.loc, ty);
                    @check(self.coerce_const_expr(expr&, field.ty.want(), false)) return;  // TODO: find_const should do this for you instead
                    expr.done = true;
                    
                    // we don't have to do it in order anymore but it means emit_bc will find it faster. 
                    bindings.insert(
                        i,
                        (
                            name = (Ident = field.name),
                            ty = (Finished = ty), 
                            nullable_tag = zeroed(*Annotations),
                            default = expr,
                            kind = .Var,
                        ),
                        self.get_alloc()
                    );
                };
                @err_assert(f.fields.len == bindings.len, "ICE: struct field count mismatch but we don't know which is missing") return;
            };

            @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
            expr.done = done;
            expr.ty = requested;
            .Ok
        }
        fn Tagged(f) => {
            @err_assert(bindings.len == 1, "% is an enum, value should have one active varient not %", self.log_type(requested), bindings.len) return;
            b := bindings.index(0);
            name := or b.ident() {
                return(@err("struct literal requires field names"))
            };
            each f.cases& { f |
                if f._0 == name {
                    ::as_ref(FatExpr);
                    value := @unwrap(b.get_default(), "struct literal needs value") return;
                    @check(self.compile_expr(value, (Specific = f._1))) return;
                    @check(self.can_assign(f._1, value.ty)) return;
                    expr.done = value.done;
                    expr.ty = requested;
                    return(.Ok);
                };
            };
            @err("No field % exists in %", self.pool.get(name), self.log_type(requested))
        }
        @default => @err("found struct literal but expected % = %", requested, self.log_type(requested));
    }
}

fn contextual_field(self: *SelfHosted, name: Symbol, expr_out: *FatExpr, type: Type) Maybe(void) = {
    @match(self.get_type(type)) {
        fn Enum(f) => {
            each f.fields { f |
                if f._0 == name {
                    value := f._1&.deep_clone(self.get_alloc());
                    expr_out.set(value, type);
                    return(.Ok);
                };
            };
            @err("contextual field % not found for %", self.pool.get(name), self.log_type(type))
        }
        fn Struct(f) => {
            if !(f.scope == NOSCOPE) {
                if find_var_in_scope(self, name, f.scope) { var |
                    value, ty := @check(self.find_const(var, .None)) return;
                    expr_out.set(value, ty);
                    return(.Ok);
                };
            };
            @err("const field $% not found for %", self.pool.get(name), self.log_type(type))
        }
        fn Tagged(f) => {
            enumerate f.cases { i, f | 
                if f._0 == name {
                    @err_assert(f._1 == void, 
                        "contextual field % of tagged union must be unit found %",
                        self.pool.get(name), self.log_type(f._1)
                    ) return;
                    
                    // We could create a StructLiteralP and let the backend deal with it, but we know the answer right now. 
                    // Note that this forces an extra case for :tagged_prims_hack
                    info := self.get_info(type);
                    @debug_assert(info.is_sized, "unsized type for tagged contextual field");
                    size: i64 = info.stride_bytes.zext(); // TODO: allow yield on sizing here. 
                    bytes := 0x00.repeated(size, self.get_alloc());
                    assert(i <= 255, "TODO: giant ass @tagged"); 
                    assert(info.align_bytes <= 8, "TODO: handle large alignment");
                    bytes[0] = i.trunc(); // :endian
                    expr_out.set(bytes.items().to_value(), type);
                    expr_out.ty = type;
                    expr_out.done = true;
                    return(.Ok);
                };
            };
            @err("contextual field % not found for %", self.pool.get(name), self.log_type(type))
        }
        fn Named(f) => self.contextual_field(name, expr_out, f._0);
        @default => @err("no contextual fields for type % (wanted %)", self.log_type(type), self.pool.get(name));
    }
}

fn compile_call_overload_set(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    f := expr.expr.Call.f;
    arg := expr.expr.Call.arg;
    os := @check(self.eval(f, OverloadSet)) return;
    data := self.dispatch.overloads&.nested_index(os.as_index());
    if data.ready.len == 1 && data.pending.len == 0 {
        self.set(f, FuncId, data.ready[0]);
    } else {
        // HACK. experiment. 
        // interesting that this doesnt work. i guess it needs help to know the type? 
        //if Flag.if.ident() == data.name {
            //self.codemap.show_error_line(arg.loc);
        //    if arg.expr&.is(.Tuple) && arg.expr.Tuple.len == 3 {
        //        parts := arg.expr.Tuple&;
        //        tt := synthetic_ty((Call = (f = parts.index(1), arg = self.make_unit_expr(expr.loc))), expr.loc, UnknownType);
        //        ff := synthetic_ty((Call = (f = parts.index(2), arg = self.make_unit_expr(expr.loc))), expr.loc, UnknownType);
        //        expr.expr = (If = (cond = parts.index(0), if_true = self.box(tt), if_false = self.box(ff)));
        //        return(self.compile_expr(expr, requested));
        //    };
        //};
    
        return(Suspend = self.wait_for(ResolveOverload = (
            os = os, 
            call = (expr = expr, requested = requested), 
            callsite = self.dispatch.enclosing_function,
            last_ready_count = 0,
            options = list(self.get_alloc()),
        )));
    };
    .Ok
}

::if(ResultType);
fn compile_call(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) #once = {
    @debug_assert(expr.expr&.is(.Call));
    f := expr.expr.Call.f;
    arg := expr.expr.Call.arg;
    req_fn: ResultType = @match(requested) {
        fn Specific(t) => (Returning = t);
        @default => .None;
    };
    
    if arg.is_raw_unit() && f.expr&.is(.Closure) && f.expr.Closure.get_flag(.AllowRtCapture) {
        f.expr.Closure.set_flag(.Once);
    };
    
    @check(self.compile_expr(f, req_fn)) return;
    
    if f.ty == self.get_or_create_type(OverloadSet) {
        @check(self.compile_call_overload_set(expr, requested)) return;
    };
    
    if f.ty == self.get_or_create_type(FuncId) {|
        return(self.call_direct(expr, requested));
    };
    
    if f.ty == self.get_or_create_type(LabelId) {
        // We're trying to early return but we didn't know the return type yet when the label was created. 
        // You should only get here for .Generic or .Infer. 
        label := @check(self.eval(f, LabelId)) return;
        fid := self.dispatch.return_labels&.nested_index(label.as_index())[];
        func := self.get_function(fid);
        @match(@check(self.infer_type(func.ret&)) return) {
            fn Some(ty) => {
                @check(self.compile_expr(arg, ty.want())) return;
                @check(self.can_assign(ty, arg.ty)) return;
            }  
            fn None() => {
                @check(self.compile_expr(arg, .None)) return;
                // It was .Infer, but we'll save it incase we have multiple returns.
                // TODO: I think this is wrong because you're allowd to have polymorphic closures but I don't clone the func.  
                func.ret = (Finished = arg.ty);
            }
        };
        f.ty = self.intern_type(Label = arg.ty);
        expr.ty = self.get_or_create_type(Never);
        expr.done = arg.done;
        return(.Ok);
    };
    
    @match(self.get_type(f.ty)) {
        fn FnPtr(it) => {
            // Feels like a pretty reasonable invarient: if we have a function pointer, we need to know exactly what its types are. 
            @check(self.compile_expr(arg, (Specific = it.ty.arg))) return;
            @check(self.can_assign(arg.ty, it.ty.arg)) return; // TODO: correct varience
        
            expr.ty = it.ty.ret;
        }
        fn Fn(it) => {
            // This happens to be an easy case where we already know the argument type.
            @check(self.compile_expr(arg, (Specific = it.arg))) return;
            return(self.call_direct(expr, requested));
        }
        fn Label(it) => {
            @check(self.compile_expr(arg, (Specific = it[]))) return;
            @check(self.can_assign(it[], arg.ty)) return;
            expr.ty = self.get_or_create_type(Never); 
        }
        @default => return(@err("not callable %", f.log(self)));
    };
    expr.done = f.done && arg.done;
    
    .Ok
}

// TODO: check function ret type against requested
// Note: when we get here, we might not know the type of the function or the argument. 
fn call_direct(self: *SelfHosted, expr: *FatExpr, requested: ResultType) Maybe(void) = {
    f_expr := expr.expr.Call.f;
    arg_expr := expr.expr.Call.arg;
    @debug_assert(!f_expr.ty.is_unknown(), "call_direct expected fn expr to be compiled");
    fid := @check(self.immediate_eval_expr(f_expr, f_expr.ty)) return;
    fid := FuncId.assume_cast(fid&)[];
    @log_event("compile direct call %", fid) self;
    func := self.get_function(fid);
    
    if func.get_flag(.UnboundGenerics) {
        @try(self.resolve_sign(func)) return;
        new_fid := @check(self.eval_where_from_expression(fid, arg_expr)) return; 
        fid  = @unwrap(new_fid, "called a #where function directly (not through an overload set) but the types did not match") return;
        f_expr[] = self.to_expr(FuncId, fid, f_expr.loc);
        func = self.get_function(fid);
        @debug_assert(!func.get_flag(.UnboundGenerics));
    };
    
    is_variadic := func.get_flag(.CVariadic);
    
    arg_ty := UnknownType;
    
    @try(self.adjust_named_arguments(fid, arg_expr)) return;
    
    if func.get_flag(.AnyConstArgs) {
        @err_assert(!is_variadic, "TODO: support #c_variadic with constant arguments") return;
        @check(self.ensure_resolved_sign(fid)) return;
        
        // This creates a new function with only runtime args, and updates the arg_expr accordingly. 
        zone := zone_begin(.BakeConstArgs);
        res := self.curry_const_args(fid, f_expr, arg_expr);
        zone_end(zone);
        fid = @check(res) return;
        func = self.get_function(fid);
        // Since we may have just made a new function, make sure we know its types,
        // and then sanity check that our types still match. 
        if arg_expr.expr&.is(.Tuple) {
            arg_expr.ty = UnknownType;
            arg_expr.done = false;
        };
        arg_ty = @check(self.infer_arguments(fid)) return;
        @check(self.compile_expr(arg_expr, arg_ty.want())) return;
        // It's probably an ICE if this fails on something non-#generic. 
        if arg_ty != arg_expr.ty {
            self.last_loc = arg_expr.loc;
            return(@err("arg type mismatch after removing const args"));
        };
    };
    
    if func.get_flag(.UnsafeNoopCast) {
        @check(self.compile_expr(arg_expr, arg_ty.want())) return;
        //@check(self.can_assign(arg_ty, arg_expr.ty)) return; TODO: type check
        @check(self.infer_return(fid)) return;
        ret_ty := func.finished_ret.expect("known ret");
        @log_event("UnsafeNoopCast % -> %", self.log_type(arg_ty), self.log_type(ret_ty)) self;
        expr.expr = (Cast = arg_expr);
        expr.ty = ret_ty;
        expr.done = arg_expr.done;
        return(.Ok);
    };
    
    capturing   := func.get_flag(.AllowRtCapture).or(func.get_flag(.MayHaveAquiredCaptures));
    will_inline := capturing.or(func.cc.expect("known cc") == .Inline);
    deny_inline := func.get_flag(.NoInline);
    if(will_inline && deny_inline, => return(@err("must inline a function marked #noinline")));
    
    allow_fold := self.dispatch.enclosing_function.is_some() && {
        func := self.get_function(self.dispatch.enclosing_function.unwrap());
        !func.get_flag(.SyntheticImmEval)
    };
    
    if will_inline {
        @err_assert(!is_variadic, "TODO: support #inline with constant arguments") return;
        if capturing {
            // We want to allow for polymorphic arguments, but its better if we can infer the annotated ones before duplicating.
            // currently this is just an optimisation.  
            @check(self.infer_arguments_partial(fid)) return;
        };
        
        // If we're just inlining for #inline, compile first so some work on the ast is only done once.
        // note: compile() checks if its ::Inline before actually generating asm so it doesn't waste its time.
        // if its a '=>' function, we can't compile it out of context, and same if it has a const arg of a '=>' function. 
        if !capturing {
            @check(self.compile_body(fid)) return;
        };
        
        // TODO: check that you're calling from the same place as the definition.
        foldable := @check(self.emit_capturing_call(fid, expr, requested)) return;
        if func.get_flag(.TryConstantFold) && allow_fold && foldable {
            // we've already inlined the call so it won't just be a call_dynamic_values, 
            // and we'll have to suspend on compiling a stub to get the value, 
            // so we make a ConstEval to remember that this needs to be folded. 
            // don't need to worry about recursing because the call node is gone. 
            e := self.box(expr[]);
            expr[] = (ty = expr.ty, done = false, expr = (ConstEval = e), loc = expr.loc);
            return(self.compile_expr(expr, requested));
        };
        return(.Ok);
    };
    
    arg_ty = @check(self.infer_arguments(fid)) return;
    if is_variadic {
        @check(self.type_check_variadic_call(arg_ty, arg_expr)) return;
    } else {
        @check(self.compile_expr(arg_expr, (Specific = arg_ty))) return;
        if arg_ty != arg_expr.ty {
            @check(self.can_assign(arg_ty, arg_expr.ty)) return;
        };
    };
    
    ret_ty := @check(self.infer_return(fid))    return;
    is_const_context := self.dispatch.enclosing_function.is_none();
    // TODO: its a bit of a hack to check is_const_context? 
    //       it fixes a problem where you try to compile too soon and then think you're already done even if you make a function for it later.
    expr.done = f_expr.done && arg_expr.done && !is_const_context;
    expr.ty = ret_ty;
    
    if f_expr.ty == self.get_or_create_type(FuncId) {|
        f_expr.ty = self.intern_type(Fn = (arg = arg_ty, ret = ret_ty, arity = func.arg.bindings.len.trunc()));
    };
    
    if !f_expr.expr&.is(.Value) {
        _ := @check(self.immediate_eval_expr(f_expr, f_expr.ty)) return;
    };
    if func.get_flag(.TryConstantFold) && allow_fold && arg_expr.is_const() {
        @log_event("TryConstantFold %", expr.log(self)) self;
        value := @check(self.immediate_eval_expr(expr, ret_ty)) return;
        expr.set(value, ret_ty); // TODO: redundant?
        return(.Ok);
    } else {
        // this fixes functions with all const args the reduce to just a value emitting useless calls to like get the number 65 or whatever if you do ascii("A"). 
        if !deny_inline && arg_expr.is_raw_unit() {
            if find_impl(func.body&, .Normal) { value |
                if value.expr&.is(.Value) {
                    expr.set(value.expr.Value.bytes&.deep_clone(self.get_alloc()), ret_ty);
                };
            };
        };
    };
    
    // you kinda want to do this but its wrong when its only available for some backends but some have a normal body. 
    // seems dumb to add callees when we know its an intrinsic we're going to emit inline. 
    // but also it's the same speed so nobody cares. 
    //  -- Dec 5                     ^
    //                               ^
    //                               ^
    if !expr.expr&.is(.Value) {  // && !func.get_flag(.Intrinsic) {
        if func.get_flag(.ComptimeOnly) {
            if self.dispatch.enclosing_function { current_f |
                outer := self.get_function(current_f);
                outer.set_flag(.ComptimeOnly);
            };
        };
        self.add_callee(fid);
    };
    
    if func.get_flag(.Once) {
        // TODO: better error message. should show the previous usage. 
        if(func.get_flag(.OnceConsumed), => return(@err("tried to call once function again")));
        func.set_flag(.OnceConsumed);
        assert(expr.done, "ICE: if we applied #once, the expression really needs to be .done");
    };
    
    if self.dispatch.enclosing_function.is_some() {
        expr.done = arg_expr.done;
    };
    
    .Ok
}

fn type_check_variadic_call(self: *SelfHosted, arg_ty: Type, arg_expr: *FatExpr) Maybe(void) = {
    positional_types := self.arg_types(arg_ty);
    args_passed := arg_expr.items();
    @err_assert(args_passed.len >= positional_types.len, "Not enough positional arguments for #c_variadic call.") return;
    types := Type.list(args_passed.len + 1, temp());
    range(0, positional_types.len) { i |
        e := args_passed.index(i);
        @check(self.compile_expr(e, (Specific = positional_types[i]))) return;
        types&.push(e.ty);
    };
    types&.push(void);  // :get_or_create_type
    range(0, args_passed.len  - positional_types.len) { i |
        e := args_passed.index(positional_types.len + i);
        @check(self.compile_expr(e, .None)) return;
        types&.push(e.ty);
    };
    @err_assert(arg_expr.expr&.is(.Tuple), "TODO: non-tuple passed to #c_variadic") return;
    e: FatExpr = (expr = .CVariadicMarker, done = true, ty = void, loc = arg_expr.loc); // :get_or_create_type
    arg_expr.expr.Tuple&.insert(positional_types.len, e, self.get_alloc());
    arg_expr.ty = self.tuple_of(types.items());
    .Ok
}

fn add_callee(self: *SelfHosted, fid: FuncId) bool = {
    is_const_context := true;
    if self.dispatch.enclosing_function { current_f |
        if(current_f == fid, => return(false)); 
        is_const_context = false;
        new := self.get_function(fid);
        current := self.get_function(current_f);
        mutual := new.callees.items().contains(current_f&);
        if mutual {
            current.mutual_callees&.add_unique(fid, self.get_alloc());
        } else {
            current.callees&.add_unique(fid, self.get_alloc());
        };
        @log_event("added % calls %", current_f, fid) self;
        record_function_call(c = self, caller = current_f, callee = fid);
    };
    if is_const_context {
        @log_event("skip adding callee for %", fid) self;
    };
    is_const_context
}

fn quick_guess_type(self: *SelfHosted, expr: *FatExpr) ?Type = {
    if !expr.ty.is_unknown() {
        return(Some = expr.ty);
    };
    @match(expr.expr&) {
        fn Block(f) => self.quick_guess_type(f.result);
        @default => .None;
    }
}

// Replace a call expr with the body of the target function.
fn emit_capturing_call(self: *SelfHosted, f: FuncId, expr_out: *FatExpr, requested: ResultType) Maybe(bool) #once = {
    @log_event("emit_capturing_call %", f) self;
    @try(self.ensure_resolved_body(f)) return; // it might not be a closure. it might be an inlined thing.
    @debug_assert(expr_out.expr&.is(.Call));
    arg_expr := expr_out.expr.Call.arg;
    
    // TODO
    //assert!(!self.currently_inlining.contains(&f), "Tried to inline recursive function.");
    //self.currently_inlining.push(f);
    
    func := self.get_function(f);
    if !func.get_flag(.AllowRtCapture) {
        t := @check(self.infer_arguments(f)) return;
    };
    foldable := false;
    if func.finished_arg { arg | 
        @check(self.compile_expr(arg_expr, (Specific = arg))) return;
        @check(self.can_assign(arg, arg_expr.ty)) return;
        foldable = arg_expr.is_const();
    };
    
    // TODO: scary to hold this reference accross doing so much shit. 
    aliased_body := find_impl(func.body&, .Normal) 
        || return(@err("emit_capturing_call: '=>' function '%' must have body expression.", self.pool.get(func.name)));
    
    // This is complicated to allow `fn() => .a` to stay polymorphic and have different inferred types at different callsites. 
    // Maybe a more robust version of this would be treating the result type like a const arg and duplicating the function? -- Aug 2
    ret_ty := or_else func.finished_ret {
        if func.ret&.is(.Infer) {
            if requested&.is(.Specific) {
                (Some = requested.Specific)
            } else {
                known := self.quick_guess_type(aliased_body);
                known
            }
        } else {
            known := @check(self.infer_return(f)) return;
            (Some = known)
        }
    };
    // Note: this relies on you never passing a garbage hint when exploring overloads. 
    if !requested&.is(.Specific) {
        if ret_ty { ret_ty |
            requested = (Specific = ret_ty);
        };
    };
    
    label_ty := ret_ty
        .map(fn(ty) => self.intern_type(Label = ty))
        .or(=> LabelId);
    
    // for inlined or things with const args, the body will already have been compiled, but the backend needs to see the whole callgraph.
    for func.callees { callee |
        self.add_callee(callee);
    };
    @err_assert(func.mutual_callees.is_empty(), "TODO: you can't really refer to inlined things so how'd you make it recursive?") return;
    if self.dispatch.enclosing_function { caller | 
        if func.get_flag(.YesContext) {
            self.get_function(caller).set_flag(.YesContext);
        };
    };
    may_have_early_return := !aliased_body.expr&.is(.Value);
    pattern := func.arg&.deep_clone(self.get_alloc());
    
    // TODO: you want to be able to share work (across all the call-sites) compiling parts of the body that don't depend on the captured variables
    old_ret_var := func.return_var.expect("return var");
    new_ret_var := self.scopes.dup_var(old_ret_var);
    ret_label: LabelId = from_index(self.dispatch.return_labels.len);
    self.dispatch.return_labels&.push(f);
    owned_body: FatExpr = if func.get_flag(.Once) {
        // TODO: better error message if they try to call it again. 
        // TODO: decide what #once with const args should mean. 
        temp := aliased_body[];
        aliased_body.expr = (Poison = .OnceUsed);
        func.set_flag(.OnceConsumed);
        temp
    } else {
        aliased_body.deep_clone(self.get_alloc())
    };
    loc := arg_expr.loc;
    // the second case would be sufficient for correctness but this is so common (if(_,!,!), loop(!), etc) that it makes me less sad. 
    if arg_expr.is_raw_unit() {
        // TODO: if !may_have_early_return, should be able to just inline the value but it doesnt work!
        //       similarly i cant have ret_label:None so im clearly wrong. its `not callable` in overloading.rs. trying to call the cond of an if??
        //       -- Jun 16
        expr_out.expr = new_block(empty(), self.box(owned_body));
    } else {
        stmts: List(FatStmt) = list(1, self.get_alloc());
        arg_stmt: Stmt = (DeclVarPattern = (binding = pattern, value = arg_expr[]));
        stmts&.push(stmt = arg_stmt, loc = loc);
        expr_out.expr = new_block(stmts.as_raw(), self.box(owned_body));
    };
    b := expr_out.expr.Block&;
    b.ret_label = (Some = ret_label);
    b.block_resolved = true;
    // TODO: self.currently_inlining.retain(|check| *check != f);
    if ret_ty { ret_ty | 
        expr_out.ty = ret_ty;
    };
    
    if may_have_early_return {
        // Note: not renumbering on the function. didn't need to clone it.
        self.renumber_expr(expr_out, (Some = (old_ret_var, new_ret_var)));
        value := self.to_values(LabelId, ret_label);
        @check(self.save_const_values(new_ret_var, value, label_ty, loc)) return;
    };
    
    @check(self.compile_expr(expr_out, requested)) return;
    if ret_ty { ret_ty |
        @check(self.can_assign(ret_ty, expr_out.ty)) return;
        expr_out.ty = ret_ty;
    };
    (Ok = foldable)
}

// TODO: this still has the problem of disallowing const args that require type hint of previous const args. 
fn const_args_key(self: *SelfHosted, original_f: FuncId, arg_expr: *FatExpr) Maybe(Values) #once = {
    func := self[original_f]&;
    ::if(Maybe(Values));
    if func.arg.bindings.len == 1 {
        // Hack to pass hint for single const arg becuase the compiler does it. 
        if !func.arg.bindings[0].ty&.is(.Infer) {
            @check(self.infer_arguments(original_f)) return;
        };
        
        @check(self.compile_expr(arg_expr, func.finished_arg.want())) return;
        ty := func.finished_arg.or(=> arg_expr.ty);
        value := @check(self.immediate_eval_expr(arg_expr, ty)) return;
        (Ok = value)
    } else {
        check_len :: fn(len: i64) => {
            if func.arg.bindings.len != len {
                return(@err( "TODO: non-trivial pattern matching for call to %", self.pool.get(func.name)));
            };
        };

        if !arg_expr.expr&.is(.Tuple) {
            // this only happens for invoke_specialized
            @check(self.compile_expr(arg_expr, func.finished_arg.want())) return;
            types := self.tuple_types(arg_expr.ty);
            if types { types |
                check_len(types.len);
                @if_let(arg_expr.expr&) fn Value(f) => {
                    // TODO: this is super dumb but better than what I did before. -- May 3 -- May 24
                    parts: List(FatExpr) = list(types.len, self.get_alloc());
                    reader: ReadBytes = (bytes = f.bytes&.bytes(), i = 0);
                    for types { ty | 
                        reader.i = align_to(reader.i, self.get_info(ty)[].align_bytes.zext()); // TODO: use field offsets
                        taken := or self.chop_prefix(ty, reader&) {| 
                            return(@err("ICE: not enough bytes to destructure value!"))
                        };
                        parts&.push(synthetic_ty((Value = (bytes = taken, coerced = false)), arg_expr.loc, ty));
                    };
                    @debug_assert_eq(reader.bytes.len, reader.i, "ICE: didn't consume all bytes.");
                    arg_expr.expr = (Tuple = parts.as_raw());
                };
            };
        };

        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple but expected % args.\n%", func.arg.bindings.len, arg_expr.log(self))));
        arg_exprs := arg_expr.expr.Tuple;
        check_len(arg_exprs.len);

        all_const_args: List(u8) = list(self.get_alloc());
        i := 0;
        has_seen_first_const := false;
        enumerate func.arg.bindings { i, binding |
            continue :: local_return;
            if(binding.kind != .Const, => continue());
            arg_ty := or binding.ty&.ty() {
                // TODO: this isn't really what you want. 
                //       the problem is you want to make the key before duplicating the function and binding const args,
                //       so its faster after the first instantiation, but to do that you need to compile the const args, 
                //       and they might need to get a type hint (like struct/enum literals), 
                //       but also thier type might depend on previous const args which this doesn't handle. 
                arg_expr := arg_exprs.index(i);
                if has_seen_first_const.or(=> binding.ty&.is(.Infer)) && !arg_expr.expr&.is(.ContextualField) { // hack to pass hint in similar situations to the old one
                    @check(self.compile_expr(arg_expr, .None)) return;
                    arg_exprs[i].ty
                } else {
                    (@check(self.infer_type(binding.ty&)) return).expect("infer type")
                }
            };
            has_seen_first_const = true;
            @debug_assert(!arg_ty.is_unknown(), "const_args_key expected arg expr to be compiled");
            arg := arg_exprs.index(i);
            value := @check(self.immediate_eval_expr(arg, arg_ty)) return;
            
            // TODO: remove? -- Aug 13
            //       You might not need this. I was hoping it would fix deconstruct_values but i think this is only used in the map. 
            self.aligned_append_value(all_const_args&, value, arg_ty);
        };
        (Ok = all_const_args.items().to_value())
    }
}

fn aligned_append_value(self: *SelfHosted, all_values: *List(u8), new_value: Values, arg_ty: Type) void = {
    alignment: i64 = self.get_info(arg_ty)[].align_bytes.zext();
    extra := all_values.len.mod(alignment);
    if extra != 0 {
        padding := alignment - extra;
        range(0, padding) { _ |
            all_values.push(0);
        };
    };
    
    all_values.push_all(new_value&.bytes());
}

fn remove_const_args(self: *SelfHosted, original_f: FuncId, arg_expr: *FatExpr) Maybe(void) = {
    func := self.get_function(original_f);
    if func.arg.bindings.len == 1 {
        arg_expr.set(unit_value, self.get_or_create_type(void));
    } else {
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple")));
        arg_exprs := arg_expr.expr.Tuple&;
        // We need to suspend out before removing anything...
        enumerate func.arg.bindings { i, binding | 
            if binding.ty&.ty() { expected |
                found := arg_exprs[i].ty;
                @check(self.can_assign(expected, found)) return;
            };
        };
        removed := 0;
        enumerate func.arg.bindings { i, binding | 
            if binding.kind == .Const {
                // TODO: this would be better if i was iterating backwards
                arg_exprs.ordered_remove(i - removed);
                removed += 1; // TODO: this sucks
            };
        };
        if arg_exprs.is_empty() {
            // Note: this started being required when I added fn while.
            arg_expr.set(unit_value, void);
        } else {
            if arg_exprs.len() == 1 {
                arg_expr[] = arg_exprs[0];
            } else {
                arg_expr.ty = UnknownType;
                arg_expr.done = false;
            };
        };
    };
    
    .Ok
}

MemoKey :: Ty(FuncId, Values);
// TODO: to allow #generic, this lazyily infers param types on the new function.
//       it would avoid redundant work to do that on the original func for params that don't depend on constants. 
//       that would jsut require compile_expr to be able to yield on a Poison.Argument, and we catch that here and just ignore. 
fn curry_const_args(self: *SelfHosted, original_f: FuncId, f_expr: *FatExpr, arg_expr: *FatExpr) Maybe(FuncId) #once = {
    // TODO: no #target_os
    key: MemoKey = (original_f, @check(self.const_args_key(original_f, arg_expr)) return);
    if self.dispatch.const_bound_memo&.get(key&) { new_f |
        @log_event("reuse baked const args % -> % (%)", original_f, new_f, key._1&) self;
        func := self.get_function(new_f);
        if func.get_flag(.AnyConstArgs) {
            original := self.get_function(original_f);
            if func.arg.bindings.len == original.arg.bindings.len {
                return(self.curry_const_args_inner(original_f, new_f, f_expr, arg_expr, 0));
            } else {
                removed_count := original.arg.bindings.len - func.arg.bindings.len;
                @debug_assert(removed_count > 0);
                return(self.curry_const_args_inner(original_f, new_f, f_expr, arg_expr, removed_count));
                return(@err("TODO: unfinished const args but started replacing some so don't really know what to do anymore. "));
            };
        };
        @check(self.remove_const_args(original_f, arg_expr)) return;
        self.set(f_expr, FuncId, new_f);
        return(Ok = new_f);
    };

    func := self.get_function(original_f);
    @err_assert(!func.get_flag(.TargetSplit), "Functions with const arguments cannot be #target_os.\nThis could be made to work for non-capturing args but i'm not ready to deal with it yet.") return;
    @debug_assert(func.get_flag(.AnyConstArgs), "baking const args but none are there");
    
    if LAZY_SCOPE {
        //@debug_assert_eq(func.get_flag(.AllowRtCapture), func.get_flag(.ResolvedBody));  // TODO: do we want to bring this back? -- Jan 8
    } else {
        @check(self.ensure_resolved_sign(original_f)) return;
        @try(self.ensure_resolved_body(original_f)) return;
    };
    
    // Some part of the argument must be known at comptime.
    new_func := func.deep_clone(self.get_alloc());
    mapping: RenumberResults = init();
    if LAZY_SCOPE {
        self.maybe_renumber_and_dup_scope(new_func&, mapping&);
    } else {
        renumber: RenumberVars = (scope = self.scopes, mapping = mapping&, compiler = self);
        renumber&.walk_func(new_func&);
    };
    new_fid := self.add_function(new_func);
    @log_event("bake const args % -> %", original_f, new_fid) self;
    @check(self.ensure_resolved_sign(new_fid)) return;
    @try(self.ensure_resolved_body(new_fid)) return;
    //@println("%", self.get_function(new_fid).log(self));
    // TODO: mark func as in progress somehow so nobody can yield on it. 
    //       tho really i feel like you want to make this whole operation yield-able,
    //       currently you could get into a situation with a lot of redundant work i think.  

    // Note: putting it in super early! this ensures that if you try to call the function while compiling it, 
    //       you don't keep spawning new variations that can never be finished. 
    self.dispatch.const_bound_memo&.insert(key, new_fid);
    
    self.curry_const_args_inner(original_f, new_fid, f_expr, arg_expr, 0)
}

LAZY_SCOPE :: true;
fn maybe_renumber_and_dup_scope(self: *SelfHosted, new_func: *Func, mapping: *RenumberResults) void #once = {
    renumber: RenumberVars = (scope = self.scopes, mapping = mapping, compiler = self);
    renumber&.walk_func(new_func);
}

fn curry_const_args_inner(self: *SelfHosted, original_f: FuncId, new_fid: FuncId, f_expr: *FatExpr, arg_expr: *FatExpr, initial_removed_count: i64) Maybe(FuncId) = {
    @log_event("work baking const args % -> %", original_f, new_fid) self;
    func := self.get_function(new_fid);
    old_func := self.get_function(original_f);
    original_arg_count := old_func.arg.bindings.len();
    ::if(Maybe(FuncId));
    if original_arg_count == 1 {
        assert(initial_removed_count == 0, "TODO: removed_count single");
        binding := func.arg.bindings[0]&;
        @debug_assert_eq(binding.kind, .Const);
        name := or binding.var() {
            return(@err("arg needs name (unreachable?)"))
        };
        // TODO: if you yield here, you spam clone the function. 
        arg_type := or @check(self.infer_type(binding.ty&)) return {
            @err_assert(func.get_flag(.AllowRtCapture), "only closure may have polymorphic args") return;
            @check(self.compile_expr(arg_expr, .None)) return;
            // It was .Infer, save what we learned for when we try to call get_type_for_arg. 
            // TODO: since there's only one argument you could even save this on the original template? 
            binding.ty = (Finished = arg_expr.ty);
            arg_expr.ty
        };
        value := @check(self.immediate_eval_expr(arg_expr, arg_type)) return;
        @check(self.bind_const_arg(new_fid, name, value, arg_expr.ty, arg_expr.loc)) return;

        func.finished_arg = .None;  // TODO: we know its void because we removed the only argument.
        self.set(f_expr, FuncId, new_fid);
        arg_expr.set(unit_value, self.get_or_create_type(void));
        func.unset_flag(.AnyConstArgs);
        func.unset_flag(.Generic);
        (Ok = new_fid)
    } else {
        if(!arg_expr.expr&.is(.Tuple), => return(@err("TODO: pattern match on non-tuple")));
        removed_count := 0;
        range(0, original_arg_count) { i |
            continue :: local_return;
            
            b := func.arg.bindings[i - removed_count]&;
            if removed_count < initial_removed_count {
                if b.kind == .Const {
                    removed_count += 1;
                };
                continue();
                
            };
            
            @log_event("%: check %/%", i, i-removed_count, original_arg_count) self;
            arg_expr := arg_expr.expr.Tuple[i]&;
            if b.kind != .Const {
                if b.ty&.is(.Infer) {
                    // No correctness reason for this requirement but it seems fair to me. -- Aug 2
                    self.last_loc = arg_expr.loc;
                    @err_assert(func.get_flag(.AllowRtCapture), "only closure may have polymorphic args %", func.log(self)) return;
                    @check(self.compile_expr(arg_expr, .None)) return;
                    b.ty = (Finished = arg_expr.ty);
                };
                continue();
            };
            name := or b.var() {
                return(@err("arg needs name (unreachable?)"))
            };
            @log_event("bake arg %: %", i, name&.log(self)) self;
            // TODO: if you yield here, you spam clone the function. 
            
            // We might not be able to know the types expected for each parameter up front (because they can reference previous consts),
            // but once we're binding an argument, we need to be able to get its type. 
            // (TODO: closures are allowed to infer so that's not even true in the long term?)
            xx := @check(self.infer_type(b.ty&)) return;
            arg_type := or xx {
                self.last_loc = func.loc;
                @err_assert(func.get_flag(.AllowRtCapture), "only closure may have polymorphic args") return;
                @check(self.compile_expr(arg_expr, .None)) return;
                // It was .Infer, save what we learned for when we try to call get_type_for_arg. 
                @debug_assert(b.ty&.is(.Infer));
                b.ty = (Finished = arg_expr.ty);
                arg_expr.ty
            };
            value := @check(self.immediate_eval_expr(arg_expr, arg_type)) return;
            
            // bind_const_arg handles adding closure captures.
            // since it needs to do a remap, it gives back the new argument names so we can adjust our bindings acordingly. dont have to deal with it above since there's only one.
            @check(self.bind_const_arg(new_fid, name, value, arg_expr.ty, arg_expr.loc)) return;
            // No remove from arg here because bind_const_arg calls remove_named.
            removed_count += 1;
        };
        @debug_assert_ne(new_fid, original_f);

        // We're leaving it to the caller to infer the new type and type-check the runtime arguments. 
        self.set(f_expr, FuncId, new_fid);
        func.unset_flag(.AnyConstArgs);
        func.unset_flag(.Generic);
        @check(self.remove_const_args(original_f, arg_expr)) return;
        // Don't need to explicitly force capturing because bind_const_arg added them if any args were closures.
        (Ok = new_fid)
    }
}

// TODO: save the bake site on the new variable. 
//       and have self.can_assign take an optional context thingy that tells why it wanted that type, 
//       and try to stick the variable in there sometimes? or show it if the type error happens from a baked thing. 
// The argument type is evaluated in the function declaration's scope, the argument value is evaluated in the caller's scope.
fn bind_const_arg(self: *SelfHosted, o_f: FuncId, arg_name: Var, arg_value: Values, arg_ty_found: Type, loc: Span) Maybe(void) = {
    // I don't want to renumber, so make sure to do the clone before resolving.
    // TODO: reslove captured constants anyway so dont haveto do the chain lookup redundantly on each speciailization. -- Apr 24
    @log_event("Bind $% = %", arg_name&.log(self), arg_value&) self;
    func := self.get_function(o_f);
    @debug_assert(func.get_flag(.ResolvedBody) && func.get_flag(.ResolvedSign));
    arg_ty := @check(self.get_type_for_arg(func.arg&, arg_name)) return;
    
    @check(self.can_assign(arg_ty, arg_ty_found)) return;

    is_function := !(!self.get_type(arg_ty).is(.Fn) && arg_ty != FuncId);
    // TODO: not sure if i actually need this but it seems like i should.
    if is_function {
        arg_func := FuncId.assume_cast(arg_value&)[];
        arg_func_obj := self.get_function(arg_func);
        
        // you want to allow passing a normal function as a const arg without forcing everything to be inlined. 
        may_have_aquired_captures := arg_func_obj.get_flag(.AllowRtCapture) || arg_func_obj.get_flag(.MayHaveAquiredCaptures);
        if may_have_aquired_captures {
            @err_assert(!func.get_flag(.NoInline), "functions with constant lambda arguments are always inlined") return;
            // :ChainedCaptures
            // TODO: HACK: captures aren't tracked properly.
            func.set_flag(.MayHaveAquiredCaptures);
            self[o_f].cc = (Some = .Inline); // just this is enough to fix chained_captures
            self[arg_func].cc = (Some = .Inline); // but this is needed too for others (perhaps just when there's a longer chain than that simple example).
            
            if arg_func_obj.get_flag(.YesContext) {
                func.set_flag(.YesContext);
            };
        };
        
        
        // If we're passing a lambda arg, infer its return type based on the param's hint (not just the callsite). 
        //      f :: fn(x: @Fn() @enum(a, b)) void = { x(); } // callsite doesn't give a hint because statement discards value. 
        //      f(=> .a);  // declaration site doesn't give a hint
        @if_let(self.get_type(arg_ty)) fn Fn(f_ty) => {
            arg_func := self.get_function(arg_func);
            if arg_func.ret&.is(.Infer) {
                arg_func.ret = (Finished = f_ty.ret);
                arg_func.finished_ret = (Some = f_ty.ret);
            };
        };
    };
    @check(self.save_const_values(arg_name, arg_value, arg_ty, loc)) return;
    func.arg&.remove_named(arg_name, self.get_alloc());

    // If it was fully resolved before, we can't leave the wrong answer there.
    // But you might want to call bind_const_arg as part of a resolving a generic signeture so its fine if the type isn't fully known yet.
    func.finished_arg = .None;
    .Ok
}

/// It's fine to call this if the type isn't fully resolved yet.
/// We just need to be able to finish infering for the referenced argument.
fn get_type_for_arg(self: *SelfHosted, arg: *Pattern, arg_name: Var) Maybe(Type) #once = {
    each arg.bindings { arg | 
        if arg.var() { name | 
            if name == arg_name {
                ty := or @check(self.infer_type(arg.ty&)) return {
                    return(@err("called get_type_for_arg on .Infer-ed arg type. expected type annotation."))
                };
                return(Ok = ty);
            };
        };
    };
    @err("missing argument %", arg_name&.log(self))
}
    
fn infer_type(self: *SelfHosted, b: *LazyType) Maybe(?Type) #inline = {
    @match(b) {
        fn PendingEval(e) => {
            ty := @check(self.eval(e, Type)) return;
            b[] = (Finished = ty);
            (Ok = (Some = ty))
        }
        fn Finished(ty) => (Ok = (Some = ty[])); // cool, we're done. 
        fn EvilUninit() => panic("ICE: nothing creates this: eviluninit binding"); 
        fn Returning(_) => return(@err("ICE: tried to infer on LazyType.Returning... figure out what to do about that..."));
        fn Infer() => (Ok = .None);
        fn UnboundGeneric() => return(@err("ICE: tried to infer on unbound generic placeholder"));
        fn Generic() => return(@err("ICE: tried to infer on bound generic placeholder"));
    }
}

fn infer_type(self: *SelfHosted, bindings: *Pattern) Maybe([]Type) = {
    types: List(Type) = list(bindings.bindings.len, temp());
    each bindings.bindings { b |
        ty := or @check(self.infer_type(b.ty&)) return {
            // TODO: really thats not what you want for closures tho... 
            return(@err("function arguments must have type annotation (cannot be inferred)"))
        };
        types&.push(ty);
    };
    (Ok = types.items())
}

fn infer_arguments(self: *SelfHosted, fid: FuncId) Maybe(Type) = {
    func := self.get_function(fid);
    if(func.finished_arg, fn(ty) => return(Ok = ty));
    @check(self.ensure_resolved_sign(fid)) return;
    
    types := @check(self.infer_type(func.arg&)) return;
    ty := self.tuple_of(types);
    func.finished_arg = (Some = ty);  // :const_args_are_not_const_in_tuple
    (Ok = ty)
}

fn get_arg_types_non_blocking(func: *Func, args: *List(ResultType)) void #once = {
    args.clear();
    for func.arg.bindings { b | 
        @match(b.ty&.ty()) {
            fn Some(ty) => args.push(Specific = ty);
            fn None()   => args.push(.None);
        };
    };
}

fn infer_arguments_partial(self: *SelfHosted, fid: FuncId) Maybe(ResultType) = {
    func := self.get_function(fid);
    if(func.finished_arg, fn(ty) => return(Ok = (Specific = ty)));
    @check(self.ensure_resolved_sign(fid)) return;
    
    args: List(ResultType) = list(func.arg.bindings.len, temp());
    all_known := true;
    each func.arg.bindings { b |
        @match(@check(self.infer_type(b.ty&)) return) {
            fn Some(known) => args&.push(Specific = known);
            fn None() => {
                args&.push(.None);
                all_known = false;
            }
        };
    };
    if all_known {
        ty := @check(self.infer_arguments(fid)) return;
        return(Ok = (Specific = ty));
    };
    (Ok = (Tuple = args.items()))
}

fn infer_return(self: *SelfHosted, fid: FuncId) Maybe(Type) = {
    func := self.get_function(fid);
    if(func.finished_ret, fn(ty) => return(Ok = ty));
    @check(self.ensure_resolved_sign(fid)) return;
    
    ty := or @check(self.infer_type(func.ret&)) return {
        // Infer is a valid return type. To deal with that, we just compile the body, and that will set the finished_ret for us. 
        @assert(!self.dispatch.function_in_progress&.get(fid.as_index()), "unhanlded mutual recursion. %", self.log_name(fid));
        self.dispatch.function_in_progress&.set(fid.as_index());
        return(Suspend = self.wait_for(CompileBody = fid))
    };
    func.finished_ret = (Some = ty);
    (Ok = ty)
}

fn type_of(self: *SelfHosted, expr: *FatExpr) Maybe(Type) = {
    if(!expr.ty.is_unknown(), => return(Ok = expr.ty));
    @match(expr.expr&) {
        fn Block(f) => self.type_of(f.result);
        fn Deref(inner) => {
            ptr := @check(self.type_of(inner[])) return;
            (Ok = self.unptr_ty(ptr).or(=> return(@err("deref non ptr"))))
        }
        fn Addr(inner) => {
            ty := @check(self.type_of(inner[])) return;
            (Ok = self.ptr_ty(ty))
        }
        //fn Quote() => (Ok = @unwrap(self.env.fat_expr_type, "quote during boot") return);
        fn As(f) => {
            type := @check(self.eval(f.type, Type)) return;
            (Ok = type)
        }
        fn GetVar(name) => {
            ::if_opt(VarInfo, void);
            if self.scopes.get_var_type(name[]) { info |
                return(Ok = info.type);
            };
            ::if(Type);
            if name.kind == .Const {
                v, type := @check(self.find_const(name[], .None)) return;
                return(Ok = type);
            };
            return(@err("type_of pending var: %", self.pool.get(name.name)))
        }
        fn Closure(f) => {  
            @check(self.compile_expr(expr, .None)) return;
            (Ok = expr.ty)
        }
        @default => @err("failed to guess type");
        //@default => @err("failed to guess type %", expr.log(self)); 
    }
}

fn adjust_named_arguments(self: *SelfHosted, fid: FuncId, arg_expr: *FatExpr) Res(void) = {
    @if_let(arg_expr.expr&) fn StructLiteralP(f) => {
        func := self.get_function(fid);
        // If there's one argument, they must be passing a struct, we don't care about that here.
        if func.arg.bindings.len == 1 && f.bindings.len != 1 {
            return(.Ok);
        };
        args: List(FatExpr) = list(func.arg.bindings.len, self.get_alloc());
        enumerate func.arg.bindings { i, want |
            continue :: local_return;
            want_name := @unwrap(want.ident(), "function param must have name") return;
            each f.bindings { found | 
                found_name := @unwrap(found.ident(), "TODO: mixed named and un-named args are not supported yet") return;
                if found_name == want_name {
                    value := @unwrap(found.get_default(), "use '=' not ':' for named arg") return;
                    @err_assert(args.len == i, "TODO: out of order named args are not supported yet") return;
                    args&.push(value[]);
                    continue();
                };
            };
            return(@err("arg not found for %", self.pool.get(want_name)));
        };
        arg_expr.expr = (Tuple = args.as_raw());
        arg_expr.done = false;
        arg_expr.ty = UnknownType;
        @log_event("after named %", arg_expr.log(self)) self;
    };
    .Ok
}

fn easy_check_type(self: *SelfHosted, e: *FatExpr) ?Type = {
    if e.ty != UnknownType {
        return(Some = e.ty);
    };
    @match(e.expr&) {
        fn GetVar(name) => 
            if self.scopes.get_var_type(name[]) { i |
                return(Some = i.type);
            };
        fn Deref(inner) => {
            if self.easy_check_type(inner[]) { inner | 
                return(self.unptr_ty(inner));
            };
        }
        fn Addr(inner) => {
            if self.easy_check_type(inner[]) { inner | 
                return(Some = self.ptr_ty(inner));
            };
        }
        @default => ();
    };
    
    .None
}

// TODO: this is very non-linear with number of overloads. :SLOW :SLOW :SLOW
fn resolve_in_overload_set_new(self: *SelfHosted, arg_expr: *FatExpr, attempt: *OverloadAttempt) Maybe(FuncId) #once = {
    overloads := self.dispatch.overloads&.nested_index(attempt.os.as_index());
    
    // this + the hash table helps a bit ~2300 -> ~2220, but it's really not enough. 
    if overloads.ready.len > 6 && arg_expr.ty.is_unknown() {
        bail :: local_return;
        args := arg_expr.items();
        types := Type.list(temp());
        each args { a | 
            if is_const(a) {
                bail();
            };
            if self.easy_check_type(a) { ty |
                types&.push(ty);
            } else {
                bail();
            }
        };
        arg_expr.ty = self.tuple_of(types.items());
    };
    
    // this does catch some but doesn't help much. TODO: maybe remove
    if !arg_expr.ty.is_unknown() {
        req := UnknownType;
        @if_let(attempt.call.requested) fn Specific(t) => {
            req = t;
        };
        key: OverloadKey = (arg = arg_expr.ty, req = req);
        if overloads.inline_cache._0& == key& {
            return(Ok = overloads.inline_cache._1);
        };
        if overloads.table& { t | 
            if t.get(key&) { f |
                return(Ok = f);
            };
        };
    };
    
    self.compute_new_overloads(overloads);
    if overloads.ready.len > attempt.last_ready_count {
        attempt.state = 0;
        new := overloads.ready.items().slice(attempt.last_ready_count, overloads.ready.len);
        attempt.options&.reserve(new.len);
        want_arity := arity(arg_expr);
        for new { option | 
            if self.filter_overload_sync(arg_expr, want_arity, option) {
                @check(self.ensure_resolved_sign(option)) return; // TODO: dont let this suspend cause it cant
                attempt.options&.push_assume_capacity(option);
            };
        };
        attempt.last_ready_count = overloads.ready.len;
    };
    @log_event("checking %/% options", attempt.options.len, overloads.ready.len) self;
    want_arity := arity(arg_expr);
    ::if([]FatExpr);
    parts := if(arg_expr.expr&.is(.Tuple), => arg_expr.expr.Tuple.items(), => (ptr = arg_expr, len = 1));
    are_we_done_yet();
    
    are_we_done_yet :: fn() => {
        if attempt.options.len == 1 {
            return(Ok = self.done_resolving_overload(attempt, overloads, arg_expr));
        };
        if attempt.options.len == 0 {
            self.last_loc = arg_expr.loc;
            return(self.report_missing_overload(parts, overloads, attempt.call.requested));
        };
    };
    
    variadic_count := 0;
    // TODO: should be able to move the infer_type into the next phase just before you need it but no -- Sep 2
    if attempt.state < 1 {
        are_we_done_yet();
        // TODO: while is it faster to write the loop like this than with for?
        f_i := attempt.options.len;
        
        while => f_i > 0 {
            continue :: local_return;
            f_i -= 1;
            fid := attempt.options[f_i];
            func := self.get_function(fid);
            if func.get_flag(.CVariadic) {
                variadic_count += 1;
            };
            
            if !func.get_flag(.Generic) && func.finished_arg.is_none() {
                all_known := true;
                each func.arg.bindings& { b |
                    res := @check(self.infer_type(b.ty&)) return;
                    all_known = all_known && res&.is_some();
                };
                if all_known {
                    @check(self.infer_arguments(fid)) return;
                };
            };
            
            if func.get_flag(.UnboundGenerics) {
                accept := @check(self.eval_where_from_expression(fid, arg_expr)) return;
                if accept { new_fid | 
                    attempt.options[f_i] = new_fid;
                } else {
                    attempt.options&.unordered_remove_discard(f_i);
                };
            };
        };
        
        if variadic_count > 0 {
            @err_assert(variadic_count == 1, "TODO: support multiple #c_variadic functions in one overload set") return;
            @err_assert(false, "TODO: support #c_variadic in overload sets") return;
        };
        
        attempt.state = 1;
    };
    
    if attempt.state < 2 {
        //@println("%) % has % options", attempt.state, self.pool.get(overloads.name), attempt.options.len);
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            type := self.type_of(arg_expr);
            if !type&.is(.Ok) {
                continue();
            };
            type := type.Ok;
            if type == FuncId {
                continue();
            };
            arg_expr.ty = type;
            
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                @check(self.infer_type(b.ty&)) return;
                if b.ty&.ty() { param | 
                    if !self.ask_can_assign(param, type) {
                        // TODO: this makes it slower and means you can't cache it but also is going to be confusingly wrong in some cases
                        //       because you might make changes based on a type hint from an overload that didn't end up getting selected. 
                        //       but also it's very convenient to have this so here we are for now.   -- Dec 23
                        if arg_expr.is_const() { // this helps with :type_through_field
                            res := self.ask_coerce_const_expr(arg_expr, param, true);
                            if res&.is(.Ok) && res.Ok {
                                @log_event("% [%] allow coerce arg % <- %", fid, arg_index, self.log_type(param), self.log_type(type)) self;
                                continue();
                            };
                        };
                        
                        //@log_event("% [%] reject arg % <- %", fid, arg_index, self.log_type(param), self.log_type(type)) self;
                        reject();
                    };
                    
                    //@log_event("% [%] allow arg % <- %", fid, arg_index, self.log_type(param), self.log_type(type)) self;
                };
            };
            are_we_done_yet();
        };
        attempt.state = 2;
    };
    
    if attempt.state < 3 {
        @if_let(attempt.call.requested) fn Specific(type) => {
            //@println("%) % has % options", attempt.state, self.pool.get(overloads.name), attempt.options.len);
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if !func.get_flag(.Generic) && func.finished_ret.is_none() {
                    @check(self.infer_return(fid)) return;
                };
                
                if func.finished_ret { ret | 
                    if !self.ask_can_assign(type, ret) {
                        reject();
                    };
                };
            };
            are_we_done_yet();
            //@println("requested %", self.log_type(type));
        };
        attempt.state = 3;
    };
    
    if attempt.state < 4 {
        //@println("%) % has % options", attempt.state, self.pool.get(overloads.name), attempt.options.len);
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            @check(self.compile_expr(arg_expr, .None)) return;
            type := arg_expr.ty;
            
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                if b.ty&.ty() { param | 
                    if !self.ask_can_assign(param, type) {
                        //@println("[%] reject arg % <- %", arg_index, self.log_type(param), self.log_type(type));
                        reject();
                    };
                    
                    //@println("[%] allow arg % <- %", arg_index, self.log_type(param), self.log_type(type));
                };
            };
            are_we_done_yet();
        };
        attempt.state = 4;
    };
    
    if attempt.state < 5 {
        //@println("%) % has % options", attempt.state, self.pool.get(overloads.name), attempt.options.len);
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            
            arg_fid := if arg_expr.is_const() && (arg_expr.ty.eq(FuncId) || self.get_type(arg_expr.ty).is(.Fn)) {
                @check(self.immediate_eval_expr(arg_expr, arg_expr.ty)) return
            } else {
                continue()
            };
            arg_fid := FuncId.assume_cast(arg_fid&)[];
            arg_func := self.get_function(arg_fid);
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                b := func.arg.bindings.index(arg_index);
                if b.ty&.ty() { param | 
                    param_info := self.get_type(param);
                    if not(arg_expr.ty.eq(FuncId).or(=> param_info.is(.Fn))) {
                        reject();
                    };
                    
                    @if_let(param_info) fn Fn(ty) => {
                        res := self.check_promote_id_to_func(arg_fid, ty[]);
                        if res&.is(.Err) {
                            reject();
                        };
                        @check(res) return;
                    };
                };
            };
            are_we_done_yet();
        };
        attempt.state = 5;
    };
    
    if attempt.state < 6 {
        @check(self.do_merges(attempt.options&, attempt.os)) return;
        attempt.state = 6;
    };
    are_we_done_yet();
    
    // this is garbage
    {
        enumerate parts { arg_index, arg_expr | 
            continue :: local_return;
            @check(self.compile_expr(arg_expr, .None)) return;
            type := arg_expr.ty;
            
            f_i := attempt.options.len;
            while => f_i > 0 {
                continue :: local_return;
                f_i -= 1;
                reject :: fn() void => {
                    attempt.options&.unordered_remove_discard(f_i);
                    continue();
                };
                fid := attempt.options[f_i];
                func := self.get_function(fid);
                if func.arg.bindings.len != want_arity {
                    continue();
                };
                
                fn ne(a: bool, b: bool) bool = !(a == b);
                b := func.arg.bindings.index(arg_index);
                if b.ty&.ty() { param | 
                    type := self.get_type(type);
                    param := self.get_type(param);
                    if type.is(.Int) && param.is(.Int) && type.Int.signed != param.Int.signed {
                        reject();
                    };
                };
            };
            are_we_done_yet();
        };
    };

    self.last_loc = arg_expr.loc;
    @println("% matching options", attempt.options.len);
    if attempt.options.len < 6 {
        for attempt.options { opt |
            func := self.get_function(opt);
            @if(DEBUG_SPAM_LOG) self.codemap.show_error_line(func.loc);
            @print("- (");
            each func.arg.bindings { b |
                @match(b.ty&.ty()) {
                    fn Some(type) => {
                        @print("%, ", self.log_type(type));
                    }
                    fn None() => {
                        @print("???, ");
                    };
                };
            };
            @println(") -> %", self.log_type(func.finished_ret));
        };
    };
    
    @err("TODO: end of loop. still too many options for '%'", self.pool.get(overloads.name))
}

fn eval_where_from_expression(self: *SelfHosted, fid: FuncId, arg_expr: *FatExpr) Maybe(?FuncId) = {
    args := arg_expr.items();
    func := self.get_function(fid);
    if func.arg.bindings.len != args.len {
        return(Ok = .None);
    };
    types := Type.list(1, temp());
    enumerate func.arg.bindings& { arg_i, b |
        if b.ty&.is(.Generic) {
            type := @check(self.type_of(args.index(arg_i))) return;
            types&.push(type);
        };
    };
    
    types_value := self.from_bytes(interpret_as_bytes(types.items()));
    key: WhereKey = (template = fid, arg = types_value);
    if self.where_memo&.get(key&) { new_fid |
        return(Ok = (Some = new_fid));
    };
    accept := @check(self.evaluate_where_clauses(fid, types.items(), arg_expr.loc)) return;
    if !accept {
        return(Ok = .None);
    };
    new_fid := @check(self.instantiate_where_generic(fid, types.items())) return;
    (Ok = (Some = new_fid))
}

fn evaluate_where_clauses(self: *SelfHosted, fid: FuncId, generic_types: []Type, loc: Span) Maybe(bool) = {
    types_value := self.from_bytes(interpret_as_bytes(generic_types));
    key: WhereKey = (template = fid, arg = types_value);
    if self.where_memo&.get(key&).is_some() {
        return(Ok = true);
    };
    
    func := self.get_function(fid);
    slow_arg_ty := {
        slow := temp().alloc(Type, generic_types.len);
        each slow { s |
            s[] = Type; // :get_or_create_type
        };
        self.tuple_of(slow)
    };
    f_ty := self.intern_type(Fn = (arg = slow_arg_ty, ret = bool, arity = generic_types.len.trunc()));  // :get_or_create_type
    
    types_arg_expr: FatExpr = (expr = (Value = (bytes = types_value, coerced = false)), done = false, ty = slow_arg_ty, loc = loc);
    each func.annotations& { ann |
        if ann.name == Flag.where.ident() {
            if ann.non_void_arg() { where_expr |
                if !where_expr.is_raw_unit() {
                    // TODO: it needs to know this is in a const context. 
                    _ := self.poll_in_place(void, => self.compile_expr(where_expr, (Specific = f_ty)));
                    condition := @check(self.eval(where_expr, FuncId)) return;
                    
                    // :get_or_create_type
                    // TODO: bad tack memory!
                    accept := @check(self.invoke(condition, types_arg_expr&, bool, loc)) return;
                    accept := bool.assume_cast(accept&)[];
                    if !accept {
                        return(Ok = false);
                    };
                };
            };
        };
    };
    
    // We accepted all #where clauses. 
    (Ok = true)
}

fn instantiate_where_generic(self: *SelfHosted, template: FuncId, generic_types: []Type) Maybe(FuncId) = {
    template_func := self.get_function(template);
    
    new_func := template_func.deep_clone(self.get_alloc());
    new_func&.unset_flag(.UnboundGenerics);
    new_func.annotations&.unordered_retain(fn(it) => it.name != Flag.where.ident()); // TODO: don't clone it in the first place

    mapping: RenumberResults = init();
    maybe_renumber_and_dup_scope(self, new_func&, mapping&);
    
    n := 0;
    each new_func.arg.bindings& { b |
        @if_let(b.ty&) fn Generic(name) => {
            new := name[];
            type := generic_types[n];
            b.ty = (Finished = type);
            //b.ty = (PendingEval = (expr = (GetVar = new), ty = Type, loc = template_func.loc, done = false));
            self.scopes.put_constant(new, self.to_expr(Type, type, template_func.loc), (Finished = Type)); // :get_or_create_type
            n += 1;
        };
    };
    
    @try(self.resolve_body(new_func&)) return;
    new_func&.unset_flag(.Generic);   
    fid := self.add_function(new_func);
    
    types_value := self.from_bytes(interpret_as_bytes(generic_types));
    key: WhereKey = (template = template, arg = types_value);
    self.where_memo&.insert(key, fid).is_some();
    
    @check(self.infer_arguments(fid)) return;
    
    (Ok = fid)
}

fn done_resolving_overload(self: *SelfHosted, attempt: *OverloadAttempt, overloads: *OverloadSetData, arg_expr: *FatExpr) FuncId = {
    fid := attempt.options[0];
    @log_event("selected % from '%'", fid, self.pool.get(overloads.name)) self;
    
    if !arg_expr.ty.is_unknown() {
        req := UnknownType;
        @if_let(attempt.call.requested) fn Specific(t) => {
            req = t;
        };
        key: OverloadKey = (arg = arg_expr.ty, req = req);
        overloads.inline_cache = (key, fid);
        if overloads.table& { t |
            t.insert(key, fid); 
        };
    };
    fid
}

fn check_promote_id_to_func(self: *SelfHosted, fid: FuncId, f_ty: FnType) Maybe(void) = {
    func := self.get_function(fid);
    //@println("check_promote_id_to_func func <- funcid %", self.log_type(self.intern_type(Fn = f_ty)));
    //partial_args := @check(self.infer_arguments_partial(fid)) return;
    // TODO: adding this fixes or_wrong_type (but only if you add type annotations) but breaks more other stuff. 
    //if !func.ret&.is(.Infer) {
    //    @try(self.poll_in_place(Type, => self.infer_return(fid))) return;
    //};
    
    // this doesn't help. doesn't happen. but its similar to what the old one did in promote_closure
    // if you take out infering from requested in fn Closure, this happens, but then its wrong in the prelude. 
    if func.ret&.is(.Infer) {
        @if_let(func.body&) fn Normal(body) => {
            guessed := self.type_of(body);
            @if_let(guessed) fn Ok(type) => {
                //@println("guessed! %", self.log_type(type));
                func.ret = (Finished = type);
                func.finished_ret = (Some = type);
            };
        };
    };
    
    ret := @check(self.infer_return(fid)) return;
    if func.finished_ret { ret | 
        @check(self.can_assign(f_ty.ret, ret)) return;
    };
    //@println("const_coerce func <- funcid %", self.log_type(want));
    .Ok
}

fn filter_overload_sync(self: *SelfHosted, arg_expr: *FatExpr, arg_expr_arity: i64, fid: FuncId) bool #once #inline = {
    func := self.get_function(fid);
    
    if func.body&.is(.Empty) && func.annotations.len == 0 {
        // foward declaration if an overload set. 
        return(false);
    };
    
    if func.arg.bindings.len != arg_expr_arity {
        if !arg_expr.expr&.is(.StructLiteralP) {
            return(false);
        };
        // Maybe they're trying to pass named arguments. 
        have_parts := arg_expr.expr.StructLiteralP.bindings&;
        want_parts := func.arg.bindings&;
        if want_parts.len != have_parts.len {| // TODO: allow default args
            return(false);
        };
        
        // :SLOW but you get here so rarely it's fine (ie. once for self-compiling).
        each want_parts { want_b | 
            next :: local_return;
            want_name := want_b.ident().or(=> return(false)); // idk if this ever happens but it sure can't be named argument
            each have_parts { have_b |
                have_name := have_b.ident().or(=> return(false)); // idk if this ever happens but it sure can't be named argument
                if want_name == have_name {
                    next();
                };
            };
            return(false);
        };
    };
    
    true
} 

// TODO: need to check for conflicts somehow but we can't assume we're able to infer all the options here,
//       because there might be cycles or more might be added later. 
//       do i need to keep a list of all the (os, arg_ty -> fn) and check at the end?
// TODO: do merges
fn resolve_in_overload_set(self: *SelfHosted, f_ty: FnType, i: OverloadSet, loc: Span) Maybe(FuncId) = {
    types := self.arg_types(f_ty.arg);
    self.last_loc = loc;
    want_arity := types.len;
    
    overloads := self.dispatch.overloads&.nested_index(i.as_index());
    self.compute_new_overloads(overloads);
    
    interesting: List(FuncId) = list(temp());
    @log_event("% total options", overloads.ready.len) self;
    for overloads.ready { opt | 
        continue :: local_return;
        func := self.get_function(opt);
        if func.arg.bindings.len != want_arity {
            continue();
        };
        if func.finished_ret { have | 
            if !self.ask_can_assign(f_ty.ret, have) {
                continue();
            };
        };
        if func.finished_arg { found | 
            if !self.ask_can_assign(f_ty.arg, found) {
                continue();
            };
        };
        interesting&.push(opt);
    };
    
    // TODO: but what if there are more overloads coming later? 
    if interesting.len == 1 {
        return(Ok = interesting[0]);
    };
    
    ::if([]FatExpr);
    blocked: List(*Action) = list(temp());
    still_interesting: List(FuncId) = list(temp());
    for interesting& { opt | 
        continue :: local_return;
        func := self.get_function(opt);         
        @check(self.ensure_resolved_sign(opt)) return;
        
        enumerate func.arg.bindings { i, b | 
            if !func.get_flag(.Generic) && !b.ty&.is(.Finished) {
                @check(self.infer_type(b.ty&)) return;
            };
            if b.ty&.ty() { param | 
                if !self.ask_can_assign(param, types[i]) {
                    continue();
                };
            };
        };
        still_interesting&.push(opt);
    };
    
    if still_interesting.len == 1 {
        return(Ok = still_interesting[0]);
    };
    temp := still_interesting;
    still_interesting = interesting;
    interesting = temp;
    still_interesting&.clear();

    want := f_ty.ret;
    for interesting& { opt | 
        continue :: local_return;
        func := self.get_function(opt);    
        // TODO: i dont want to check .Generic because i want to remove the need for the annotation eventually. 
        if !func.get_flag(.Generic) && func.finished_ret&.is_none() {
            @match(self.infer_return(opt)) {
                fn Ok() => ();
                fn Err(e) => return(Err = e);
                fn Suspend(f) => {
                    blocked&.push(f); 
                    // TODO: write a test where this matters
                    still_interesting&.push(opt);
                    continue();
                }
            };
        };
        
        if func.finished_ret { found | 
            if !self.ask_can_assign(want, found) {
                continue();
            };
        };
        still_interesting&.push(opt);
    };

    interesting = still_interesting;
    
    if !blocked.is_empty() {
        @log_event("block len = %", blocked.len) self;
        return(Suspend = blocked[0]);  // TODO: return all? don't just pick one. 
    };
    
    if interesting.len > 1 {
        @check(self.do_merges(interesting&, i)) return;
    };
    if interesting.len == 1 {
        return(Ok = interesting[0]);
    };
    
    
    // TODO: don't spam log since we might discard the error.
    @println("ambigous overload for % -> %", self.log_type(f_ty.arg), self.log_type(f_ty.ret));
    self.last_loc = loc;
    @println("% matching options", interesting.len);
    for interesting { opt |
        func := self.get_function(opt);
        @if(DEBUG_SPAM_LOG) self.codemap.show_error_line(func.loc);
        @println("- % -> %", self.log_type(func.finished_arg), self.log_type(func.finished_ret));
    };
    overloads := self.dispatch.overloads&.nested_index(i.as_index());
    @err("TODO: resolve_in_overload_set % %", self.pool.get(overloads.name), interesting.len)
}

// TODO: you want something more powerful than this so you can have a function thats inline asm on some arch/os but normal code on others. 
// TODO: this discards link_renamer and any other tags that have special meaning. 
// TODO: simplier tests that use this. right now its just the compiler. 
fn do_merges(self: *SelfHosted, options: *List(FuncId), os: OverloadSet) Maybe(void) = {
    overloads := self.dispatch.overloads&.nested_index(os.as_index());
    
    for options { opt | 
        func := self.get_function(opt);
        if !func.get_flag(.BodyIsSpecial) {
            // since we know we do this at the end, we aren't going to be able to help you if you have multiple overloads and not all are special. 
            return(.Ok);
        };
        @check(self.infer_arguments(opt)) return;
        @check(self.infer_return(opt)) return;
        @try(self.ensure_resolved_body(opt)) return;
        @check(self.emit_special_body(opt)) return;
        func.set_flag(.EnsuredCompiled);
    };
    
    //@println("doing merges %", self.pool.get(overloads.name));
    f_i := options.len;
    while => f_i > 0 {
        continue :: local_return;
        f_i -= 1;
        fid := options[f_i];
        func := self.get_function(fid);
        f_ty := func.finished_ty().expect("types to be infered");
        
        merged := func.body;
        func.body = .Empty; // this will get reset after the loop. 
        
        f_j := f_i;
        while => f_j > 0 {
            continue :: local_return; 
            f_j -= 1;
            other_fid := options[f_j];
            @debug_assert(other_fid != fid, "function appeared twice in overload set");
            other_func := self.get_function(other_fid);
            
            other_f_ty := other_func.finished_ty().expect("types to be infered");
            if f_ty.arg == other_f_ty.arg && f_ty.ret == other_f_ty.ret {
                if !other_func.body&.is(.Redirect) {  // TODO: i'd rather not get here a billion times
                    append_impl(merged&, self.get_alloc(), other_func.body);
                    other_func.body = (Redirect = fid);
                    options.unordered_remove_discard(f_j);
                    if f_j < f_i {
                        f_i -= 1;
                    };
                    overloads.ready&.unordered_retain() { (f: *FuncId) bool | 
                        f[] != other_fid
                    };
                };
            };
        };
        func.body = merged;
    };
    // TODO: test getting here multiple times
    
    .Ok
}

fn log_type(self: *SelfHosted, ty: ?Type) Str = {
    @match(ty) {
        fn Some(ty) => self.log_type(ty);
        fn None() => "???";
    }
}

fn compute_new_overloads(self: *SelfHosted, overloads: *OverloadSetData) void = {
    overloads.ready&.push_all(overloads.pending&.items(), self.get_alloc());
    if overloads.table.is_none() && overloads.ready.len > 6 {
        overloads.table = (Some = init(self.get_alloc()));
    };
    overloads.pending.len = 0;
}

// - you want macros to be able to create new constant declarations in macro expansions and const arg functions.
// - for now constants are always stored globally and restricted visibility is just handled by scope resolution.
// So we delay taking constants until you try to compile the expression that contains them.
// Also, to be order independent, we don't actually evaluate or type-check them yet, that's done lazily the first time they're referenced. 
// TODO: the old compiler did #when here, but I think its better to delay until we know we care and can hope more stuff is ready. 
fn hoist_constants(self: *SelfHosted, body: []FatStmt) Maybe(void) #once = {
    each body { stmt |
        @match(stmt.stmt&) {
            fn DeclFunc(func) => {
                // TODO: aren't the `@check`s below a problem because if they suspend you're totally fucked cause the function's gone but you might not know its overload set or whatever? 
                fid := self.add_function(func[][]);
                stmt.stmt = .Noop;
                func := self.get_function(fid);
                overload_out: ?OverloadSet = .None;
                
                // I thought i dont have to add to constants here because we'll find it on the first call when resolving overloads.
                // But it does need to have an empty entry in the overload pool because that allows it to be closed over so later stuff can find it and share if they compile it.
                if func.var_name& { var |
                    // TODO: allow function name to be any expression that resolves to an OverloadSet so you can overload something in a module with dot syntax.
                    // TODO: distinguish between overload sets that you add to and those that you re-export
                    //@debug_assert(!func.get_flag(.ResolvedSign), "unresolved"); // TODO!
                    //@debug_assert(!func.get_flag(.ResolvedBody), "unresolved"); // TODO!
                    // TODO: assert placeholdervalue
                    if self.is_empty_constant(var[]) {
                        // We're the first to reference this overload set so create it. 
                        i: OverloadSet = from_index(self.dispatch.overloads.len);
                        self.dispatch.overloads&.push(
                            ready = empty(),
                            name = var.name,
                            pending = fid.single(self.get_alloc()).as_raw_list(),
                            // zero is UnknownType which we'll never query so its a fine .None replacement value since i don't have niches yet. 
                            inline_cache = Ty(OverloadKey, FuncId).zeroed(),
                        );
                        os_value := self.to_values(OverloadSet, i);
                        @check(self.save_const_values(var[], os_value, self.get_or_create_type(OverloadSet), func.loc)) return;
                        @log_event("create os % (%) for %", i.as_index(), var.log(self), fid) self;
                        overload_out = (Some = i);
                    } else {
                        overloads := @check(self.find_const(var[], self.get_or_create_type(OverloadSet).want())) return;
                        i := OverloadSet.assume_cast(overloads._0&)[];
                        os := self.dispatch.overloads&.nested_index(i.as_index());
                        os.pending&.push(fid, self.get_alloc());
                        @log_event("add to os % (%) for %", i.as_index(), var.log(self), fid) self;
                        overload_out = (Some = i);
                    };
                };
                
                @try(self.update_function_metadata(func, overload_out)) return;
            }
            fn Decl(f) => if f.kind == .Const {
                @debug_assert(f.name&.is(.Var), "tried to hoist unresovled decl");
                self.scopes.put_constant(f.name.Var, f.default, f.ty);
                stmt.stmt = .Noop;
            };
            @default => ();
        }
    };
    .Ok
}

fn update_function_metadata(self: *SelfHosted, func: *Func, os: ?OverloadSet) Res(void) = {
    any_const_args :: fn(self: *Func) bool = {
        each self.arg.bindings { b |
            if(b.kind == .Const, => return(true));
        };
        false
    };
    if func.any_const_args() {
        func.set_flag(.AnyConstArgs);
    };

    each func.annotations { tag | 
        @switch(tag.name) {
            @case(Flag.fold.ident())     => func.set_flag(.TryConstantFold);
            @case(Flag.macro.ident())    => {
                func.set_flag(.Macro);
                func.set_flag(.ComptimeOnly);
            };
            @case(Flag.noinline.ident()) => func.set_flag(.NoInline);
            @case(Flag.no_context.ident()) => func.set_flag(.NoContext);
            @case(Flag.yes_context.ident()) => func.set_flag(.YesContext);
            // TODO: generic+unsafe_noop_cast+cold are done in scope for old sema but i want to move them here. -- Jul 30
            @case(Flag.generic.ident())  => func.set_flag(.Generic);
            @case(Flag.cold.ident())     => func.set_flag(.Cold);
            @case(Flag.log_bc.ident())   => func.set_flag(.LogBc);
            @case(Flag.log_asm.ident())  => func.set_flag(.LogAsm);
            @case(Flag.log_ir.ident())     => func.set_flag(.LogIr);
            @case(Flag.log_ast.ident())    => func.set_flag(.LogAst);
            @case(Flag.unsafe_noop_cast.ident()) => {
                func.set_flag(.UnsafeNoopCast);
                func.set_flag(.BodyIsSpecial);
            };
            @case(Flag.target_os.ident()) => {
                func.set_flag(.TargetSplit);
            };
            @case(Flag.bc.ident())       => func.set_flag(.BodyIsSpecial);
            @case(Flag.import.ident())   => {
                func.set_flag(.BodyIsSpecial);
                //func.set_flag(.NoMangle);
            };
            @case(Flag.libc.ident())     => {
                func.set_flag(.BodyIsSpecial);
                //func.set_flag(.NoMangle);
            };
            @case(Flag.redirect.ident()) => {
                @err_assert(func.body&.is(.Empty), "#redirect conflicts with body expression") return;
                os := @unwrap(os, "#redirect function only makes sense as part of an overload set") return;
                os := self.to_values(OverloadSet, os);
                expr := @unwrap(tag.non_void_arg(), "#redirect requires argument") return;
                os := synthetic_ty((Value = (bytes = os, coerced = false)), expr.loc, OverloadSet);
                @err_assert(expr.expr&.is(.Tuple), "#redirect expected tuple") return;
                parts := expr.expr.Tuple&;
                parts.push(os, self.get_alloc());
                func.set_flag(.BodyIsSpecial);
            };
            // TODO: actually im not sure if its better to do this later...
            //       it would be nice to do only one pass over the annotations 
            //       but it would also be nice to do absolutely no work if you never try to call the function. 
            @case(Flag.comptime_addr.ident()) => {
                @err_assert(func.body&.is(.Empty), "#comptime_addr conflicts with body expression") return;
                func.set_flag(.BodyIsSpecial);
                //func.set_flag(.ComptimeOnly);
            };
            @case(Flag.intrinsic.ident()) => {
                func.set_flag(.BodyIsSpecial);
            };
            @case(Flag.ir.ident()) => {
                func.set_flag(.BodyIsSpecial);
            };
            @case(Flag.asm.ident()) => {
                @err_assert(!func.body&.is(.Empty), "#asm requires body expression") return;
                func.set_flag(.BodyIsSpecial);
            };
            @case(Flag.inline.ident()) => {
                // TODO: error on conflicting annotations. 
                func.cc = (Some = .Inline);
            };
            @case(Flag.ct.ident()) => {
                func.set_flag(.ComptimeOnly);
                func.set_flag(.YesContext);
            };
            @case(Flag.c_variadic.ident()) => {
                func.set_flag(.CVariadic);
                func.set_flag(.NoInline);  // TODO
            };
            @default => ();
        };
    };
    
    if func.cc.is_none() {
        func.cc = (Some = .CCallReg);
    };
    .Ok
}

::if(Maybe(void));
::if_opt(Type, Maybe(void));
fn compile_get_var(self: *SelfHosted, expr: *FatExpr, requested: ResultType, old: bool) Maybe(void) #once = {
    @debug_assert(expr.expr&.is(.GetVar));
    var := expr.expr.GetVar;
    if var.kind == .Const {
        value, ty := @check(self.find_const(var, requested)) return;
        expr.set(value, ty);
        @check(self.coerce_const_expr(expr, requested, false)) return;
        .Ok
    } else {
        if self.scopes.get_var_type(var) { info | 
            expr.ty = info.type;
            expr.done = true;
            // Reading a variable. Convert it to `var&[]`. 
            ptr_ty := self.ptr_type(info.type);
            // :?
            if old {
                info.took_address = true;
                expr[] = synthetic_ty((Addr = self.box(expr[])), expr.loc, ptr_ty);
                expr.done = true;
                // Note: not using deref_one, because don't want to just remove the ref, we want raw variable expressions to not exist. kinda HACK
                expr[] = synthetic_ty((Deref = self.box(expr[])), expr.loc, info.type);
                expr.done = true;
            };
            return(.Ok);
        };
        // For now runtime vars are always declared in order so we always know thier type.
        // This might change to allow peeking into return-ed expressions when infering closure return type? -- Jul 15
        @err("Unknown type for runtime var %", self.log(var))
    }
}

fn took_pointer_value(self: *SelfHosted, fid: FuncId) void = {
    func := self.get_function(fid);
    if !func.get_flag(.TookPointerValue) { // TODO: only if already compiled?
        old := self.comptime_codegen.m.get_addr(self.comp().fmt_fn_name(fid));
        // Note: else is not `Suspend = self.wait_for(Jit = fid)` because of cycles, we defer to the backend to deal with finializing it. 
        //       a better system would be to generate a shim right now so we'd always have an address? 
        if old { ptr |
            // We already compiled the function but didn't know we'd need to remember its address. (ie. #test fn large_struct_ret_return).
            self.created_jit_fn_ptr_value(fid, ptr.int_from_rawptr())
        } else {
            id := self.comptime_codegen.m.intern(self.comp().fmt_fn_name(fid));  // looks silly to cache this but time it before removing!
            self.pending_took_pointer&.push(@as(Ty(FuncId, u32)) (fid, id));
        };
    };
    func.set_flag(.TookPointerValue);
}

// TODO: this is kinda weird. `fn` statements create an overload set or add to an existing one. 
fn is_empty_constant(self: *SelfHosted, name: Var) bool #once = {
    var := self.scopes.get_constant(name);
    var := or(var, => return(true));
    var._0.expr&.is(.Poison)
}

// Passing in the requested type here feels a bit weird, but I think it will make anon-functions less painful. 
fn find_const(self: *SelfHosted, name: Var, requested: ResultType) Maybe(Ty(Values, Type)) = {
    // If someone else is already trying to compile this, we don't want to fight over it. 
    if self.dispatch.const_var_in_progress&.get(name.id.zext()) {
        return(Suspend = self.wait_for(EvalConstant = (name = name, requested = requested)));
    };
    
    var := self.scopes.get_constant(name);
    if var& { var | 
        //@println("find_const %", name&.log(self));
        // If we've compiled this before, great.
        //     We can't coerce_constant here because we don't have a unique expression node to stick changes into if needed. 
        //     (need to change the expression to create function pointers because they might not be compiled yet)
        @if_let(var._0.expr&) fn Value(f) => {
            return(Ok = (f.bytes&.clone(self.get_alloc()), var._0.ty));
        };
    };
    
    self.dispatch.const_var_in_progress&.set(name.id.zext());
    
    // TODO: -- Jul 21
    // its sad to always yield here because _most_ of the time you could just do it now and it would be fine.
    // so an easy optimisation might be just trying now, if it wasn't already const_var_in_progress so we know nobody else is working on it. 
    // but since any _could_ yield, its a bug if we can't compile with _everything_ yielding,
    // so at least keep a flag to toggle this behaviour? 
    
    (Suspend = self.wait_for(EvalConstant = (name = name, requested = requested)))
}

fn find_const_non_blocking(self: *SelfHosted, name: Var) ?Ty(Values, Type) #once = {
    // If someone else is already trying to compile this, we don't want to fight over it. 
    if self.dispatch.const_var_in_progress&.get(name.id.zext()) {
        return(.None);
    };
    var := self.scopes.get_constant(name);
    var := or(var, => return(.None));
    @if_let(var._0.expr&) fn Value(f) => {
        return(Some = (f.bytes&.clone(self.get_alloc()), var._0.ty));
    };
    .None
}

fn is_rec_hack(value: *FatExpr) bool = {
    ::AutoEq(?Symbol); ::RefEq(?Symbol);
    value.expr&.is(.PrefixMacro) && value.expr.PrefixMacro.handler.ident() == (Some = Flag.rec.ident())
}

fn handle_declare_constant(self: *SelfHosted, name: Var, ty: *LazyType, value: *FatExpr) Maybe(void) #once = {
    @debug_assert(self.dispatch.enclosing_function.is_none(), "ICE: handle_declare_constant should have no enclosing function");
    
    is_rec := is_rec_hack(value);
    
    // infer the more useful name when you do `foo :: fn() = { ... };`
    @if_let(value.expr) fn Closure(func) => {
        if func.name == @run Flag.Anon.ident() {
            func.name = name.name;
        };
    };
    
    want := ResultType.None;
    if @check(self.infer_type(ty)) return { known |
        want = (Specific = known);
        if(is_rec, => @err_assert(known == Type, "@rec expected Type") return);
    };
    
    // TODO: HACK that doesn't respect scoping!
    //       eventually it should just notice when a type tries to reference itself and switch to this path without the explicit @rec. 
    if is_rec {
        invocation := value.expr.PrefixMacro&;
        if invocation.arg.is_raw_unit() {
            invocation.arg[] = invocation.target[];
        };
        real_value_box := invocation.arg;
        hole := self.intern_type(.Placeholder);
        @log_event("% = @rec %", name&.log(self), hole) self;
        self.set(value, Type, hole);
        // This can't just fallthrough and call update_placeholder at the end of this function like the old version did,
        // because we might suspend while compiling and then next time around we'll forget this value is special. 
        // (we'll just think its already done because we'll see the placeholder).
        return(Suspend = self.wait_for(FinishRecType = (hole = hole, name = name, value = real_value_box)));
    };
    
    @check(self.compile_expr(value, want)) return;
    
    if value.ty == UnknownType {
        return(@err("TODO: needed type hint for %", self.pool.get(name.name)));
    };
    if !value.expr&.is(.Value) {
        val := @check(self.immediate_eval_expr(value, value.ty)) return;
        value.set(val, value.ty);
    };
    
    @check(self.coerce_const_expr(value, want, false)) return;
    //@if_let(want) fn Specific(ty) => {
    //    @check(self.can_assign(ty, value.ty)) return;
    //};
    
    self.finish_layout_deep(value.ty);
    
    if value.ty == Type {
        @debug_assert(value.expr&.is(.Value));
        type_value := Type.assume_cast(value.expr.Value.bytes&)[];
        // You're still gonna get a lot of T/Self/RAW which is unhelpful. 
        self.save_guessed_name(type_value, name.name);
    };
    
    .Ok
}

fn eval(self: *SelfHosted, expr: *FatExpr, $T: Type) Maybe(T) #generic = {
    t := self.get_or_create_type(T);
    value := @check(self.immediate_eval_expr(expr, t)) return;
    if value&.len() != size_of(T) {
        return(@err("size mismatch in constant evaluation %\nTODO: the compiler should have caught this before and failed a typecheck.", log(value&, self, t)))
    };
    (Ok = T.assume_cast(value&)[])
}

fn set(self: *SelfHosted, expr: *FatExpr, $T: Type, value: T) void #generic = {
    value := self.to_values(T, value);
    expr.set(value, self.get_or_create_type(T));
}

fn handle_finish_rec_type(self: *SelfHosted, hole: Type, name: Var, value: *FatExpr) Maybe(void) = {
    result_type := @check(self.eval(value, Type)) return;
    assert(result_type != hole, "ICE: @rec resolved to itself");
    self.update_placeholder(hole, result_type, name.name);
    @log_event("update_placeholder @rec % -> %", hole, result_type) self;
    .Ok
}

fn save_const_values(self: *SelfHosted, name: Var, value: Values, final_ty: Type, loc: Span) Maybe(void) = {
    self.save_const(name, (Value = (bytes = value, coerced = false)), final_ty, loc)
}

fn save_const(self: *SelfHosted, name: Var, val_expr: Expr, final_ty: Type, loc: Span) Maybe(void) #once = {
    @debug_assert(name.kind == .Const, "tried to save non-constant");
    // TODO: do i have to check if someone is already working on this constant? -- Jul 30
    val := self.scopes.get_constant(name);
    val := or val {
        // dup_var of return label in emit_capturing_call gets here.
        // eventually should remove the ? so its smaller and just do this there instead. 
        ptr := self.scopes.constants&.nested_index(name.id.zext());
        ptr[] = ((expr = (Poison = .Unknown), loc = loc, ty = UnknownType, done = false), .Infer);
        ptr
    };
    
    // TODO: this used to work before new overloading rework. -- Aug 27
    //if val._1&.is(.Finished) {
    //    return(@err("tried to re-save constant %", name&.log(self)));
    //};
    //if !val._0.expr&.is(.Poison) {
    //    return(@err("tried to stomp constant %", name&.log(self)));
    //};
    
    val._0.expr = val_expr;
    val._0.ty = final_ty;
    val._1 = (Finished = final_ty);
    self.scopes.put_var_type(name, final_ty);
    .Ok
}

fn ask_coerce_const_expr(self: *SelfHosted, expr: *FatExpr, want: Type, query: bool) Maybe(bool) = {
    found := expr.ty;
    if found == want {
        return(Ok = true);
    };
    // can't do this because it mightn ot be a value
    //@assert(!expr.expr.Value.coerced, "const mismatch. % vs % but already coerced", self.log_type(found), self.log_type(want));
    // TODO: set coereced? 
    
    want_info := self.get_type(want);
    found_info := self.get_type(found);
    if found == OverloadSet {
        @match(want_info) {
            fn Fn(f_ty) => {
                expr.done = false;  // do this early in case we suspend, we don't want to give up on doing the coersion
                os := @check(self.eval(expr, OverloadSet)) return;
                fid := @check(self.resolve_by_type(os, f_ty[], expr.loc)) return;
                if(query, => return(Ok = true));
                self.set(expr, FuncId, fid); // :HERE
                return(Ok = true);
            }
            fn FnPtr(f) => {
                expr.done = false;  // do this early in case we suspend, we don't want to give up on doing the coersion
                os := @check(self.eval(expr, OverloadSet)) return;
                fid := @check(self.resolve_by_type(os, f.ty, expr.loc)) return;
                if(query, => return(Ok = true));
                self.set(expr, FuncId, fid);
                expr.done = false;
                self.took_pointer_value(fid);
                e := self.box(expr[]);  // :fucked segfault if you inline this!!!!!! -- Aug 6
                expr.expr = (FnPtr = e);
                expr.ty = want;
                expr.done = false;
                @check(self.compile_fn_ptr(expr)) return;
                return(Ok = true);
            }
            @default => ();
        };
    };
    if want == FuncId {
        @if_let(found_info) fn Fn() => {
            if(query, => return(Ok = true));
            expr.ty = FuncId;
            return(Ok = true);
        };
    };
    
    if found == FuncId {
        @match(want_info) {
            fn Fn(f_ty) => {
                expr.done = false;  // do this early in case we suspend, we don't want to give up on doing the coersion
                fid := @check(self.eval(expr, FuncId)) return;
                @check(self.check_promote_id_to_func(fid, f_ty[])) return;
                if(query, => return(Ok = true));
                
                expr.ty = want;
                // TODO: typecheck args
                return(Ok = true);
            }
            fn FnPtr(f_ty) => {
                expr.done = false;
                inner := self.box(expr[]);
                expr.expr = (FnPtr = inner);
                expr.done = false;
                expr.ty = UnknownType;
                // TODO: typecheck
                @check(self.compile_expr(expr, (Specific = want))) return;
                return(Ok = true);
            }
            fn VoidPtr() => {
                if(query, => return(Ok = true));
                fid := @check(self.eval(expr, FuncId)) return;
                inner := self.box(expr[]);
                expr.expr = (FnPtr = inner);
                expr.done = inner.done;
                expr.ty = rawptr;
                self.took_pointer_value(fid);
                // TODO: could call compile_expr/compile_fn_expr instead of imm+took
                return(Ok = true);
            }
            @default => ();
        };
    };

    @match(found_info) {
        fn FnPtr(f_ty) => {
            if want == rawptr {
                if(query, => return(Ok = true));
                expr.ty = rawptr;
                return(Ok = true);
            };
        }
        fn void() => {
            while => want_info.is(.Named) {
                want_info = self.get_type(want_info.Named._0);
            };
            @if_let(want_info) fn Struct(f) => {
                each f.fields { f | 
                    if f.get_default().is_none() {
                        // TODO: better error message
                        //return(@err("had non-default fields (or if it's a call: expected function args) (or if its a block expected non-void return)"));
                        return(Ok = false);
                    };
                };
                if(query, => return(Ok = true));
                expr[] = empty_struct_literal(expr.loc);
                expr.ty = want;
                @check(self.construct_struct_literal(expr, (Specific = want))) return;
                return(Ok = true);
            };
        }
        fn Int(have_int) => {
            @match(want_info) {
                fn Int(want_int) => {
                    min, max := want_int[].range();
                    @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
                    value := expr.expr.Value.bytes&;
                    v := @try(value.int_value(have_int[])) return;
                    //@println(" % % %", min, max, v);
                    if v <= max && v >= min {
                        if(query, => return(Ok = true));
                        value.adjust_int_length(want_int);
                        expr.ty = want;
                        return(Ok = true);
                    };
                }
                fn F64() => {
                    max := 1.shift_left(MANTISSA_DIGITS_f64) - 1;
                    min := -max; // TODO: is that true? 
                    @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
                    value := expr.expr.Value.bytes&;
                    i := @try(value.int_value(have_int[])) return;
                    if i <= max && i >= min {
                        if(query, => return(Ok = true));
                        self.set(expr, f64, i.float());
                        return(Ok = true);
                    };
                }
                fn F32() => { // TODO: copy paste
                    max := 1.shift_left(MANTISSA_DIGITS_f32) - 1;
                    min := -max; // TODO: is that true? 
                    @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
                    value := expr.expr.Value.bytes&;
                    i := @try(value.int_value(have_int[])) return;
                    if i <= max && i >= min {
                        if(query, => return(Ok = true));
                        f: f32 = i.float().cast();
                        self.set(expr, f32, f);
                        return(Ok = true);
                    };
                }
                @default => ();
            };
        }
        fn F64() => {
            @err_assert(expr.expr&.is(.Value), "TODO: imm_eval const int cast") return;
            value := expr.expr.Value.bytes&;
            vf := f64.assume_cast(value)[];
            @match(want_info) {
                fn Int(want_int) => {
                    v := vf.int();
                    if v.float() == vf {
                        min, max := want_int[].range();
                        if v <= max && v >= min {
                            if(query, => return(Ok = true));
                            value[] = self.to_values(i64, v);
                            value.adjust_int_length(want_int);
                            expr.ty = want;
                            return(Ok = true);
                        };
                    };
                }
                fn F32() => {
                    v: f32 = vf.cast();
                    // if v.cast() == vf {  // TODO: maybe you want this but it seems too strict to be usable
                    // TODO: if vf < MAX_F32 && vf > MIN_F32 {
                    if(query, => return(Ok = true));
                    self.set(expr, f32, v);
                    return(Ok = true);
                }
                @default => ();
            };
        }
        @default => ();
    };
    (Ok = false)
}

// TODO: you can never do this to a constant directly in case its aliased once i have const ptrs. 
fn coerce_const_expr(self: *SelfHosted, expr: *FatExpr, req: ResultType, query: bool) Maybe(void) = {
    if req.specific() { want | 
        ok := @check(self.ask_coerce_const_expr(expr, want, query)) return;
        if !ok {
            if self.ask_can_assign(want, expr.ty) {
                return(.Ok);
            };
            return(Err = self.error(CoerceConst = (span = Span.zeroed(), wanted = want, found = expr.ty, expr = expr)));
        };
    };
    .Ok
}

fn can_assign(self: *SelfHosted, want: Type, found: Type) Maybe(void) = {
    ::if(Maybe(void));
    if self.ask_can_assign(want, found) {
        .Ok
    } else {
        // TODO: dont allocate + fmt here beacause overload resolution spams this. 
        (Err = self.error(TypeMismatch = (span = Span.zeroed(), wanted = want, found = found)))
    }
}

fn ask_can_assign(self: *SelfHosted, want: Type, found: Type) bool #inline = 
    want == found || found == Never || ask_can_assign_inner(self, want, found);

fn ask_can_assign_inner(self: *SelfHosted, want: Type, found: Type) bool = {
    // do need this tho sadly. TODO: fix me!
    if(found == UnknownType, => return(true)); // TODO: remove
    
    want_info  := self.get_type(want);
    found_info := self.get_type(found);
    @match(found_info) {
        fn Named(f) => {
            // TODO: hack because normally Named means @rec which should be transparent but the special id types also use it. 
            //       they should be changed to unique types
            // found != OverloadSet && found != FuncId && found != Type && found != Symbol && found != LabelId && found != ScopeId
            if found.as_index() > 16 {
                return(self.ask_can_assign(want, f._0));
            };
        }
        fn Fn(f) => {
            if want == FuncId {
                //@println("can_assign func -> funcid %", self.log_type(want));
                return(true);
            };
        }
        fn Label(f) => {
            if want == LabelId {
                // TODO: typecheck!!
                return(true);
            };
        }
        fn Struct(found_struct) => {
            @if_let(want_info) fn Struct(want_struct) => {
                if found_struct.is_tuple && want_struct.is_tuple && found_struct.fields.len == want_struct.fields.len {
                    range(0, want_struct.fields.len) { i | 
                        w := want_struct.fields[i].ty;
                        f := found_struct.fields[i].ty;
                        if !self.ask_can_assign(w, f) {
                            return(false);
                        };
                    };
                    return(true);
                };
            };
        }
        fn Int(a) => {
            @if_let(want_info) fn Int(b) => {
                if a.bit_count == 64 && b.bit_count != 32 && b.bit_count != 16 && b.bit_count != 8 && b.bit_count != 64 {
                    return(true);
                };
                if b.bit_count == 64 && a.bit_count != 32 && a.bit_count != 16 && a.bit_count != 8 && a.bit_count != 64 {
                    return(true);
                };
            };
        };
        @default => ();
    };
    @match(want_info) {
        fn Named(f) => {
            // TODO: hack because normally Named means @rec which should be transparent but the special id types also use it. 
            //       they should be changed to unique types
            // want != OverloadSet && want != FuncId && want != Type && want != Symbol && want != LabelId && want != ScopeId 
            if want.as_index() > 16 {
                return(self.ask_can_assign(f._0, found));
            };
        }
        fn Fn(f) => {
            if found == FuncId {
                // TODO: typecheck!!
                //@println("can_assign func <- funcid %", self.log_type(want));
                return(true);
            };
        }
        fn Label(f) => {
            if found == LabelId {
                // TODO: typecheck!!
                return(true);
            };
        }
        @default => ();
    };
    
    false
}

fn resolve_by_type(self: *SelfHosted, os: OverloadSet, f_ty: FnType, loc: Span) Maybe(FuncId) = {
    // This is mostly used for macros so its a big win to not spin up a whole operation just to notice that @match still goes to the same place. 
    // saves ~10% of the time Aug 30.
    data := self.dispatch.overloads&.nested_index(os.as_index());
    key: OverloadKey = (arg = f_ty.arg, req = f_ty.ret);
    if data.inline_cache._0& == key& {
        return(Ok = data.inline_cache._1);
    };
    zone := zone_begin(.SemaOverloads); // TODO: defer
    @if(ENABLE_TRACY) {
        real_name := self.pool.get(data.name);
        ___tracy_emit_zone_name(zone, real_name);
    };
    res := self.resolve_in_overload_set(f_ty, os, loc);
    zone_end(zone);
    fid := @check(res) return;
    data.inline_cache = (key, fid);
    (Ok = fid)
}

::?FnType;
fn immediate_eval_expr(self: *SelfHosted, expr: *FatExpr, ret_ty: Type) Maybe(Values) = {
    self.last_loc = expr.loc;
    @log_event("imm_eval %", expr.log(self)) self;
    @debug_assert(!ret_ty.is_unknown(), "immediate_eval_expr unknown");
    old_func := self.dispatch.enclosing_function;
    self.dispatch.enclosing_function = .None;
    ::?Values;
    
    @match(expr.expr&) {
        fn Call(f) => {
            // goes from 30000 lit_fn to 7000. saved 55/645 ms.   -- Sep 2
            // this might fold so it should go before a check_quick_eval. 
            @check(self.compile_call(expr, (Specific = ret_ty))) return;
        }
        fn ContextualField(name) => {
            // saves 500
            @check(self.contextual_field(name[], expr, ret_ty)) return;
        }
        fn String(ident) => {
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        fn Closure() => {
            // saves 500
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        fn FromBitLiteral() => {
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        fn PrefixMacro() => {
            @check(self.compile_expr(expr, (Specific = ret_ty))) return;
        }
        @default => ();
    };
    
    // this makes size match for int literals that get down casted in const_eval_any
    // TODO: this should work on non-int types but it seems to get confused by Never.  -- Nov 12
    if expr.expr&.is(.Value) && expr.ty != ret_ty && self.get_type(ret_ty).is(.Int) {
        @check(self.coerce_const_expr(expr, (Specific = ret_ty), false)) return;
    };
    
    if self.check_quick_eval(expr, ret_ty) { val |
        self.dispatch.enclosing_function = old_func;
        return(Ok = val);
    };
    
    // Can't just try to compile_expr here!
    // Since were evaluating in const context, any functions that are called in the expression weren't added to anyone's callees. 
    // So we want to say we need to recompile the expression, adding to callees of the lit_fn we're about to make. 

    // If its already a trivial function call, there's nothing else we can do to simplify, 
    // so we have to just yield on the function. 
    @if_let(expr.expr&) fn Call(f) => {
        // TODO: `if self.check_quick_eval(f.arg, f_ty.arg) { arg_value |` 
        //       instead of requiring void so its consistant with the fn_ptr version.
        //       tho also the call_dynamic is kinda sketchy (can only handle simple cases) and 
        //       i could just get rid of it and always generate a function that passes the arguments.    -- Jul 31
        if expr.expr&.is(.Call) && f.arg.is_raw_unit() && f.f.expr&.is(.Value) && self.get_type(f.f.ty).is(.Fn).or(f.f.ty == FuncId) {
            fid := FuncId.assume_cast(f.f.expr.Value.bytes&)[];
            self.dispatch.enclosing_function = old_func;
            func := self.get_function(fid);
            if func.body&.is(.Normal) {
                if func.get_flag(.EnsuredCompiled) {
                    if self.check_quick_eval(func.body.Normal&, ret_ty) { val |
                        self.dispatch.enclosing_function = old_func;
                        return(Ok = val);
                    };
                    //@println("imm call %", self.get_function(fid).log(self));
                } else {
                    self.dispatch.function_in_progress&.set(fid.as_index());
                    self.dispatch.enclosing_function = old_func;
                    return(Suspend = self.wait_for(CompileBody = fid));
                };
            };
            return(Suspend = self.wait_for(Jit = fid));
        };
    };
    
    // Different from the version in check_quick_eval because this accepts unfinished ones too. 
    // This is just an optimisation to avoid an intermediate Func on the first use of a given constant. 
    @if_let(expr.expr&) fn GetVar(name) => {
        if name.kind == .Const {
            // Don't need to call find_const here because check_quick_eval already did the easy case. 
            self.dispatch.enclosing_function = old_func;
            return(Suspend = self.wait_for(EvalConstant = (name = name[], requested = (Specific = ret_ty))));
        };
    };
    @if_let(expr.expr&) fn FnPtr(f_expr) => {
        fid := @check(self.eval(f_expr[], FuncId)) return;
        fn_ptr := or self.get_fn_callable(fid) {
            opts := self.get_build_options();
            if opts.do_jit_shims {
                // TODO: really should be putting this in someone's callees list.
                //       but for jit we fix when you try to call and for aot we notice when you relocate it so its kinda fine... 
                //       --- Sep 21
                @check(self.infer_arguments(fid)) return;
                @check(self.infer_return(fid)) return;
                // TODO: fixth ecases wehre you blindly return .Ok
                @try(self.create_jit_shim(fid)) return;
                self.get_fn_callable(fid).expect("jit shim to be ready")
            } else {
                self.dispatch.enclosing_function = old_func;
                return(Suspend = self.wait_for(Jit = fid))
            }
        };
        
        values := self.to_values(rawptr, fn_ptr);
        @debug_assert(values.Small._0 != 0, "Expr:FnPtr cannot return null");
        expr.expr = (Value = (bytes = values, coerced = false));
        expr.done = true;
        return(Ok = values);
    };
    
    // TODO: should probably include coerce_const_expr in check_quick_eval so it nests better. 
    if expr.expr&.is(.Value) {
        ok := @check(self.ask_coerce_const_expr(expr, ret_ty, false)) return;
        if ok {
            if self.check_quick_eval(expr, ret_ty) { val |
                self.dispatch.enclosing_function = old_func;
                return(Ok = val);
            };
        };
    };
    
    // The expression is too complex to deal with here. 
    // So box it into a function, compile that normally, and then just call into it with no arguments when we come around again.
    // This operation is make_lit_function from the old compiler. 
    // We know we'll yield to compile the new function, and don't want its body to alias the old expr. 
    
    //@println("slow imm eval %", expr.log(self));
    bindings: List(Binding) = list(self.get_alloc());
    bindings&.if_empty_add_unit();
    arg: Pattern = (bindings = bindings.as_raw(), loc = expr.loc);
    def: FnDef = (name = .None, arg = arg, ret = (Finished = ret_ty), tags = list(temp()), loc = expr.loc);

    // we might have already tried to compile the expression in a different context,
    // but we need to do it again so we're sure to add callees to the newly created function,
    // so we don't try to call something that isn't compiled yet. 
    walk: MarkNotDone = ();
    walk&.walk_expr(expr);
    
    fake_func := make_func(def, (Some = expr[]), false);
    // We didn't bother setting a scope because we don't need one, the expression will already have been resolved. 
    fake_func&.set_flag(.ResolvedBody);
    fake_func&.set_flag(.ResolvedSign);
    fake_func&.set_flag(.SyntheticImmEval);
    self.dispatch.lit_fn_count += 1;
    fake_func&.set_flag(.ComptimeOnly);
    // Since we know this function has no args, we'll always fold it. 
    // This makes sure that if we start analyzing the expression without noticing that its actually in const-context,
    // we won't add a callee that becomes unnecessary when the function call gets reduced to an inlined value eventually. 
    // This fixes a bunch of .ComptimeOnly callees that the AOT backends choke on.  -- Aug 21
    // The SyntheticImmEval is used to prevent a loop where we try to fold by calling imm_eval on inner nested expressions? maybe?
    // TODO: i cant quite understand why, but you need something like that even if you don't add this extra TryConstFold. 
    // TODO: this introduces a ICE: Tried to call un-compiled function. for the test `floats`
    //       happens at `assert_eq(5.neg(), 5.7.neg().int());` and only if neg is #fold
    fake_func&.set_flag(.TryConstantFold);
    fake_func.finished_arg = (Some = void);
    fake_func.finished_ret = (Some = ret_ty);
    
    @if(TRACE_CALLS) {  
        s := expr.log(self);
        s.len = min(s.len, 45);
        each s { c |
            if c[] == "\n".ascii() {
                c[] = "\\".ascii();
            };
        };
        fake_func.name = self.pool.insert_owned(s.clone(self.get_alloc()).items());
    };
    
    fake_func.cc = (Some = .CCallReg);    
    fid := self.add_function(fake_func);
    @log_event("imm eval wait for lit %: %", fid, expr.log(self)) self;
    f_expr := self.box(self.to_expr(FuncId, fid, expr.loc));
    expr.expr = (Call = (f = f_expr, arg = self.make_unit_expr(expr.loc)));  // so we try this task again, we get an easy function call.
    expr.done = false;
    self.dispatch.enclosing_function = old_func;
    self.dispatch.function_in_progress&.set(fid.as_index());
    (Suspend = self.wait_for(CompileBody = fid))
}

// TODO: decide if need to set 
fn check_quick_eval(self: *SelfHosted, expr: *FatExpr, ret_ty: Type) ?Values = {
    @match(expr.expr&) {
        fn Value(f) => (Some = f.bytes&.clone(self.get_alloc()));
        fn GetVar(f) => {
            if f.kind == .Const {
                // This is a super common case because you type `arg: i64` a lot. 
                res := self.find_const_non_blocking(f[]);
                if res { f |
                    // TODO: this is wrong, you need to const coerce before doing it and not lose information if there's a type error. 
                    //       but without this you get an infinite loop on #link_rename
                    // TODO: make a test that fails if you just do this now!
                    expr.expr = (Value = (bytes = f._0&.clone(self.get_alloc()), coerced = false));
                    expr.ty = f._1;
                    // We can't report a type erorr from here so just let the slow path deal with it. 
                    if(ret_ty == f._1, => return(Some = f._0));
                };
            };
            .None
        }
        fn Block(it) => {
            if it.block_resolved && it.body.is_empty() {
                return(self.check_quick_eval(it.result, ret_ty));
            };
            .None
        }
        fn Tuple(parts) => {
            all: List(u8) = list(self.get_alloc());
            each parts { part |
                // TODO: tuple_types
                val := or self.check_quick_eval(part, part.ty) {
                    return(.None)
                };
                // Required since deconstruct_values (which we use for dyn_call) expects correctly aligned fields. 
                self.aligned_append_value(all&, val, part.ty);
            };
            (Some = (Big = all.as_raw()))
        }
        fn Call(it) => {
            if it.f.expr&.is(.Value) {
                @if_let(self.get_type(it.f.ty)) fn FnPtr(f) => {
                    if self.check_quick_eval(it.arg, f.ty.arg) { arg_value |
                        if it.arg.ty != f.ty.arg {| // TODO
                            return(.None);
                        };
                        f_ptr := i64.assume_cast(it.f.expr.Value.bytes&)[];
                        self.finish_layout_deep(it.f.ty);
                        zone := zone_begin(.CallDynamic);
                        res := self.call_dynamic_values(f_ptr, f.ty, arg_value&.bytes());
                        zone_end(zone);
                        expr.set(res&.clone(self.get_alloc()), f.ty.ret);
                        return(Some = res);
                    };
                };
                
                @if_let(self.get_type(it.f.ty)) fn Fn(f) => {
                    if self.check_quick_eval(it.arg, f.arg) { arg_value |
                        if it.arg.ty != f.arg {| // TODO
                            return(.None);
                        };
                        f_id := FuncId.assume_cast(it.f.expr.Value.bytes&)[];
                        if self.get_fn_callable(f_id) { f_ptr |
                            func := self.get_function(f_id);
                            self.finish_layout_deep(it.f.ty);
                            res := self.call_dynamic_values(f_ptr.int_from_rawptr(), f[], arg_value&.bytes());
                            expr.set(res&.clone(self.get_alloc()), f.ret);
                        };
                    };
                };
            };
            .None
        }
        fn Deref(it) => {
            if self.check_quick_eval(it[], self.ptr_ty(ret_ty)) { ptr |
                info := self.get_info(ret_ty);
                if info.is_sized {
                    @debug_assert(ptr&.is(.Small), "want inline ptr");
                    bytes: []u8 = (ptr = u8.ptr_from_int(ptr.Small._0), len = info.stride_bytes.zext());
                    return(Some = self.from_bytes(bytes));
                };
            };
            .None
        }
        fn PtrOffset(it) => {
            if self.check_quick_eval(it.ptr, it.ptr.ty) { ptr |
                @debug_assert(ptr&.is(.Small) && ptr.Small._1 == 8);
                ptr.Small._0 += it.bytes;
                expr.expr = (Value = (bytes = ptr, coerced = false));
                return(Some = ptr);
            };
            .None
        }
        @default => .None;
    }
}

fn create_slice_type(self: *SelfHosted, inner: Type, loc: Span) Maybe(Type) = {
    // TODO: this is called 3208 times when self-compiling. Aug 29. 
    @log_event("Create slice %", inner) self;
    f     := @unwrap(self.env.make_slice_t, "slice type not ready!") return;  
    arg   := self.to_expr(Type, inner, loc);
    value := @check(self.invoke(f, self.box(arg), Type, loc)) return;  // :get_or_create_type
    (Ok = Type.assume_cast(value&)[])
}

fn invoke(self: *SelfHosted, callee: FuncId, argument: *FatExpr, return_type: Type, loc: Span) Maybe(Values) = {
    f     := self.to_expr(FuncId, callee, loc);
    s_ty  := synthetic_ty((Call = (f = self.box(f), arg = argument)), loc, Type);
    s_ty  := self.box(s_ty);
    value := self.poll_in_place(Values, => self.immediate_eval_expr(s_ty, return_type));
    value := @try(value) return;  // TODO: miscompilation if you inline this ^ :fucked
    (Ok = value)
} 

///////////////////////
/// Macro Expansion ///

fn compile_prefix_macro(self: *SelfHosted, expr: *FatExpr) Maybe(void) #once = {
    // TODO: Bring back tag checks so i don't have to be paranoid!!
    //       this annoys be enough that im tempted to go back to inlining all of these. 
    //       tho maybe #once is reassuring enough. think about it. -- Jul 22
    @debug_assert(expr.expr&.is(.PrefixMacro)); 
    // TODO: you probably want this but constants don't have it (really they shouldn't be compiled without being wrapped in a function?)
    //@err_assert( self.dispatch.enclosing_function.is_some(), "ice: macro expansion must be in function context") return;
    invocation := expr.expr.PrefixMacro&;
    
    // This allows @a E; instead of @a(E);
    if invocation.arg.is_raw_unit() {
        temp := invocation.arg;
        invocation.arg = invocation.target;
        invocation.target = temp;
    };
    
    fat_expr_type := or self.env.fat_expr_type {
        return(self.early_builtin_prefix_macro(expr))
    };
    pair := self.tuple_of(@slice(fat_expr_type, fat_expr_type));
    
    @log_event("call user macro %", invocation.handler.log(self)) self;
    single := invocation.target.is_raw_unit();
    if single {
        @log_event("macro input: %", invocation.arg.log(self)) self;
    } else {
        @log_event("macro input: (%, %)", invocation.arg.log(self), invocation.target.log(self)) self;
    };
    
    @check(self.compile_expr(invocation.handler, .None)) return;
    if invocation.handler.ty == OverloadSet {
        os := @check(self.eval(invocation.handler, OverloadSet)) return; 
        ::if(FnType); 
        f_ty: FnType = if single {|  // TODO: wrong! what if they actually passed unit?
            (arg = fat_expr_type, ret = fat_expr_type, arity = 1)
        } else {
            (arg = pair, ret = fat_expr_type, arity = 2)
        };
        fid := @check(self.resolve_by_type(os, f_ty, invocation.handler.loc)) return;
        self.set(invocation.handler, FuncId, fid);
    };
    
    fid := @check(self.eval(invocation.handler, FuncId)) return; 
    func := self.get_function(fid);
    if !func.get_flag(.Macro) {
        self.codemap.show_error_line(func.loc);
    };
    @err_assert(func.get_flag(.Macro), "Tried to invoke non-macro % (missing #macro or missing overload)", self.pool.get(func.name)) return;
    fn_ptr := or self.get_fn_callable(fid) {| 
        return(Suspend = self.wait_for(Jit = fid))
    };
    
    self.last_loc = expr.loc;
    f_ty := func.finished_ty().expect("known type once compiled");
    assert(f_ty.ret == fat_expr_type, "tried to call macro with bad ret type. missing overload? \nresolve_in_overloadset assumes someone later will typecheck so giving a garabge match is fine");
    assert(f_ty.arg.eq(fat_expr_type).or(f_ty.arg == pair), "tried to call macro with bad arg type. missing overload? \nresolve_in_overloadset assumes someone later will typecheck so giving a garabge match is fine");
    expr.expr = (Poison = .InProgressMacro);  // they call me RefCell<FatExpr> because im always refing cells 
    callee := assume_types_fn(Arg = Ty(FatExpr, FatExpr), Ret = FatExpr, ptr = fn_ptr);
    self.bump_dirty_new();
    zone := zone_begin(.CallDynamic);
    expr[] = callee(invocation.arg[], invocation.target[]);  // :call_dynamic_values
    zone_end(zone);
    @log_event("macro output: %", expr.log(self)) self;
    // :InlineFoldHack
    // TODO: garbage garbage garbage HACK. 
    //       i want `not` to be #inline and also #fold, but something about @debug_assert gets confused. 
    //       tho also ive occasionally had to move conditions out of @debug_assert before so really ive just revealed a problem,
    //       but how can that be the only one. 
    //       if could do it for all macros that would be fine (but sad because slower), 
    //       but it doesn't work because some type checking stuff. 
    //       maybe it would be easier to fix that than this. 
    //       -- Dec 5
    if self.log_name(fid).starts_with("debug_assert") {
        walk: MarkNotDone = ();
        walk&.walk_expr(expr); // TODO: might not need this :SLOW
    };
    .Ok
}

// If we're early in bootstrapping and haven't compiled the FatExpr type yet, so some special handling.
fn early_builtin_prefix_macro(self: *SelfHosted, expr: *FatExpr) Maybe(void) = {
    invocation := expr.expr.PrefixMacro&;
    name := @match(invocation.handler.expr&) {
        fn GetVar(v) => v.name;
        fn UndeclaredVar(v) => v.name;
        fn GetNamed(v) => v[];
        @default => return(@err("macro calls must be GetVar while bootstrapping. tried to run something too compilicated too soon: %: %", invocation.handler.expr&.tag(), invocation.handler.log(self)));
    };
    @switch(name) {
        @case(Flag.builtin.ident()) => {
            expr[] = @try(self.builtin_macro(invocation.arg[])) return;
        };
        @case(Flag.struct.ident()) => {
            expr[] = @check(self.struct_macro(invocation.arg)) return;
        };
        @case(Flag.enum.ident()) => {
            expr[] = @check(self.enum_macro(invocation.arg, invocation.target)) return;
        };
        @case(Flag.tagged.ident()) => {
            @err_assert(invocation.target.is_raw_unit(), "@tagged expected single arg") return;
            expr[] = @check(self.tagged_macro(invocation.arg)) return;
        };
        @case(Flag.late.ident()) => {
            expr.set(unit_value, self.get_or_create_type(void));
        };
        @default => {
            return(@err("tried to call non-builtin macro '%' while bootstrapping.", self.pool.get(name)));
        };
    };
    
    .Ok
}

fn builtin_macro(self: *SelfHosted, arg: FatExpr) Res(FatExpr) = {
    name := @unwrap(arg&.ident(), "@builtin arg must be String literal") return;
    value, type := @try(self.builtin_macro(name)) return;
    arg&.set(value, type);
    (Ok = arg)
}

fn builtin_macro(self: *SelfHosted, builtin_name: Symbol) Result(Ty(Values, Type), *CompileError) = {
    builtin_type :: fn($T: Type) void => {
        ptr := self.get_or_create_type(T);
        val := ptr_cast_unchecked(Type, u32, ptr&)[];
        return(Ok = ((Small = (val.zext(), 4)), self.get_or_create_type(Type)));
    };
    @switch(builtin_name) {
        @case(Flag.i64.ident())         => builtin_type(i64);
        @case(Flag.bool.ident())        => builtin_type(bool);
        @case(Flag.OverloadSet.ident()) => builtin_type(OverloadSet);
        @case(Flag.ScopeId.ident())     => builtin_type(ScopeId);
        @case(Flag.FuncId.ident())      => builtin_type(FuncId);
        @case(Flag.LabelId.ident())     => builtin_type(LabelId);
        @case(Flag.Symbol.ident())      => builtin_type(Symbol);
        @case(Flag.rawptr.ident())      => builtin_type(rawptr);
        @case(Flag.Type.ident())        => builtin_type(Type);
        @case(Flag.void.ident())        => builtin_type(void);
        @case(Flag.Never.ident())       => builtin_type(Never);
        @case(Flag.true.ident())        => return(Ok = (((Small = (1, 1)), self.get_or_create_type(bool))));
        @case(Flag.false.ident())       => return(Ok = (((Small = (0, 1)), self.get_or_create_type(bool))));
        @case(Flag.f64.ident())         => builtin_type(f64);
        @case(Flag.f32.ident())         => builtin_type(f32);
        @case(Flag.UnknownType.ident()) => builtin_type(UnknownType);
        @case(Flag.compiler_debug_assert_eq_i64.ident()) => {
            my_assert_eq: rawptr : fn(a: i64, b: i64) i64 = {
                assert_eq(a, b);
                a
            };
            ty := self.tuple_of(@slice(i64, i64));
            ty := self.intern_type(FnPtr = (ty = (arg = ty, ret = i64, arity = 2), cc = .CCallReg));
            return(Ok = ((Small = (my_assert_eq.int_from_rawptr(), 8)), ty));
        };
        @default => {
            return(@err("unknown @builtin '%'.", self.pool.get(builtin_name)));
        };
    };
    unreachable()
}

fn struct_type(self: *SelfHosted, pattern: *Pattern, is_union: bool) Maybe(Type) #once = {
    fields: List(Field) = list(pattern.bindings.len, self.get_alloc());
    scope := NOSCOPE;
    each pattern.bindings { b | 
        if b.kind == .Const {
            if scope == NOSCOPE {
                // TODO: this scope means a totally different thing (nothing), its not where variables in the struct will be looked up
                scope = self.scopes.new_scope(scope_from_index(0), self.get_alloc(), pattern.loc);
            };
            name := b.ident().expect("field name");
            expr := @unwrap(b.get_default(), "constant field % must have a value.", self.pool.get(name)) return;
            v := self.unique_const(name);
            self.scopes.put_constant(v, expr[], b.ty);
            self.scopes[scope].const_lookup&.insert(name, v);
            expr.expr = (GetVar = v);  // Save our work because we might yield on a later field. 
        };
    };
    
    // TODO: you want the above to stay in a seperate loop and create the type before we start evaluating types for runtime fields, 
    //       so those expressions can reference local constants also declared in the struct. 
    
    each pattern.bindings { b | 
        if b.kind != .Const {
            fields&.push(@check(self.make_runtime_field(b)) return);
        };
    };
    
    @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
    (Ok = self.intern_type(Struct = (
        fields = fields.as_raw(),
        layout_done = false,
        is_tuple = false,
        is_union = is_union,
        scope = scope,
    )))
}

fn make_runtime_field(self: *SelfHosted, b: *Binding) Maybe(Field) #inline = {
    name := b.ident().expect("field name");
    ty := or @check(self.infer_type(b.ty&)) return {
        // We need to know the type before we can size the struct. 
        default := @unwrap(b.get_default(), "@struct field without type requires default value.") return;
        @err_assert(!is_rec_hack(default), "cannot use @rec on a non-constant value.") return;
        @check(self.compile_expr(default, .None)) return;
        b.ty = (Finished = default.ty);
        default.ty
    };
    
    ::if_opt(*FatExpr, Var);
    default := if b.get_default() { (expr: *FatExpr) |
        v := self.unique_const(name);
        self.scopes.put_constant(v, expr[], b.ty);
        expr.expr = (GetVar = v);  // Save our work because we might yield on a later field. 
        v
    } else {
        zeroed(Var)
    };
    (Ok = (
        name = name,
        ty = ty,
        nullable_tag = b.nullable_tag,
        default = default,
        byte_offset = 99999999999,
    ))
}

fn union_macro(self: *SelfHosted, fields: *FatExpr) Maybe(FatExpr) = {
    @err_assert(fields.expr&.is(.StructLiteralP), "expected map literal: (name: Type, ... ) for @union") return;
    ty := @check(self.struct_type(fields.expr.StructLiteralP&, true)) return;
    self.set(fields, Type, ty);
    (Ok = fields[])
}

fn struct_macro(self: *SelfHosted, expr: *FatExpr) Maybe(FatExpr) = {
    msg :: "expected map literal: (name: Type, ... ) for @struct";
    ty := @match(expr.expr&) {
        fn StructLiteralP(it) => {
            @check(self.struct_type(it, false)) return
        }
        fn Value(f) => {
            if expr.ty == self.get_or_create_type(void) {
                self.intern_type(Struct = (
                    fields = empty(),
                    layout_done = false,
                    is_tuple = false,
                    is_union = false,
                    scope = NOSCOPE,
                ))
            } else {
                return(@err(msg))
            }
        }
        fn Block(it) => {
            @err_assert(it.result.is_raw_unit(), "@struct {} block should not have a result expression") return; 
            @check(self.ensure_resolved(it)) return;    
            
            fields := Field.list(self.get_alloc());
            
            // TODO: don't iterate twice 
            if !it.hoisted_constants {
                @check(self.hoist_constants(it.body.items())) return;
            };
            
            each it.body { s | 
                // TODO: super confusing if you suspend here. same problem for the other fn struct_type
                @match(s.stmt&) {
                    fn Decl(b) => fields&.push(@check(self.make_runtime_field(b[])) return);
                    fn Noop()      => ();
                    // TODO: allow ConstEval
                    @default => @err_assert(false, "invalid stmt type in @struct {} block: %", s.stmt&.tag()) return; 
                };
            };
            scope := it.scope;
            self.intern_type(Struct = (
                fields = fields.as_raw(),
                layout_done = false,
                is_tuple = false,
                is_union = false,
                scope = scope, 
            ))
        }
        @default => return(@err(msg));
    };
    
    self.set(expr, Type, ty);
    (Ok = expr[])
}

fn enum_macro(self: *SelfHosted, arg: *FatExpr, target: *FatExpr) Maybe(FatExpr) = {
    type := @check(self.eval(arg, Type)) return;

    F :: Ty(Symbol, Values);
    fields: List(F) = list(self.get_alloc());
    as_int: ?IntTypeInfo = @match(self.get_type(type)) {
        fn Int(it) => (Some = it[]);
        @default => .None;
    };
    sequential := as_int.is_some();
    last := -1;
    @match(target.expr&) {
        fn StructLiteralP(pattern) => {
            each pattern.bindings { b | 
                name := @unwrap(b.ident(), "@enum case requires name") return;
                @err_assert(b.get_default().is_some(), "@enum expected case value") return;
                @err_assert(b.ty&.is(.Infer), "@enum case cannot have type annotation (use '=' instead of ':'") return;

                expr := b.get_default().expect("enum value");
                val  := @check(self.immediate_eval_expr(expr, type)) return;
                if sequential {
                    current := @try(val&.int_value(as_int.expect("int"))) return;
                    if current == last + 1 {
                        last += 1;
                    } else {
                        sequential = false;
                    };
                };
                fields&.push(@as(F) (name, val));
            };
            @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
        }
        fn Tuple(names) => {
            @err_assert(sequential, "@enum on tuple of names must be of int type") return;
            as_int := as_int.unwrap();
            repr := if(as_int.bit_count == 8 || as_int.bit_count == 16 || as_int.bit_count == 32, => as_int.bit_count / 8, => 8);
            //@err_assert(type == i64, "TODO: @enum(other-int-types)(Tuple)") return;
            enumerate names { i, name |
                name := @unwrap(name.ident(), "@enum expected ident") return;
                value := self.to_values(i64, i);
                value.Small._1 = repr.trunc();
                fields&.push(@as(F) (name, value));
            };
        }
        @default => return(@err("@enum expected struct literal or tuple of names"));
    };
    unique_ty := self.intern_type(Enum = (raw = type, fields = fields.as_raw(), sequential = sequential));
    self.set(arg, Type, unique_ty);
    (Ok = arg[])
}

fn tagged_macro(self: *SelfHosted, cases_expr: *FatExpr) Maybe(FatExpr) = {
    @err_assert(cases_expr.expr&.is(.StructLiteralP), "@tagged expected map literal like `(name: Type, ...)`") return;
    pattern := cases_expr.expr.StructLiteralP&;
    
    F :: Ty(Symbol, Values);
    C :: Ty(Symbol, Type);
    tag_fields: List(F) = list(self.get_alloc());
    cases: List(C) = list(self.get_alloc());
    enumerate pattern.bindings { i, b |
        // TODO: allow as default so you can use .Name like you can with void?
        //       then need to store default in TypeInfo::Tagged as well. -- Jul 5
        @err_assert(b.get_default().is_none(), "use ':' not '=' with @tagged") return;
        type := @check(self.infer_type(b.ty&)) return;
        type := or type {
            // @tagged(s: i64, n) is valid and infers n as void.
            b.ty = (Finished = self.get_or_create_type(void));
            b.ty.Finished
        };
        name := @unwrap(b.ident(), "@tagged field requires name") return;
        tag_value := self.to_values(i64, i);
        tag_fields&.push(@as(F) (name, tag_value)); // :tag_enums_are_sequential
        cases&.push(@as(C) (name, type))
    };
    
    @try(self.require_unique_fields(pattern)) return;  // doing this at the end, after all suspends
    tag_type := self.intern_type(Enum = (raw = self.get_or_create_type(i64), fields = tag_fields.as_raw(), sequential = true));
    tagged_type := self.intern_type(Tagged = (cases = cases.as_raw(), tag = tag_type));
    self.set(cases_expr, Type, tagged_type);
    (Ok = cases_expr[])
}

fn compile_body(self: *SelfHosted, fid: FuncId) Maybe(void) #inline = {
    if self.get_function(fid).get_flag(.EnsuredCompiled) {
        .Ok
    } else {
        @assert(!self.dispatch.function_in_progress&.get(fid.as_index()), "unhanlded mutual recursion %", self.log_name(fid));
        self.dispatch.function_in_progress&.set(fid.as_index());
        (Suspend = self.wait_for(CompileBody = fid))
    }
}

// TODO: track if we're in unquote mode or placeholder mode.
Unquote :: @struct(compiler: *SelfHosted, placeholders: List(FatExpr));

// :UnquotePlaceholders
:: WalkAst(Unquote, *CompileError);

fn handle_expr(self: *Unquote, expr: *FatExpr) Result(DoMore, *CompileError) #once = {
    @match(expr.expr) {
        fn Unquote(arg) => {
            //expr_ty := self.compiler.env.fat_expr_type.expect("used unquote ast while bootstrapping");
            // Note: take <arg> but replace the whole <expr>
            idx := self.placeholders.len;
            //@log_event("Placeholder(%) <- %", idx, arg.log(self.compiler.pool));
            self.placeholders&.push(arg[]);
            expr[] = (expr = (Placeholder = idx), loc = expr.loc, ty = UnknownType, done = false);
        }
        fn Placeholder(idx) => {
            @err_assert(idx < self.placeholders.len, "ICE: invalid unquote placeholder index %", idx) return;
            value := self.placeholders.index(idx); // TODO: make it more obvious that its only one use and the slot is empty.
            //@log_event("Placeholder(%) -> %", idx, value.log(self.compiler.pool));
            @err_assert(!value.expr&.is(.Poison), "ICE: missing placeholder for unquote") return;
            // This clone fixes a renumbering problem. :double_use_quote
            expr[] = value.deep_clone(self.compiler.get_alloc());
            value.expr = (Poison = .PlaceholderUsed);
            value.done = false;
        }
        fn Quote() => {
            // TODO: add a simpler test case than the derive thing (which is what discovered this problem).
            // Don't go into nested !quote. This allows having macros expand to other macro calls without stomping eachother.
            // TODO: feels like you might still end up with two going on at once so need to have a monotonic id number for each expansion stored in the !placeholder.
            //       but so far this is good enough.
            return(Ok = .Break);
        }
        @default => ();
    };
    (Ok = .Continue)
}
fn handle_stmt(self: *Unquote, stmt: *FatStmt) Result(DoMore, *CompileError) #once = (Ok = .Continue);
fn handle_func(self: *Unquote, func: *Func)    Result(DoMore, *CompileError) #once = (Ok = .Continue);
fn handle_type(self: *Unquote, ty: *LazyType)  Result(DoMore, *CompileError) #once = (Ok = .Continue);
fn handle_pattern(self: *Unquote, p: *Pattern) Result(DoMore, *CompileError) #once = (Ok = .Continue);

fn ensure_resolved(self: *SelfHosted, block: *get_variant_type(Expr, .Block)) Maybe(void) = {
    if(block.block_resolved, => return(.Ok));
    
    r: ResolveScope = new(self, block.scope, self.last_loc);
    zone := zone_begin(.Scope); // TODO: defer
    res := r&.finish_resolving_block(block);
    zone_end(zone);
    @try(res) return;
    .Ok
}
