// This includes the indirect ret pointer!!
fn arg_slots(self: *PrimSig) i64 = {
    if(self.first_arg_is_indirect_return, => self.args.len + 1, => self.args.len)
}

fn get_info(c: CompCtx, type: Type) *TypeMeta = {
    {c.vtable.get_type_meta}(c.data, type)
}

fn get_type(c: CompCtx, type: Type) *TypeInfo = {
    {c.vtable.get_type_info}(c.data, type)
}

fn get_baked(c: CompCtx, id: BakedVarId) *Ty(rawptr, BakedVar) = {
    {c.vtable.get_baked}(c.data, id)
}

fn get_build_options(c: CompCtx) *BuildOptions = {
    {c.vtable.get_build_options}(c.data)
}

fn emit_relocatable_constant(c: CompCtx, ty: Type, value: []u8) CRes(BakedVarId) = {
    {c.vtable.emit_relocatable_constant}(c.data, ty, value)
}

fn log(c: CompCtx, e: *FatExpr) Str #inline = {
    {c.vtable.log_expr}(c.data, e)
}

fn log(c: CompCtx, e: *FatStmt) Str #inline = {
    {c.vtable.log_stmt}(c.data, e)
}

fn log(c: CompCtx, e: Type) Str #inline = {
    {c.vtable.log_type}(c.data, e)
}

fn log(c: CompCtx, e: *Func) Str #inline = {
    {c.vtable.log_func}(c.data, e)
}

fn follow_redirects(program: CompCtx, f_id: FuncId) FuncId = {
    dowhile() {
        @match(program.get_function(f_id)[].body&) {
            fn Redirect(target) => {
                f_id = target[];
                true
            }
            @default => false;
        }
    };
    f_id
}

// TODO: prepending all this shit to every program is getting a bit dumb. 

fn display(self: *FuncId, out: *List(u8)) void = {
    @fmt(out, "F%", self[].as_index());
}
fn display(self: FuncId, out: *List(u8)) void = {
    @fmt(out, "F%", self.as_index());
}

fn finished_ty(self: *Func) ?FnType = {
    if self.finished_arg { arg |
        if self.finished_ret { ret |
            return(Some = (arg = arg, ret = ret, arity = @as(u16) self.arg.bindings.len().max(1).trunc()));
        };
    };
    .None
}

fn arity(expr: *FatExpr) i64 = {
    @match(expr.expr&) {
        fn Tuple(parts) => parts.len;
        @default => 1;
    }
} 

fn unptr_ty(c: CompCtx, ty: Type) ?Type = {
    @match(c.get_type(ty)) {
        fn Ptr(inner) => (Some = inner[]);
        @default => .None; // TODO: do i need to raw_type? 
    }
}

fn flat_tuple_types(self: CompCtx, ty: Type) List(Type) = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            out: List(Type) = list(temp());
            each f.fields { f | 
                if !f.kind.eq(.Const) {| 
                    out&.push_all(self.flat_tuple_types(f.ty).items());
                };
            };
            out
        }
        fn Enum(f) => self.flat_tuple_types(f.raw);
        fn Named(f) => self.flat_tuple_types(f._0);
        // TODO: this is sketchy
        fn Tagged(f) => {
            slots: i64 = self.slot_count(ty).zext();
            varients: List(List(Type)) = list(f.cases.len, temp());
            for f.cases { f |
                var := self.flat_tuple_types(f._1);
                if var.len > 0 {
                    varients&.push(var);
                };
            };
            if varients.len() == 1 {
                varients[0]&.insert(0, i64);
                @debug_assert_eq(slots, varients[0].len());
                return(varients[0]);
            };
            // TODO: hack
            i64.repeated(slots, temp())
        }
        fn Array(f) => {
            out := self.flat_tuple_types(f.inner);
            single := out.len;
            // TODO: fix underflow. better ban 0 len array -- Jul 9
            range(0, f.len.zext() - 1) { i|
                // reslice everytime because we might reallocate. TODO: just reserve at the beginning.  -- Jul 6
                out&.push_all(out.items().slice(0, single));
            };
            out
        }
        fn void() => list(temp());
        @default => ty.repeated(1, temp());
    }
}

fn get_comptime_env(comp: CompCtx) *ComptimeEnvironment = 
    ComptimeEnvironment.ptr_from_raw(comp.get_build_options()[].env);

fn fmt_fn_name(program: CompCtx, f: FuncId) Str = {
    opts := program.get_build_options();
    func := program.get_function(f);
    //if func.get_flag(.NoMangle) { 
    //    return(program.get_string(func.name));
    //};
    if opts.retain_function_names {
        real_name := program.get_string(func.name);
        @tfmt("%__%", real_name, f.to_index())
    } else {
        @tfmt("F%", f.to_index())
    }
}

fn tuple_types(self: CompCtx, ty: Type) ?[]Type = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            out: List(Type) = list(f.fields.len, temp());
            // :const_args_are_not_const_in_tuple
            @debug_assert(f.is_tuple, "called tuple_types on a struct, which might be fine but i don't think happens. %", self.log(ty));
            @debug_assert(f.const_field_count == 0, "tuples don't have constant fields");
            each f.fields { f |
                @debug_assert(f.kind != .Const); // we don't know what the caller's expecting
                out&.push(f.ty);
            };
            (Some = out.items())
        }
        fn Named(f) => self.tuple_types(f._0);
        fn Array(f) => (Some = f.inner.repeated(f.len.zext(), temp()).items());
        @default => .None;
    }
}

max_homogeneous_tuple :: 50;

fn arg_types(self: CompCtx, ty: Type) []Type = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            if f.is_tuple {
                types := self.tuple_types(ty);
                return(types.unwrap());
            };
            (@list(ty) temp()).items()
        };
        // :hacky_type_compression
        // this is a :HACK to fix @slice(>50 elements)
        // TODO: but does this miscompile `fn foo(a: Array(Bar, 51)) void;`? 
        //       or do we makes sure it's wrapped in an extra tuple? if so then we don't need the second case maybe? -- Nov 18
        fn Array(it) => {
            if it.len.zext() > max_homogeneous_tuple {
                return(it.inner.repeated(it.len.zext(), temp()).items()); // TODO: aaa :SLOW
            } else {
                return((@list(ty) temp()).items());
            };
            unreachable()
        }
        @default => {
            xx := @list(ty) temp();
            xx.items()  
        };
    }
}

///////////////////////////
// TODO: should try to clean these up.

// @err_assert(cond, "msg %", 123) return;
fn err_assert(arg: FatExpr, ret: FatExpr) FatExpr #macro = {
    loc := arg.loc;
    arg := compile_ast(arg);
    arg_err :: "@err_assert expected '(cond, fmt_string, args)'";
    if arg.ty == bool {
        arg = compile_ast(@{ (@[arg], "Assertion Failed") });
    };
    @ct_assert(arg.expr&.is(.Tuple), loc, arg_err);
    parts := arg.expr.Tuple.items();
    @ct_assert(parts.len >= 2, loc, arg_err);
    @ct_assert(parts[1].ty == Str, loc, arg_err);
    
    if parts.len == 2 {
        @{ 
            if !@[parts[0]] {
                @[ret](@err(@[parts[1]]));
            };
        }
    } else {
        @{
            if !@[parts[0]] {
                out: List(u8) = list(temp());
                @[format_into(@{ out& }, parts.rest(1), arg.loc)];
                @[ret](@err(out.items()));
            };
        }
    }
}

fn unwrap(arg: FatExpr, ret: FatExpr) FatExpr #macro = {
    loc := arg.loc;
    arg := compile_ast(arg);
    if arg.ty == bool {
        arg = compile_ast(@{ (@[arg], "Assertion Failed") });
    };
    arg_err :: "@assert expected '(nullable, fmt_string, args)'";
    @ct_assert(arg.expr&.is(.Tuple), loc, arg_err);
    parts := arg.expr.Tuple.items();
    @ct_assert(parts.len >= 2, loc, arg_err);
    @ct_assert(parts[1].ty == Str, loc, arg_err);
    @{
        cond := @[parts[0]];
        if cond&.is_none() {
            out: List(u8) = list(temp());
            out&.push_all("Missing Value: ");
            @[format_into(@{ out& }, parts.rest(1), arg.loc)];
            @[ret](@err(out.items()));
        };
        cond.unwrap()
    }
}

fn err(arg: FatExpr) FatExpr #macro = {
    //@if(BOOTSTRAP_ONLY_MODE) {
    //    arg = @{"---"};
    //};
    if !arg.expr&.is(.Tuple) {
        return(@{ (Err = make_error(@[arg])) });
    };
    @{
        out: List(u8) = list(libc_allocator);
        @[ format_into(@{ out& }, arg.expr.Tuple.items(), arg.loc) ];
        (Err = make_error(out.items()))
    }
}

// TODO: dont call malloc here!
fn make_error(msg: Str) *CompileError = {
    mem := libc_allocator.alloc(CompileError, 1);
    mem.ptr[] = (Msg = (span = Span.zeroed(), msg = msg));
    mem.ptr
}

fn update_main_span(err: *CompileError, new_loc: Span) void = {
    @match(err) {
        fn Msg(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        fn TypeMismatch(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        fn CoerceConst(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        fn InvalidField(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        @default => (); // TODO
    };
}

fn choose_impl(self: *FuncImpl, prio: []FuncImpl.Tag()) ?*FuncImpl = {
    @if_let(self) fn Merged(parts) => {
        // :SLOW make prio $ and have a lookup table for value and just find max in one pass, but N is like 3 so meh.
        for prio { p | 
            each parts { check |
                @debug_assert(!check.is(.Merged), "nested FuncImpl.Merged");
                if check.tag() == p {
                    return(Some = check);
                };
            };
        };
        return(.None);
    };
    
    for prio { p |
        if self.tag() == p {
            return(Some = self);
        };
    };
    .None
} 

// :get_or_create_type
fn is_unit(t: Type) bool = t == void;
fn is_unknown(t: Type) bool = t == UnknownType;
fn is_never(t: Type) bool = t == Never;

fn raw_type(c: CompCtx, ty: Type) Type = {
    loop {
        @match(c.get_type(ty)) {
            fn Named(f) => {
                ty = f._0;
            }
            fn Enum(f) => {
                ty = f.raw;
            }
            @default => {
                return(ty);
            };
        };
    };
    ty
}

// Mote: this is weak! often you'd rather use immediate_eval_expr.
fn as_const(self: *FatExpr) ?Values = {
    @if_let(self.expr&)
        fn Value(f) => { return(Some = f.bytes); };  // TODO: this should work when the branch returns never (no extra { ..; }) -- Jul 8
    
    .None
}

// fn from_values
fn assume_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    assert_eq(b.len, T.size_of());  // TODO: check alignment
    ptr_cast_unchecked(u8, T, b.ptr)
}

::enum(CallConv);

fn find_impl(self: *FuncImpl, $p: FuncImpl.Tag()) ?*get_variant_type(FuncImpl, p) #generic = {
    if self.tag() == p {
        return(Some = get_variant_ptr(FuncImpl, self, p));
    };
    @if_let(self) fn Merged(parts) => {
        each parts { check | 
            @debug_assert(!check.is(.Merged), "nested FuncImpl.Merged");
            if check.tag() == p {
                return(Some =  get_variant_ptr(FuncImpl, check, p));
            };
        };
    };
    .None
} 

:: {
    H :: TrivialHasher;
    
    fn DerefHash($T: Type) void = {
        fn hash(hasher: *H, a: *T) void #inline = hasher.hash(a[]);
    }
    
    #redirect(Ty(*H, *u32), void) fn hash(h: *H, s: *LabelId) void;    AutoHash(TypeInfo, H); AutoEq(TypeInfo);
    DerefHash(*TypeInfo); DerefEq(*TypeInfo);
    enum(CallConv); DerefEq(CallConv);
    enum(VarType); DerefEq(VarType);
    
    fn hash(h: *H, s: *CallConv) void #redirect(Ty(*H, *i64), void);
    fn hash(h: *H, s: *VarType) void #redirect(Ty(*H, *i64), void);
    
    fn hash(h: *H, s: *Var) void = {
        h.hash(s.id&);
    }
    
    fn hash(h: *H, s: *Values) void = {
        b := s.bytes();
        h.hash(b&);
    }
    
    AutoHash(FnType, H); AutoEq(FnType);
    AutoHash(IntTypeInfo, H); AutoEq(IntTypeInfo);
    AutoHash(?Var, H); AutoEq(?Var);
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().FnPtr), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().FnPtr));
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().Array), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().Array));
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().Tagged), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().Tagged));
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().Enum), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().Enum));
    AutoHash(Ty(Symbol, Type), H); AutoEq(Ty(Symbol, Type));
    AutoHash(Ty(Type, Symbol), H); AutoEq(Ty(Type, Symbol));
    AutoHash(Ty(Symbol, Values), H); AutoEq(Ty(Symbol, Values));
    HashEach(RsVec(Field), H);
    HashEach(RsVec(Ty(Symbol, Type)), H);
    HashEach(RsVec(Ty(Symbol, Values)), H);
    //// :struct_layout_in_type_info important that some things aren't included by hash and eq!
    fn hash(hasher: *H, self: *Field) void = {
        hasher.hash(self.name&);
        hasher.hash(self.ty&);
        hasher.hash(self.default&);
        hasher.hash(self.kind&);
    }
    TypeInfoStruct :: get_variant_type(TypeInfo, TypeInfo.Tag().Struct);
    fn hash(hasher: *H, self: *TypeInfoStruct) void = {
        hasher.hash(self.fields&);
        hasher.hash(self.is_tuple&);
    }
};

fn bytes(self: *Values) Slice(u8) = {
    @match(self) {
        (fn Small(v) Slice(u8) => (ptr = ptr_cast_unchecked(i64, u8, v._0&), len = v._1.zext()));
        (fn Big(v) Slice(u8) => v.items());
    }
}

fn len(self: *Values) i64 = {
    @match(self) {
        (fn Small(v) i64 => { v._1.zext() });
        (fn Big(v) i64 => { v.len });
    }
}

fn var(self: *Binding) ?Var = {
    @match(self.name) {
        fn Var(v) => (Some = v);
        @default => .None;
    }
}

fn ne(a: Symbol, b: Symbol) bool #redirect(Ty(u32, u32), bool);

fn hash(i: *Var) i64 = {
    i := (@as(i64) i.id.zext());
    i&.hash()
}
fn eq(a: Var, b: Var) bool = a.id == b.id;
fn eq(a: *Var, b: *Var) bool = a[] == b[]; // TODO: this should be automatic, or == should always look for the ref version? or i should commit to rls like zig so it doesn't matter. 
fn hash(i: *Symbol) i64 = {
    i := (@as(i64) i[].id().zext());
    i&.hash()
}

fn contains_pointers(s: *TypeMeta) bool = s.contains_pointers;

fn jit_addr(v: *Values) i64 = {
    s := v.bytes();
    u8.int_from_ptr(s.ptr)
}

fn unwrap(self: Name) Symbol = {
    @match(self) {
        fn Var(v) => v.name;
        fn Ident(v) => v;
        @default => panic("Expected name!");
    }
}

// It makes me feel better to guess that they provided the fields in order before scanning. 
// The arrays are so short its not a measurable difference tho -- Sep 18
fn find_struct_field(f: *get_variant_type(TypeInfo, .Struct), name: Symbol, index_guess: i64) ?*Field = {
    field := index_guess;
    if field >= f.fields.len || f.fields[field].name != name { 
        miscompilation_if_you_inline_this := f.fields&.position(fn(f) => f.name == name); // :fucked // TODO: check if thats still true
        field = or miscompilation_if_you_inline_this {
            return(.None)
        };
    };
    (Some = f.fields.index(field))
}

fn log(v: *Var, pool: CompCtx) Str = 
    items(@format("%%%", pool.get_string(v.name), "%", v.id) temp());

fn unwrap_ty(self: *Binding) Type = {
    assert(self.ty&.is(.Finished), "type not ready!");
    self.ty.Finished
}

// :link_rename
fn check_link_rename(comp: CompCtx, base_import_name: Symbol, func: *Func, target: TargetEnv) Str = {
    if func.nullable_link_rename_func != 0 {
        out: List(u8) = list(temp());
        arg: LinkRenameArg = (
            target = target,
            out = out&,
            old_name = comp.get_string(base_import_name),
        );
        addr := rawptr_from_int(func.nullable_link_rename_func);
        addr := assume_types_fn(*LinkRenameArg, void, addr);
        addr(arg&);
        out.items()
    } else {
        comp.get_string(base_import_name)
    }
}

fn get_default(b: *Binding) ?*FatExpr = {
    e := b.default.expr&;
    ::enum(@type e.Poison);
    if e.is(.Poison) && e.Poison == .EmptyBindingValue {
        return(.None);
    };
    (Some = b.default&)
}

fn binding_missing_default()  FatExpr = (expr = (Poison = .EmptyBindingValue), ty = UnknownType, done = false, loc = Span.zeroed());

fn get_default(b: *Field) ?Var = {
    if(b.default.id == 0, => return(.None));
    (Some = b.default)
}

fn get_alloc(self: CompCtx) Alloc = {
    {self.vtable.get_alloc}(self.data)
}

ReadBytes :: @struct(bytes: [] u8, i: i64);

fn read_next(self: *ReadBytes, $T: Type) T #generic = {
    @debug_assert_eq(self.i.mod(T.size_of()), 0);
    @safety(.Bounds) self.i + T.size_of() - 1 < self.bytes.len();
    
    ptr := self.bytes.ptr.offset(self.i);
    self.i += T.size_of();
    ptr_cast_unchecked(u8, T, ptr)[]
}

fn take(self: *ReadBytes, size: i64) ?[]u8 = {
    if(self.i + size - 1 >= self.bytes.len(), => return(.None));
    ptr := self.bytes.ptr.offset(self.i);
    self.i += size;
    (Some = (ptr = ptr, len = size))
}

::enum(ExecStyle);

Flag :: @enum(i64) (
    SYMBOL_ZERO,
    toplevel,
    include_std,
    Anon,
    if,
    
    // TODO: HACK
    // TODO: use this
    first_dumb_type_name, Self, RAW, last_dumb_type_name,
    
    // These are function #tags that the compiler knows about. 
    log_bc, log_asm, log_ir, log_ast, 
    cold, inline, noinline, asm, c_call, unsafe_noop_cast, 
    import, intrinsic, libc, redirect, comptime_addr,
    once, fold, no_trace, ct, outputs, macro, 
    link_rename, generic, target_os, 
    llvm, aarch64, x86, qbe, c, x86_bytes, bc,
    ir,
    
    // These are variables that are automatically shadowed in new scopes. 
    return, local_return,
    
    // These are macros that need to be called before we've compiled enough to evaluate the type FatExpr.
    builtin, type, struct, tagged, enum, late,
    
    // This is a magic blessed macro with no implementation but recognised by name. 
    rec, 
    
    // These are operators that have syntax desugaring. 
    add, sub, mul, div, lt, gt, ge, le, eq, ne, neg, not, and, or,  
    operator_squares_prefix, operator_star_prefix, operator_question_prefix, operator_index,
    operator_plus_equal, operator_minus_equal, operator_star_equal, operator_slash_equal,
    __string_escapes,
    
    // These can be used with @builtin.
    OverloadSet, ScopeId, FuncId, LabelId, Symbol, i64, bool, true, false, void, Type, rawptr, Never, f64, f32, UnknownType,
    compiler_debug_assert_eq_i64,  // this is maybe helpful for debugging the compiler when nothing works at all. 
    
    // These must be in order because tuple field names use them and compute by offsetting Flag._0.raw().
     _0,  _1,  _2,  _3,  _4,  _5,  _6,  _7,  _8,  _9, 
    _10, _11, _12, _13, _14, _15, _16, _17, _18, _19, _20,
    
    // These are libc functions the compiler can provide for comptime code even when it can't dlopen a libc. 
    // These are chosen arbitrarily based on what tests i want to run in blink emulator. 
    mmap, abort, write, read, munmap, open, close, mprotect, __clear_cache, uname, clock_gettime, malloc, free, 
    
    ___this_function_is_not_real_im_just_testing_the_franca_compiler,
    no_context,
    yes_context,
    c_variadic,
    where,
    syscall,
);

fn ident(name: Flag) Symbol = {
    symbol_from_id((@as(i64) name).trunc())
}

:: {
    fn eq(a: FuncId, b: FuncId) bool #redirect(Ty(u32, u32), bool);
    DerefEq(FuncId);
};

fn get_whole_line(c: CompCtx, loc: Span) FrancaCodeLine = {
    {c.vtable.get_whole_line}(c.data, loc)
}

// TODO: this is a dumb hack to make it less painful that i can't access fields on a value (because you need a pointer to offset)
//       fix in the compiler and remove this! -- Jul 7
//       now the problem is you can't access fields on a pointer if its not a dereference of it 
//       (i changed get_info when i ported it, i didn't fix the old problem yet).  -- Jul 21
fn stride_bytes(s: *TypeMeta) u16 = s.stride_bytes;
fn align_bytes(s: *TypeMeta) u16 = s.align_bytes;
fn size_slots(s: *TypeMeta) u16 = s.size_slots;

fn slot_count(self: CompCtx, ty: Type) u16 = {
    self.get_info(ty)[].size_slots
}

fn emit_qbe_included_dyn(m: *QbeModule, comp: *CompCtx, fns: [] FuncId, entry: ProgramEntry) BucketArray(u8) = {  
    {comp.vtable.emit_qbe_included}(QbeModule.raw_from_ptr(m), comp, fns, entry)
}

fn init_default_module_dyn(m: *QbeModule, vtable: *ImportVTable, goal: QbeTargetEnv) void = {  
    @assert(ptr_cast_unchecked(@type vtable.init_default_qbe_module, i64, vtable.init_default_qbe_module&)[] != 0, "init_default_qbe_module is not enabled");
    {vtable.init_default_qbe_module}(QbeModule.raw_from_ptr(m), QbeTargetEnv.raw_from_ptr(goal&));
}
