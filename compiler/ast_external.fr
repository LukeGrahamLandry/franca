// This includes the indirect ret pointer!!
fn arg_slots(self: *PrimSig) i64 = {
    if(self.first_arg_is_indirect_return, => self.args.len + 1, => self.args.len)
}

fn get_info(c: CompCtx, type: Type) *TypeMeta = {
    {c.vtable.get_type_meta}(c.data, type)
}

fn get_type(c: CompCtx, type: Type) *TypeInfo = {
    {c.vtable.get_type_info}(c.data, type)
}

fn get_build_options(c: CompCtx) *BuildOptions = {
    {c.vtable.get_build_options}(c.data)
}

fn emit_relocatable_constant(c: CompCtx, ty: Type, value: []u8, loc: Span) CRes(BakedVarId) = {
    {c.vtable.emit_relocatable_constant}(c.data, ty, value, loc)
}

fn log(c: CompCtx, e: *FatExpr) Str #inline = {
    {c.vtable.log_expr}(c.data, e)
}

fn log(c: CompCtx, e: *FatStmt) Str #inline = {
    {c.vtable.log_stmt}(c.data, e)
}

fn log(c: CompCtx, e: Type) Str #inline = {
    {c.vtable.log_type}(c.data, e)
}

fn log(c: CompCtx, e: *Func) Str #inline = {
    {c.vtable.log_func}(c.data, e)
}

fn follow_redirects(program: CompCtx, f_id: FuncId) FuncId = {
    dowhile() {
        @match(program.get_function(f_id)[].body&) {
            fn Redirect(target) => {
                f_id = target[];
                true
            }
            @default => false;
        }
    };
    f_id
}

// TODO: prepending all this shit to every program is getting a bit dumb. 

fn display(self: *FuncId, out: *List(u8)) void = {
    @fmt(out, "F%", self[].as_index());
}
fn display(self: FuncId, out: *List(u8)) void = {
    @fmt(out, "F%", self.as_index());
}

fn finished_ty(self: *Func) ?FnType = {
    if self.finished_arg { arg |
        if self.finished_ret { ret |
            return(Some = (arg = arg, ret = ret, arity = @as(u16) self.arg.bindings.len().max(1).trunc()));
        };
    };
    .None
}

fn arity(expr: *FatExpr) i64 = {
    @match(expr.expr&) {
        fn Tuple(parts) => parts.len;
        @default => 1;
    }
} 

fn unptr_ty(c: CompCtx, ty: Type) ?Type = {
    @match(c.get_type(ty)) {
        fn Ptr(inner) => (Some = inner[]);
        @default => .None; // TODO: do i need to raw_type? 
    }
}

fn get_comptime_env(comp: CompCtx) *ComptimeEnvironment = 
    ComptimeEnvironment.ptr_from_raw(comp.get_build_options()[].env);

fn fmt_fn_name(program: CompCtx, f: FuncId) Str = {
    opts := program.get_build_options();
    func := program.get_function(f);
    //if func.get_flag(.NoMangle) { 
    //    return(program.get_string(func.name));
    //};
    if opts.retain_function_names {
        real_name := program.get_string(func.name);
        @tfmt("%__%", real_name, f.to_index())
    } else {
        @tfmt("F%", f.to_index())
    }
}

fn tuple_types(self: CompCtx, ty: Type) ?[]Type = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            out: List(Type) = list(f.fields.len, temp());
            if !f.is_tuple {
                out&.push(ty);
            } else {
                // :const_args_are_not_const_in_tuple
                @debug_assert(f.scope == NOSCOPE, "tuples don't have constant fields");
                each f.fields { f |
                    out&.push(f.ty);
                };
            };
            (Some = out.items())
        }
        fn Named(f) => self.tuple_types(f._0);
        fn Array(f) => (Some = f.inner.repeated(f.len.zext(), temp()).items());
        @default => .None;
    }
}

max_homogeneous_tuple :: 50;

fn arg_types(self: CompCtx, ty: Type) []Type = {
    @match(self.get_type(ty)) {
        fn Struct(f) => {
            if f.is_tuple {
                types := self.tuple_types(ty);
                return(types.unwrap());
            };
            (@list(ty) temp()).items()
        };
        // :hacky_type_compression
        // this is a :HACK to fix @slice(>50 elements)
        // TODO: but does this miscompile `fn foo(a: Array(Bar, 51)) void;`? 
        //       or do we makes sure it's wrapped in an extra tuple? if so then we don't need the second case maybe? -- Nov 18
        fn Array(it) => {
            if it.len.zext() > max_homogeneous_tuple {
                return(it.inner.repeated(it.len.zext(), temp()).items()); // TODO: aaa :SLOW
            } else {
                return((@list(ty) temp()).items());
            };
            unreachable()
        }
        @default => {
            xx := @list(ty) temp();
            xx.items()  
        };
    }
}

fn tuple_of(self: CompCtx, types: []Type) Type = {
    @switch(types.len()) {
        @case(0) => void;
        @case(1) => types[0];
        @default => {
            // :hacky_type_compression
            // TODO: always. would need to change how you go arg_ty -> prim_sig.
            if types.len > max_homogeneous_tuple {
                first := types[0];
                if !types.contains(fn(t) => t[] != first) {
                    return(self.intern_type(Array = (
                        inner = first,
                        len = types.len.trunc(),
                    )));
                };
            };
            ::enum(Flag);
            ::if(Symbol);
            fields := Field.list(types.len, self.get_alloc());
            enumerate types { i, ty |
                fields&.push(
                    name = if i <= 20 {  // don't allocate for common numbers
                        @as(Symbol) @as(u32) trunc(@as(i64) Flag._0.raw() + i)
                    } else {
                        self.intern_string(@tfmt("_%", i))
                    },
                    ty = ty[],
                    nullable_tag = zeroed(*Annotations),
                    default = zeroed(Var),
                    byte_offset = FIELD_LAYOUT_NOT_DONE,
                );
            };
            self.intern_type(Struct = (
                fields = fields.as_raw(),
                layout_done = false,
                is_tuple = true,
                is_union = false,
                scope = NOSCOPE,
            ))
        };
    }
}

FIELD_LAYOUT_NOT_DONE :: 99999999999;

///////////////////////////
// TODO: should try to clean these up.

// @err_assert(cond, "msg %", 123) return;
fn err_assert(arg: FatExpr, ret: FatExpr) FatExpr #macro = {
    loc := arg.loc;
    arg := compile_ast(arg);
    arg_err :: "@err_assert expected '(cond, fmt_string, args)'";
    if arg.ty == bool {
        arg = compile_ast(@{ (@[arg], "Assertion Failed") });
    };
    @ct_assert(arg.expr&.is(.Tuple), loc, arg_err);
    parts := arg.expr.Tuple.items();
    @ct_assert(parts.len >= 2, loc, arg_err);
    @ct_assert(parts[1].ty == Str, loc, arg_err);
    
    if parts.len == 2 {
        @{ 
            if !@[parts[0]] {
                @[ret](@err(@[parts[1]]));
            };
        }
    } else {
        @{
            if !@[parts[0]] {
                out: List(u8) = list(temp());
                @[format_into(@{ out& }, parts.rest(1), arg.loc)];
                @[ret](@err(out.items()));
            };
        }
    }
}

fn unwrap(arg: FatExpr, ret: FatExpr) FatExpr #macro = {
    loc := arg.loc;
    arg := compile_ast(arg);
    if arg.ty == bool {
        arg = compile_ast(@{ (@[arg], "Assertion Failed") });
    };
    arg_err :: "@assert expected '(nullable, fmt_string, args)'";
    @ct_assert(arg.expr&.is(.Tuple), loc, arg_err);
    parts := arg.expr.Tuple.items();
    @ct_assert(parts.len >= 2, loc, arg_err);
    @ct_assert(parts[1].ty == Str, loc, arg_err);
    @{
        cond := @[parts[0]];
        if cond&.is_none() {
            out: List(u8) = list(temp());
            out&.push_all("Missing Value: ");
            @[format_into(@{ out& }, parts.rest(1), arg.loc)];
            @[ret](@err(out.items()));
        };
        cond.unwrap()
    }
}

fn err(arg: FatExpr) FatExpr #macro = {
    //@if(BOOTSTRAP_ONLY_MODE) {
    //    arg = @{"---"};
    //};
    if !arg.expr&.is(.Tuple) {
        return(@{ (Err = make_error(@[arg])) });
    };
    @{
        out: List(u8) = list(general_allocator());  // TODO: leak!
        @[ format_into(@{ out& }, arg.expr.Tuple.items(), arg.loc) ];
        (Err = make_error(out.items()))
    }
}

// TODO: dont call malloc here!
fn make_error(msg: Str) *CompileError = {
    mem := general_allocator().alloc(CompileError, 1);  // TODO: leak!
    mem.ptr[] = (Msg = (span = Span.zeroed(), msg = msg));
    mem.ptr
}

fn update_main_span(err: *CompileError, new_loc: Span) void = {
    @match(err) {
        fn Msg(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        fn TypeMismatch(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        fn CoerceConst(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        fn InvalidField(it) => {
            if it.span.low == 0 && it.span.high == 0 {
                it.span = new_loc;
            };
        }
        @default => (); // TODO
    };
}

fn choose_impl(self: *FuncImpl, prio: []FuncImpl.Tag()) ?*FuncImpl = {
    @if_let(self) fn Merged(parts) => {
        // :SLOW make prio $ and have a lookup table for value and just find max in one pass, but N is like 3 so meh.
        for prio { p | 
            each parts { check |
                @debug_assert(!check.is(.Merged), "nested FuncImpl.Merged");
                if check.tag() == p {
                    return(Some = check);
                };
            };
        };
        return(.None);
    };
    
    for prio { p |
        if self.tag() == p {
            return(Some = self);
        };
    };
    .None
} 

// :get_or_create_type
fn is_unit(t: Type) bool = t == void;
fn is_unknown(t: Type) bool = t == UnknownType;
fn is_never(t: Type) bool = t == Never;

fn raw_type(c: CompCtx, ty: Type) Type = {
    loop {
        @match(c.get_type(ty)) {
            fn Named(f) => {
                ty = f._0;
            }
            fn Enum(f) => {
                ty = f.raw;
            }
            @default => {
                return(ty);
            };
        };
    };
    ty
}

// Mote: this is weak! often you'd rather use immediate_eval_expr.
fn as_const(self: *FatExpr) ?Values = {
    @if_let(self.expr&)
        fn Value(f) => { return(Some = f.bytes); };  // TODO: this should work when the branch returns never (no extra { ..; }) -- Jul 8
    
    .None
}

// fn from_values
fn assume_cast($T: Type, self: *Values) *T #generic = {
    b := self.bytes();
    @assert_eq(b.len, T.size_of(), "assume_cast() FIXME , $body: @Fn(),");  // TODO: check alignment
    ptr_cast_unchecked(u8, T, b.ptr)
}

::enum(CallConv);

fn find_impl(self: *FuncImpl, $p: FuncImpl.Tag()) ?*get_variant_type(FuncImpl, p) #generic = {
    if self.tag() == p {
        return(Some = get_variant_ptr(FuncImpl, self, p));
    };
    @if_let(self) fn Merged(parts) => {
        each parts { check | 
            @debug_assert(!check.is(.Merged), "nested FuncImpl.Merged");
            if check.tag() == p {
                return(Some =  get_variant_ptr(FuncImpl, check, p));
            };
        };
    };
    .None
} 

:: {
    H :: TrivialHasher;
    
    fn DerefHash($T: Type) void = {
        fn hash(hasher: *H, a: *T) void #inline = hasher.hash(a[]);
    }
    
    #redirect(Ty(*H, *u32), void) fn hash(h: *H, s: *LabelId) void;    AutoHash(TypeInfo, H); AutoEq(TypeInfo);
    DerefHash(*TypeInfo); DerefEq(*TypeInfo);
    enum(CallConv); DerefEq(CallConv);
    enum(VarType); DerefEq(VarType); DerefEq(ScopeId);
    
    fn hash(h: *H, s: *CallConv) void #redirect(Ty(*H, *i64), void);
    fn hash(h: *H, s: *VarType) void #redirect(Ty(*H, *i64), void);
    
    fn hash(h: *H, s: *Var) void = {
        h.hash(s.id&);
    }
    fn hash(h: *H, s: *ScopeId) void #redirect(Ty(*H, *u32), void);
    fn hash(h: *H, s: *Values) void = {
        b := s.bytes();
        h.hash(b&);
    }
    
    AutoHash(FnType, H); AutoEq(FnType);
    AutoHash(IntTypeInfo, H); AutoEq(IntTypeInfo);
    AutoHash(?Var, H); AutoEq(?Var);
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().FnPtr), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().FnPtr));
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().Array), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().Array));
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().Tagged), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().Tagged));
    AutoHash(get_variant_type(TypeInfo, TypeInfo.Tag().Enum), H); AutoEq(get_variant_type(TypeInfo, TypeInfo.Tag().Enum));
    AutoHash(Ty(Symbol, Type), H); AutoEq(Ty(Symbol, Type));
    AutoHash(Ty(Type, Symbol), H); AutoEq(Ty(Type, Symbol));
    AutoHash(Ty(Symbol, Values), H); AutoEq(Ty(Symbol, Values));
    HashEach(RawList(Field), H);
    HashEach(RawList(Ty(Symbol, Type)), H);
    HashEach(RawList(Ty(Symbol, Values)), H);
    //// :struct_layout_in_type_info important that some things aren't included by hash and eq!
    fn hash(hasher: *H, self: *Field) void = {
        hasher.hash(self.name&);
        hasher.hash(self.ty&);
        hasher.hash(self.default&);
    }
    TypeInfoStruct :: get_variant_type(TypeInfo, TypeInfo.Tag().Struct);
    fn hash(hasher: *H, self: *TypeInfoStruct) void = {
        hasher.hash(self.fields&);
        hasher.hash(self.is_tuple&);
        s := self.scope.id();
        hasher.hash(s&);
    }
};

fn bytes(self: *Values) Slice(u8) = {
    @match(self) {
        (fn Small(v) Slice(u8) => (ptr = ptr_cast_unchecked(i64, u8, v._0&), len = v._1.zext()));
        (fn Big(v) Slice(u8) => v.items());
    }
}

fn len(self: *Values) i64 = {
    @match(self) {
        (fn Small(v) i64 => { v._1.zext() });
        (fn Big(v) i64 => { v.len });
    }
}

fn var(self: *Binding) ?Var = {
    @match(self.name) {
        fn Var(v) => (Some = v);
        @default => .None;
    }
}

fn ne(a: Symbol, b: Symbol) bool #redirect(Ty(u32, u32), bool);

fn hash(i: *Var) i64 = {
    i := (@as(i64) i.id.zext());
    i&.hash()
}
fn eq(a: Var, b: Var) bool = a.id == b.id;
fn eq(a: *Var, b: *Var) bool = a[] == b[]; // TODO: this should be automatic, or == should always look for the ref version? or i should commit to rls like zig so it doesn't matter. 
fn hash(i: *Symbol) i64 = {
    i := (@as(i64) i[].id().zext());
    i&.hash()
}

fn contains_pointers(s: *TypeMeta) bool = s.contains_pointers;

fn jit_addr(v: *Values) i64 = {
    s := v.bytes();
    u8.int_from_ptr(s.ptr)
}

fn unwrap(self: Name) Symbol = {
    @match(self) {
        fn Var(v) => v.name;
        fn Ident(v) => v;
        @default => panic("Expected name!");
    }
}

// It makes me feel better to guess that they provided the fields in order before scanning. 
// The arrays are so short its not a measurable difference tho -- Sep 18
fn find_struct_field(f: *get_variant_type(TypeInfo, .Struct), name: Symbol, index_guess: i64) ?*Field = {
    field := index_guess;
    if field >= f.fields.len || f.fields[field].name != name { 
        miscompilation_if_you_inline_this := f.fields&.position(fn(f) => f.name == name); // :fucked // TODO: check if thats still true
        field = or miscompilation_if_you_inline_this {
            return(.None)
        };
    };
    (Some = f.fields.index(field))
}

fn log(v: *Var, pool: CompCtx) Str = 
    @tfmt("%%%", pool.get_string(v.name), "%", v.id);

fn unwrap_ty(self: *Binding) Type = {
    assert(self.ty&.is(.Finished), "type not ready!");
    self.ty.Finished
}

// :link_rename
fn check_link_rename(comp: CompCtx, base_import_name: Symbol, func: *Func, target: TargetEnv) Str = {
    if func.nullable_link_rename_func != 0 {
        out: List(u8) = list(temp());
        arg: LinkRenameArg = (
            target = target,
            out = out&,
            old_name = comp.get_string(base_import_name),
        );
        addr := rawptr_from_int(func.nullable_link_rename_func);
        addr := assume_types_fn(*LinkRenameArg, void, addr);
        addr(arg&);
        out.items()
    } else {
        comp.get_string(base_import_name)
    }
}

fn get_default(b: *Binding) ?*FatExpr = {
    e := b.default.expr&;
    ::enum(@type e.Poison);
    if e.is(.Poison) && e.Poison == .EmptyBindingValue {
        return(.None);
    };
    (Some = b.default&)
}

fn binding_missing_default()  FatExpr = (expr = (Poison = .EmptyBindingValue), ty = UnknownType, done = false, loc = Span.zeroed());

fn get_default(b: *Field) ?Var = {
    if(b.default.id == 0, => return(.None));
    (Some = b.default)
}

fn get_alloc(self: CompCtx) Alloc = {
    {self.vtable.get_alloc}(self.data)
}

ReadBytes :: @struct(bytes: [] u8, i: i64);

fn read_next(self: *ReadBytes, $T: Type) T #generic = {
    @debug_assert_eq(self.i.mod(T.size_of()), 0);
    @safety(.Bounds) self.i + T.size_of() - 1 < self.bytes.len();
    
    ptr := self.bytes.ptr.offset(self.i);
    self.i += T.size_of();
    ptr_cast_unchecked(u8, T, ptr)[]
}

fn take(self: *ReadBytes, size: i64) ?[]u8 = {
    if(self.i + size - 1 >= self.bytes.len(), => return(.None));
    ptr := self.bytes.ptr.offset(self.i);
    self.i += size;
    (Some = (ptr = ptr, len = size))
}

::enum(ExecStyle);

Flag :: @enum(i64) (
    SYMBOL_ZERO,
    toplevel,
    include_std,
    Anon,
    if,
    
    // TODO: HACK
    // TODO: use this
    first_dumb_type_name, Self, RAW, last_dumb_type_name,
    
    // These are function #tags that the compiler knows about. 
    log_asm, log_ir, log_ast, 
    inline, noinline, asm, c_call, unsafe_noop_cast, 
    import, intrinsic, libc, redirect, comptime_addr,
    once, fold, no_trace, ct, outputs, macro, 
    link_rename, generic, target_os, 
    aarch64, x86_bytes, ir,
    
    // These are variables that are automatically shadowed in new scopes. 
    return, local_return,
    
    // These are macros that need to be called before we've compiled enough to evaluate the type FatExpr.
    builtin, type, struct, tagged, enum, late,
    
    // This is a magic blessed macro with no implementation but recognised by name. 
    rec, 
    
    // These are operators that have syntax desugaring. 
    add, sub, mul, div, lt, gt, ge, le, eq, ne, neg, not, and, or,  
    operator_squares_prefix, operator_star_prefix, operator_question_prefix, operator_index,
    operator_plus_equal, operator_minus_equal, operator_star_equal, operator_slash_equal,
    __string_escapes,
    
    // These can be used with @builtin.
    OverloadSet, ScopeId, FuncId, LabelId, Symbol, i64, bool, true, false, void, Type, rawptr, Never, f64, f32, UnknownType,
    compiler_debug_assert_eq_i64,  // this is maybe helpful for debugging the compiler when nothing works at all. 
    
    // These must be in order because tuple field names use them and compute by offsetting Flag._0.raw().
     _0,  _1,  _2,  _3,  _4,  _5,  _6,  _7,  _8,  _9, 
    _10, _11, _12, _13, _14, _15, _16, _17, _18, _19, _20,
    
    // These are libc functions the compiler can provide for comptime code even when it can't dlopen a libc. 
    // These are chosen arbitrarily based on what tests i want to run in blink emulator. 
    mmap, abort, write, read, munmap, open, close, mprotect, __clear_cache, uname, clock_gettime, malloc, free, 
    
    ___this_function_is_not_real_im_just_testing_the_franca_compiler,
    c_variadic,
    where,
    syscall,
    use,
    reexport,
    _,
    self,
    duplicated,
    align,
);

fn ident(name: Flag) Symbol = {
    symbol_from_id((@as(i64) name).trunc())
}

:: {
    fn eq(a: FuncId, b: FuncId) bool #redirect(Ty(u32, u32), bool);
    DerefEq(FuncId);
};

fn get_whole_line(c: CompCtx, loc: Span) FrancaCodeLine = {
    {c.vtable.get_whole_line}(c.data, loc)
}

// TODO: this is a dumb hack to make it less painful that i can't access fields on a value (because you need a pointer to offset)
//       fix in the compiler and remove this! -- Jul 7
//       now the problem is you can't access fields on a pointer if its not a dereference of it 
//       (i changed get_info when i ported it, i didn't fix the old problem yet).  -- Jul 21
fn align_bytes(s: *TypeMeta) u16 = s.align_bytes;
fn size_slots(s: *TypeMeta) u16 = s.size_slots;

fn slot_count(self: CompCtx, ty: Type) u16 = {
    self.get_info(ty)[].size_slots
}

fn emit_qbe_included_dyn(m: *QbeModule, comp: *CompCtx, fns: [] FuncId, entry: ProgramEntry) BucketArray(u8) = {  
    {comp.vtable.emit_qbe_included}(QbeModule.raw_from_ptr(m), comp, fns, entry)
}

fn non_void_arg(a: *Annotation) ?*FatExpr = {
    if(a.args&.is_raw_unit(), => return(.None));
    (Some = a.args&)
}

fn add_file(c: CompCtx, name: Str, src: Str) Span = {  // TODO: this shouldn't need the squiggles 
    {c.vtable.add_file}(c.data, name, src).shrink()
}

fn intern_type(c: CompCtx, info: TypeInfo) Type #inline = {
    {c.vtable.intern_type}(c.data, info&)
}

fn intern_func(c: CompCtx, info: *Func) FuncId = {
    {c.vtable.intern_func}(c.data, info)
}

fn fill_backend_vtable(vtable: *ImportVTable) void = {
    vtable.init_default_qbe_module = fn(module_out: rawptr, qbe_env_goal: rawptr) void = {
        m := QbeModule.ptr_from_raw(module_out);
        e := QbeTargetEnv.ptr_from_raw(qbe_env_goal);
        init_default_module(m, e[], false);
    };
    vtable.run_qbe_passes = fn(f) = {
        f := Qbe.Fn.ptr_from_raw(f);
        run_qbe_passes(f);
    };
    vtable.finish_qbe_module = fn(m) = {
        m := QbeModule.ptr_from_raw(m);
        m.emit_suspended_inlinables();
        {m.target.finish_module}(m)
    };
    vtable.drop_qbe_module = fn(m) = {
        m := QbeModule.ptr_from_raw(m);
        m.drop();
    };
    
    // TODO: this is ugly. but it's a really convenient hack to make the feedback loop for import_wasm/import_c faster. 
    hack := @as(CodegenWorker) codegen_thread_main;
    vtable.codegen_thread_main = ptr_cast_unchecked(@type hack, @type vtable.codegen_thread_main, hack&)[];
}

////////////////////// 
/// Bake Constants /// 
// See compiler/values.fr

BakedValue :: @struct {
    template: @tagged(Bytes: []u8, Zeroes: i64);
    relocations: []BakedReloc = empty();  // These must be in order (by .off)
    // TODO: expose these to the frontend in a sane way. 
    align: u32 = 1;
    export: bool = false;
    loc: Span;
    jit_addr := 0;
};

BakedReloc :: @struct {
    target: @tagged(FuncId: FuncId, BakedVarId: BakedVarId);
    off: u32;
    // TODO: you don't want this here if you want to allow multiple AOT modules in one frontend run.  
    backend_symbol: u32 = MAX_u32;
};

fn get_baked(c: CompCtx, id: BakedVarId) *BakedValue = 
    BakedValue.ptr_from_raw((c.vtable.get_baked)(c.data, id));

fn bake_translate_legacy(legacy: []BakedEntry, bytes: *List(u8), relocs: *List(BakedReloc), base: u32) void = {
    off: u32 = 0;
    for legacy { it |
        continue :: local_return;
        off += 8;
        
        // TODO: This (not just setting the value to 0 when it's a pointer)
        //       is a hack because it aliases the comptime bytes and sometimes
        //       we need to use it at comptime even after we emit for AOT. 
        dest := bytes.reserve_type(i64);
        @if_let(it) fn Num(int) => {
            dest[] = int.value;
        };
        relocs.push(off = off - 8 + base, target = @match(it) {
            fn Num(_) => continue();
            fn FnPtr(it) => (FuncId = it);
            fn AddrOf(it) => (BakedVarId = it);
        });
    };
}

fn bake_collect_legacy(value: *BakedValue, a: Alloc) []BakedEntry = {
    out := BakedEntry.list(value.len() / 8, a);
    bake_iter_legacy(value, fn(it) => out&.push(it));
    out.items()
}

fn bake_iter_legacy(value: *BakedValue, $body: @Fn(legacy: BakedEntry) void) void = {
    off := 0;
    size := value.len();
    next := 0;
    bytes: []u8 = @match(value.template) {
        fn Bytes(it) => it;
        fn Zeroes() => empty();
    };
    while => off < size {
        legacy: BakedEntry = (Num = (value = if(bytes.len == 0, => 0, => bytes&.pop_type(i64)[])));
        if value.relocations.len > next {
            r := value.relocations.index(next);
            if r.off == off.trunc() {
                next += 1;
                legacy = @match(r.target) {
                    fn FuncId(it) => (FnPtr = it);
                    fn BakedVarId(it) => (AddrOf = it);
                };
            };
        };
        off += 8;
        body(legacy);
    };
    @debug_assert_eq(next, value.relocations.len, "some relocs not in range");
}

fn len(self: *BakedValue) i64 = @match(self.template&) {
    fn Bytes(it)   => it.len;
    fn Zeroes(len) => len[];
};

fn add_to_scope(c: CompCtx, s: ScopeId, name: Symbol, foreign_type: Type, value: ~T) void #where = {
   (c.vtable.add_to_scope)(c.data, s, name, foreign_type, T.raw_from_ptr(value&));
}

fn empty_fn(fn_type: FnType, name: Symbol, loc: Span) Func #inline = (
    annotations = empty(),
    callees = empty(),
    mutual_callees = empty(),
    var_name = .None,
    finished_arg = (Some = fn_type.arg),
    finished_ret = (Some = fn_type.ret),
    cc = (Some = .CCallReg),
    return_var = .None,
    scope = .None,
    body = .Empty,
    arg = (bindings = empty(), loc = loc),
    ret = (Finished = fn_type.ret),
    name = name,
    loc = loc,
);

fn fill_bindings(func: *Func, fr: CompCtx, args: []Type) void = {
    func.arg.bindings = init(fr.get_alloc(), args.len);
    for args { a |
        func.arg.bindings&.push((
            name = .None,
            ty = (Finished = a),
            nullable_tag = zeroed(*Annotations),
            default = binding_missing_default(),
            kind = .Var,
        ), panicking_allocator);
    }
}
