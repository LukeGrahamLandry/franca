// TODO: HACK: i only have i64 but code needs to be a packed array of u32. 
fn pack_i64(dest: Slice$i64, src: Slice$i64) Unit = {
    assert(dest.len().mul(2).eq(src.len()).or(dest.len().mul(2).eq(src.len().add(1))), "bad len");
    
    range(0, dest.len()) {(i: i64)|
        // Note: which gets shifted feels backwards because of endianness 
        var value = src.get(i.mul(2));
        let next = i.mul(2).add(1);
        
        if(src.len().gt(next)) {|
            let v = src.get(next).shift_left(32);
            value = value.bit_or(v);
        }; // else: there's an odd number of i64s so we just do the last and leave 0 in the final slot. 
        
        dest.index(i)[] = value;
    };
}

#aarch64 
fn call_jit(input: i64, var code: Ty(*i64, i64)) i64 = {
    let code: Slice$i64 = (ptr = code.(0), len = code.(1));
    var bytes = code.len().mul(4);
    code.len().mod(2).eq(1).if {|
        bytes = bytes.add(4);
    };
    
    var mapped = os_alloc(bytes);
    pack_i64(mapped, code);
    let beg: rawptr = mapped.ptr;
    let end: rawptr = beg.ptr_to_int().add(bytes).int_to_ptr();
    var exec = make_exec(mapped);
    let f: FnPtr(i64, i64) = exec.ptr;
    __clear_cache(beg, end);
    let out = f(input);
    os_dealloc(exec);
    out
}

#test fn manual_mmap(canary: i64) i64 = {
    let code = (
        movz(Bits.X64, x1, 0b0000000000001010, Hw.Left0),
        add_sr(Bits.X64, x0, x1, x0, Shift.LSL, 0b000000), 
        ret()
    )!slice;
    assert_eq(call_jit(90, code), 100);
    
    // Decimal literals auto cast to any bit size. 
    // TODO: bounds checking for constants. 
    let code = (
        movz(Bits.X64, x1, @as(u16) 120, Hw.Left0), 
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000), 
        ret()
    )!slice;
    assert_eq(call_jit(80, code), 200);
    
    canary
}

#test fn math_ops(canary: i64) i64 = {
    #c_call #aarch64
    fn add_by_asm(a: i64, b: i64) i64 = (
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
        ret(),
    )!asm;
    
    let three = add_by_asm(1, 2);
    assert_eq(three, 3);
    
    #c_call #aarch64
    fn add_one(a: i64) i64 = (
        add_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    )!asm;
    assert_eq(add_one(9), 10);
    
    #c_call #aarch64
    fn sub_one(a: i64) i64 = (
        sub_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    )!asm;
    assert_eq(sub_one(10), 9);
    
    #c_call #aarch64
    fn bool_to_int(b: bool) i64 = (
        ret(),
        ret(), // TODO: single element tuple
    )!asm;
    assert_eq(bool_to_int(true), 1);
    assert_eq(bool_to_int(false), 0);
    
    canary
}

#test fn branching(canary: i64) i64 = {
    #c_call #aarch64
    fn add_or_sub(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.EQ),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    )!asm;
    assert_eq(add_or_sub(30, 20, 7), 50);
    assert_eq(add_or_sub(30, 20, 8), 10);

    #c_call #aarch64
    fn add_or_sub2(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.NE),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    )!asm;
    assert_eq(add_or_sub2(30, 20, 7), 50);
    assert_eq(add_or_sub2(30, 20, 8), 10);
    
    #c_call #aarch64
    fn is_seven(n: i64) i64 = (
        cmp_im(Bits.X64, x0, @as(u12) 7, 0b0),
        cset(Bits.X64, x0, Cond.NE),
        ret(),
    )!asm;
    assert_eq(is_seven(7), 1);
    assert_eq(is_seven(8), 0);

    canary
}

#test fn using_stack(canary: i64) i64 = {
    #c_call #aarch64
    fn truncate_to_byte(in: *i64, out: *i64) Unit = (
        ldrb_uo(x2, x0, @as(u12) 0),
        strb_uo(x2, x1, @as(u12) 0),
        ret(),
    )!asm;
    var in = 257;
    var out = 0;
    truncate_to_byte(in&, out&);
    assert_eq(out, 1);
    
    #c_call #aarch64
    fn use_stack(a: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 1),
        movz(Bits.X64, x0, @as(u16) 10, Hw.Left0), 
        ldr_uo(Bits.X64, x1, sp, @as(u12) 1),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(use_stack(15), 25);

    // Note: you make 16 bytes of room on the stack, then use offset 0 words and 1 words above sp. 
    //       NOT offset 1 and 2, that will stomp the first slot of the outer function (in this case canary so luckily you notice)
    //       but... its fine on interp because canary is stored on the fake stack and there happens to be a gap on the real one i guess. 
    //       I also didn't notice that I forgot to reset sp, I guess rust set the value after the call instead of trusing callee to offset it back? 
    #c_call #aarch64
    fn read_pair(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 0),
        str_uo(Bits.X64, x1, sp, @as(u12) 1),
        ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(read_pair(30, 20), 10);
    
    #c_call #aarch64
    fn write_pair(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
        ldr_uo(Bits.X64, x2, sp, @as(u12) 0),
        ldr_uo(Bits.X64, x3, sp, @as(u12) 1),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(write_pair(30, 20), 10);
    
    #c_call #aarch64
    fn both_pair(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
        ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),  // sub means order matters, so you can catch if flipped in encoding.
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(both_pair(30, 20), 10);

    canary
}
