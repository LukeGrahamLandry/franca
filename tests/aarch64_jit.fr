#aarch64
fn call_jit(input: i64, var code: Slice(u32)) i64 = {
    var bytes = code.len().mul(4);
    var mapped = page_allocator.alloc(u8, bytes);
    
    assert(eq(0, (@as(rawptr) mapped.ptr).ptr_to_int().mod(4)), "bad alignment");
    dest: Slice(u32) = (ptr = @as(*u32) mapped.ptr, len = code.len);
    dest.copy_from(code);
    
    let beg: rawptr = mapped.ptr;
    let end: rawptr = beg.ptr_to_int().add(bytes).int_to_ptr();
    var exec = make_exec(mapped);
    let f: FnPtr(i64, i64) = exec.ptr;
    __clear_cache(beg, end);
    let out = f(input);
    os_dealloc(exec);
    out
}

#aarch64
#test fn manual_mmap(canary: i64) i64 = {
    let code = (
        movz(Bits.X64, x1, 0b0000000000001010, Hw.Left0),
        add_sr(Bits.X64, x0, x1, x0, Shift.LSL, 0b000000), 
        ret()
    )!slice;
    assert_eq(call_jit(90, code), 100);
    
    // Decimal literals auto cast to any bit size. 
    // TODO: bounds checking for constants. 
    let code = (
        movz(Bits.X64, x1, @as(u16) 120, Hw.Left0), 
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000), 
        ret()
    )!slice;
    assert_eq(call_jit(80, code), 200);
    
    canary
}

#aarch64
#test fn math_ops(canary: i64) i64 = {
    #c_call #aarch64
    fn add_by_asm(a: i64, b: i64) i64 = (
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
        ret(),
    )!asm;
    
    let three = add_by_asm(1, 2);
    assert_eq(three, 3);
    
    #c_call #aarch64
    fn add_one(a: i64) i64 = (
        add_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    )!asm;
    assert_eq(add_one(9), 10);
    
    #c_call #aarch64
    fn sub_one(a: i64) i64 = (
        sub_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    )!asm;
    assert_eq(sub_one(10), 9);
    
    #c_call #aarch64
    fn bool_to_int(b: bool) i64 = (
        ret(),
        ret(), // TODO: single element tuple
    )!asm;
    assert_eq(bool_to_int(true), 1);
    assert_eq(bool_to_int(false), 0);
    
    canary
}

#aarch64
#test fn branching(canary: i64) i64 = {
    #c_call #aarch64
    fn add_or_sub(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.EQ),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    )!asm;
    assert_eq(add_or_sub(30, 20, 7), 50);
    assert_eq(add_or_sub(30, 20, 8), 10);

    #c_call #aarch64
    fn add_or_sub2(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.NE),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    )!asm;
    assert_eq(add_or_sub2(30, 20, 7), 50);
    assert_eq(add_or_sub2(30, 20, 8), 10);
    
    #c_call #aarch64
    fn is_seven(n: i64) i64 = (
        cmp_im(Bits.X64, x0, @as(u12) 7, 0b0),
        cset(Bits.X64, x0, Cond.NE),
        ret(),
    )!asm;
    assert_eq(is_seven(7), 1);
    assert_eq(is_seven(8), 0);

    canary
}

#aarch64
#test fn using_stack(canary: i64) i64 = {
    #c_call #aarch64
    fn truncate_to_byte(in: *i64, out: *i64) Unit = (
        ldrb_uo(x2, x0, @as(u12) 0),
        strb_uo(x2, x1, @as(u12) 0),
        ret(),
    )!asm;
    var in = 257;
    var out = 0;
    truncate_to_byte(in&, out&);
    assert_eq(out, 1);
    
    #c_call #aarch64
    fn use_stack(a: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 1),
        movz(Bits.X64, x0, @as(u16) 10, Hw.Left0), 
        ldr_uo(Bits.X64, x1, sp, @as(u12) 1),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(use_stack(15), 25);

    // Note: you make 16 bytes of room on the stack, then use offset 0 words and 1 words above sp. 
    //       NOT offset 1 and 2, that will stomp the first slot of the outer function (in this case canary so luckily you notice)
    //       but... its fine on interp because canary is stored on the fake stack and there happens to be a gap on the real one i guess. 
    //       I also didn't notice that I forgot to reset sp, I guess rust set the value after the call instead of trusing callee to offset it back? 
    #c_call #aarch64
    fn read_pair(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 0),
        str_uo(Bits.X64, x1, sp, @as(u12) 1),
        ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(read_pair(30, 20), 10);
    
    #c_call #aarch64
    fn write_pair(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
        ldr_uo(Bits.X64, x2, sp, @as(u12) 0),
        ldr_uo(Bits.X64, x3, sp, @as(u12) 1),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(write_pair(30, 20), 10);
    
    #c_call #aarch64
    fn both_pair(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
        ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),  // sub means order matters, so you can catch if flipped in encoding.
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    )!asm;
    assert_eq(both_pair(30, 20), 10);

    canary
}
