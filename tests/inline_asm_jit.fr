//! This is nice for sanity checking instruction encoding seperatly from the comptime jit because you can cross compile. 

#test fn manual_mmap(canary: i64) i64 = {
    arch := query_current_arch().unwrap();  // note: at comptime we don't know if we're cross compiling, so both branches are compiled. 
    if arch == .aarch64 {
        code := @slice (
            movz(Bits.X64, x1, 0b0000000000001010, Hw.Left0),
            add_sr(Bits.X64, x0, x1, x0, Shift.LSL, 0b000000), 
            ret()
        );
        assert_eq(call_jit(90, code), 100);
        
        // TODO: Decimal literals auto cast to any bit size. 
        // TODO: bounds checking for constants. 
        code := @slice (
            movz(Bits.X64, x1, 0x0078, Hw.Left0), 
            add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000), 
            ret()
        );
        assert_eq(call_jit(80, code), 200);
    };
    if arch == .x86_64 {
        code: List(u8) = list(temp());
        
        RAX_FROM_RDI :: 0b11111000;  // modrm. (mode:u2= registers not memory= 3, reg_src:u3=.rdi=7, reg_dest:u3=rax=0)
        JUST_RAX :: 0b11000000;
        
        @asm_x64(
            PrimaryOp.MovImm32, JUST_RAX, @as(u32) 10, // mov .rax, 10
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(0, code.items()), 10);
        
        code&.clear();
        code&.encode_bin(PrimaryOp.MovReg, .rax, .rdi);
        code&.push(@as(u8) PrimaryOp.Ret);
        assert_eq(call_jit(90, code.items()), 90);
    
        code&.clear();
        @asm_x64(
            PrimaryOp.MovImm32, JUST_RAX, @as(u32) 10, // mov .rax, 10
            PrimaryOp.AddReg, RAX_FROM_RDI,   // add .rax, .rdi
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(90, code.items()), 100);
        
        // same thing but with fancier macro usage
        code&.clear();
        @asm_x64(
            encode_imm(X86Reg.rax, 10),
            encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rdi),
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(90, code.items()), 100);
        
        // big immediate works 
        code&.clear();
        @asm_x64(
            PrimaryOp.MovImm32, JUST_RAX, @as(u32) 1234567,
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(0, code.items()), 1234567);
    };
    
    canary
}

// TODO: compiler hangs when trying to jit inline asm if it doesn't support the comptime arch.
// TODO: go through and implement on x86 everything with `@if(@run query_current_arch().unwrap() == .aarch64) {`
// TODO: output llvm asm for #x86_bytes like i do for #aarch64

#test fn math_ops(canary: i64) i64 = {
    #c_call #aarch64 #asm
    fn add_by_asm(a: i64, b: i64) i64 = (
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
        ret(),
    );
    // now using inline asm that the compiler knows about
    #c_call #x86_bytes #asm
    fn add_by_asm(a: i64, b: i64) i64 = fn(out) = @asm_x64(
        encode_imm(X86Reg.rax, 0),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rdi),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rsi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(add_by_asm(10, 15), 25);
    
    three := add_by_asm(1, 2);
    assert_eq(three, 3);
    
    #c_call #aarch64 #asm
    fn add_one(a: i64) i64 = (
        add_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    );
    #c_call #x86_bytes #asm
    fn add_one(a: i64) i64 = fn(out) = @asm_x64(
        encode_imm(PrimaryOp.AddImm32, X86Reg.rdi, 1),
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(add_one(9), 10);
    
    #c_call #aarch64 #asm
    fn sub_one(a: i64) i64 = (
        sub_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    );
    #c_call #x86_bytes #asm
    fn sub_one(a: i64) i64 = fn(out) = @asm_x64(
        encode_imm(PrimaryOp.AddImm32, X86Reg.rdi, -1),
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(sub_one(10), 9);
    
    #c_call #aarch64 #asm
    fn bool_to_int(b: bool) i64 = (
        ret(),
        ret(), // TODO: single element tuple
    );
    #c_call #x86_bytes #asm
    fn bool_to_int(b: bool) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(bool_to_int(true), 1);
    assert_eq(bool_to_int(false), 0);
    
    canary
}

#test fn branching(canary: i64) i64 = {
    #c_call #aarch64 #asm
    fn is_seven(n: i64) i64  = (
        cmp_im(Bits.X64, x0, @as(u12) 7, 0b0),
        cset(Bits.X64, x0, Cond.NE),
        ret(),
    );
    #c_call #x86_bytes #asm
    fn is_seven(n: i64) i64 = fn(out) = @asm_x64(  // :slow, if you care they have setcc
        encode_imm(X86Reg.rax, 7),
        encode_bin(PrimaryOp.CmpReg, X86Reg.rdi, X86Reg.rax),
        encode_jmp(X86cc.e, 13),
        encode_imm(X86Reg.rax, 0),    // 7 bytes
        encode_jmp(7),                // 6 bytes
        encode_imm(X86Reg.rax, 1),    // 7 bytes
        PrimaryOp.Ret,
    ) out;
    assert_eq(is_seven(7), 1);
    assert_eq(is_seven(8), 0);
    
    #c_call #aarch64 #asm
    fn add_or_sub(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.EQ),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    );
    #c_call #x86_bytes #asm
    fn add_or_sub(a: i64, b: i64, add_if_seven: i64) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        encode_imm(X86Reg.r11, 7),
        encode_bin(PrimaryOp.CmpReg, X86Reg.r11, X86Reg.rdx),
        encode_jmp(X86cc.e, 4),
        encode_bin(PrimaryOp.SubReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
    ) out;
    assert_eq(add_or_sub(30, 20, 7), 50);
    assert_eq(add_or_sub(30, 20, 8), 10);
    
    #c_call #aarch64 #asm
    fn add_or_sub2(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.NE),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    );
    #c_call #x86_bytes #asm
    fn add_or_sub2(a: i64, b: i64, add_if_seven: i64) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        encode_imm(X86Reg.rcx, 7),
        encode_bin(PrimaryOp.CmpReg, X86Reg.rcx, X86Reg.rdx),
        encode_jmp(X86cc.ne, 4),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
        encode_bin(PrimaryOp.SubReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
    ) out;
    assert_eq(add_or_sub2(30, 20, 7), 50);
    assert_eq(add_or_sub2(30, 20, 8), 10);
    
    canary
}

#aarch64
#test fn using_stack(canary: i64) i64 = {
    #c_call #aarch64 #asm
    fn swap_ints(a: *i64, b: *i64) void = (
        ldr_uo(Bits.X64, x3, x0, @as(u12) 0),
        ldr_uo(Bits.X64, x4, x1, @as(u12) 0),
        str_uo(Bits.X64, x3, x1, @as(u12) 0),
        str_uo(Bits.X64, x4, x0, @as(u12) 0),
        ret(),
    );
    // TODO: rsp, rbp, r12, r13 use different addressing modes. 
    #c_call #x86_bytes #asm
    fn swap_ints(a: *i64, b: *i64) void = fn(out) = @asm_x64(
        // memory on the left!
        encode_bin_mem(PrimaryOp.MovRegLoad, X86Reg.rdi, X86Reg.r10),
        encode_bin_mem(PrimaryOp.MovRegLoad, X86Reg.rsi, X86Reg.r11),
        encode_bin_mem(PrimaryOp.MovReg, X86Reg.rdi, X86Reg.r11),
        encode_bin_mem(PrimaryOp.MovReg, X86Reg.rsi, X86Reg.r10),
        PrimaryOp.Ret
    ) out;
    a := 123;
    b := 456;
    swap_ints(a&, b&);
    @println("% %", a, b); // without this print it works if you get the registers backwards which is odd. 
    assert_eq(a, 456);
    assert_eq(b, 123);
    
    #c_call #aarch64 #asm
    fn use_stack(a: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 1),
        movz(Bits.X64, x0, 0x000A, Hw.Left0), 
        ldr_uo(Bits.X64, x1, sp, @as(u12) 1),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    );
    #c_call #x86_bytes #asm
    fn use_stack(a: i64) i64 = fn(out) = @asm_x64(
        encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -16),
        encode_imm(PrimaryOp.MovImm32, X86Reg.rsi, 0),
        encode_tri_mem(PrimaryOp.MovReg, X86Reg.rdi, X86Reg.rsp, X86Reg.rsi),
        encode_tri_mem(PrimaryOp.MovRegLoad, X86Reg.rax, X86Reg.rsp, X86Reg.rsi),
        encode_imm(PrimaryOp.AddImm32, X86Reg.rax, 10),
        encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, 16),
        PrimaryOp.Ret
    ) out;
    assert_eq(use_stack(15), 25);
        
    // TODO: fancy enough driver that you can warn on this (@run query_current_arch()) becuase it's a footgun for cross compiling. 
    //       but i don't want to just keep adding shit to the compiler. 
    @if(@run query_current_arch().unwrap() == .aarch64) {
        #c_call #aarch64 #asm
        fn truncate_to_byte(in: *i64, out: *i64) void = (
            ldrb_uo(x2, x0, @as(u12) 0),
            strb_uo(x2, x1, @as(u12) 0),
            ret(),
        );
        in := 257;
        out := 0;
        truncate_to_byte(in&, out&);
        assert_eq(out, 1);
        
        // Note: you make 16 bytes of room on the stack, then use offset 0 words and 1 words above sp. 
        //       NOT offset 1 and 2, that will stomp the first slot of the outer function (in this case canary so luckily you notice)
        //       but... its fine on interp because canary is stored on the fake stack and there happens to be a gap on the real one i guess. 
        //       I also didn't notice that I forgot to reset sp, I guess rust set the value after the call instead of trusing callee to offset it back? 
        #c_call #aarch64 #asm
        fn read_pair(a: i64, b: i64) i64 = (
            sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
            str_uo(Bits.X64, x0, sp, @as(u12) 0),
            str_uo(Bits.X64, x1, sp, @as(u12) 1),
            ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
            sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
            add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
            ret(),
        );
        assert_eq(read_pair(30, 20), 10);
        
        #c_call #aarch64 #asm
        fn write_pair(a: i64, b: i64) i64 = (
            sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
            stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
            ldr_uo(Bits.X64, x2, sp, @as(u12) 0),
            ldr_uo(Bits.X64, x3, sp, @as(u12) 1),
            sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
            add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
            ret(),
        );
        assert_eq(write_pair(30, 20), 10);
        
        #c_call #aarch64 #asm
        fn both_pair(a: i64, b: i64) i64 = (
            sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
            stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
            ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
            sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),  // sub means order matters, so you can catch if flipped in encoding.
            add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
            ret(),
        );
        assert_eq(both_pair(30, 20), 10);
    };
    canary
}

#test fn asm_fn_ptr(canary: i64) i64 = {
    // TODO: need to do_merges when resolving by type. 
    //       should clean up the old resolve_in_overload_set
    @if(@run query_current_arch().unwrap() == .aarch64) {
        fn double_it(a: i64) i64 #c_call #aarch64 #asm = (
            add_sr(Bits.X64, x0, x0, x0, Shift.LSL, @as(u6) 0),
            ret(),
        );
        //#c_call #x86_bytes #asm
        //fn double_it(a: i64) i64 = fn(out) = @asm_x64(
        //    encode_bin(PrimaryOp.MovReg, .rax, .rdi),
        //    encode_bin(PrimaryOp.AddReg, .rax, rax),
        //    PrimaryOp.Ret,
        //) out;
        
        f: @FnPtr(a: i64) i64 : double_it; // constant ':'
        assert_eq(f(5), 10);
        
        fn do_nothing(a: i64) i64 #c_call #aarch64 #asm = (
            add_sr(Bits.X64, x0, x0, xzr, Shift.LSL, @as(u6) 0),
            ret(),
        );
        //#c_call #x86_bytes #asm
        //fn do_nothing(a: i64) i64 = fn(out) = @asm_x64(
        //    encode_bin(PrimaryOp.MovReg, .rax, .rdi),
        //    PrimaryOp.Ret,
        //) out;
        
        f2: @FnPtr(a: i64) i64 = do_nothing; // runtime '='
        assert_eq(f2(5), 5);
    };
    canary
}

// aarch64 instructions are u32, x86_64 are u8

fn call_jit(input: i64, code: Slice(u32)) i64 = {
    assert(query_current_arch().unwrap() == .aarch64, "wrong size instructions wont go well");
    bytes := code.len().mul(4);
    mapped := page_allocator.alloc(u8, bytes);
    
    assert(eq(0, u8.int_from_ptr(mapped.ptr).mod(4)), "bad alignment");
    dest: Slice(u32) = (ptr = ptr_cast_unchecked(From = u8, To = u32, ptr = mapped.ptr), len = code.len);
    dest.copy_from(code);
    
    beg := u8.raw_from_ptr(mapped.ptr);
    end := beg.int_from_rawptr().add(bytes).rawptr_from_int();
    exec := make_exec(mapped);
    f := assume_types_fn(i64, i64, exec.ptr);
    __clear_cache(beg, end);
    out := f(input);
    os_dealloc(exec);
    out
}

fn call_jit(input: i64, code: Slice(u8)) i64 = {
    assert(query_current_arch().unwrap() == .x86_64, "wrong size instructions wont go well");
    mapped := page_allocator.alloc(u8, code.len);
    mapped.copy_from(code);
    
    beg := u8.raw_from_ptr(mapped.ptr);
    end := beg.int_from_rawptr().add(code.len).rawptr_from_int();
    exec := make_exec(mapped);
    f := assume_types_fn(i64, i64, exec.ptr);
    __clear_cache(beg, end);
    out := f(input);
    os_dealloc(exec);
    out
}
