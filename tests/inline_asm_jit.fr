//! This is nice for sanity checking instruction encoding seperatly from the comptime jit because you can cross compile. 

#use("@/lib/sys/process.fr");

#test fn manual_mmap(canary: i64) i64 = {
    arch := query_current_arch();  // note: at comptime we don't know if we're cross compiling, so both branches are compiled. 
    if arch == .aarch64 {
        code := @slice (
            movz(Bits.X64, x1, 0b0000000000001010, Hw.Left0),
            add_sr(Bits.X64, x0, x1, x0, Shift.LSL, 0b000000), 
            ret()
        );
        assert_eq(call_jit(90, code), 100);
        
        // TODO: Decimal literals auto cast to any bit size. 
        // TODO: bounds checking for constants. 
        code := @slice (
            movz(Bits.X64, x1, 0x0078, Hw.Left0), 
            add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000), 
            ret()
        );
        assert_eq(call_jit(80, code), 200);
    };
    if arch == .x86_64 {
        code: List(u8) = list(temp());
        
        RAX_FROM_RDI :: 0b11111000;  // modrm. (mode:u2= registers not memory= 3, reg_src:u3=.rdi=7, reg_dest:u3=rax=0)
        JUST_RAX :: 0b11000000;
        
        @asm_x64(
            PrimaryOp.MovImm32, JUST_RAX, @as(u32) 10, // mov .rax, 10
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(0, code.items()), 10);
        
        code&.clear();
        code&.encode_bin(PrimaryOp.MovReg, .rax, .rdi);
        code&.push(@as(u8) PrimaryOp.Ret);
        assert_eq(call_jit(90, code.items()), 90);
    
        code&.clear();
        @asm_x64(
            PrimaryOp.MovImm32, JUST_RAX, @as(u32) 10, // mov .rax, 10
            PrimaryOp.AddReg, RAX_FROM_RDI,   // add .rax, .rdi
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(90, code.items()), 100);
        
        // same thing but with fancier macro usage
        code&.clear();
        @asm_x64(
            encode_imm(X86Reg.rax, 10),
            encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rdi),
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(90, code.items()), 100);
        
        // big immediate works 
        code&.clear();
        @asm_x64(
            PrimaryOp.MovImm32, JUST_RAX, @as(u32) 1234567,
            PrimaryOp.Ret,
        ) code&;
        assert_eq(call_jit(0, code.items()), 1234567);
    };
    
    canary
}

#test fn math_ops(canary: i64) i64 = {
    add_by_asm :: AsmFunctionOld(add_by_asm0, add_by_asm1);
    add_by_asm0 :: fn(a: i64, b: i64) i64 = (
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, 0b000000),
        ret(),
    );
    // now using inline asm that the compiler knows about
    add_by_asm1 :: fn(a: i64, b: i64) i64 = fn(out) = @asm_x64(
        encode_imm(X86Reg.rax, 0),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rdi),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rsi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(add_by_asm(10, 15), 25);
    
    three := add_by_asm(1, 2);
    assert_eq(three, 3);
    
    add_one :: AsmFunctionOld(add_one0, add_one1);
    add_one0 :: fn(a: i64) i64 = (
        add_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    );
    add_one1 :: fn(a: i64) i64 = fn(out) = @asm_x64(
        encode_imm(PrimaryOp.AddImm32, X86Reg.rdi, 1),
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(add_one(9), 10);
    
    sub_one :: AsmFunctionOld(sub_one0, sub_one1);
    sub_one0 :: fn(a: i64) i64 = (
        sub_im(Bits.X64, x0, x0, @as(u12) 1, 0b0),
        ret(),
    );
    sub_one1 :: fn(a: i64) i64 = fn(out) = @asm_x64(
        encode_imm(PrimaryOp.AddImm32, X86Reg.rdi, -1),
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(sub_one(10), 9);
    
    bool_to_int :: AsmFunctionOld(bool_to_int0, bool_to_int1);
    bool_to_int0 :: fn(b: bool) i64 = (
        ret(),
    );
    bool_to_int1 :: fn(b: bool) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    assert_eq(bool_to_int(true), 1);
    assert_eq(bool_to_int(false), 0);
    
    canary
}

#test fn branching(canary: i64) i64 = {
    is_seven :: AsmFunctionOld(is_seven0, is_seven1);
    is_seven0 :: fn(n: i64) i64  = (
        cmp_im(Bits.X64, x0, @as(u12) 7, 0b0),
        cset(Bits.X64, x0, Cond.NE),
        ret(),
    );
    is_seven1 :: fn(n: i64) i64 = fn(out) = @asm_x64(  // :slow, if you care they have setcc
        encode_imm(X86Reg.rax, 7),
        encode_bin(PrimaryOp.CmpReg, X86Reg.rdi, X86Reg.rax),
        encode_jmp(X86cc.e, 13),
        encode_imm(X86Reg.rax, 0),    // 7 bytes
        encode_jmp(7),                // 6 bytes
        encode_imm(X86Reg.rax, 1),    // 7 bytes
        PrimaryOp.Ret,
    ) out;
    assert_eq(is_seven(7), 1);
    assert_eq(is_seven(8), 0);
    
    add_or_sub :: AsmFunctionOld(add_or_sub_0, add_or_sub_1);
    add_or_sub_0 :: fn(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.EQ),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    );
    add_or_sub_1 :: fn(a: i64, b: i64, add_if_seven: i64) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        encode_imm(X86Reg.r11, 7),
        encode_bin(PrimaryOp.CmpReg, X86Reg.r11, X86Reg.rdx),
        encode_jmp(X86cc.e, 4),
        encode_bin(PrimaryOp.SubReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
    ) out;
    assert_eq(add_or_sub(30, 20, 7), 50);
    assert_eq(add_or_sub(30, 20, 8), 10);
    
    add_or_sub2 :: AsmFunctionOld(add_or_sub2_0, add_or_sub2_1);
    add_or_sub2_0 :: fn(a: i64, b: i64, add_if_seven: i64) i64 = (
        cmp_im(Bits.X64, x2, @as(u12) 7, 0b0),
        b_cond(@as(i19) 3, Cond.NE),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
        sub_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        ret(),
    );
    add_or_sub2_1 :: fn(a: i64, b: i64, add_if_seven: i64) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        encode_imm(X86Reg.rcx, 7),
        encode_bin(PrimaryOp.CmpReg, X86Reg.rcx, X86Reg.rdx),
        encode_jmp(X86cc.ne, 4),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
        encode_bin(PrimaryOp.SubReg, X86Reg.rax, X86Reg.rsi),   // 3
        PrimaryOp.Ret, // 1
    ) out;
    assert_eq(add_or_sub2(30, 20, 7), 50);
    assert_eq(add_or_sub2(30, 20, 8), 10);
    
    canary
}

#aarch64
#test fn using_stack(canary: i64) i64 = {
    swap_ints :: AsmFunctionOld(swap_ints0, swap_ints1);
    swap_ints0 :: fn(a: *i64, b: *i64) void = (
        ldr_uo(Bits.X64, x3, x0, @as(u12) 0),
        ldr_uo(Bits.X64, x4, x1, @as(u12) 0),
        str_uo(Bits.X64, x3, x1, @as(u12) 0),
        str_uo(Bits.X64, x4, x0, @as(u12) 0),
        ret(),
    );
    // TODO: rsp, rbp, r12, r13 use different addressing modes. 
    swap_ints1 :: fn(a: *i64, b: *i64) void = fn(out) = @asm_x64(
        // memory on the left!
        encode_bin_mem(PrimaryOp.MovRegLoad, X86Reg.rdi, X86Reg.r10),
        encode_bin_mem(PrimaryOp.MovRegLoad, X86Reg.rsi, X86Reg.r11),
        encode_bin_mem(PrimaryOp.MovReg, X86Reg.rdi, X86Reg.r11),
        encode_bin_mem(PrimaryOp.MovReg, X86Reg.rsi, X86Reg.r10),
        PrimaryOp.Ret
    ) out;
    a := 123;
    b := 456;
    swap_ints(a&, b&);
    //@println("% %", a, b); // without this print it works if you get the registers backwards which is odd. 
    esc(@tfmt("% %", a, b)); // hopefully this has the same bug catching effect? probably not cause it won't stomp as much. oh well 
    esc :: fn(s: Str) void #noinline = ();
    assert_eq(a, 456);
    assert_eq(b, 123);
    
    use_stack :: AsmFunctionOld(use_stack0, use_stack1);
    use_stack0 :: fn(a: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 1),
        movz(Bits.X64, x0, 0x000A, Hw.Left0), 
        ldr_uo(Bits.X64, x1, sp, @as(u12) 1),
        add_sr(Bits.X64, x0, x0, x1, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    );
    use_stack1 :: fn(a: i64) i64 = fn(out) = @asm_x64(
        encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -16),
        encode_imm(PrimaryOp.MovImm32, X86Reg.rsi, 0),
        encode_tri_mem(PrimaryOp.MovReg, X86Reg.rdi, X86Reg.rsp, X86Reg.rsi),
        encode_tri_mem(PrimaryOp.MovRegLoad, X86Reg.rax, X86Reg.rsp, X86Reg.rsi),
        encode_imm(PrimaryOp.AddImm32, X86Reg.rax, 10),
        encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, 16),
        PrimaryOp.Ret
    ) out;
    assert_eq(use_stack(15), 25);
        
    truncate_to_byte :: AsmFunctionOld(truncate_to_byte0, truncate_to_byte1);
    truncate_to_byte0 :: fn(in: *i64, out: *i64) void = (
        ldrb_uo(x2, x0, @as(u12) 0),
        strb_uo(x2, x1, @as(u12) 0),
        ret(),
    );
    truncate_to_byte1 :: fn(in: *i64, out: *i64) void = (fn(out) = @asm_x64(
        encode_bin_mem(PrimaryOp.MovByteLoad, X86Reg.rdi, X86Reg.rax),
        encode_bin_mem(PrimaryOp.MovByte, X86Reg.rsi, X86Reg.rax),
        PrimaryOp.Ret,
    ) out);
    in := 257;
    out := 0;
    truncate_to_byte(in&, out&);
    assert_eq(out, 1);

    // These tests are only interesting for arm. 
    ten :: fn(out: *List(u8)) => @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        encode_bin(PrimaryOp.SubReg, X86Reg.rax, X86Reg.rsi), 
        PrimaryOp.Ret
    ) out;
    
    read_pair :: AsmFunctionOld(read_pair0, read_pair1);
    // Note: you make 16 bytes of room on the stack, then use offset 0 words and 1 words above sp. 
    //       NOT offset 1 and 2, that will stomp the first slot of the outer function (in this case canary so luckily you notice)
    //       but... its fine on interp because canary is stored on the fake stack and there happens to be a gap on the real one i guess. 
    //       I also didn't notice that I forgot to reset sp, I guess rust set the value after the call instead of trusing callee to offset it back? 
    read_pair0 :: fn(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        str_uo(Bits.X64, x0, sp, @as(u12) 0),
        str_uo(Bits.X64, x1, sp, @as(u12) 1),
        ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    );
    read_pair1 :: fn(a: i64, b: i64) i64 = ten;
    assert_eq(read_pair(30, 20), 10);
    
    write_pair :: AsmFunctionOld(write_pair0, write_pair1);
    write_pair0 :: fn(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
        ldr_uo(Bits.X64, x2, sp, @as(u12) 0),
        ldr_uo(Bits.X64, x3, sp, @as(u12) 1),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    );
    write_pair1 :: fn(a: i64, b: i64) i64 = ten;
    assert_eq(write_pair(30, 20), 10);
    
    both_pair :: AsmFunctionOld(both_pair0, both_pair1);
    both_pair0 :: fn(a: i64, b: i64) i64 = (
        sub_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        stp_so(Bits.X64, x0, x1, sp, @as(i7) 0),
        ldp_so(Bits.X64, x2, x3, sp, @as(i7) 0),
        sub_sr(Bits.X64, x0, x2, x3, Shift.LSL, @as(u6) 0),  // sub means order matters, so you can catch if flipped in encoding.
        add_im(Bits.X64, sp, sp, @as(u12) 16, 0b0),
        ret(),
    );
    both_pair1 :: fn(a: i64, b: i64) i64 = ten;
    assert_eq(both_pair(30, 20), 10);
    
    canary
}

#use("@/backend/amd64/bits.fr");
#use("@/backend/arm64/bits.fr");
#use("@/tests/removed.fr");  // for AsmFunctionOld

#test fn asm_fn_ptr(canary: i64) i64 = {
    double_it :: AsmFunctionOld(double_it0, double_it1);
    double_it0 :: fn(a: i64) i64 = (
        add_sr(Bits.X64, x0, x0, x0, Shift.LSL, @as(u6) 0),
        ret(),
    );
    double_it1 :: fn(a: i64) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        encode_bin(PrimaryOp.AddReg, X86Reg.rax, X86Reg.rax),
        PrimaryOp.Ret,
    ) out;
    
    f: @FnPtr(a: i64) i64 : double_it; // constant ':'
    assert_eq(f(5), 10);
    
    do_nothing :: AsmFunctionOld(do_nothing0, do_nothing1);
    do_nothing0 :: fn(a: i64) i64 = (
        add_sr(Bits.X64, x0, x0, xzr, Shift.LSL, @as(u6) 0),
        ret(),
    );
    do_nothing1 :: fn(a: i64) i64 = fn(out) = @asm_x64(
        encode_bin(PrimaryOp.MovReg, X86Reg.rax, X86Reg.rdi),
        PrimaryOp.Ret,
    ) out;
    
    f2: @FnPtr(a: i64) i64 = do_nothing; // runtime '='
    assert_eq(f2(5), 5);
    
    // this won't work if branch target integrity is enabled! 
    // TODO: decide what the policy for that should be in AsmFunction. 
    // actually rn the backend inserts that instruction for you when you add_code_bytes
    foozle :: fn(f: @FnPtr(a: i64) i64, a: i64) i64 #noinline = 
        f(a);
    foozle(f2, 5);
    
    canary
}

// aarch64 instructions are u32, x86_64 are u8

fn call_jit(input: i64, code: Slice(u32)) i64 = {
    assert(query_current_arch() == .aarch64, "wrong size instructions wont go well");
    call_jit(input, code.interpret_as_bytes())
}

fn call_jit(input: i64, code: Slice(u8)) i64 = {
    mapped := code.shallow_copy(page_allocator);
    
    beg := u8.raw_from_ptr(mapped.ptr);
    end := beg.int_from_rawptr().add(code.len).rawptr_from_int();
    exec := make_exec(mapped);
    f := assume_types_fn(i64, i64, exec.ptr);
    clear_instruction_cache(beg, end);
    out := f(input);
    os_dealloc(exec);
    out
}
