// RV64I Zicond M Zacas F D B

// in contrast to amd/arm, the riscv 32 bit instructions generally sign extend the result. 
// TODO: a test for overflowing 32 bit add to make sure it's sign extended after discarding 
//       the overflow instead of using a 64 bit add. (if those are different, idk)
// TODO: what should go in the static float rounding field? 

EmitRv :: @struct {
    f: *Qbe.Fn;
    m: *Qbe.Module;
    start_of_function: i64;
    patches: List(Patch);
    pending_immediate_fixup_got: List(Ty(Fixup, Qbe.Sym));
};

rv64_emitfn :: fn(f: *Qbe.Fn) []Ty(Fixup, Qbe.Sym) = {
    e: EmitRv = (
        f = f,
        m = f.globals,
        start_of_function = f.globals.segments&.index(.Code).len(),
        patches = Patch.list(f.nblk.zext(), temp()),
        pending_immediate_fixup_got = list(temp()),
    );
    emit_the_code(e&);
    e.pending_immediate_fixup_got.items()
}

rv_scratch_int :: TMP(RvReg._T6);

// TODO: stop calling getcon here, it might reallocate when loading a cache file which seems sad  
emit_the_code :: fn(e: *EmitRv) void = {
    f := e.f;
    if f.vararg {
        e.emit(make_ins(.add, .Kl, TMP(RvReg.SP), TMP(RvReg.SP), f.getcon(-64)));
        range(0, 8) { i |
            e.emit_store_off(TMP(@as(i64) RvReg.A0.raw().zext() + i), TMP(RvReg.SP), 8*i);
        }
    }
    
    // TODO: elide this for leaf without stack frame
    //       (be careful! might have to update franca_runtime_init)
    e.emit_store_off(TMP(RvReg.FP), TMP(RvReg.SP), -16);
    e.emit_store_off(TMP(RvReg.RA), TMP(RvReg.SP), -8);
    e.emit(make_ins(.add, .Kl, TMP(RvReg.FP), TMP(RvReg.SP), f.getcon(-16)));

    to_save   := f.reg.bit_and(Abi.rv64_rclob);
    save_area := to_save.count_ones().zext() * 8;
    frame := f.slot.zext().add(16).align_to(16).add(save_area).align_to(16);

    if frame <= 2048 {  // '<=' is not a mistake; there are more negative numbers than positive ones 
        e.emit(make_ins(.add, .Kl, TMP(RvReg.SP), TMP(RvReg.SP), f.getcon(-frame)));
    } else {
        e.emit(make_ins(.copy, .Kl, rv_scratch_int, f.getcon(frame), Qbe.Null));
        e.emit(make_ins(.sub, .Kl, TMP(RvReg.SP), TMP(RvReg.SP), rv_scratch_int));
    }

    off := 0;
    for_bits to_save { i |
        r := @as(RvReg) @as(u8) i.trunc();
        e.emit_store_off(TMP(r), TMP(RvReg.SP), off);
        off += 8;
    };
    
    local_labels := temp().alloc_zeroed(i64, e.f.nblk.zext());
    for_blocks f { b | 
        local_labels[b.id.zext()] = e.next_inst_offset();
        for_insts_forward b { i |
            e.emit(i);
        };
        
        @match(b.jmp.type) {
            fn hlt() => e.inst(ebreak);
            fn ret0() => {
                if f.dynalloc {
                    if frame - 16 <= 2048 {
                        e.emit(make_ins(.add, .Kl, TMP(RvReg.SP), TMP(RvReg.FP), f.getcon(-(frame - 16))));
                    } else {
                        e.emit(make_ins(.copy, .Kl, rv_scratch_int, f.getcon(frame - 16), Qbe.Null));
                        e.emit(make_ins(.sub, .Kl, TMP(RvReg.SP), TMP(RvReg.FP), rv_scratch_int));
                    }
                };
                
                off := 0;
                for_bits to_save { i |
                    r := @as(RvReg) @as(u8) i.trunc();
                    e.emit_load_off(TMP(r), TMP(RvReg.SP), off);
                    off += 8;
                };
                
                e.emit_add_off(TMP(RvReg.SP), TMP(RvReg.FP), 16 + f.vararg.int() * 64);
                e.emit_load_off(TMP(RvReg.RA), TMP(RvReg.FP), 8);
                e.emit_load_off(TMP(RvReg.FP), TMP(RvReg.FP), 0);
                e.inst(jalr(.RA, 0, .Zero)); // ret
            }
            fn jmp() => if !b.s1.identical(b.link) {
                e.jmp(b.s1.id, -1, Qbe.Null);
            };
            fn jnz() => {
                swap := b.link.identical(b.s2);
                if swap {
                    s := b.s1;
                    b.s1 = b.s2;
                    b.s2 = s;
                };
                @debug_assert(isreg(b.jmp.arg));
                
                // conditional branch only has 4kb of range which is really not enough. 
                // add a little trampoline as needed. 
                src := e.next_inst_offset();
                bid := b.s2.id;
                dest := local_labels[bid.intcast()];
                far := dest == 0 || src - dest >= 1.shift_left(12);
                if far {
                    cond := @if(!swap, Qbe.Cmp.Cine, .Cieq);
                    e.inst(cond_jmp(8, cond, b.jmp.arg));
                    e.jmp(bid, -1, Qbe.Null);
                } else {
                    cond := @if(swap, Qbe.Cmp.Cine, .Cieq);
                    e.jmp(bid, cond.raw(), b.jmp.arg);
                }
                
                if !b.s1.identical(b.link) {
                    e.jmp(b.s1.id, -1, Qbe.Null);
                };
            };
            @default => unreachable();
        }
    };
    
    each e.patches& { p |
        target := local_labels[p.target_bid.zext()];
        @debug_assert(target != 0);  
        distance := (target - p.offset_from_start);
        code  := e.m.segments&[.Code]&;
        patch := code.mmapped.ptr.offset(e.start_of_function + p.offset_from_start);
        patch := ptr_cast_unchecked(u8, u32, patch);
        @debug_assert_eq(ebreak, patch[], "not expecting patch");
        if p.cond >= 0 {
            patch[] = cond_jmp(distance, @as(Qbe.Cmp) p.cond, p.reg);
        } else {
            @debug_assert(distance.fits_sext(20), "TODO: rv branch too far");
            patch[] = jal(@as(i21) distance, .Zero); 
        };
    };
}

fn cond_jmp(distance: i64, cond: Qbe.Cmp, r: Qbe.Ref) u32 = {
    kind: Bcmp = @match(cond) {
        fn Cieq() => .beq;
        fn Cine() => .bne;
        @default => todo();
    };
    @debug_assert(distance.fits_sext(12), "TODO: rv branch too far: %", distance);
    B(@as(i13) distance, RvReg.Zero.bits(), r.rv().bits(), @as(u3) kind, .bcmp)
}


fn jmp(e: *EmitRv, bid: i32, cond: i32, r: Qbe.Ref) void = {
    e.patches&.push(target_bid = bid, offset_from_start = e.next_inst_offset(), cond = cond, reg = r);
    e.inst(ebreak);
}

fixmem :: fn(e: *EmitRv, pr: *Qbe.Ref) void = {
    r := pr[];
    if rtype(r) == .RSlot {
        s := e.f.slot(r);
        if s < -2048 || s > 2047 {
            e.emit(make_ins(.copy, .Kl, rv_scratch_int, e.f.getcon(s), Qbe.Null));
            e.emit(make_ins(.add, .Kl, rv_scratch_int, TMP(RvReg.FP), rv_scratch_int));
            pr[] = rv_scratch_int;
        }
    }
}

fn emit(e: *EmitRv, o: Qbe.O, k: Qbe.Cls, r: Qbe.Ref, a: Qbe.Ref, b: Qbe.Ref) void = 
    emit(e, make_ins(o, k, r, a, b));

fn emit(e: *EmitRv, i: Qbe.Ins) void = 
    emit(e, i&);
    
fn emit(e: *EmitRv, i: *Qbe.Ins) void = {
    f := e.f;
    a0 := i.arg&[0];
    @match(i.op()) {
        fn nop() => ();
        fn dbgloc() => e.m.add_debug_info(i, e.next_inst_offset());
        fn copy() => {
            if(i.to == a0, => return());
            if rtype(i.to) == .RSlot {
                if rtype(i.arg&[0]) != .RTmp {
                    @debug_assert(i.cls().is_int(), "TODO: copy slot float");
                    e.emit(.copy, i.cls(), rv_scratch_int, i.arg&[0], Qbe.Null);
                    i.arg&[0] = rv_scratch_int;
                };
                e.emit(i.cls().store_op(), .Kw, Qbe.Null, i.arg&[0], i.to);
                return();
            }
            @debug_assert(i.to.isreg(), "% copy", i.to);
            @match(rtype(a0)) {
                fn RCon() => {
                    c := f.get_constant(a0);
                    @match(c.type()) {
                        fn CBits() => e.load_number(c.bits(), i.to, i.cls());
                        fn CAddr() => e.load_symbol(c, i.to.rv());
                    }
                }
                fn RSlot() => {
                    i.set_op(.load);
                    e.emit(i);
                }
                fn RTmp() => {
                    @debug_assert(a0.isreg(), "copy %", a0);
                    if i.cls().is_int() {
                        // op_imm not zero register so it disassembles to `mv` (more readable)
                        e.emit(.add, .Kl, i.to, a0, Qbe.ConZero);
                    } else {
                        i.arg&[1] = i.arg&[0];
                        e.float_op(i, .sgnj, 0b000);  // FSGNJ = FMV
                    }
                }
                @default => unreachable();
            }
        }
        fn neg() => {
            @debug_assert(i.cls().is_float());
            i.arg&[1] = i.arg&[0];
            e.float_op(i, .sgnj, 0b001);  // FSGNJN = FNEG
        }
        fn addr() => {
            @debug_assert_eq(rtype(a0), .RSlot);
            s := f.slot(a0);
            // TODO: very unfortunate precedence :compiler
            if (-s < 2048) {
                e.emit_add_off(i.to, TMP(RvReg.FP), s);
            } else {
                e.emit(make_ins(.copy, .Kl, i.to, f.getcon(s), Qbe.Null));
                e.emit(make_ins(.add, .Kl, i.to, TMP(RvReg.FP), i.to));
            }
        }
        fn call() => {
            @match(rtype(i.arg&[0])) {
                fn RCon() => {
                    c := f.get_constant(a0);
                    // TODO: use jal instead of wasting an extra instruction
                    // RA is stomped by the call instruction anyway so it's fine to use it as scratch. 
                    e.emit(make_ins(.copy, .Kl, TMP(RvReg.RA), a0, Qbe.Null));
                    e.inst(jalr(.RA, 0, .RA));
                }
                fn RTmp() => {
                    e.inst(jalr(a0.rv(), 0, .RA));
                }
                @default => unreachable();
            }
        }
        fn salloc() => {
            e.emit(make_ins(.sub, .Kl, TMP(RvReg.SP), TMP(RvReg.SP), i.arg&[0]));
            if i.to != Qbe.Null {
                e.emit(make_ins(.copy, .Kl, i.to, TMP(RvReg.SP), Qbe.Null));
            }
        }
        // ADDIW sign extends the result, so adding zero is just a sign extension. 
        fn extsw() => e.inst(R(0, 0, a0.rv().bits(), @as(u3) OpI.add, i.to.rv().bits(), .opw));
        // TODO: ADDIW, ADDW, SUBW, MULW, DIVW, DIVUW, REMW, REMUW
        fn add() => @if(i.cls().is_int(), 
            e.simple_op(i, .add, 0),
            e.float_op(i, .add, 0b000));
        fn sub() => @if(i.cls().is_int(), 
            e.simple_op(i, .add, 0b0100000),
            e.float_op(i, .sub, 0b000));
        fn and() => e.simple_op(i, .and, 0);
        fn xor() => e.simple_op(i, .xor, 0);
        fn or() => e.simple_op(i, .or, 0);
        fn mul() => @if(i.cls().is_int(), 
            e.simple_op(i, @as(OpI) 0b000, 1),
            e.float_op(i, .mul, 0b000));
        fn div()  => @if(i.cls().is_int(), 
            e.simple_op(i, @as(OpI) 0b100, 1),
            e.float_op(i, .div, 0b000));
        fn udiv() => e.simple_op(i, @as(OpI) 0b101, 1);
        fn rem()  => e.simple_op(i, @as(OpI) 0b110, 1);
        fn urem() => e.simple_op(i, @as(OpI) 0b111, 1);
        fn min() => e.float_op(i, .min_max, 0b000);
        fn max() => e.float_op(i, .min_max, 0b001);
        fn sqrt() => e.float_op(i, .sqrt, 0b000);
        
        fn csltl() => e.simple_op(i, .slt, 0);
        fn cultl() => e.simple_op(i, .sltu, 0);
        fn ceqs() => e.float_cmp(i, .eq, false);
        fn ceqd() => e.float_cmp(i, .eq, true);
        fn clts() => e.float_cmp(i, .lt, false);
        fn cltd() => e.float_cmp(i, .lt, true);
        fn cles() => e.float_cmp(i, .le, false);
        fn cled() => e.float_cmp(i, .le, true);
        fn swap() => {
            // TODO: this is the only place the reserved _FT11 is used,
            //       is it worth the weirdness over just using cast to int and back to use rv_scratch_int for both cases here?  
            t := @if(i.cls().is_int(), rv_scratch_int, TMP(RvReg._FT11));
            e.emit(.copy, i.cls(), t, i.arg&[0], Qbe.Null); 
            e.emit(.copy, i.cls(), i.arg&[0], i.arg&[1], Qbe.Null); 
            e.emit(.copy, i.cls(), i.arg&[1], t, Qbe.Null); 
        }
        fn cast() => {
            to, a0 := (i.to.rv().bits(), i.arg&[0].rv().bits());
            wide := i.cls().is_wide();
            to_f := i.cls().is_float();
            e.inst(@bits(0b111, to_f, 0b00, wide, 0b00000, a0, 0b000, to, Opcode.op_fp));  // R
        }
        fn selieq() => e.simple_op(i, @as(OpI) 0b101, 0b0000111);  // czero.eqz
        fn seline() => e.simple_op(i, @as(OpI) 0b111, 0b0000111);  // czero.nez
        
        // have_ints, from_int, double, long, unsigned
        fn stosi()  => e.fcnvt(i,  true, false, false, i.cls().is_wide(), false);
        fn stoui()  => e.fcnvt(i,  true, false, false, i.cls().is_wide(),  true);
        fn dtosi()  => e.fcnvt(i,  true, false,  true, i.cls().is_wide(), false);
        fn dtoui()  => e.fcnvt(i,  true, false,  true, i.cls().is_wide(),  true);
        fn swtof()  => e.fcnvt(i,  true,  true, i.cls().is_wide(), false, false);
        fn uwtof()  => e.fcnvt(i,  true,  true, i.cls().is_wide(), false,  true);
        fn sltof()  => e.fcnvt(i,  true,  true, i.cls().is_wide(),  true, false);
        fn ultof()  => e.fcnvt(i,  true,  true, i.cls().is_wide(),  true,  true);
        fn exts()   => e.fcnvt(i, false, false,  true,             false,  false);
        fn truncd() => e.fcnvt(i, false, false, false,             false, true);
        
        // :paste from arm/emit
        fn cas1() => {
            m := e.f.get_memory(i.arg&[0]);
            k := i.cls();
            t := rv_scratch_int;
            if i.to == QbeNull {
                i.to = t;
            };
            p, new, old_in, res := (i.arg&[1], m.index, m.base, i.to);
            real_res := res;
            if res != old_in {
                if res == new || res == p {
                    res = t;
                };
                e.emit(.copy, k, res, old_in, QbeNull);
            };
            rd, rs2, rs1 := (@as(u5) res.rv().bits(), @as(u5) new.rv().bits(), @as(u5) p.rv().bits());
            e.inst(@bits(0b00101, 0b1, 0b1, rs2, rs1, 0b01, i.cls().is_wide(), rd, Opcode.amo)); 
            if res != real_res {
                e.emit(.copy, k, real_res, res, QbeNull);
            };
        }
        
        fn byteswap() => e.unary(i, 0b011010111000, 0b101, .op_imm);  // rev8
        fn ones()     => e.unary(i, 0b011000000010, 0b001, @if(i.cls() == .Kw, .opw, .op_imm)); // cpop
        fn clz()      => e.unary(i, 0b011000000000, 0b001, @if(i.cls() == .Kw, .opw, .op_imm));
        fn ctz()      => e.unary(i, 0b011000000001, 0b001, @if(i.cls() == .Kw, .opw, .op_imm));
        fn extsb()    => e.unary(i, 0b011000000100, 0b001, .op_imm);
        fn extsh()    => e.unary(i, 0b011000000101, 0b001, .op_imm);
        fn extuh()    => e.unary(i, 0b000010000000, 0b100, .op32);
        fn extuw()    => e.unary(i, 0b000010000000, 0b000, .op32);  // add.uw 0
        
        @default => {
            if i.maybe_load() { size |
                addr, off := decode_mem(e, a0, i.arg&[1]);
                zero_ext := @is(i.op(), .loadub, .loaduh, .loaduw);
                if i.op() == .loaduw && i.cls() == .Kw {
                    // confusing! Kw comparisons aren't a seperate instruction, 
                    // ceqw is done as a 64 bit xor between the values and then comparing the result to zero, 
                    // so sign extension on Kw values needs to be consistant. 
                    zero_ext = false;
                };
                e.inst(load(i.to.rv(), addr, off, size.intcast(), zero_ext, i.cls().is_float()));
                return();
            };
            if i.maybe_store() { size |
                addr, off := decode_mem(e, i.arg&[1], Qbe.Null);
                e.inst(store(a0.rv(), addr, off, size.intcast(), @is(i.op(), .stores, .stored)));
                return();
            };
            if @is(i.op(), .shl, .shr, .sar, .rotl, .rotr) {
                e.encode_shift(i);
                return();
            }
        
            @panic("TODO: rv encoding for %", i.op());
        };
    }
}

fn unary(e: *EmitRv, i: *Qbe.Ins, funct12: i12, funct3: u3, opcode: Opcode) void = {
    e.inst(I(funct12, i.arg&[0].rv().bits(), funct3, i.to.rv().bits(), opcode))
}

fn fcnvt(e: *EmitRv, i: *Qbe.Ins, have_ints: bool, from_int: bool, double: bool, long: bool, unsigned: bool) void = {
    rm := @if(from_int || i.op() == .truncd, 0b000, 0b001);
    e.inst(encode_fcnvt(i.arg&[0].rv(), rm, i.to.rv(), have_ints, from_int, double, long, unsigned))
}

// TODO: factor out the encoding part so it can be used with AsmFunction, etc. 
fn encode_shift(e: *EmitRv, i: *Qbe.Ins) void = {
    rd, rs1 := (i.to.rv().bits(), i.arg&[0].rv().bits());
    con, rs2 := @match(e.f.get_int(i.arg&[1])) { 
        fn Some(imm) => {
            imm := imm.bit_and(@if(i.cls().is_wide(), 63, 31));
            (false, @as(u5) imm)
        }
        fn None() => (true, i.arg&[1].rv().bits());
    };
    right := @is(i.op(), .rotr, .shr, .sar);
    kw := i.cls() == .Kw;
    rot := @is(i.op(), .rotl, .rotr);
    arith := i.op() == .sar || rot;
    e.inst(@bits(0b0, arith, rot, 0b0000, rs2, rs1, right, 0b01, rd, 0b0, con, 0b1, kw, 0b011));
}

fn load_number(e: *EmitRv, off: i64, to: Qbe.Ref, k: Qbe.Cls) void = {
    fits_double_extend :: fn(i) => true
        && i >= -(1.shift_left(31)) - 1.shift_left(11) 
        && i <= 1.shift_left(31) - 1.shift_left(11) - 1;
    @debug_assert(k.is_int());
    @debug_assert_lt(to.val(), 32);
    if k == .Kw {
        // if the high bits don't matter, sign extended is easier to encode. 
        off = off.sign_extend_low32();
    };
    r := to.rv().bits();
    @if_else {
        @if(off.fits_sext(11)) => {
            e.inst(I(@as(i12) off, 0b00000, @as(u3) OpI.add, r, .op_imm));
        }
        @if(off.fits_double_extend()) => {
            lo, hi := double_sign_extend_imm(off);
            e.inst(U(hi, r, .lui));
            e.inst(I(lo, r, @as(u3) OpI.add, r, .op_imm));
        }
        @if(fits_double_extend(off.bit_not())) => {
            e.load_number(off.bit_not(), to, .Kl);
            e.inst(R(0b0100000, r, 0b00000, 0b110, r, .op));  // orn. r = 0 | ~r
        }
        @else => {
            // TODO: this is convoluted. just put it in memory and load it. 
            hi, lo := (off.shift_right_logical(32), off.bit_and(MAX_u32));
            
            if lo == 0 {
                e.load_number(hi, to, .Kw);
                e.emit(.shl, .Kl, to, to, e.f.getcon(32));
                return();
            };
            @debug_assert_ne(to, rv_scratch_int);
            
            if hi == 0 || k == .Kw {
                hi := @as(i64) lo.imm(31, 12);
                lo := bit_and(@as(i64) lo.imm(11, 0), 1.shift_left(13) - 1);
            
                e.load_number(lo, rv_scratch_int, .Kl);
                e.inst(U(hi, r, .lui));
                e.inst(R(0b0000100, rv_scratch_int.rv().bits(), r, 0b000, r, .op32));  // add.uw 
                return();
            };
            
            e.load_number(lo, to, .Kw);
            e.load_number(hi, rv_scratch_int, .Kw);
            
            e.emit(.shl, .Kl, rv_scratch_int, rv_scratch_int, e.f.getcon(32));
            // add.uw (zero extends rs1, thats why high bits of lo don't matter)
            e.inst(R(0b0000100, rv_scratch_int.rv().bits(), r, 0b000, r, .op32));
        };
    };
}

// TODO: sad how similar this is to the arm one
fn decode_mem(e: *EmitRv, a: Qbe.Ref, other: Qbe.Ref) Ty(RvReg, i12) = {
    e.fixmem(a&);
    offset := 0;
    addr := @match(rtype(a)) {
        fn RSlot() => {
            offset = e.f.slot(a);
            TMP(RvReg.FP)
        }
        fn RTmp() => a;
        fn RMem() => {
            m := e.f.get_memory(a);
            offset = m.offset&.bits();
            m.base
        }
        @default => panic("bad address type");
    };
    // TODO: isel address folding
    if rtype(other) == .RInt {
        offset += other.rsval().zext();
    };
    @debug_assert(offset.fits_sext(11), "decode_mem: %", offset);
    (addr.rv(), @as(i12) offset)
}

// TODO: factor together with arm/loadaddr_bits>
fn load_symbol(e: *EmitRv, c: *Qbe.Con, dest_reg: RvReg) void = {
    import := e.f.could_be_import(c);
    inc := @if(import, 0, c.bits());
    use_symbol(e.m, c.sym) { symbol |
        patch_addr := e.m.segment_address(.Code, e.m.current_offset(.Code));
        patch: Fixup = (patch_at = patch_addr, type = (InReg = (r = dest_reg.raw().zext(), increment = inc)));
        @match(symbol.kind) {
            fn DynamicPatched() => {
                patch.type.InReg.got_load = true;
                if e.m.goal.type != .JitOnly || !e.m.goal.got_indirection_instead_of_patches {
                    push_fixup(e.m, symbol, patch); 
                };
                e.inst(ebreak);
                e.inst(ebreak);
                push(e.pending_immediate_fixup_got&, (patch, symbol.id));
            }
            fn Local() => {
                dest := e.m.segment_address(symbol.segment, symbol.offset + inc);
                i_page, i_off := rel_off(dest, patch_addr, dest_reg, false);
                e.inst(i_page);
                e.inst(i_off);
                
                if e.m.goal.type == .Relocatable {
                    push_fixup(e.m, symbol, patch);
                };
            }
            // TODO: load from got now if got_indirection_instead_of_patches
            fn Pending() => {
                e.inst(ebreak);
                e.inst(ebreak);
                if e.m.goal.got_indirection_instead_of_patches {
                    patch.type.InReg.got_load = true;
                    push(e.pending_immediate_fixup_got&, (patch, symbol.id));
                };
                if e.m.goal.type != .JitOnly || !e.m.goal.got_indirection_instead_of_patches {
                    push_fixup(e.m, symbol, patch);
                }
            }
        };
    };
    
    if e.m.goal.type == .Relocatable {
        // :StupidEncodingsWinStupidRelocations
        // need to make a fake symbol so the lo reloc can reference the hi one. 
        // the linker can't do them seperatly because of the weird double sign extension (see ./bits.fr/rel_off). 
        // note also that the addend doesn't apply to the instruction reference so you really do need a unique symbol each time. 
        // TODO: represent that link more explicitly, having both places re-mangle this name is fragile. 
        // TODO: have a more sane thing for anonymous symbols that doesn't required allocating strings for names and hoping they're unique. 
        off := e.m.segments&[.Code]&.len() - 8;  // local offset not vaddr; important for repro
        anon := e.m.intern(@tfmt(".auipc%", off));
        use_symbol(e.m, anon) { s |
            s.kind = .Local;
            s.offset = off;
            s.segment = .Code;
            e.m.local_needs_reloc&.push(s.id);
        };
    };
    
    // TODO: decide if this is better or if i should just allow multiple GOT slots with different addends. 
    //       if i keep this way, use add immediate when it's small. 
    //       (same question applies to arm but im not sure if mach-o can do negative addends)
    if import {
        @debug_assert(TMP(dest_reg) != rv_scratch_int);
        e.emit(.copy, .Kl, rv_scratch_int, e.f.getcon(c.bits()), Qbe.Null);
        e.emit(.add, .Kl, TMP(dest_reg), TMP(dest_reg), rv_scratch_int);
    };
}

// TODO: add,sub,mul,div,rem have a W version
fn simple_op(e: *EmitRv, i: *Qbe.Ins, o: OpI, funct7: u7) void = {
    @debug_assert(i.cls().is_int(), "float %", i.op());
    a0, a1 := (i.arg&[0], i.arg&[1]);
    @debug_assert(isreg(i.to));
    i := if rtype(a1) == .RCon {
        c := e.f.get_constant(a1);
        off := c.bits();
        @debug_assert(c.type() == .CBits && off.fits_sext(11), "out of range %", off);
        @debug_assert_eq(0, @as(i64) funct7, "bad simple_op % should not have immediate", i.op());
        I(@as(i12) off, a0.rv().bits(), @as(u3) o, i.to.rv().bits(), .op_imm)
    } else {
        R(funct7, a1.rv().bits(), a0.rv().bits(), @as(u3) o, i.to.rv().bits(), .op)
    };
    e.inst(i);
}

fn float_op(e: *EmitRv, i: *Qbe.Ins, o: OpF, rm: u3) void = {
    to, a0, a1 := (i.to.rv().bits(), i.arg&[0].rv().bits(), i.arg&[1].rv().bits());
    size := @if(i.cls().is_wide(), 0b01, 0b00);
    e.inst(@bits(o, size, a1, a0, rm, to, Opcode.op_fp));  // R
}

fn float_cmp(e: *EmitRv, i: *Qbe.Ins, c: OpF_cmp, wide: bool) void = {
    to, a0, a1 := (i.to.rv().bits(), i.arg&[0].rv().bits(), i.arg&[1].rv().bits());
    e.inst(@bits(OpF.cmp, 0b0, wide, a1, a0, c, to, Opcode.op_fp));  // R
}

fn rv(r: Qbe.Ref) RvReg = {
    @debug_assert(r.isreg() || r == Qbe.Null/*zero register is a legal argument*/, "rv(non-reg: %)", r);
    @as(RvReg) @as(u8) r.val().trunc()
}

slot :: fn(f: *Qbe.Fn, r: Qbe.Ref) i64 = {
    s := rsval(r);
    -intcast @if(s < 0, 8 * s, f.slot - s)
}

fn inst(e: *EmitRv, opcode: u32) void = {
    code := e.m.segments&[.Code]&;
    ptr_cast_unchecked(u8, u32, code.next)[] = opcode;  // SAFETY: aligned becuase i don't do compressed instructions yet
    code.next = code.next.offset(4);
}

fn emit_store_off(e: *EmitRv, value: Qbe.Ref, addr: Qbe.Ref, off: i64) void = {
    @debug_assert(off <= 2047 && off >= -2048);
    e.inst(store(value.rv(), addr.rv(), @as(i12) off, 8, value.val() >= 32));
}

fn emit_load_off(e: *EmitRv, dest: Qbe.Ref, addr: Qbe.Ref, off: i64) void = {
    @debug_assert(off <= 2047 && off >= -2048);
    e.inst(load(dest.rv(), addr.rv(), @as(i12) off, 8, false, dest.val() >= 32));
}

fn emit_add_off(e: *EmitRv, dest: Qbe.Ref, a: Qbe.Ref, off: i64) void = {
    e.emit(make_ins(.add, .Kl, dest, a, e.f.getcon(off)));
}

fixup_rv64 :: fn(self: *QbeModule, symbol: *SymbolInfo, fixup: *Fixup, new_got_reloc: *?Fixup) void = {
    address := symbol.jit_addr;
    local  := symbol.kind == .Local;
    @match(fixup.type&) {
        fn Call(it) => {
            panic("TODO: rv fixup Call");
        }
        fn InReg(it) => {
            inst: []u32 = (ptr = u32.ptr_from_raw(fixup.patch_at), len = 2);
            if local {
                i_page, i_off := rel_off(address.offset(it.increment), fixup.patch_at, @as(RvReg) @as(u8) it.r.trunc(), false);
                inst[0] = i_page;
                inst[1] = i_off;
            } else {
                // todo: it would be fine, the problem is i only handle one new_got_reloc per symbol rn (its an option instead of a list) 
                @assert_eq(it.increment, 0, "TODO: [rv/emit.fr/fixup] offset from dynamic import %+%", symbol.name, it.increment);
                new_got_reloc[] = load_from_got(self, symbol, it.r, inst, address).or_else(=> new_got_reloc[]);
                it.got_load = true;
            };
        }
        fn DataAbsolute(it) => {
            rawptr.ptr_from_raw(fixup.patch_at)[] = address.offset(it.increment);
            m := self;
            @debug_assert(!(m.goal.got_indirection_instead_of_patches && m.goal.type == .JitOnly && address.is_null()), "you cant do that bro");
        }
        @default => @panic("unexpected fixup type for %", symbol.name);
    }
}

load_from_got :: fn(self: *QbeModule, symbol: *SymbolInfo, reg_encoding: i64, insts: []u32, jit_address: rawptr) ?Fixup = {
    new_got_reloc := ensure_got_slot(self, symbol, jit_address);
    @debug_assert(symbol.got_lookup_offset >= 0 && symbol.got_lookup_offset.mod(8) == 0);
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    here := u32.raw_from_ptr(insts.ptr);
    i_page, i_off := rel_off(got, here, @as(RvReg) @as(u8) reg_encoding.trunc(), true);
    insts[0] = i_page;
    insts[1] = i_off;
    new_got_reloc
}

fn next_inst_offset(self: *EmitRv) i64 = {
    code := self.m.segments&[.Code]&;
    ptr_diff(code.mmapped.ptr, code.next) - self.start_of_function
}

#use("@/backend/lib.fr");
#use("@/backend/rv64/bits.fr");
Abi :: import("@/backend/rv64/abi.fr");

// same as qbe
/*
+=============+
| varargs     |
|  save area  |
+-------------+
|  saved ra   |
|  saved fp   |
+-------------+ <- fp
|    ...      |
| spill slots |
|    ...      |
+-------------+
|    ...      |
|   locals    |
|    ...      |
+-------------+
|   padding   |
+-------------+
| callee-save |
|  registers  |
+=============+
*/
