Qbe :: import("@/backend/ir.fr");

Simplify :: import("@/backend/opt/simplify.fr");
#include_std("backend/util.fr");
#include_std("backend/emit.fr");
#use("@/backend/opt/ssa.fr");
#use("@/backend/opt/cfg.fr");
#use("@/backend/opt/spill.fr");
#include_std("backend/opt/rega.fr");
#use("@/backend/meta/dump.fr");
#include_std("backend/opt/alias.fr");
#use("@/backend/opt/inline.fr");
#use("@/backend/abi.fr");
#use("@/backend/meta/template.fr");
#use("@/lib/encoding/leb128.fr");
#use("@/lib/bit_fields.fr");
#use("@/lib/sys/sync/mutex.fr");
#use("@/lib/sys/process.fr");
Fold :: import("@/backend/opt/fold.fr");
OpTab :: import("@/backend/meta/ops.fr");

backend :: import_module("@/backend/lib.fr");
exports :: Type.scope_of(@struct {
    init_module: (@Fn(m: *QbeModule, goal: QbeTargetEnv) void) :init_default_module;
    compile_fn: (@Fn(f: *Qbe.Fn) void) :run_qbe_passes;
    compile_dat: (@Fn(m: *QbeModule, d: Dat2) void) :emit_data;
    compile_suspended: (@Fn(m: *QbeModule) void) :emit_suspended_inlinables;
    finish_writer: (@Fn(self: *Incremental.Writer, meta: Incremental.Meta, m: *QbeModule) [][]u8) :to_bytes;
    load_one: (@Fn(h: *Incremental.Header, f: *Qbe.Fn, it: Incremental'One(Incremental.Sym), syms: []Qbe.Sym) bool) :load;
    load_all: (@Fn(header: *Incremental.Header, m: *QbeModule) void) :compile_all_symbols;
    drop_module: (@Fn(m: *QbeModule) void) :drop;
    fold_op: (@Fn(rt_o: Qbe.O, rt_k: Qbe.Cls, a0: i64, a1: i64) i64) :Fold.do_fold;
    try_fold_op: (@Fn(op: Qbe.O, cls: Qbe.Cls, cl: *Qbe.Con, cr: *Qbe.Con) ?Qbe.Con) :Fold.try_fold;
    worker_thread: (@Fn(s: *CodegenShared) void) : codegen_thread_main;
    worker_start: (@Fn(m: *QbeModule, arena: Alloc, worker: CodegenWorker, threaded: bool, enqueue: @FnPtr(*CodegenShared, *CodegenEntry) void, join: CodegenWorker) *CodegenShared) : init_codegen_worker;
    worker_enqueue: (@Fn(*CodegenShared, *CodegenEntry) void) : enqueue_task;
    worker_join: (@Fn(s: *CodegenShared) void) :join_codegen_thread_impl;
    writer_finish_imports: (@Fn(writer: *Incremental.Writer, m: *QbeModule) void) : finish_imports;
});

   ENABLE_INLINING :: true;
ENABLE_ELIDE_SLOTS :: true;
    ENABLE_PROMOTE :: true;
 ENABLE_CONST_FOLD :: true &&
    matching_comptime_incremental_abi;
    // opt/fold.fr generates a module at comptime and includes that at comptime, 
    // so it can only be enabled when the host compiler has the same ir abi as the source code being compiled. 

matching_comptime_incremental_abi :: { 
    c := current_compiler_context(); 
    c'vtable'frc_module_magic_v == Incremental.MAGIC 
};
ENABLE_STATIC_MEMMOVE :: true 
    && matching_comptime_incremental_abi && ENABLE_INLINING;

// replaces the old fold and copy passes. 
ENABLE_GVN :: true
    && !(__driver_abi_version < 1349);
    
ALSO_OLD_COPY :: false 
    && ENABLE_GVN;

// doesn't help speed but makes the ir more readable in spill and rega. 
LATE_COPY_ELIM :: true;

Arm  :: import("@/backend/arm64/bits.fr");
Amd  :: import("@/backend/amd64/bits.fr");
Wasm :: import("@/backend/wasm/bits.fr");

fn init_default_module_dyn(m: *QbeModule, vtable: *ImportVTable, goal: QbeTargetEnv) void = {  
    @assert(ptr_cast_unchecked(@type vtable.init_default_qbe_module, i64, vtable.init_default_qbe_module&)[] != 0, "init_default_qbe_module is not enabled");
    {vtable.init_default_qbe_module}(QbeModule.raw_from_ptr(m), QbeTargetEnv.raw_from_ptr(goal&));
}

fn emit_qbe_included_dyn(m: *QbeModule, comp: *CompCtx, fns: [] FuncId, entry: ProgramEntry) [][]u8 = {  
    {comp.vtable.emit_qbe_included}(QbeModule.raw_from_ptr(m), comp, fns, entry)
}

QbeModule :: @rec @struct(
    save: ?@if(enable_incremental(), *Incremental.Writer, i64),
    types: RawList(Qbe.Typ),
    insb: *Array(Qbe.Ins, 1.shift_left(20)), // MaxInstructions
    curi: *Qbe.Ins,
    target: Qbe.Target,
    debug: i64,  // bitset of 1.shift_left(Qbe.DebugKey - "A".char())
    
    debug_out: *List(u8),
    symbols: []SymbolBucket,
    
    // stuff for emitting code.
    segments: EnumMap(SegmentType, SegmentCursor),
    fixups_locked := false,
    goal: QbeTargetEnv,
    start_of_writable_code: *u8,
    // these are special sections that live inside the main segments. 
    got: SegmentCursor,
    commands: SegmentCursor,
    wasm_types: SegmentCursor,
    
    exports: List(Qbe.Sym),
    imports: List(Qbe.Sym),
    local_needs_reloc: List(Qbe.Sym),  // OR for wasm: all local functions in order
    inlinable_but_referenced_later: List(Qbe.Sym),
    
    initialized: bool,
    currently_emitting_function: u32,
    
    forever_storage: Arena.Allocator,
    forever: LockedAlloc,
    symbol_memmove: Qbe.Sym,
    types_mutex: Mutex,
    last_function_end: *u8,
    __: Ty(i64, i64),  // :UpdateBoot
    
    wasm_function_type_count: i32,
    wasm_symbol_stackbase: Qbe.Sym,
    _0: u32,
    wasm_has_any_indirect_calls: bool,
    _1: bool,
    
    need_static_memmove: bool,
    have_static_memmove: bool,
    
    last_perfmap_end: *u8,
    
    debug_info: @struct {
        headers: List(u8);  // crash_report.AotDebugInfo.headers
        data: List(u8);     // crash_report.AotDebugInfo.payload
        last_off_func := 0;
        last_off_source := 0;
        enabled: bool;
        macho_function_starts: List(u8);
    },
    
    debug_out_storage: List(u8),
    need_write_protect: bool,
    use_map_jit_on_apple_silicon: bool,
    wasm_types_cache: HashMap([]u8, i32),
    libraries: RawList(Str),  // :SLOW but n=4 so like it's probably fine 
    scratch_for_blink_smc: List(u8),
    wasm_exec_handles: List(i64),
    
    codemap: ?*CodeMap,
    
    // waste of time factory. 
    // same blob of xml as in an xcode .entitlements file. 
    // see examples/os/vzf.fr for an example. 
    // i use this so little the api is just poke a value into this field. 
    macho_entitlements: Str,
);

// TODO: clearly this has to go in @/lib somewhere
#use("@/compiler/codemap.fr");

// :UpdateBoot
enable_incremental :: fn() bool #fold = !IS_BOOTSTRAPPING;

Incremental :: import("@/backend/incremental.fr");
#use("@/lib/collections/map.fr");

DisassemblerFunc :: @FnPtr(out: *List(u8), code: []u8, arch: Arch) void;
    
Goal :: QbeTargetEnv;
QbeTargetEnv :: @struct(
    arch: Arch,
    // - should we use apple's abi changes?
    // - should we emit Mach-O or ELF?
    os: Os,
    type: QbeOutputPurpose,
    
    // only matters when type=JitOnly. 
    // does the code need to be writable and executable at the same time? 
    // this lets you freely interweave running code in a module and adding more functions to it
    // if false, you have to call make_exec before trying to run the code. 
    // Regardless, be careful of cache coherency on aarch64, see comment on lib/sys/process.fr/clear_instruction_cache().
    expecting_wx := false,
    
    // When this is set, forward references to symbols always emit an inline load from the global 
    // offset table instead of using an instruction with a direct offset that must be patched later.
    // this is useful for jitting without needing to go back and modify code. 
    // false: faster runtime, true: faster comptime + more flexibility 
    // note: this setting is NOT fully respected on amd. it's more important on arm where cache coherency rules are more strict. 
    got_indirection_instead_of_patches := false,
    
    _0 := true,  // :updateboot this used to be link_libc

    // When outputting an exe directly, local symbol names are not included in the symbol table (because the os 
    // doesn't care about them and they make your binary larger). However, if you want to see readable names in 
    // a debugger, disassemble correct ranges of functions, etc., you can force symbol tables to be emitted. 
    // For wasm, this just exports everything so the browser gets names to show in the stack trace. 
    exe_debug_symbol_table := false,
    
    blink_smc := true,
    
    _1 := false,
    
    // call __libc_start_main and then call the entry point. 
    wrap_entry_for_linux_libc := false,
    
    have_entry_point := false,  // ==Exe    
    
    segment_size        := 1.shift_left(25),
    max_got_size        := 1.shift_left(15),
    _       := 1.shift_left(11),  // :UpdateBoot
    commands_size_guess := 16384,
    chained_size_guess  := 2000000,
    
    // called for debug["D"]. the default one just execs 'llvm-mc' and hopes for the best. 
    dis: DisassemblerFunc = llvm_mc_dis,
    
    default_wasm_import_module := "env",
    entry_point := "main",
    
    // for https://github.com/kengorab/abra-lang
    HACK_convert_illegal_ret_to_hlt := false,
    
    // if this is true, when creating the WebAssembly.Memory in the browser you need to add `shared: true` to the arguments. 
    // this only works if window.crossOriginIsolated=true which requires setting http headers:
    // `Cross-Origin-Opener-Policy: same-origin` and `Cross-Origin-Embedder-Policy: require-corp` (or `credentialless`). 
    // it must also be https not http. pain in the ass. 
    //
    // web people have a *Meltdown* about anything that could be used to create high precision timers 
    // because they're afraid there might be a spooky ghost (or as some would say... *Spectre*)
    //
    // this also makes the data section mode=passive. the main thread must do memory.init. 
    // (if you have mode=active, your static variables will be reset when you spawn a new thread). 
    wasm_shared_memory := true,
);

QbeOutputPurpose :: @enum(i64) (Cached, JitOnly, Exe, Relocatable, Dynamic, CachedEarly);

fn text_padding(m: *QbeModule) i64 = 
    m.goal.commands_size_guess;

fn init_default_module(m: *QbeModule, goal: QbeTargetEnv) void = {
    :: import("@/lib/sys/subprocess.fr");  // this includes it in comptime_compute_self_hash even when -syscalls
    
    init(m, general_allocator(), goal);
    
    // :DefaultToLibc
    // just make it work with the new library tracking. 
    // assume any imports are from libc like before.
    m.new_library("libc");
}

fn init(m: *QbeModule, gpa: Alloc, goal: QbeTargetEnv) void = {
    init_common(m, gpa, goal);
    m.set_target_vtable();
}

// 
// :JitMemProtect (use_map_jit_on_apple_silicon)
//
// They won't let you have RWX pages, but if you call pthread_jit_write_protect_np, you can have pages that 
// are writable on one thread but executable on another by passing MAP_JIT to mmap, which would be great, 
// except that you control the protection for ~every~ MAP_JIT page together per thread. So you can't 
// have a jitted program create a new MAP_JIT area and write code into it. A work around would be having the 
// jitted code write into other memory and then call a fully AOT function that toggles protection and copies 
// the into the MAP_JIT memory. This also isn't a problem if there's a clear boundery between when you're creating 
// code and executing it because you can just call mprotect at the end when you're done. 
//
// The main take away is the franca compiler wants to use this so comptime franca code cannot set expecting_rwx to true on aarch64 macos. 
//

fn init_common(m: *QbeModule, gpa: Alloc, goal: QbeTargetEnv) void = {
    // gpa needs to not be temp() because we reset temp()
    m[] = QbeModule.zeroed();
    m.goal = goal; goal := m.goal&;
    goal.have_entry_point = goal.type == .Exe;
    
    @if(TODOWASM)
    if m.goal.arch == .wasm32 {
        m.goal.commands_size_guess *= 8;
        if goal.type == .JitOnly {
            Jit :: import("@/backend/wasm/jit.fr");
            FR_wasm_jit_event :: fn(event: Jit.Event) i64 #weak #libc;
            if goal.wasm_shared_memory {
                m.target.wasm_jit_event = FR_wasm_jit_event;
            } else {
                m.target.wasm_jit_event = Jit.impl.run;
            };
        };
    };
    
    segment_size := goal.segment_size;
    m.codemap = .None;
    m.save = .None;
    m.forever_storage = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    m.forever = (parent = m.forever_storage&.borrow());
    forever := m.forever&.borrow();
    @if(enable_incremental())
    if @is(goal.type, .Cached, .CachedEarly) {
        writer := forever.boxed(Incremental.Writer, init(gpa));
        m.save = (Some = writer);
    };
    goal.expecting_wx = goal.expecting_wx && goal.type == .JitOnly && goal.arch != .wasm32;
    m.use_map_jit_on_apple_silicon = goal.expecting_wx && goal.arch == .aarch64 && goal.os == .macos;
    
    if goal.type == .JitOnly {
        @assert_eq(goal.arch, query_current_arch(), "trying to jit to the wrong architecture");
        msg :: "trying to jit to the wrong os.\nthe abi will be wrong if you try to call external functions (like apple disagrees about how varargs should be passed).";
        @if(goal.arch == .aarch64) @assert_eq(goal.os, query_current_os(), msg);
    };
    need_segments := !@is(goal.type, .Cached, .CachedEarly);
    
if need_segments {
    mem := if !m.use_map_jit_on_apple_silicon {
        // relies on page allocator always giving zeroed memory
        page_allocator.alloc_uninit(u8, segment_size * 3)
    } else {
        F :: LibcType(.macos).MapFlag; // only apple makes us the MAP_JIT shenanigans. 
        fd: Fd = (fd = -1);
        flag :i64= bit_or(Posix'MAP'PRIVATE, Posix'MAP'ANON); 
        
        // Reserve the whole contiguous area we want.  
        // It seems that MAP_JIT and then MAP_FIXED over it is fine, but the reverse is not. 
        prot_WX :i64= bit_or(Posix'PROT'WRITE, Posix'PROT'EXEC);
        flag_WX :i64= bit_or(flag, Posix'MAP'JIT);  // :JitMemProtect .Jit means one thread will be able to write and one will be able to execute. 
        ptr := Posix'mmap(zeroed(rawptr), segment_size * 3, prot_WX, flag_WX, fd, 0) || {
            @panic("mmap Jit failed")
        };
        
        // Now remap over the data sections so they're not executable and both threads can write to them. 
        prot_RW :i64= bit_or(Posix'PROT'READ, Posix'PROT'WRITE);
        flag_RW := bit_or(flag, Posix'MAP'FIXED); // .Fixed means the address hint will not be ignored. 
        ptr_rw := ptr.offset(segment_size);
        res := or Posix'mmap(ptr_rw, segment_size * 2, prot_RW, flag_RW, fd, 0) {
            @panic("mmap Fixed failed")
        };
        
        // So now we have a contigous chunk where the first third is MAP_JIT and the rest is normal.
        mem: []u8 = (ptr = u8.ptr_from_raw(ptr), len = segment_size * 3);
        // TODO: if we wanted to allow jit and then emit macho from the same module, you'd have to remap the `commands: SegmentCursor` as well. 
        mem
    };
    
    if !m.use_map_jit_on_apple_silicon && goal.expecting_wx {
        // Only the code segment needs to be executable. 
        prot := bit_or(@as(i64) MapProt.Exec, bit_or(@as(i64) MapProt.Read, @as(i64) MapProt.Write));
        Syscall'mprotect(u8.raw_from_ptr(mem.ptr), segment_size, prot) 
            || @panic("mprotect failed while creating Module");
    };
   
    // :SegmentsAreTheSameSize 
    // order matters!
    make_segment(mem, .Code, 0, segment_size);  // :CodeIsFirst
    make_segment(mem, .ConstantData, segment_size, segment_size*2);
    make_segment(mem, .MutableData, segment_size*2, segment_size*3);
    
    // No backing allocation. It's just for aot for now. 
    m.segments&[.ZeroInitData].mmapped.ptr = mem.ptr.offset(segment_size * 3);
    m.segments&[.ZeroInitData].next = m.segments&[.ZeroInitData].mmapped.ptr;
    
    make_segment :: fn(mem: []u8, s: SegmentType, start: i64, end: i64) void => {
        m.segments&.insert(s, (mmapped = mem.slice(start, end), next = mem.index(start)));
    };
};

    m.initialized = true;
    m.exports = list(gpa);
    m.imports = list(gpa);
    m.local_needs_reloc = list(gpa);
    m.inlinable_but_referenced_later = list(gpa);
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    m.debug_info = (
        headers = list(gpa), 
        data = list(gpa),
        // the debug format is based on machine code offsets so it only matters if we're actually generating machine code. 
        // you still want it when jitting tho because the stack trace report thingy uses it
        enabled = !@is(goal.type, .Cached, .CachedEarly),
        macho_function_starts = list(gpa),
    );
    // :leak
    m.wasm_types_cache      = init(gpa);
    if need_segments {
        // :GotAtConstantDataBase
        // TODO: this is kinda ass
        next := m.segments&[.ConstantData].next&;
        m.got = (mmapped = (ptr = next[], len = goal.max_got_size), next = next[]);
        next[] = next[].offset(goal.max_got_size); 
        
        if goal.type == .JitOnly {
            range(0, goal.max_got_size / 8) { i |
                tried_to_call_uncompiled :: fn() void = {
                    panic("ICE: jitted code tried to call an uncompiled function through __GOT");
                };
                ptr_cast_unchecked(u8, @FnPtr() void, m.got.mmapped.ptr.offset(i * 8))[] = tried_to_call_uncompiled;
            };
        };
        
        next := m.segments&[.Code].next&;
        m.commands = (mmapped = (ptr = next[], len = m.goal.commands_size_guess), next = next[]);
        next[] = next[].offset(m.goal.commands_size_guess); 
        m.start_of_writable_code = next[];
        m.last_perfmap_end = m.start_of_writable_code;
        
        if m.goal.arch == .wasm32 {
            // :WasmZeroPage Skip the low page so we don't live in crazy town where the null pointer is valid. 
            next := m.segments&[.MutableData].next&;
            next[] = next[].offset(wasm_page_size); 
            // TODO: make this size configurable on .goal
            mem := forever.alloc_uninit(u8, 1.shift_left(11));
            m.wasm_types = (mmapped = mem, next = mem.ptr);
            
            // :WasmTypeZeroIsVoidToVoid HACK
            s := m.wasm_types&;
            s.reserve(Wasm.ValType)[] = .ResultStart;
            s.reserve(u8)[] = 0;
            s.reserve(u8)[] = 0;
            i := import("@/backend/wasm/emit.fr")'pack_wasm_result_type_outlined(m, 0);
            @assert_eq(i, 0);
            
            if m.goal.type == .JitOnly {
                m.wasm_exec_handles = list(forever);
            };
        };
    };
    
    m.goal.blink_smc = goal.blink_smc && goal.expecting_wx && goal.arch == .x86_64;
    if m.goal.blink_smc {
        m.scratch_for_blink_smc = list(func_size_limit_bytes, forever);
    };
    
    m.debug_out_storage = list(gpa);
    m.debug_out = m.debug_out_storage&;
    
    m.insb   = forever.box_uninit(@type m.insb[]);
    
    m.curi = m.insb.index_unchecked(Qbe.MaxInstructions);

    m.symbol_memmove = m.intern("__franca_builtin_static_memmove");
    @if(!IS_BOOTSTRAPPING)
    if m.goal.type == .JitOnly {
        // when jitting it's not a big deal to add imports, so don't even bother recompiling the tiny function.
        // doing this up front instead of in emit_suspended_inlinables means it works when got_indirection_instead_of_patches. 
        m.have_static_memmove = true;
        use_symbol(m, m.symbol_memmove) { s |
            f: @Fn(rawptr, rawptr, i64) void : copy_bytes;
            m.put_jit_addr(m.symbol_memmove, @as(rawptr) f);
        };
    };
    
    // :leak
    m.libraries&.push("", general_allocator());
    
    if !@is(m.goal.type, .Exe, .Dynamic) {
        m.goal.exe_debug_symbol_table = false;
    };
    if m.goal.exe_debug_symbol_table && m.goal.os == .macos {
        leb128_unsigned(m.debug_info.macho_function_starts&, m.text_padding());
    };
    
    // TODO: annoying that for relocatable or .frc, it could reasonably be a whole program (that should have wrap)
    //       or a library (that shouldn't) so i can't really just do the right thing automatically. 
    //       since i do emit_linux_start in elf/emit, it won't happen for .frc. 
    //       and when doing Relocatable, even if this is the one containing the entry point, you have to use a linker anyway so it can give you a _start. 
    if goal.os != .linux || goal.arch == .wasm32 || goal.type != .Exe {
        goal.wrap_entry_for_linux_libc = false;
    };
}

fn set_target_vtable(m: *QbeModule) void = {
    EmitWasm :: import("@/backend/wasm/emit.fr");
    t := m.target&;
    t.apple = m.goal.os == .macos;
    @match(m.goal.arch) {
        fn aarch64() => import("@/backend/arm64/abi.fr")'fill_target_arm(t);
        fn x86_64()  => import("@/backend/amd64/sysv.fr")'fill_target_amd(t);
        fn wasm32()  => {
            @assert(!@is(m.goal.type, .Cached), "TODO: cached wasm. need to save types more?");  // CachedEarly is fine tho
            t.finish_passes = finish_qbe_passes_wasm;
            t.emit_fn = EmitWasm.emit_func_wasm32;
            t.fixup = EmitWasm.fixup_wasm32;
            m.wasm_symbol_stackbase = m.intern("__stackbase");
        }
        fn rv64() => @if(IS_BOOTSTRAPPING, @panic("riscv target is disabled"),
            import("@/backend/rv64/abi.fr")'fill_target_rv64(t));
    };

    t.gpr = 1.shift_left(t.ngpr.intcast()).sub(1).shift_left(t.gpr0.intcast()).bitcast();
    t.fpr = 1.shift_left(t.nfpr.intcast()).sub(1).shift_left(t.fpr0.intcast()).bitcast();
    @debug_assert_eq(t.gpr.bit_and(t.fpr), 0);
    t.nrsave = init(@slice(
        @as(i32) t.caller_saved.bit_and(t.gpr).count_ones().bitcast(),
        t.caller_saved.bit_and(t.fpr).count_ones().bitcast(),
    ));
    @debug_assert_eq(@as(i64) t.caller_saved.count_ones().zext(), zext(t.nrsave&[0] + t.nrsave&[1]));
    
    @if(enable_incremental())
    if m.goal.type == .CachedEarly {
        t.finish_passes = (fn(f: *Qbe.Fn) void = {  
            save := f.globals.save.unwrap(); 
            save.push(f, false); 
        });
    };
    
    t.finish_module = @if_else {
        @if(@is(m.goal.type, .Cached, .CachedEarly))  => @if(enable_incremental(), Incremental'finish_module, 
            panic("tried to init cached module but that feature was disabled at comptime"));
        @if(m.goal.arch == .wasm32)  => EmitWasm.output_wasm_module;
        @if(m.goal.type == .JitOnly) => (fn(m: *QbeModule) [][]u8 = empty());
        @if(m.goal.os == .macos)     => import("@/backend/macho/emit.fr").output_macho;
        @if(m.goal.os == .linux)     => import("@/backend/elf/emit.fr").output_elf;
        @else => panic("TODO: we don't support this target");
    };
} 

fn init_empty_for_template_only(m: *QbeModule, gpa: Alloc) void = {
    m[] = QbeModule.zeroed();
    m.initialized = true;
    m.forever_storage = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    m.forever = (parent = m.forever_storage&.borrow());
    forever := m.forever&.borrow();
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    mem := forever.alloc_uninit(Qbe.Ins, Qbe.MaxInstructions);
    ::?*Array(Qbe.Ins, Qbe.MaxInstructions);
    m.insb = mem.as_array().unwrap();
    m.curi = m.insb.index_unchecked(Qbe.MaxInstructions);
    m.debug_out_storage = list(forever);
    m.debug_out = m.debug_out_storage&;
    m.codemap = .None;
}

// TODO: this should go away eventually 
fn show_backend_stats() bool = 
    Foreign.ENV.FRANCA_LOG_STATS.get_environment_variable().is_some();

fn drop(m: *QbeModule) void = {
    m.write_perfmap_symbols();
    if m.wasm_exec_handles.len != 0 {
        (m.target.wasm_jit_event)(Close = (events = m.wasm_exec_handles.items()));
    };

    @run assert_eq(enum_count(SegmentType), 4);
    fst := m.segments.data&[0].mmapped;
    mem := fst.ptr.slice(fst.len * 3);  // ZeroInitData has no allocation
    if m.permap_enabled() {
        // :LEAK
        // if i free the memory it can be reused for a new module which makes the profile completely useless. 
        Syscall'mprotect(u8.raw_from_ptr(fst.ptr), fst.len, 0); 
        page_allocator.dealloc(u8, mem.rest(fst.len)); 
    } else {
        page_allocator.dealloc(u8, mem); 
    };
    @if(enable_incremental()) if m.save { save |
        save.drop();
    };
    for m.libraries { it |
        general_allocator().dealloc(u8, it);
    };
    drop(m.libraries&, general_allocator());
    drop(m.exports&);
    drop(m.imports&);
    drop(m.local_needs_reloc&);
    drop(m.inlinable_but_referenced_later&);
    drop(m.debug_info.headers&);
    drop(m.debug_info.data&);
    drop(m.debug_info.macho_function_starts&);
    drop(m.debug_out_storage&);
    drop(m.wasm_types_cache&);
    m.forever_storage&.deinit();
    m[] = zeroed(@type m[]);  // just to make sure you crash if you try to use it again. 
}

// returns false if suspended for inlining 
run_qbe_passes_common :: fn(f: *Qbe.Fn) bool = {
    ::ptr_utils(*Qbe.Blk);
    #use("@/backend/opt/mem.fr");
    
    ::if(Qbe.Ref);
    m := f.globals;
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "function module not set");
    
    flush_debug(f.globals);
    when_debug_printfn(f, .Parsing, "\n## After parsing:\n");
     
    replace_switches(f);
    
    @if(m.goal.HACK_convert_illegal_ret_to_hlt)
    @if(f.ret_cls != .Kx)
    for_blocks f { b |
        if b.jmp.type.is_ret() && b.jmp.arg == Qbe.Null {
            b.jmp.type = .hlt;
        };
    };
    
    f.set_block_id();  // fails_typecheck uses them in bit sets and its bad vibes to only mutate when debug checks enabled
    if fails_typecheck(f) { err | 
        printfn(f, f.globals.debug_out);
        @panic("IR failed typecheck ($%)\n %", f.name(), err);
    };
    
    extsb_parargret(f);
    
    DEBUG :: safety_check_enabled(.DebugAssertions);
    
    inlcalls(f);
    fillcfg(f);
    
    // just to be less flaky, make for_blocks order more deterministic 
    // instead of depending on the order blocks were made in the input. 
    set_link_from_rpo(f);
    
    fill_use(f);
    promote(f); // optional
    convert_to_ssa(f);
    @if(DEBUG) fill_use(f);
    ssacheck(f); // debug
    fillalias(f); // !
    
    ::if(Qbe.O);
    import("@/backend/opt/load.fr")'loadopt(f); // optional
    fill_use(f);
    fillalias(f); // for coalesce
    coalesce(f); // optional
    fill_use(f); 
    ssacheck(f); // debug
    
    @if(@run(!ENABLE_GVN)) {
        import("@/backend/opt/copy.fr")'copy_elimination(f);
        fill_use(f);
        Fold'fold_constants(f);
        fillcfg(f);
    };
    @if(ALSO_OLD_COPY) {
        import("@/backend/opt/copy.fr")'copy_elimination(f);
        fill_use(f);
    };
    
    @if(ENABLE_GVN) {
        import("@/backend/opt/gvn.fr")'gvn(f);
        fillcfg(f);
        fill_use(f);
        filldom(f);
        import("@/backend/opt/gcm.fr")'gcm(f);
        @if(DEBUG) fill_use(f);
        ssacheck(f);
    };
    
    if save_for_inlining(f) {
        // all uses might be inlined so we suspend processing until we know you took a function pointer later. 
        when_debug(f, .Inlining, fn(out) => @fmt(out, "\n# Suspending: %\n", f.name()));
        return(false);
    };
    true
};

run_qbe_passes :: fn(f: *Qbe.Fn) void = {
    mark := mark_temporary_storage();
    if run_qbe_passes_common(f) {
        {f.globals.target.finish_passes}(f);
    };
    reset_temporary_storage(mark);
};

fn finish_qbe_passes_native(f: *Qbe.Fn) void = {
    ::[]u64;
    if f.mem.first.is_null() {
        f.mem = new(0);
    };
    m := f.globals;
    with m.types_mutex& {
        {m.target.abi1}(f);
    };
    import("@/backend/opt/blit.fr")'elide_blits(f); 
    
    Simplify'simplify(f);
    fillpreds(f);
    fill_use(f); 
    
    fillrpo(f);   // for fillalias. 
    fillalias(f); // for late slots mem opt, escaping slots are tracked at the beginning of isel. 
        
    // arm isel inserts RMem (with different meaning from x64) for mul/store and RInt for load.
    {m.target.isel}(f);
    import("@/backend/opt/slots.fr")'elide_abi_slots(f); 
    
    fillrpo(f);
    fill_liveness(f);
    fillloop(f); // optional
    fillcost(f); // optional
    spill(f);
    
    register_allocation(f);
    fillrpo(f);
    simplify_jump_chains(f); // optional
    fillpreds(f);
    fillrpo(f);
    
    // :LinkIsNowRpo
    set_link_from_rpo(f);
    
    emit_fn(f);
}

fn finish_qbe_passes_wasm(f: *Qbe.Fn) void = {
    if f.mem.first.is_null() {
        f.mem = new(0);
    };
    m := f.globals;
    with m.types_mutex& {
        import("@/backend/wasm/abi.fr")'wasm_abi(f);
    };
    import("@/backend/opt/blit.fr")'elide_blits(f); 

    simplify(f);
    fillpreds(f);
    fill_use(f); 
    
    //fillrpo(f);
    //fillalias(f);
    import("@/backend/wasm/isel.fr")'wasm_isel(f);
    
    // TODO: elide_abi_slots
    //       (requires isel to use RSlot instead of directly converting to SP math)
    
    // TODO: maybe we still want liveness info even though we're not doing register allocation?
    
    // TODO: you want to do this but it breaks Blk.wasm_type. can it just go before isel? but still need to add something because split insts from jmp arg needs to be treated like phis. 
    //simplify_jump_chains(f); // optional
    
    
    fill_use(f); 
    
    emit_fn(f);
}

fn fails_typecheck(f: *Qbe.Fn) ?Str = {
    @if(!::safety_check_enabled(.DebugAssertions)) return(.None);
    
    fillpreds(f);
    pb  := init_bitset(f.nblk.zext());
    ppb := init_bitset(f.nblk.zext());
    for_blocks f { b | 
        for_phi b { p |
            if rtype(p.to) != .RTmp {
                return(Some = @tfmt("phi destination must be a tmp"));
            };
            f.get_temporary(p.to)[].cls = p.cls;
        };
        for_insts_forward b { i | 
            if rtype(i.to) == .RTmp {
                t := f.get_temporary(i.to);
                if clsmerge(t.cls&, i.cls()) {
                    return(Some = @tfmt("temporary %%.% is assigned with multiple types (% vs %)", "%", t.name(), i.to.val(), t.cls, i.cls()));
                };
            };
        };
    };
    for_blocks f { b |
        bszero(pb&);
        range(0, b.npred.zext()) { n |
            bsset(pb&, b.pred[n].id.zext());
        };
        for_phi b { p |
            bszero(ppb&);
            t := f.get_temporary(p.to);
            range(0, p.narg.zext()) { n |
                k := t.cls;
                if bshas(ppb&, p.blk[n].id.zext()) {
                    return(Some = @tfmt("multiple entries for @% in phi %%", p.blk[n].name(), "%", t.name()));
                };
                if !usecheck(p.arg[n], k, f) {
                    return(Some = @tfmt("invalid type (%) for operand %% in phi %%", k.raw(), "%", f.get_temporary(p.arg[n]).name(), "%", t.name()));
                };
                bsset(ppb&, p.blk[n].id.zext());
            };
            if !bsequal(pb&, ppb&) {
                return(Some = @tfmt("predecessors not matched in phi %%", "%", t.name()));
            };
        };
        j := -1;
        for_insts_forward b { i |
            continue :: local_return;
            error :: fn(e: Str) => return(report(f, b, j, e));
            report :: fn(f: *Qbe.Fn, b: *Qbe.Blk, i: i64, e: Str) ?Str = {
                out := e.clone(temp());
                @fmt(out&, "\nat @%[%] (", b.name(), i);
                printins(f, b.ins.index(i), out&);
                @fmt(out&, ")");
                (Some = out.items())
            };
            
            j += 1;
            no_r := i.to == Qbe.Null;
            if no_r != no_result(i.op()) && i.op() != .call {
                error "bad output ref";
            };
            range(0, 2) { n |
                continue :: local_return;
                
                k := argcls(i, n);
                r := i.arg&[n];
                if k == .Ke {
                    error "invalid instruction output type";
                };
                word := if(n == 1, => "second", => "first");
                if rtype(r) == .RType {
                    N := @match(i.op()) {
                        fn parc() => 0;
                        fn argc() => 0;
                        fn call() => 1;
                        @default => -1;
                    };
                    if n != N {
                        error @tfmt("% operand cannot be RType", word);
                    };
                    continue();
                };
                if rtype(r) != .RNull && k == .Kx {
                    error @tfmt("no % operand expected", word);
                };
                if rtype(r) == .RNull && k != .Kx {
                    error @tfmt("missing % operand", word);
                };
                if !usecheck(r, k, f) {
                    error @tfmt("invalid type for % operand", word);
                };
            };
            if is_store(i.op()) && i.to != QbeNull { 
                error "store instruction must not have result";
            };
            if i.op() == .blit1 && rtype(i.arg&[0]) != .RInt {
                error @tfmt("blit1 expected a0 to be RInt not %", rtype(i.arg&[0]));
            };
            if i.op() == .vastart && !f.vararg {
                error "used vastart in non-variadic function";
            };
        };
    
        r := b.jmp.arg;
        jump_error := false;
        if is_ret(b.jmp.type) {
            scalar := b.jmp.type != .retc;
            k: Qbe.Cls = @if(!scalar, .Kl, @if(b.jmp.type.raw() >= Qbe.J.retsb.raw(), .Kw, b.jmp.type.cls()));
            jump_error = !usecheck(r, k, f);
            @if(!jump_error) if !scalar {
                jump_error = rtype(f.retty) != .RType || f.ret_cls != .Ke;
            } else {
                if b.jmp.type == .ret0 {
                    jump_error = f.ret_cls != .Kx || b.jmp.arg != QbeNull;
                } else {
                    jump_error = !usecheck(f.ret_cls, k) || b.jmp.arg == QbeNull;
                };
            };
        };
        if b.jmp.type != .jnz && !b.s2.is_null() {
            return(Some = @tfmt("@%: % cannot have a second target", b.name(), b.jmp.type));
        };
        if b.jmp.type != .jnz && b.jmp.type != .jmp && !b.s1.is_null() {
            return(Some = @tfmt("@%: % cannot have a target", b.name(), b.jmp.type));
        };
        if b.jmp.type == .jnz {
            jump_error = !usecheck(r, .Kw, f) || !@is(rtype(b.jmp.arg), .RTmp, .RCon);
            if b.s1.is_null() || b.s2.is_null() {
                return(Some = @tfmt("@%: jnz must have targets", b.name()));
            };
        };
        if b.jmp.type == .jnz && b.s1.is_null() {
            return(Some = @tfmt("@%: jmp must have target", b.name()));
        };
        if @is(b.jmp.type, .hlt, .jmp) {
            jump_error = b.jmp.arg != QbeNull;
        };
        if jump_error {
            return(Some = @tfmt("invalid type for jump argument in block @%", b.name()));
        };
        
        @debug_assert_ne(b.jmp.type, .switch, "B");
        if !b.s1.is_null() && b.s1.jmp.type == .Jxxx {   
            return(Some = @tfmt("block @% is used undefined", b.s1.name()));
        };
        if !b.s2.is_null() && b.s2.jmp.type == .Jxxx {
            return(Some = @tfmt("block @% is used undefined", b.s2.name()));
        };
    };
    .None
}

fn usecheck(r: Qbe.Ref, k: Qbe.Cls, f: *Qbe.Fn) bool = {
    rtype(r) == .RTmp || return(true);
    t := f.get_temporary(r);
    usecheck(k, t.cls)
}

fn usecheck(k_ref: Qbe.Cls, k_tmp: Qbe.Cls) bool #inline = 
    k_tmp == k_ref || (k_tmp == .Kl && k_ref == .Kw);

wasm_page_size :: 65536;

// This is not what you want if you care about cross compiling!
Dyn :: import("@/lib/dynamic_lib.fr");
fill_from_libc :: fn(self: *Qbe.Module) void = {
    for find_os_libc_dylib() { libc_path | 
        if Dyn'open(libc_path) { libc |
            self.fill_pending_dynamic_imports(libc);
        };
    };
    seal_imports(self);
};

// :SLOW insanely inefficient way of doing this. especially when someone tries to add multiple libs. 
// TODO: store a list of the pending ones so don't have to iterate them all. 
// This is not what you want if you care about cross compiling!
fill_pending_dynamic_imports :: fn(self: *Qbe.Module, lib: Dyn.Handle) void = {
    for_symbols self { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending && (symbol.fixups.len != 0 || query_current_arch() == .wasm32) {
            self.imports&.push(id);
            if lib.get(symbol.name) { found |
                self.put_jit_addr(id, found);
            };
        };
    };
};

seal_imports :: fn(self: *Qbe.Module) void = {
    if TODOWASM && query_current_arch() == .wasm32 {
        self.imports&.clear();
    };
    
    // :UnfilledGotAccessStillNeedsPatch
    @debug_assert_eq(self.goal.type, .JitOnly, "fill_from_libc is only sane when jitting because i want to be able to cross compile.");
    unfilled := 0;
    for self.imports& { id | 
        use_symbol(self, id) { s |
            if s.kind == .Pending {
                if s.strong {
                    @eprintln("unfilled strong import: %", s.name);
                    unfilled += 1;
                } else {
                    self.put_jit_addr(id, zeroed(rawptr));
                };
            }
        };
    };
    @assert_eq(unfilled, 0, "unfilled strong imports");
    
    self.flush_debug();
};

// 
// This api makes it easy to have your frontend and the backend run on seperate threads. 
// 
// Call enter_task, generate some code, and return a CodegenTask which will be sent to the backend thread. 
// Inside enter_task, temp() is tied to the task so the generated code can be in temp memory but you can't let any of your own temp allocations escape. 
// Most backend functions are not thread safe, so you can't mix this threaded api with the direct calls it replaces.  
// Sym intern()/str() on either thread is safe. When generating code, make sure to use emit(*Blk) instead of emit(*Fn). 
// Call join_codegen_thread() at the end to finalize everything. 
// 

CodegenShared :: @rec @struct(
    entries: []CodegenEntry,
    // It's important that these are processed in the order they're created so it's not a race to get a reproducible build. 
    pipe: QRing'Cycle(*CodegenEntry),

    no_more_functions: bool,
    no_more_codegen: u32,
    m: *QbeModule,
    codegen_time: i64,
    // TODO: allow multiple backend threads. 
    thread: *Thread,
    threaded: bool,
    alloc: Alloc,
    enqueue: @FnPtr(*CodegenShared, *CodegenEntry) void,
    join: @FnPtr(*CodegenShared) void,
);

CodegenTask :: @tagged(
    Dead: void,
    Func: *Qbe.Fn,
    Asm: @struct(lnk: *Qbe.Lnk, code: *MultiArchAsm),
    JitImport: @struct(lnk: *Qbe.Lnk, addr: rawptr),
    Bounce: @struct(lnk: *Qbe.Lnk, target: Qbe.Sym),
    Shim: @struct(f: *Qbe.Fn, shim_for: Qbe.Sym),
    AotVar2: []Dat2,
    // :UpdateBoot
    SetLibrary: []Ty(@slice(Qbe.Sym, Str, bool)), // id, lib, weak
    FromCache: @struct(f: *Qbe.Fn, did_regalloc: bool),
    SaveSign: *Qbe.Fn,
    Done: void,
    All: *Incremental.Header,
);

CodegenEntry :: @struct(
    arena_storage: Arena.Allocator, 
    arena: Alloc,
    task: CodegenTask,
    logging := "",
);

fn init_codegen_worker(m: *QbeModule, threaded: bool) *CodegenShared = {
    a := general_allocator();
    @if(IS_BOOTSTRAPPING, 
        backend'worker_start(m, a, codegen_thread_main, threaded, enqueue_task, join_codegen_thread_impl),
        backend'worker_start(m, a, backend.worker_thread, threaded, backend.worker_enqueue, backend.worker_join),
    )
}

fn init_codegen_worker(m: *QbeModule, threaded: bool, vtable: *ImportVTable) *CodegenShared = {
    worker := bit_cast_unchecked(@type vtable.codegen_thread_main, CodegenWorker, vtable.codegen_thread_main);
    join := bit_cast_unchecked(@type vtable.codegen_thread_join, CodegenWorker, vtable.codegen_thread_join);
    enqueue := bit_cast_unchecked(@type vtable.enqueue_task, @FnPtr(*CodegenShared, *CodegenEntry) void, vtable.enqueue_task);
    init_codegen_worker(m, general_allocator(), worker, threaded, enqueue, join)
}

CodegenWorker :: @FnPtr(s: *CodegenShared) void;
fn init_codegen_worker(m: *QbeModule, arena: Alloc, worker: CodegenWorker, threaded: bool, enqueue: @FnPtr(*CodegenShared, *CodegenEntry) void, join: CodegenWorker) *CodegenShared = {
    s := arena.box_zeroed(CodegenShared); 
    s.threaded = threaded && use_threads;
    m.need_write_protect = !threaded && m.use_map_jit_on_apple_silicon;
    s.m = m;
    n := codegen_queue_size;
    s.entries = arena.alloc_zeroed(CodegenEntry, n);
    s.pipe = init(arena, n);
    s.enqueue = enqueue;
    s.join = join;
    each s.entries { it |
        if s.threaded {
            it.arena_storage = init(general_allocator(), 1.shift_left(16));  // these are freed in join_codegen_thread
            it.arena = it.arena_storage&.borrow();
        };
        s.pipe.done&.push(it);
    };
    s.alloc = arena;
    @if(s.threaded) {
        s.thread = start_thread(CodegenShared, worker, s);
    };
    s
}

// TODO: bring back `@if(ENABLE_ENABLE_TRACY, zone_begin(.Wait), ());`
// don't reset the arena here, the other guy might want it.
fn acquire(shared: *CodegenShared, $i_am_bouba: bool) ?*CodegenEntry = {
    q := @if(i_am_bouba, shared.pipe.done&, shared.pipe.work&);
    value := q.pop();
    @if(value.task&.tag() == .Done) return(.None);
    (Some = value)
}

QRing :: import("@/lib/sys/sync/qring.fr");

// don't reset the arena here, the other guy might want it.
fn release(shared: *CodegenShared, entry: *CodegenEntry, $i_am_bouba: bool) void = {
    @debug_assert(shared.threaded);
    q := @if(i_am_bouba, shared.pipe.work&, shared.pipe.done&);
    q.push(entry);
}

fn join_codegen_thread(shared: *CodegenShared) void = 
    shared'join(shared);

fn join_codegen_thread_impl(shared: *CodegenShared) void = {
    @assert(!shared.no_more_functions);
    shared.no_more_functions = true;
    fence();
    if shared.threaded {
        enter_task shared { e |
            e.task = .Done;
        };
        shared.thread.join();
        shared.thread = zeroed(@type shared.thread);
    };
    @if(shared.threaded) each shared.entries { q |
        q.arena.deinit();
    };
    arena := shared.alloc;
    arena.dealloc(@type shared.entries[0], shared.entries);
    arena.dealloc(@type shared.pipe.work.ring[0], shared.pipe.work.ring);
    arena.dealloc(@type shared.pipe.done.ring[0], shared.pipe.done.ring);
    
    if !shared.threaded {
        shared.m.emit_suspended_inlinables();
    } else {
        @debug_assert_eq(shared.m.inlinable_but_referenced_later.len, 0, "this happens on the other thread so it works with apple's MAP_JIT");
    };
    arena.dealloc_one(@type shared[], shared);
}

fn enter_task(shared: *CodegenShared, $body: @Fn(it: *CodegenEntry) void) void = {
    old, old_i := (tls(.temporary_allocator)[], tls(.temporary_allocator_i)[]);
    marker := zeroed Alloc.Mark;
    entry := acquire(shared, true).unwrap();
    if shared.threaded {
        set_temporary_allocator(entry.arena_storage&);
        entry.arena.reset_retaining_capacity();
    } else {
        marker = temp().mark();
    };
    @must_return body(entry);
    shared'enqueue(shared, entry);
    if shared.threaded {
        if __driver_abi_version < 1263 {
            tls(.temporary_allocator)[] = old;
        };
        tls(.temporary_allocator_i)[] = old_i;
    } else {
        temp().reset(marker);
    };
}

fn enqueue_task(shared: *CodegenShared, old_entry: *CodegenEntry) void = {
    @debug_assert(!old_entry.task&.is(.Dead), "tried to enqueue dead task");
    if !shared.threaded {
        do_codegen(shared.m, old_entry);
        shared.pipe.done&.push(old_entry);
    } else {
        release(shared, old_entry, true);
    };
}

// (this is not the number of threads! just the amount of backlog we allow). 
codegen_queue_size :: 128;

codegen_thread_main :: fn(shared: *CodegenShared) void = {
    @debug_assert(use_threads && shared.threaded && !shared.m.need_write_protect);
    @if(shared.m.use_map_jit_on_apple_silicon) apple_thread_jit_write_protect(false);  // :JitMemProtect this thread wants to write
    
    idx := 0;
    loop {
        @debug_assert(shared.m.initialized, "destroyed module before joining codegen thread.");
        if acquire(shared, false) { entry |
            set_temporary_allocator(entry.arena_storage&);
            do_codegen(shared.m, entry);
            release(shared, entry, false);
        } else {
            @assert(shared.no_more_functions, "expected no_more_functions");
            emit_suspended_inlinables(shared.m); // needs to happen on codegen thread for apple's jit shit.
        
            // pthread_getcpuclockid doesn't exist? so we have to do this here
            shared.codegen_time = clock_ms(MacosLibc.CLOCK_THREAD_CPUTIME_ID);  // // :TodoLinux
            shared.no_more_codegen = 1;
            Futex'wake(shared.no_more_codegen&, 1);
            return();
        };
    };
};

ENABLE_ENABLE_TRACY :: Foreign.ENV.FRANCA_TRACY.get_environment_variable().is_some();

fn do_codegen(m: *QbeModule, entry: *CodegenEntry) void = {
    zone := @if(ENABLE_ENABLE_TRACY, zone_begin(.Backend), ());
    logging := entry.logging;
    entry.logging = "";
    prev_debug := m.debug;
    m.set_debug_types(logging, true);
    @debug_assert(identical(m.curi, m.insb.index_unchecked(Qbe.MaxInstructions)), "leaked scratch");
    @match(entry.task) {
        fn Dead() => @panic("dead codegen task %", m.initialized);
        fn Done() => unreachable();
        fn Func(f) => {
            @if(ENABLE_ENABLE_TRACY) @if(ENABLE_TRACY) ___tracy_emit_zone_name(zone, f.globals.str(f.lnk.id));
            run_qbe_passes(f);
        }
        // TODO: this is more confusing than it needs to be
        fn Shim(it) => {
            m := it.f.globals;
            @debug_assert_eq(m.goal.type, .JitOnly);
            run_qbe_passes(it.f);
            
            // The other guy's got slot points to us until it's ready. 
        use_symbol(m, it.f.lnk.id) { shim_s | 
            jit_addr := shim_s.jit_addr;
            use_symbol(m, it.shim_for) { s | 
                break :: local_return;
                // :wasmgotunify
                @if(TODOWASM) if m.goal.arch == .wasm32 {
                    import("@/backend/wasm/abi.fr")'save_signature(m, it.shim_for, shim_s.wasm_type_index);
                    @debug_assert(shim_s.got_lookup_offset == -1);
                    if s.got_lookup_offset == -1 {
                        s.got_lookup_offset = (m.target.wasm_jit_event)(Grow = (delta = 1));
                    };
                    shim_s.got_lookup_offset = s.got_lookup_offset;
                    jit_addr = s.got_lookup_offset.rawptr_from_int();
                };
                @debug_assert(s.jit_addr.is_null(), "making a shim but it already has a jit_addr slot. %", s.name);
                s.jit_addr = jit_addr;
                s.shim_addr = jit_addr;
                s.size = shim_s.size;
                // TODO: it sucks that im doing this differently on wasm than on native. 
                
                @if(TODOWASM)
                if m.goal.arch == .wasm32 {
                    // TODO: make this code more uniform. but you don't need to make a GOT in linear memory, 
                    //       just use the indirect_table instead. then i won't need to make_exec as often. 
                    //       the change the makes my problems go away is just assigning jit_addr table indices
                    //       early instead of forcing indirect_table to be in order. but then to update 
                    //       jit_instantiate_module. maybe you just end up with duplicate table entries 
                    //       because i don't want to deal with letting you touch externref yet so it can
                    //       only give you the function values in a table. yeah thats fine. twice as big a table
                    //       probably consts nothing if it saves you even one jit_instantiate_module call.  
                    break();
                };
                
                @debug_assert(!jit_addr.is_null(), "we just compiled this");
                if ensure_got_slot(m, s, jit_addr) { fix |
                    push_fixup(m, s, fix);
                };
                @debug_assert(s.got_lookup_offset != -1);
                
                // for normal franca code this doesn't matter because you can only create a data fixup by already having a jit shim, 
                // but FrcImport changes that and this matters for symbols only referenced through a vtable. without this, the patch 
                // will only be filled when the real function gets compiled and it will only get compiled the first time you try to 
                // call through the shim. only matters for the comptime module or if FRANCA_NO_CACHE=true.   -- Jun 9, 2025
                each s.fixups { fix |
                    @if_let(fix.type) fn DataAbsolute(it) => {
                        fix.patch_at.raw[] = jit_addr.offset(it.increment);
                    };
                };
            };};
        }
        fn Asm(it) => {
            // This is trivial but we want to do it without threads fighting. 
            // TODO: support it being exported, etc. 
            m.add_code_bytes(it.lnk.id, it.code);  
        }
        fn JitImport(it) => {
            //@if(TODOWASM) @if(m.goal.arch == .wasm32 && m.goal.type == .JitOnly)
            //    @println("JitImport % %", m.str(it.lnk.id), it.addr);
            
            m.put_jit_addr(it.lnk.id, it.addr);
        }
        // Note: this does not work on wasm because they care about signatures
        fn Bounce(it) => {
            declare_alias(m = m, target = it.target, alias = it.lnk);
        }
        fn AotVar2(it) => each it { d |
            m.emit_data(d[]);
        };
        fn SetLibrary(it) => for it { id, lib, weak |
            // This is trivial but we want to do it without threads fighting. 
            m.set_library(id, lib, weak);
        };
        fn FromCache(it) => {  
            f := it.f;
            @if(ENABLE_ENABLE_TRACY) @if(ENABLE_TRACY) ___tracy_emit_zone_name(zone, f.globals.str(f.lnk.id));
            
            // TODO: factor out this and from incremental.fr/compile_all_symbols() so i don't have to remember to keep them in sync
            if it.did_regalloc {
                emit_fn(f);
            } else {
                // TODO: still inlsave(), that's kinda the whole point of this operation
                {f.globals.target.finish_passes}(f);
            };
        }
        fn SaveSign(f) => {
            f.lnk.id = m.intern("xx");  // in case you want to printf debug in sel_call
            @if(!IS_BOOTSTRAPPING)
            if m.goal.arch == .wasm32 {
                xxx := Qbe.Ins.list(temp());
                i := f.start.ins.items();
                with m.types_mutex& {
                    import("@/backend/wasm/abi.fr")'sel_call(f, i.slice(0, i.len-1), xxx&);
                };
                // sel call outputs instructions for its normal usage, but here i just wanted the type
                f.reset_scratch();
            };
        }
        fn All(h) => {
            @if(IS_BOOTSTRAPPING) unreachable();
            compile_all_symbols(h, m);
        }
    };
    entry.task = .Dead; 
    m.debug = prev_debug;
    @if(ENABLE_ENABLE_TRACY) zone_end(zone);
}

QbeNull :: Qbe.Null;
QbeUndef :: Qbe.Undef;
QbeConZero :: Qbe.ConZero;
Module :: QbeModule;
OutputPurpose :: QbeOutputPurpose;

#use("@/lib/collections/enum_map.fr");
#use("@/lib/sort.fr");
#use("@/lib/sys/threads.fr");
#use("@/lib/sys/sync/atomics.fr");
#use("@/lib/collections/deque.fr");

// This can be used with push_resolver. 
// used for .frc when there's no debug info. also for jitting in import_wasm. etc. 
// TODO: always push_resolver() when you create a module? 
// TODO: "you really want to lock the module in case there's another thread trying to compile into it"
find_ip_in_module :: fn(m: *QbeModule, addr: rawptr) ?Str = {
    #use("@/lib/crash_report.fr");
    if(!m.initialized, => return(.None));
    
    code := m.segments&.index(.Code);
    code_segment := code.mmapped.ptr.slice(code.len());
    if !code_segment.contains_address(u8.ptr_from_raw(addr)) {
        return(.None);
    };
    
    @if(!m.debug_info.enabled) return(.None);
    
    debug: AotDebugInfo = (
        code_segment = code_segment.rest(m.text_padding()),
        headers = m.debug_info.headers.items(),
        payload = m.debug_info.data.items(),
        source = empty(),
        files = empty(),
        nullable_cached_codemap = zeroed rawptr,
    );
    
    find_in_debug_info(AotDebugInfo.raw_from_ptr(debug&), addr)
};

permap_enabled :: fn(m: *Qbe.Module) bool = !(false
    || m.goal.type != .JitOnly
    || m.goal.arch == .wasm32
    || Foreign.ENV.FRANCA_PERFMAP.get_environment_variable().is_none()
);

fn write_perfmap_symbols(m: *Qbe.Module) void = {
    @if(identical(m.last_perfmap_end, m.last_function_end)) return();
    @if(!m.permap_enabled()) return();
    
    write_perfmap_symbols { $yield | 
        for_symbols m { _, s |
            @if(s.segment == .Code)
            @if(s.alias == Qbe.no_symbol_S)
            @if(!identical(s.jit_addr, s.shim_addr))
            @if(ptr_diff(m.last_perfmap_end, s.jit_addr) >= 0)  
                yield(s.jit_addr, s.size, s.name);
        };
    };
    // don't spam the file if called multiple times
    m.last_perfmap_end = m.last_function_end;
}

// - TODO: https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jitdump-specification.txt 
//         so it can show me the assembly as well. 
// - it's fine to do this all at the end (giving them names after they've been called)
// - useful even when not os=linux because samply understands the same symbol info and works on macos. 
// - TODO: is there some way to detect when running under perf without setting an extra environment variable? 
// https://github.com/torvalds/linux/blob/0fe7d7e9761ec7e23350b5543ddac470bb3cde1e/tools/perf/Documentation/jit-interface.txt
fn write_perfmap_symbols($iter: @Fn(yield: @Fn(p: rawptr, size: i64, name: Str) void) void) void = {
    out := @ref u8.list(temp());
    iter { ptr, len, name |
        if !ptr.is_null() && len > 0 {
            // they say don't put the 0x but it seems fine
            @fmt(out, "% % %\n", fmt_hex(ptr), fmt_hex(len), name);
        }
    };
    pid := Syscall'getpid_cached();
    path := @tfmt(Foreign.PATH.tmp_perf_map, pid);
    #use("@/lib/sys/fs.fr");
    if !append_to_file(path, out.items()) {
        @eprintln("failed to write symbol info for profiling (%)", path);
    };
}

// TODO: /tmp/marker-pid-tid.txt (start end name\n)
//       for zone_begin/zone_end

// franca examples/default_driver.fr build compiler/main.fr -o target/qq.out -unsafe -keep-names && FRANCA_PERFMAP=1 samply record --unlink-aux-files --output target/profile.json.gz target/qq.out examples/default_driver.fr build compiler/main.fr -unsafe -keep-names
// https://github.com/mstange/samply
