Qbe :: import("@/backend/ir.fr");

Simplify :: import("@/backend/opt/simplify.fr");
#include_std("backend/util.fr");
#include_std("backend/emit.fr");
#use("@/backend/opt/ssa.fr");
#use("@/backend/opt/cfg.fr");
#use("@/backend/opt/spill.fr");
#include_std("backend/opt/rega.fr");
#use("@/backend/meta/dump.fr");
#include_std("backend/opt/alias.fr");
#use("@/backend/opt/inline.fr");
#use("@/backend/abi.fr");
#use("@/backend/meta/template.fr");
#use("@/lib/leb128.fr");
#use("@/lib/bit_fields.fr");
#use("@/lib/sys/sync/mutex.fr");
#use("@/lib/sys/process.fr");
Fold :: import("@/backend/opt/fold.fr");

backend :: import_module("@/backend/lib.fr");
exports :: Type.scope_of(@struct {
    init_module: (@Fn(m: *QbeModule, goal: QbeTargetEnv) void) :init_default_module;
    compile_fn: (@Fn(f: *Qbe.Fn) void) :run_qbe_passes;
    compile_dat: (@Fn(m: *QbeModule, d: Dat2) void) :emit_data;
    compile_suspended: (@Fn(m: *QbeModule) void) :emit_suspended_inlinables;
    finish_writer: (@Fn(self: *Incremental.Writer, meta: Incremental.Meta, m: *QbeModule) [][]u8) :to_bytes;
    load_one: (@Fn(h: *Incremental.Header, f: *Qbe.Fn, it: Incremental'One(Incremental.Sym), syms: []Qbe.Sym) bool) :load;
    load_all: (@Fn(header: *Incremental.Header, m: *QbeModule) void) :compile_all_symbols;
    drop_module: (@Fn(m: *QbeModule) void) :drop;
    fold_op: (@Fn(rt_o: Qbe.O, rt_k: Qbe.Cls, out: rawptr, a0: i64, a1: i64) void) :Fold.do_fold;
    try_fold_op: (@Fn(op: Qbe.O, cls: Qbe.Cls, cl: *Qbe.Con, cr: *Qbe.Con) ?Qbe.Con) :Fold.try_fold;
    worker_thread: (@Fn(s: *CodegenShared) void) : codegen_thread_main;
    worker_start: (@Fn(m: *QbeModule, arena: Alloc, worker: CodegenWorker, threaded: bool) *CodegenShared) : init_codegen_worker;
    worker_enqueue: (@Fn(*CodegenShared, *CodegenEntry) void) : enqueue_task;
    worker_join: (@Fn(s: *CodegenShared) void) :join_codegen_thread;
    writer_finish_imports: (@Fn(writer: *Incremental.Writer, m: *QbeModule) void) : finish_imports;
});

// It can be interesting to disable optional optimisation passes to see how much they help. 
//                           June 2025 (arm self compile)
ENABLE_INLINING    :: true;   // 480ms r
ENABLE_ELIDE_SLOTS :: true;   // 240ms l
ENABLE_PROMOTE     :: true;   // 230ms q
ENABLE_CONST_FOLD  :: true && //  40ms q
    matching_comptime_incremental_abi;
    // opt/fold.fr generates a module at comptime and includes that at comptime, 
    // so it can only be enabled when the host compiler has the same ir abi as the source code being compiled. 

matching_comptime_incremental_abi :: { 
    c := current_compiler_context(); 
    c'vtable'frc_module_magic_v == Incremental.MAGIC 
};

Arm  :: import("@/backend/arm64/bits.fr");
Amd  :: import("@/backend/amd64/bits.fr");
Wasm :: import("@/backend/wasm/bits.fr");

fn init_default_module_dyn(m: *QbeModule, vtable: *ImportVTable, goal: QbeTargetEnv) void = {  
    @assert(ptr_cast_unchecked(@type vtable.init_default_qbe_module, i64, vtable.init_default_qbe_module&)[] != 0, "init_default_qbe_module is not enabled");
    {vtable.init_default_qbe_module}(QbeModule.raw_from_ptr(m), QbeTargetEnv.raw_from_ptr(goal&));
}

fn emit_qbe_included_dyn(m: *QbeModule, comp: *CompCtx, fns: [] FuncId, entry: ProgramEntry) [][]u8 = {  
    {comp.vtable.emit_qbe_included}(QbeModule.raw_from_ptr(m), comp, fns, entry)
}

// easy-ish (?) things to try to get used to making changes. 
// TODO: if they don't do it already: 
//       - track known truthyness per block (based on jumps in dominators)
//         to remove redundant checks (like bounds checks). that use kinda relies on inlining tho.
//       - jmp on the not (xor -1) of a cmp should just flip the cmp. 
// TODO: don't bother emitting nops (when copying with emiti() anyway)? is avoiding a branch worth your blocks using extra memory
// TODO: :Explain :UnacceptablePanic :SketchyIterTargets :TodoPort :nullable
//       :MakeATestThatFails places where i don't know why qbe did something and all my current tests work if you change it. 
//                           so try to make a test case that requires qbe's behaviour or remove the extra code. 
//       :HardcodeAlloc 
//       :force_dep_on_memmove_hack
// TODO: fix tests/broken/todo.fr
// TODO: the trick of having a `BitSet w[1]` so it auto decays to a pointer is cute. it's kinda a shame i can't do that. 
// TODO: fix panic on overflow block name in rega
// TODO: look at the asm and make sure im not uxtb before every cmp #0 on a bool. have to fix the ir i generate.
//       do i need to be able to express that a function returns exactly 0 or 1?
//       once i expose w ops to the language i can just generate those instead of l for working with bools and then it should be fine?
// TODO: dead store elimination would make small examples generate tighter code. 
// TODO: don't do extra copies of arg/ret when just passing through to another call.
// TODO: arm isel can use ldp/stp for consecutive loads/stores and simplify:blit can do it in pairs to make that happen. 
//       i do a lot of copies so that might be an easy win for code size at least. 
// TODO: ir test that uses opaque types

QbeModule :: @rec @struct(
    save: ?@if(enable_incremental(), *Incremental.Writer, i64),
    types: RawList(Qbe.Typ),
    insb: *Array(Qbe.Ins, 1.shift_left(20)), // MaxInstructions
    curi: *Qbe.Ins,
    target: Qbe.Target,
    debug: i64,  // bitset of 1.shift_left(Qbe.DebugKey - "A".char())
    
    debug_out: *List(u8),
    symbols: []SymbolBucket,
    
    // stuff for emitting code.
    segments: EnumMap(SegmentType, SegmentCursor),
    fixups_locked := false,
    goal: QbeTargetEnv,
    start_of_writable_code: *u8,
    // these are special sections that live inside the main segments. 
    got: SegmentCursor,
    commands: SegmentCursor,
    wasm_types: SegmentCursor,
    
    exports: List(Qbe.Sym),
    imports: List(Qbe.Sym),
    local_needs_reloc: List(Qbe.Sym),  // OR for wasm: all local functions in order
    inlinable_but_referenced_later: List(Qbe.Sym),
    
    initialized: bool,
    currently_emitting_function: bool,
    
    forever_storage: ArenaAlloc,
    forever: LockedAlloc,
    symbol_memmove: Qbe.Sym,
    types_mutex: Mutex,
    last_function_end: *u8,
    icache_mutex: Mutex,
    
    wasm_function_type_count: i32,
    wasm_symbol_stackbase: Qbe.Sym,
    wasm_symbol_env_parameter: Qbe.Sym,
    wasm_has_any_indirect_calls: bool,
    wasm_any_pare: bool,
    
    need_static_memmove: bool,
    have_static_memmove: bool,
    
    _: *u8,
    
    debug_info: @struct {
        headers: List(u8);  // crash_report.AotDebugInfo.headers
        data: List(u8);     // crash_report.AotDebugInfo.payload
        last_off_func := 0;
        last_off_source := 0;
        enabled: bool;
        macho_function_starts: List(u8);
    },
    
    debug_out_storage: List(u8),
    need_write_protect: bool,
    use_map_jit_on_apple_silicon: bool,
    wasm_types_cache: HashMap([]u8, i32),
    libraries: RawList(Str),  // :SLOW but n=4 so like it's probably fine 
    scratch_for_blink_smc: List(u8),
    
    // only added when f.lnk.shrink=true, 
    // not relying on frontend telling you which group it's in 
    // because (im lazy but also) it would be neat to have multiple versions of a library with small changes and deduplicate across them. 
    deduplicate: RawHashMap(i64 /*Fn.hash*/, RawList(Qbe.Sym)),
    // to improve deduplication
    type_lookup: RawHashMap(Qbe.Typ, i64),
);

// :UpdateBoot
enable_incremental :: fn() bool #fold = !IS_BOOTSTRAPPING;

Incremental :: import("@/backend/incremental.fr");
#use("@/lib/collections/map.fr");

DisassemblerFunc :: @FnPtr(out: *List(u8), code: []u8, arch: Arch) void;
    
QbeTargetEnv :: @struct(
    arch: Arch,
    // - should we use apple's abi changes?
    // - should we emit Mach-O or ELF?
    os: Os,
    type: QbeOutputPurpose,
    
    // only matters when type=JitOnly. 
    // does the code need to be writable and executable at the same time? 
    // this lets you freely interweave running code in a module and adding more functions to it
    // if false, you have to call make_exec before trying to run the code. 
    // Regardless, be careful of cache coherency on aarch64, see comment on lib/sys/process.fr/clear_instruction_cache().
    expecting_wx := false,
    
    // When this is set, forward references to symbols always emit an inline load from the global 
    // offset table instead of using an instruction with a direct offset that must be patched later.
    // this is useful for jitting without needing to go back and modify code. 
    // false: faster runtime, true: faster comptime + more flexibility 
    // note: this setting is NOT fully respected on amd. it's more important on arm where cache coherency rules are more strict. 
    got_indirection_instead_of_patches := false,
    
    link_libc := true,

    // When outputting an exe directly, local symbol names are not included in the symbol table (because the os 
    // doesn't care about them and they make your binary larger). However, if you want to see readable names in 
    // a debugger, disassemble correct ranges of functions, etc., you can force symbol tables to be emitted. 
    exe_debug_symbol_table := false,
    
    blink_smc := true,
    
    // TODO: kinda a hack but its not the same thing as link_libc 
    // it's slower to emit very large blits inline so we generate a call to a function to do the copy. 
    // when this is false, builtin_static_memmove will be emitted and called instead of trying to import lib's memmove. 
    prefer_libc_memmove := true,
    
    segment_size        := 1.shift_left(25),
    max_got_size        := 1.shift_left(15),
    _       := 1.shift_left(11),  // :UpdateBoot
    commands_size_guess := 16384,
    chained_size_guess  := 2000000,
    
    // called for debug["D"]. the default one just execs 'llvm-mc' and hopes for the best. 
    dis: DisassemblerFunc = llvm_mc_dis,
);

QbeOutputPurpose :: @enum(i64) (Cached, JitOnly, Exe, Relocatable, Dynamic, CachedEarly);

fn text_padding(m: *QbeModule) i64 = 
    m.goal.commands_size_guess;

fn init_default_module(m: *QbeModule, goal: QbeTargetEnv) void = {
    :: import("@/lib/sys/subprocess.fr");  // this includes it in comptime_compute_self_hash even when -syscalls
    ::enum(QbeOutputPurpose);
    
    init(m, general_allocator(), goal);
    
    // :DefaultToLibc
    // just make it work with the new library tracking. 
    // assume any imports are from libc like before.
    m.new_library("libc");
}

fn init(m: *QbeModule, gpa: Alloc, goal: QbeTargetEnv) void = {
    init_common(m, gpa, goal);
    m.set_target_vtable();
}

// 
// :JitMemProtect (use_map_jit_on_apple_silicon)
//
// They won't let you have RWX pages, but if you call pthread_jit_write_protect_np, you can have pages that 
// are writable on one thread but executable on another by passing MAP_JIT to mmap, which would be great, 
// except that you control the protection for ~every~ MAP_JIT page together per thread. So you can't 
// have a jitted program create a new MAP_JIT area and write code into it. A work around would be having the 
// jitted code write into other memory and then call a fully AOT function that toggles protection and copies 
// the into the MAP_JIT memory. This also isn't a problem if there's a clear boundery between when you're creating 
// code and executing it because you can just call mprotect at the end when you're done. 
//
// The main take away is the franca compiler wants to use this so comptime franca code cannot set expecting_rwx to true on aarch64 macos. 
//

fn init_common(m: *QbeModule, gpa: Alloc, goal: QbeTargetEnv) void = {
    // gpa needs to not be temp() because we reset temp()
    m[] = QbeModule.zeroed();
    m.goal = goal; goal := m.goal&;
    
    if goal.type == .JitOnly && !is_linking_libc() {
        goal.link_libc = false;
    };
    
    segment_size := goal.segment_size;
    m.save = .None;
    m.forever_storage = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    m.forever = (parent = m.forever_storage&.borrow());
    forever := m.forever&.borrow();
    @if(enable_incremental())
    if @is(goal.type, .Cached, .CachedEarly) {
        writer := forever.box(Incremental.Writer);
        writer[] = init(gpa);
        m.save = (Some = writer);
    };
    m.segments = EnumMap(SegmentType, SegmentCursor).zeroed();
    goal.expecting_wx = goal.expecting_wx && goal.type == .JitOnly;
    m.use_map_jit_on_apple_silicon = goal.expecting_wx && goal.arch == .aarch64 && goal.os == .macos;
    
    if goal.type == .JitOnly {
        @assert_eq(goal.arch, query_current_arch(), "trying to jit to the wrong architecture");
        msg :: "trying to jit to the wrong os.\nthe abi will be wrong if you try to call external functions (like apple disagrees about how varargs should be passed).";
        @if(goal.arch == .aarch64) @assert_eq(goal.os, query_current_os(), msg);
    };
    
    mem := if !m.use_map_jit_on_apple_silicon {
        // relies on page allocator always giving zeroed memory
        page_allocator.alloc_uninit(u8, segment_size * 3)
    } else {
        F :: LibcType(.macos).MapFlag; // only apple makes us the MAP_JIT shenanigans. 
        fd: Fd = (fd = -1);
        flag :i64= bit_or(Posix'MAP'PRIVATE, Posix'MAP'ANON); 
        
        // Reserve the whole contiguous area we want.  
        // It seems that MAP_JIT and then MAP_FIXED over it is fine, but the reverse is not. 
        prot_WX :i64= bit_or(Posix'PROT'WRITE, Posix'PROT'EXEC);
        flag_WX :i64= bit_or(flag, Posix'MAP'JIT);  // :JitMemProtect .Jit means one thread will be able to write and one will be able to execute. 
        ptr := Posix'mmap(zeroed(rawptr), segment_size * 3, prot_WX, flag_WX, fd, 0) || {
            @panic("mmap Jit failed")
        };
        
        // Now remap over the data sections so they're not executable and both threads can write to them. 
        prot_RW :i64= bit_or(Posix'PROT'READ, Posix'PROT'WRITE);
        flag_RW := bit_or(flag, Posix'MAP'FIXED); // .Fixed means the address hint will not be ignored. 
        ptr_rw := ptr.offset(segment_size);
        res := or Posix'mmap(ptr_rw, segment_size * 2, prot_RW, flag_RW, fd, 0) {
            @panic("mmap Fixed failed")
        };
        
        // So now we have a contigous chunk where the first third is MAP_JIT and the rest is normal.
        mem: []u8 = (ptr = u8.ptr_from_raw(ptr), len = segment_size * 3);
        // TODO: if we wanted to allow jit and then emit macho from the same module, you'd have to remap the `commands: SegmentCursor` as well. 
        mem
    };
    
    if !m.use_map_jit_on_apple_silicon && goal.expecting_wx {
        // Only the code segment needs to be executable. 
        prot := bit_or(@as(i64) MapProt.Exec, bit_or(@as(i64) MapProt.Read, @as(i64) MapProt.Write));
        Syscall'mprotect(u8.raw_from_ptr(mem.ptr), segment_size, prot) 
            || @panic("mprotect failed while creating Module");
    };
   
    // :SegmentsAreTheSameSize 
    // order matters!
    make_segment(mem, .Code, 0, segment_size);  // :CodeIsFirst
    make_segment(mem, .ConstantData, segment_size, segment_size*2);
    make_segment(mem, .MutableData, segment_size*2, segment_size*3);
    
    // No backing allocation. It's just for aot for now. 
    m.segments&[.ZeroInitData].mmapped.ptr = mem.ptr.offset(segment_size * 3);
    m.segments&[.ZeroInitData].next = m.segments&[.ZeroInitData].mmapped.ptr;
    
    make_segment :: fn(mem: []u8, s: SegmentType, start: i64, end: i64) void => {
        m.segments&.insert(s, (mmapped = mem.slice(start, end), next = mem.index(start)));
    };
    
    m.initialized = true;
    m.exports = list(gpa);
    m.imports = list(gpa);
    m.local_needs_reloc = list(gpa);
    m.inlinable_but_referenced_later = list(gpa);
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    m.debug_info = (
        headers = list(gpa), 
        data = list(gpa),
        // the debug format is based on machine code offsets so it only matters if we're actually generating machine code. 
        // you still want it when jitting tho because the stack trace report thingy uses it
        enabled = !@is(goal.type, .Cached, .CachedEarly),
        macho_function_starts = list(gpa),
    );
    {   // TODO: this is kinda ass
        // :GotAtConstantDataBase
        next := m.segments&[.ConstantData].next&;
        m.got = (mmapped = (ptr = next[], len = goal.max_got_size), next = next[]);
        next[] = next[].offset(goal.max_got_size); 
        
        if goal.type == .JitOnly {
            range(0, goal.max_got_size / 8) { i |
                tried_to_call_uncompiled :: fn() void = {
                    panic("ICE: jitted code tried to call an uncompiled function through __GOT");
                };
                ptr_cast_unchecked(u8, @FnPtr() void, m.got.mmapped.ptr.offset(i * 8))[] = tried_to_call_uncompiled;
            };
        };
        
        next := m.segments&[.Code].next&;
        m.commands = (mmapped = (ptr = next[], len = m.goal.commands_size_guess), next = next[]);
        next[] = next[].offset(m.goal.commands_size_guess); 
        m.start_of_writable_code = next[];
        
        if m.goal.arch == .wasm32 {
            // :WasmZeroPage Skip the low page so we don't live in crazy town where the null pointer is valid. 
            next := m.segments&[.MutableData].next&;
            next[] = next[].offset(wasm_page_size); 
            mem := forever.alloc_uninit(u8, 1.shift_left(11));
            m.wasm_types = (mmapped = mem, next = mem.ptr);
        };
    };
    
    m.goal.blink_smc = m.goal.blink_smc && goal.expecting_wx;
    if m.goal.blink_smc {
        m.scratch_for_blink_smc = list(func_size_limit_bytes, forever);
    };
    
    m.debug_out_storage = list(gpa);
    m.debug_out = m.debug_out_storage&;
    m.deduplicate = init();
    
    m.insb   = forever.box(@type m.insb[]);
    // coconut.jpg
    //forever.box_zeroed(Qbe.Ins);
    
    m.curi = m.insb.index_unchecked(Qbe.MaxInstructions);
    if !m.goal.link_libc {
        // TODO: should allow user to specify a symbol to use instead of importing from libc or including a new one? 
        m.goal.prefer_libc_memmove = false;
    };
    if m.goal.prefer_libc_memmove {
        m.symbol_memmove = m.intern("memmove");
    } else {
        m.symbol_memmove = m.intern("__franca_builtin_static_memmove");
    };
    
    // :leak
    m.wasm_types_cache      = init(gpa);
    m.libraries&.push("", general_allocator());
    
    if m.goal.arch == .wasm32 || !@is(m.goal.type, .Exe, .Dynamic) {
        m.goal.exe_debug_symbol_table = false;
    };
    if m.goal.exe_debug_symbol_table && m.goal.os == .macos {
        leb128_unsigned(m.debug_info.macho_function_starts&, m.text_padding());
    };
}

fn set_target_vtable(m: *QbeModule) void = {
    EmitWasm :: import("@/backend/wasm/emit.fr");
    t := m.target&;
    @match(m.goal.arch) {
        fn aarch64() => import("@/backend/arm64/target.fr")'fill_target_arm(t, m.goal.os == .macos, false);
        fn x86_64()  => import("@/backend/amd64/target.fr")'fill_target_amd(t, m.goal.os == .macos);
        fn wasm32()  => {
            @assert(!@is(m.goal.type, .Cached), "TODO: cached wasm. need to save types more?");  // CachedEarly is fine tho
            t.finish_passes = finish_qbe_passes_wasm;
            t.emit_fn = EmitWasm.emit_func_wasm32;
            t.fixup = EmitWasm.fixup_wasm32;
            m.wasm_symbol_stackbase = m.intern("__stack_base");
            m.wasm_symbol_env_parameter = m.intern("__env_parameter");
        }
        fn rv64() => @if(IS_BOOTSTRAPPING, @panic("riscv target is disabled"),
            import("@/backend/rv64/abi.fr")'fill_target_rv64(t));
    };

    t.gpr = 1.shift_left(t.ngpr.intcast()).sub(1).shift_left(t.gpr0.intcast()).bitcast();
    t.fpr = 1.shift_left(t.nfpr.intcast()).sub(1).shift_left(t.fpr0.intcast()).bitcast();
    @debug_assert_eq(t.gpr.bit_and(t.fpr), 0);
    t.nrsave = init(@slice(
        @as(i32) t.caller_saved.bit_and(t.gpr).count_ones().bitcast(),
        t.caller_saved.bit_and(t.fpr).count_ones().bitcast(),
    ));
    @debug_assert_eq(@as(i64) t.caller_saved.count_ones().zext(), zext(t.nrsave&[0] + t.nrsave&[1]));
    
    @if(enable_incremental())
    if m.goal.type == .CachedEarly {
        t.finish_passes = (fn(f: *Qbe.Fn) void = {  
            save := f.globals.save.unwrap(); 
            save.push(f, false); 
        });
    };
    
    t.finish_module = @if_else {
        @if(@is(m.goal.type, .Cached, .CachedEarly))  => @if(enable_incremental(), Incremental'finish_module, 
            panic("tried to init cached module but that feature was disabled at comptime"));
        @if(m.goal.arch == .wasm32)  => EmitWasm.output_wasm_module;
        @if(m.goal.type == .JitOnly) => (fn(m: *QbeModule) [][]u8 = empty());
        @if(m.goal.os == .macos)     => import("@/backend/macho/emit.fr").output_macho;
        @if(m.goal.os == .linux)     => import("@/backend/elf/emit.fr").output_elf;
        @else => panic("TODO: we don't support this target");
    };
} 

fn init_empty_for_template_only(m: *QbeModule, gpa: Alloc) void = {
    m[] = QbeModule.zeroed();
    m.initialized = true;
    m.forever_storage = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    m.forever = (parent = m.forever_storage&.borrow());
    forever := m.forever&.borrow();
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    mem := forever.alloc_uninit(Qbe.Ins, Qbe.MaxInstructions);
    ::?*Array(Qbe.Ins, Qbe.MaxInstructions);
    m.insb = mem.as_array().unwrap();
    m.curi = m.insb.index_unchecked(Qbe.MaxInstructions);
    m.debug_out_storage = list(forever);
    m.debug_out = m.debug_out_storage&;
}

// TODO: this should go away eventually 
fn show_backend_stats() bool = 
    get_environment_variable("FRANCA_LOG_STATS").is_some();

fn drop(m: *QbeModule) void = {
    //each m.deduplicate& { k, v |
    //    @print("[");
    //    for v { s |
    //        @print("%, ", m.str(s));
    //    };
    //    @println("]%", k);
    //};
    fst := m.segments.data&[0].mmapped;
    page_allocator.dealloc(u8, (ptr = fst.ptr, len = fst.len * 3)); // ZeroInitData has no allocation
    @if(enable_incremental()) if m.save { save |
        save.drop();
    };
    for m.libraries { it |
        general_allocator().dealloc(u8, it);
    };
    drop(m.libraries&, general_allocator());
    drop(m.exports&);
    drop(m.imports&);
    drop(m.local_needs_reloc&);
    drop(m.inlinable_but_referenced_later&);
    drop(m.debug_info.headers&);
    drop(m.debug_info.data&);
    drop(m.debug_info.macho_function_starts&);
    drop(m.debug_out_storage&);
    drop(m.wasm_types_cache&);
    m.forever_storage&.deinit();
    m[] = zeroed(@type m[]);  // just to make sure you crash if you try to use it again. 
}


// returns false if suspended for inlining 
run_qbe_passes_common :: fn(f: *Qbe.Fn) bool = {
    ::ptr_utils(*Qbe.Blk);

    #use("@/backend/opt/mem.fr");
    
    ::if(Qbe.Ref);
    ::enum_basic(Qbe.UseType);
    m := f.globals;
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "function module not set");
    
    flush_debug(f.globals);
    when_debug_printfn(f, .Parsing, "\n## After parsing:\n");
     
    replace_frontend_ops(f);
    
    if fails_typecheck(f) { err | 
        printfn(f, f.globals.debug_out);
        @panic("IR failed typecheck: %", err);
    };
    
    extsb_parargret(f);
    
    inlcalls(f);
    fillrpo(f);
    fillpreds(f);
    fill_use(f);
    promote(f); // optional
    fill_use(f);
    convert_to_ssa(f);
    fill_use(f);
    ssacheck(f); // debug
    fillalias(f); // !
    import("@/backend/opt/load.fr")'loadopt(f); // optional
    fill_use(f);
    fillalias(f); // for coalesce
    coalesce(f); // optional
    fill_use(f); 
    filldom(f);  // TODO: do we need this here? i dont think anything breaks it after convert_to_ssa does it
    ssacheck(f); // debug
    
    import("@/backend/opt/copy.fr")'copy_elimination(f); // optional
    fill_use(f);
    Fold'fold_constants(f);
    if save_for_inlining(f) {
        // all uses might be inlined so we suspend processing until we know you took a function pointer later. 
        // also happens if got deduplicated
        when_debug(f, .Inlining, fn(out) => @fmt(out, "\n# Suspending: %\n", f.name()));
        return(false);
    };
    true
};

run_qbe_passes :: fn(f: *Qbe.Fn) void = {
    mark := mark_temporary_storage();
    if run_qbe_passes_common(f) {
        {f.globals.target.finish_passes}(f);
    };
    reset_temporary_storage(mark);
};

fn finish_qbe_passes_native(f: *Qbe.Fn) void = {
    ::[]u64;
    if f.mem.first.is_null() {
        f.mem = new(0);
    };
    m := f.globals;
    with m.types_mutex& {
        {m.target.abi1}(f);
    };
    
    simplify(f);
    fillpreds(f);
    fill_use(f); 
    
    // TODO: don't always need extra fillrpo? only if vararg or any_dead_blocks in fold_constants
    fillrpo(f);   // for fillalias. 
    fillalias(f); // for late slots mem opt, escaping slots are tracked at the beginning of isel. 
        
    // arm isel inserts RMem (with different meaning from x64) for mul/store and RInt for load.
    {m.target.isel}(f);
    import("@/backend/opt/slots.fr")'elide_abi_slots(f); 
    
    fillrpo(f);
    fill_liveness(f);
    fillloop(f); // optional
    fillcost(f); // optional? includes filluse (counts only)
    spill(f);
    
    register_allocation(f);
    fillrpo(f);
    simplify_jump_chains(f); // optional
    fillpreds(f);
    fillrpo(f);
    
    // :LinkIsNowRpo
    set_link_from_rpo(f);
    
    emit_fn(f);
}

fn finish_qbe_passes_wasm(f: *Qbe.Fn) void = {
    if f.mem.first.is_null() {
        f.mem = new(0);
    };
    m := f.globals;
    with m.types_mutex& {
        import("@/backend/wasm/abi.fr")'wasm_abi(f);
    };

    // TODO: wasm has memory.copy which we should use for blit instead of a bunch of ldr+str
    simplify(f);
    fillpreds(f);
    fill_use(f); 
    
    import("@/backend/wasm/isel.fr")'wasm_isel(f);
    
    // TODO: elide_abi_slots
    
    // TODO: maybe we still want liveness info even though we're not doing register allocation?
    
    // TODO: you want to do this but it breaks Blk.wasm_type. can it just go before isel? but still need to add something because split insts from jmp arg needs to be treated like phis. 
    //simplify_jump_chains(f); // optional
    
    
    fill_use(f); 
    
    emit_fn(f);
}

// TODO: detect non-unique block ids. 
fn fails_typecheck(f: *Qbe.Fn) ?Str = {
    @if(!::safety_check_enabled(.DebugAssertions)) return(.None);
    
    fillpreds(f);
    pb  := init_bitset(f.nblk.zext());
    ppb := init_bitset(f.nblk.zext());
    for_blocks f { b | 
        for_phi b { p |
            if rtype(p.to) != .RTmp {
                return(Some = @tfmt("phi destination must be a tmp"));
            };
            f.get_temporary(p.to)[].cls = p.cls;
        };
        for_insts_forward b { i | 
            if rtype(i.to) == .RTmp {
                t := f.get_temporary(i.to);
                if clsmerge(t.cls&, i.cls()) {
                    return(Some = @tfmt("temporary %%.% is assigned with multiple types (% vs %)", "%", t.name(), i.to.val(), t.cls, i.cls()));
                };
            };
        };
    };
    for_blocks f { b |
        bszero(pb&);
        range(0, b.npred.zext()) { n |
            bsset(pb&, b.pred[n].id.zext());
        };
        for_phi b { p |
            bszero(ppb&);
            t := f.get_temporary(p.to);
            range(0, p.narg.zext()) { n |
                k := t.cls;
                if bshas(ppb&, p.blk[n].id.zext()) {
                    return(Some = @tfmt("multiple entries for @% in phi %%", p.blk[n].name(), "%", t.name()));
                };
                if !usecheck(p.arg[n], k, f) {
                    return(Some = @tfmt("invalid type (%) for operand %% in phi %%", k.raw(), "%", f.get_temporary(p.arg[n]).name(), "%", t.name()));
                };
                bsset(ppb&, p.blk[n].id.zext());
            };
            if !bsequal(pb&, ppb&) {
                return(Some = @tfmt("predecessors not matched in phi %%", "%", t.name()));
            };
        };
        for_insts_forward b { i |
            continue :: local_return;
            range(0, 2) { n |
                continue :: local_return;
                k := argcls(i, n);
                r := i.arg&[n];
                if k == .Ke {
                    return(Some = @tfmt("invalid instruction output type % in %", i.cls(), i.op()));
                };
                if(rtype(r) == .RType, => continue());
                word :: fn() => if(n == 1, => "second", => "first");
                if rtype(r) != .RNull && k == .Kx {
                    return(Some = @tfmt("no % operand expected in %", word(), i.op()));
                };
                if rtype(r) == .RNull && k != .Kx {
                    if i.op() == .copy && isreg(i.to) {
                        continue();  // :ExprLevelAsm
                    };
                    return(Some = @tfmt("missing % operand in %", word(), i.op()));
                };
                if !usecheck(r, k, f) {
                    return(Some = @tfmt("invalid type for % operand % of % in block @%", word(), r, i.op(), b.name()));
                };
            };
            if is_store(i.op()) && i.to != QbeNull { 
                return(Some = @tfmt("store instruction must not have result"));
            };
            if i.op() == .blit1 && rtype(i.arg&[0]) != .RInt {
                return(Some = @tfmt("blit1 expected a0 to be RInt not %", rtype(i.arg&[0])));
            };
        };
    
        r := b.jmp.arg;
        jump_error := false;
        if is_ret(b.jmp.type) {
            k: Qbe.Cls = @if(b.jmp.type == .retc, .Kl, @if(b.jmp.type.raw() >= Qbe.J.retsb.raw(), .Kw, b.jmp.type.cls()));
            jump_error = !usecheck(r, k, f);
        };
        if b.jmp.type == .jnz {
            jump_error = !usecheck(r, .Kw, f) || !@is(rtype(b.jmp.arg), .RTmp, .RCon);
        };
        if @is(b.jmp.type, .ret0, .hlt) {
            jump_error = b.jmp.arg != QbeNull;
        };
        if jump_error {
            return(Some = @tfmt("invalid type for jump argument in block @%", b.name()));
        };
        
        @debug_assert_ne(b.jmp.type, .switch, "B");
        if !b.s1.is_null() && b.s1.jmp.type == .Jxxx {   
            return(Some = @tfmt("block @% is used undefined", b.s1.name()));
        };
        if !b.s2.is_null() && b.s2.jmp.type == .Jxxx {
            return(Some = @tfmt("block @% is used undefined", b.s2.name()));
        };
    };
    .None
}

fn usecheck(r: Qbe.Ref, k: Qbe.Cls, f: *Qbe.Fn) bool = {
    rtype(r) == .RTmp || return(true);
    t := f.get_temporary(r);
    t.cls == k || (t.cls == .Kl && k == .Kw)
}

// 
// Some operations are provided for frontend convenience but don't actually have a target dependent implementation. 
// - J.switch -> jnz chain
// - O.assert -> jnz+hlt
//
fn replace_frontend_ops(f: *Qbe.Fn) void = {
    new_blocks_list := Qbe.Blk.ptr_from_int(0);
    last_block := f.start;
    changed := false;
    switches := f.switches.slice(0, f.switch_count.zext());
    fail_assertion: ?*Qbe.Blk = .None;
    
    xxx := 0;
    for_blocks f { b | 
        continue :: local_return;
        last_block = b;
        
        { // O.assert
            j := 0;
            while => j < b.ins.len {
                i := b.ins.index(j);
                j += 1;
                // TODO: explain more
                // TODO: nothing uses this instruction rn
                if i.op() == .assert {
                    r := f.newtmp("front", .Kw);
                    i[] = make_ins(.cnew, .Kw, r, i.arg&[0], QbeConZero);  // TODO: only looking at low 32 bits rn so it could just be jnz directly. but should really look at all 64. 
                    fail := or fail_assertion { 
                        b3 := add_block();
                        b3.jmp.type = .hlt;
                        fail_assertion = (Some = b3);
                        b3
                    };
                    b2 := add_block();
                    b2.s1 = b.s1; b2.s2 = b.s2; b2.jmp = b.jmp;
                    len := b.ins.len - j;
                    b2.ins = init(temp(), len);
                    b2.ins.len = len;
                    b2.ins.items().copy_from(b.ins.slice(j, b.ins.len));
                    b.jmp = (type = .jnz, arg = r);
                    b.s1 = b2; b.s2 = fail;
                    b.ins.len = j;
                    // this matters if the frontend uses phis 
                    for_jump_targets b2 { s |
                        for_phi s { p |  // paste from replacepreds
                            replacepred(p.blk, p.narg.zext(), b2, b);
                        };
                    };
                    b = b2;
                    j = 0;
                };
            };
        };
        
        if b.jmp.type != .switch {
            continue();
        };
        @debug_assert_eq(rtype(b.jmp.arg), .RInt, "J.switch expected RInt index into Fn.switches");
        payload := switches.index(b.jmp.arg.val());
        @debug_assert(@is(rtype(payload.inspect), .RCon, .RTmp), "invalid inspect value for switch");
        
        if payload.case_count == 0 {
            b.jmp = (type = .jmp, arg = QbeNull);
            b.s1 = payload.default;
            continue();
        };
        
        set_s1 :: fn(old_src: *Qbe.Blk, new_src: *Qbe.Blk, dest: *Qbe.Blk) void => {
            new_src.s1 = dest;
            for_phi dest { p | 
                n := index_in_phi(old_src, p);
                p.blk[n] = new_src;
            };
        };
        
        add_block :: fn() => {
            blk := newblk(); 
            f.nblk += 1;
            // build up a chain to add at the end
            blk.link = new_blocks_list;
            new_blocks_list = blk;
            blk
        };
        
        next_block := add_block();
        first_dispatch_block := next_block;
        cases := payload.cases.slice(0, payload.case_count);
        for cases { case |
            continue :: local_return;
            dest, value := case;
            if(dest.identical(payload.default), => continue());
            
            check := next_block;  
            next_block = add_block();
            
            matches := f.newtmp("switch", .Kw);
            check.ins = init(temp(), 1);
            check.ins.len = 1;
            check.ins[0] = make_ins(.ceql, .Kw, matches, payload.inspect, f.getcon(value));
            
            set_s1(b, check, dest);
            check.jmp = (type = .jnz, arg = matches);
            check.s2 = next_block;
        };
        
        set_s1(b, next_block, payload.default);
        next_block.jmp = (type = .jmp, arg = QbeNull);
        
        b.jmp = (type = .jmp, arg = QbeNull);
        b.s1 = first_dispatch_block;
    };
    
    f.switch_count = 0;
    @debug_assert(last_block.link.is_null());
    last_block.link = new_blocks_list;
}

wasm_page_size :: 65536;

// 
// This api makes it easy to have your frontend and the backend run on seperate threads. 
// 
// Call enter_task, generate some code, and return a CodegenTask which will be sent to the backend thread. 
// Inside enter_task, temp() is tied to the task so the generated code can be in temp memory but you can't let any of your own temp allocations escape. 
// Most backend functions are not thread safe, so you can't mix this threaded api with the direct calls it replaces.  
// Sym intern()/str() on either thread is safe. When generating code, make sure to use emit(*Blk) instead of emit(*Fn). 
// Call join_codegen_thread() at the end to finalize everything. 
// 

CodegenShared :: @struct(
    entries: []CodegenEntry,
    waiting_for_bouba: RawDeque(*CodegenEntry),
    // It's important that these are processed in the order they're created so it's not a race to get a reproducible build. 
    waiting_for_kiki: RawDeque(*CodegenEntry),
    mutex: import("@/lib/sys/sync/mutex.fr").Mutex,
    waiting_for_bouba_futex: u32,
    waiting_for_kiki_futex: u32,

    no_more_functions: bool,
    no_more_codegen: u32,
    m: *QbeModule,
    codegen_time: i64,
    // TODO: allow multiple backend threads. 
    thread: *Thread,
    threaded: bool,
    alloc: Alloc,
);

CodegenTask :: @tagged(
    Dead: void,
    Func: *Qbe.Fn,
    Asm: @struct(lnk: *Qbe.Lnk, code: *MultiArchAsm),
    JitImport: @struct(lnk: *Qbe.Lnk, addr: rawptr),
    Bounce: @struct(lnk: *Qbe.Lnk, target: Qbe.Sym),
    Shim: @struct(f: *Qbe.Fn, shim_for: Qbe.Sym),
    AotVar2: []Dat2,
    // :UpdateBoot
    SetLibrary: []Ty(@slice(Qbe.Sym, Str, bool)), // id, lib, weak
    FromCache: @struct(f: *Qbe.Fn, did_regalloc: bool),
);

CodegenEntry :: @struct(
    arena_storage: ArenaAlloc, 
    arena: Alloc,
    task: CodegenTask,
    logging := "",
);

CodegenWorker :: @FnPtr(s: *CodegenShared) void;
fn init_codegen_worker(m: *QbeModule, arena: Alloc, worker: CodegenWorker, threaded: bool) *CodegenShared = {
    s := arena.box_zeroed(CodegenShared); 
    s.mutex&.lock();
    s.threaded = threaded && use_threads;
    m.need_write_protect = !threaded && m.use_map_jit_on_apple_silicon;
    s.m = m;
    n := codegen_queue_size;
    s.entries = arena.alloc_zeroed(CodegenEntry, n);
    s.waiting_for_bouba = init(arena, n);
    s.waiting_for_kiki = init(arena, n);
    each s.entries { it |
        if s.threaded {
            it.arena_storage = init(general_allocator(), 16000);  // these are freed in join_codegen_thread
            it.arena = it.arena_storage&.borrow();
        };
        s.waiting_for_bouba&.push_back(it, arena);
    };
    s.alloc = arena;
    s.waiting_for_bouba_futex = n.trunc();
    @if(s.threaded) {
        s.thread = start_thread(CodegenShared, worker, s);
    };
    s.mutex&.unlock();
    s
}

// don't reset the arena here, the other guy might want it.
fn acquire(shared: *CodegenShared, $i_am_bouba: bool) ?*CodegenEntry = {
    futex := @if(i_am_bouba, shared.waiting_for_bouba_futex&, shared.waiting_for_kiki_futex&);
    if shared.threaded {
        zone := @if(ENABLE_ENABLE_TRACY, zone_begin(.Wait), ());
        fence();
        while => futex[] == 0 {
            fence();
            if shared.no_more_functions {
                @debug_assert(!i_am_bouba);
                return(.None);
            };
            Futex'wait(futex, 0);
        };
        zone_end(zone);
        fence();
    };
    
    shared.mutex&.lock();
    source := @if(i_am_bouba, shared.waiting_for_bouba&, shared.waiting_for_kiki&);
    it := source.pop_front().expect("acquire CodegenEntry");
    if shared.threaded {
        futex[] -= 1;
    };
    shared.mutex&.unlock();
    (Some = it)
}

// don't reset the arena here, the other guy might want it.
fn release(shared: *CodegenShared, entry: *CodegenEntry, $i_am_bouba: bool) void = {
    @debug_assert(shared.threaded);
    shared.mutex&.lock();
    if i_am_bouba {
        // when the frontend is done, that entry goes on the end of the queue 
        // to make sure compilation order is deterministic (required for repro)
        shared.waiting_for_kiki&.push_back(entry, panicking_allocator);
    } else {
        // when the backend is done, have that entry be reused as soon as possible. 
        // this avoids needing to warm up as many arenas (and reduces memory usage). 
        // Aug 3, 2025: this saves ~10ms on self compile (1.08%)
        shared.waiting_for_bouba&.push_front(entry, panicking_allocator);
    };
    futex := @if(!i_am_bouba, shared.waiting_for_bouba_futex&, shared.waiting_for_kiki_futex&);
    
    prev := u32.atomic_add(futex, 1);
    need_wake := prev == 0 || shared.no_more_functions;
    @if(need_wake) Futex'wake(futex, 1);
    shared.mutex&.unlock();
}

fn join_codegen_thread(shared: *CodegenShared) void = {
    @assert(!shared.no_more_functions);
    shared.no_more_functions = true;
    fence();
    if shared.threaded {
        while => shared.no_more_codegen == 0 {
            Futex'wake(shared.waiting_for_bouba_futex&, 1);
            Futex'wake(shared.waiting_for_kiki_futex&, 1);
            Futex'wait(shared.no_more_codegen&, 0);
        };
        shared.thread.join();
        shared.thread = zeroed(@type shared.thread);
    };
    shared.mutex&.lock();
    
    @debug_assert_eq(shared.waiting_for_bouba&.len(), shared.entries.len);
    @debug_assert_eq(shared.waiting_for_bouba_futex, shared.entries.len.trunc());
    @debug_assert_eq(shared.waiting_for_kiki&.len(), 0);
    @if(shared.threaded) each shared.entries { q |
        q.arena.deinit();
    };
    arena := shared.alloc;
    arena.dealloc(@type shared.entries[0], shared.entries);
    shared.waiting_for_bouba&.drop(arena);
    shared.waiting_for_kiki&.drop(arena);
    
    if !shared.threaded {
        shared.m.emit_suspended_inlinables();
    } else {
        @debug_assert_eq(shared.m.inlinable_but_referenced_later.len, 0, "this happens on the other thread so it works with apple's MAP_JIT");
    };
    arena.dealloc_one(@type shared[], shared);
}

fn enter_task(shared: *CodegenShared, $body: @Fn(it: *CodegenEntry) void) void = {
    c := context(DefaultContext);
    old, old_i := (c.temporary_allocator, c.temporary_allocator_i);
    marker := zeroed Alloc.Mark;
    entry := acquire(shared, true).unwrap();
    if shared.threaded {
        set_temporary_allocator(entry.arena_storage&);
        entry.arena.reset_retaining_capacity();
    } else {
        marker = temp().mark();
    };
    @must_return body(entry);
    enqueue_task(shared, entry);
    if shared.threaded {
        c.temporary_allocator = old;
        c.temporary_allocator_i = old_i;
        //set_temporary_allocator(old);
    } else {
        temp().reset(marker);
    };
}

fn enqueue_task(shared: *CodegenShared, old_entry: *CodegenEntry) void = {
    ::tagged(@type old_entry.task);
    @debug_assert(!old_entry.task&.is(.Dead), "tried to enqueue dead task");
    if !shared.threaded {
        do_codegen(shared.m, old_entry);
        shared.waiting_for_bouba&.push_back(old_entry, panicking_allocator);
    } else {
        release(shared, old_entry, true);
    };
}

// (this is not the number of threads! just the amount of backlog we allow). 
codegen_queue_size :: 128;

codegen_thread_main :: fn(shared: *CodegenShared) void = {
    @debug_assert(use_threads && shared.threaded && !shared.m.need_write_protect);
    @if(shared.m.use_map_jit_on_apple_silicon) apple_thread_jit_write_protect(false);  // :JitMemProtect this thread wants to write
    
    idx := 0;
    loop {
        @debug_assert(shared.m.initialized, "destroyed module before joining codegen thread.");
        if acquire(shared, false) { entry |
            set_temporary_allocator(entry.arena_storage&);
            do_codegen(shared.m, entry);
            release(shared, entry, false);
        } else {
            @assert(shared.no_more_functions, "expected no_more_functions");
            emit_suspended_inlinables(shared.m); // needs to happen on codegen thread for apple's jit shit.
        
            // pthread_getcpuclockid doesn't exist? so we have to do this here
            shared.codegen_time = clock_ms(MacosLibc.CLOCK_THREAD_CPUTIME_ID);  // // :TodoLinux
            shared.no_more_codegen = 1;
            Futex'wake(shared.no_more_codegen&, 1);
            return();
        };
    };
};

ENABLE_ENABLE_TRACY :: get_environment_variable("FRANCA_TRACY").is_some();

fn do_codegen(m: *QbeModule, entry: *CodegenEntry) void = {
    zone := @if(ENABLE_ENABLE_TRACY, zone_begin(.Backend), ());
    logging := entry.logging;
    entry.logging = "";
    prev_debug := m.debug;
    m.set_debug_types(logging, true);
    @match(entry.task) {
        fn Dead() => @panic("dead codegen task %", m.initialized);
        fn Func(f) => {
            @if(ENABLE_ENABLE_TRACY) @if(ENABLE_TRACY) ___tracy_emit_zone_name(zone, f.globals.str(f.lnk.id));
            run_qbe_passes(f);
        }
        // TODO: this is more confusing than it needs to be
        fn Shim(it) => {
            m := it.f.globals;
            run_qbe_passes(it.f);
            
            // The other guy's got slot points to us until it's ready. 
            jit_addr := rawptr.zeroed();
            use_symbol(m, it.f.lnk.id) { s | 
                jit_addr = s.jit_addr;
            };
            @debug_assert(!jit_addr.is_null(), "we just compiled this");
            use_symbol(m, it.shim_for) { s | 
                @debug_assert(s.jit_addr.is_null(), "making a shim but it already has a jit_addr slot. %", s.name);
                s.jit_addr = jit_addr;
                s.shim_addr = jit_addr;
                if ensure_got_slot(m, s, jit_addr) { fix |
                    push_fixup(m, s, fix);
                };
                @debug_assert(s.got_lookup_offset != -1);
                
                // for normal franca code this doesn't matter because you can only create a data fixup by already having a jit shim, 
                // but FrcImport changes that and this matters for symbols only referenced through a vtable. without this, the patch 
                // will only be filled when the real function gets compiled and it will only get compiled the first time you try to 
                // call through the shim. only matters for the comptime module or if FRANCA_NO_CACHE=true.   -- Jun 9, 2025
                each s.fixups { fix |
                    @if_let(fix.type) fn DataAbsolute(it) => {
                        rawptr.ptr_from_raw(fix.patch_at)[] = jit_addr.offset(it.increment);
                    };
                };
            };
        }
        fn Asm(it) => {
            // This is trivial but we want to do it without threads fighting. 
            // TODO: support it being exported, etc. 
            m.add_code_bytes(it.lnk.id, it.code);  
        }
        fn JitImport(it) => {
            m.put_jit_addr(it.lnk.id, it.addr);
        }
        // Note: this does not work on wasm because they care about signatures
        fn Bounce(it) => {
            declare_alias(m = m, target = it.target, alias = it.lnk);
        }
        fn AotVar2(it) => each it { d |
            m.emit_data(d[]);
        };
        fn SetLibrary(it) => for it { id, lib, weak |
            // This is trivial but we want to do it without threads fighting. 
            m.set_library(id, lib, weak);
        };
        fn FromCache(it) => {  
            f := it.f;
            @if(ENABLE_ENABLE_TRACY) @if(ENABLE_TRACY) ___tracy_emit_zone_name(zone, f.globals.str(f.lnk.id));
            
            // TODO: factor out this and from incremental.fr/compile_all_symbols() so i don't have to remember to keep them in sync
            if it.did_regalloc {
                emit_fn(f);
            } else {
                // TODO: still inlsave(), that's kinda the whole point of this operation
                {f.globals.target.finish_passes}(f);
            };
        }
    };
    entry.task = .Dead; 
    m.debug = prev_debug;
    @if(ENABLE_ENABLE_TRACY) zone_end(zone);
}

QbeNull :: Qbe.Null;
QbeUndef :: Qbe.Undef;
QbeConZero :: Qbe.ConZero;
Module :: QbeModule;
OutputPurpose :: QbeOutputPurpose;

#use("@/lib/collections/enum_map.fr");
#use("@/lib/sort.fr");
#use("@/lib/sys/threads.fr");
#use("@/lib/sys/sync/atomics.fr");
#use("@/lib/collections/deque.fr");
