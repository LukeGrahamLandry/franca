#include_std("backend/ir.fr");
#include_std("backend/opt/simplify.fr");
#include_std("backend/opt/live.fr");
#include_std("backend/util.fr");
#include_std("backend/opt/copy.fr");
#include_std("backend/arm64/isel.fr");
#include_std("backend/arm64/emit.fr");
#include_std("backend/emit.fr");
#include_std("backend/opt/ssa.fr");
#include_std("backend/opt/cfg.fr");
#include_std("backend/opt/spill.fr");
#include_std("backend/opt/rega.fr");
#include_std("backend/arm64/abi.fr");
#include_std("backend/meta/dump.fr");
#include_std("backend/arm64/target.fr");
#include_std("backend/opt/load.fr");
#include_std("backend/opt/alias.fr");
#include_std("backend/opt/inline.fr");
#include_std("backend/macho/bits.fr");
#include_std("backend/macho/emit.fr");
#include_std("backend/opt/mem.fr");
#include_std("backend/opt/minimal.fr");
#include_std("backend/opt/fold.fr");
#include_std("backend/amd64/target.fr");
#include_std("backend/amd64/emit.fr");
#include_std("backend/amd64/sysv.fr");
#include_std("backend/meta/parse.fr");
#include_std("backend/abi.fr");
#include_std("backend/meta/template.fr");
#include_std("backend/amd64/isel.fr");
#include_std("backend/opt/slots.fr");
#include_std("backend/wasm/bits.fr");
#include_std("backend/wasm/isel.fr");
#include_std("backend/wasm/abi.fr");
#include_std("backend/wasm/emit.fr");

// easy-ish (?) things to try to get used to making changes. 
// TODO: if they don't do it already: 
//       - track known truthyness per block (based on jumps in dominators)
//         to remove redundant checks (like bounds checks). that use kinda relies on inlining tho.
//       - jmp on the not (xor -1) of a cmp should just flip the cmp. 
// TODO: add instructions for clz, ctz, popcnt, byteswap/bit reverse,
//          when i do firstbit, why not do whatever __builtin_ctzl does? x64 has an instruction for that. arm has reverse bits and then clz. wasm has it. 
//          the c version works with converting to that. clang -O2 doesn't treat the functions the same so maybe im missing something. 
//          have to add that as an Intrinsic to my language... which i guess means adding it to my new qbe. 
// TODO: don't bother emitting nops (when copying with emiti() anyway)? is avoiding a branch worth your blocks using extra memory
// TODO: :Explain :UnacceptablePanic :SketchyIterTargets :TodoPort :SketchPlatformRegister :nullable
//       :MakeATestThatFails places where i don't know why qbe did something and all my current tests work if you change it. 
//                           so try to make a test case that requires qbe's behaviour or remove the extra code. 
//       :HardcodeAlloc 
//       :force_dep_on_memmove_hack
// TODO: fix tests/broken/todo.fr
// TODO: the trick of having a `BitSet w[1]` so it auto decays to a pointer is cute. it's kinda a shame i can't do that. 
// TODO: fix panic on overflow block name in rega
// TODO: have a flag on module to turn off formatting name strings when we're not going to be debug printing. 
// TODO: don't use libc file structs all over the place. have my own non-painful buffered io. 
// TODO: look at the asm and make sure im not uxtb before every cmp #0 on a bool. have to fix the ir i generate.
//       do i need to be able to express that a function returns exactly 0 or 1?
//       once i expose w ops to the language i can just generate those instead of l for working with bools and then it should be fine?
// TODO: dead store elimination would make small examples generate tighter code. 
// TODO: don't do extra copies of arg/ret when just passing through to another call.
// TODO: arm isel can use ldp/stp for consecutive loads/stores and simplify:blit can do it in pairs to make that happen. 
//       i do a lot of copies so that might be an easy win for code size at least. 
// TODO: ir test that uses opaque types

QbeModule :: @rec @struct(
    // When we linked with qbe, these were pointers to it's externs so both sides could use them. TODO: some should be values now. 
    types: *QList(Qbe.Typ), // typ
    insb: *Array(Qbe.Ins, 1.shift_left(20)), // MaxInstructions
    curi: **Qbe.Ins,
    target: *Qbe.Target,
    debug: *Array(bool, "Z".char() + 1),
    
    // These we need to track but Qbe doesn't use internally. 
    number_of_types := 0, // ntyp. only used by parser
    debug_out: *FILE,
    // :ThreadSafety
    namel := QbeName.ptr_from_int(0), // TODO: this is just lazy, should manually pass it around now that im using temp() and resetting between functions. 
    symbols: []SymbolBucket,
    
    // stuff for emitting code.
    segments: EnumMap(SegmentType, SegmentCursor),
    fixups_locked := false,
    goal: QbeTargetEnv,
    start_of_writable_code: *u8,
    // these are special sections that live inside the main segments. 
    got: SegmentCursor,
    commands: SegmentCursor,
    stubs: SegmentCursor,
    
    exports: List(u32),
    imports: List(u32),
    local_needs_reloc: List(u32),
    inlinable_but_referenced_later: List(u32),
    
    supports_self_sign: bool,  // this was in zeroed padding before so the old version in ../boot will have false and current default_driver will use codesign binary like it used to -- Nov 28
    initialized := false,
    
    // When this is set, forward references to symbols always emit an inline load from the global 
    // offset table instead of using an instruction with a direct offset that must be patched later.
    // this is useful for jitting without needing to go back and modify code. 
    got_indirection_instead_of_patches := false,
    
    // :JitMemProtect the length of the name is proportional to how much they are intent on wasting my time.  
    //
    // They won't let you have RWX pages, but if you call pthread_jit_write_protect_np, you can have pages that 
    // are writable on one thread but executable on another by passing MAP_JIT to mmap, which would be great, 
    // except that you control the protection for ~every~ MAP_JIT page together per thread. So you can't 
    // have a jitted program create a new MAP_JIT area and write code into it. A work around would be having the 
    // jitted code write into other memory and then call a fully AOT function that toggles protection and copies 
    // the into the MAP_JIT memory. This also isn't a problem if there's a clear boundery between when you're creating 
    // code and executing it because you can just call mprotect at the end when you're done. 
    //
    // The main take away is the franca compiler wants to use this so comptime franca code cannot set this to true on aarch64 macos. 
    //
    use_map_jit_on_apple_silicon: bool, 
    
    outf := zeroed(*FILE),
    
    forever: ArenaAlloc,
    symbol_memmove: u32,
    types_mutex: PMutex,
    last_function_end: *u8,
    icache_mutex: PMutex,
    intern_mutex: PMutex,
);

QbeTargetEnv :: @struct(
    arch: Arch,
    // - should we use apple's abi changes?
    // - should we emit Mach-O or ELF? (can't do ELF yet)
    os: Os,
    type: QbeOutputPurpose,
);

// TODO: remove AsmText
QbeOutputPurpose :: @enum(i64) (AsmText, JitOnly, Exe, Relocatable, Dynamic);
::enum(QbeOutputPurpose);

fn making_macho(m: *QbeModule) bool = 
    m.goal.os == .macos && (m.goal.type == .Exe || m.goal.type == .Relocatable || m.goal.type == .Dynamic);

MAX_GOT_SIZE :: 1.shift_left(15);
SIZE_OF_STUBS :: 1.shift_left(10); // TODO: not this :explain
COMMANDS_SIZE_GUESS :: macos_page_size; // :explain

fn init_default_module(m: *QbeModule, goal: QbeTargetEnv, expecting_rwx: bool) void = 
    init(m, 1.shift_left(25), libc_allocator, goal, expecting_rwx);

// - the allocators need to not be temp() because we reset temp()
// - expecting_rwx only matters when jitting. See the comment on use_map_jit_on_apple_silicon if running on macos. 
//   if you jit with expecting_rwx=false, you have to call make_exec before trying to run the code. 
//   Regardless, be careful of cache coherency on aarch64, see comment on lib/sys/process.fr/clear_instruction_cache().
fn init(m: *QbeModule, segment_size: i64, gpa: Alloc, goal: QbeTargetEnv, expecting_rwx: bool) void = {
    gpa = todo_allocator;
    m[] = QbeModule.zeroed();
    m.supports_self_sign = true;
    m.forever = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    forever := m.forever&.borrow();
    m.segments = EnumMap(SegmentType, SegmentCursor).zeroed();
    m.use_map_jit_on_apple_silicon = expecting_rwx && goal.type == .JitOnly && goal.arch == .aarch64 && goal.os == .macos;
    
    // don't forget to update init_empty_for_template_only too!
    @if(use_threads) {
        pthread_mutex_init(m.types_mutex&, 0).unwrap();
        pthread_mutex_init(m.icache_mutex&, 0).unwrap();
        pthread_mutex_init(m.intern_mutex&, 0).unwrap();
    };
    
    mem := if !m.use_map_jit_on_apple_silicon {
        page_allocator.alloc(u8, segment_size * 3)
    } else {
        F :: LibcType(.macos).MapFlag; // only apple makes us the MAP_JIT shenanigans. 
        fd: Fd = (fd = -1);
        flag := bit_or(@as(i64) F.Private, @as(i64) F.Anonymous); 
        
        // Reserve the whole contiguous area we want.  
        // It seems that MAP_JIT and then MAP_FIXED over it is fine, but the reverse is not. 
        prot_WX := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Write);
        flag_WX := bit_or(flag, @as(i64) F.Jit);  // :JitMemProtect .Jit means one thread will be able to write and one will be able to execute. 
        ptr := mmap(zeroed(rawptr), segment_size * 3, prot_WX, flag_WX, fd, 0);
        @assert_gt(ptr.int_from_rawptr(), 0, "mmap Jit failed");
        
        // Now remap over the data sections so they're not executable and both threads can write to them. 
        prot_RW := bit_or(@as(i64) MapProt.Read, @as(i64) MapProt.Write);
        flag_RW := bit_or(flag, @as(i64) F.Fixed); // .Fixed means the address hint will not be ignored. 
        ptr_rw := ptr.offset(segment_size);
        res := mmap(ptr_rw, segment_size * 2, prot_RW, flag_RW, fd, 0);
        @assert_eq(ptr_rw, res, "mmap Fixed failed");
        
        // So now we have a contigous chunk where the first third is MAP_JIT and the rest is normal.
        mem: []u8 = (ptr = u8.ptr_from_raw(ptr), len = segment_size * 3);
        // TODO: if we wanted to allow jit and then emit macho from the same module, you'd have to remap the `commands: SegmentCursor` as well. 
        mem
    };
    
    if !m.use_map_jit_on_apple_silicon && expecting_rwx {
        // Only the code segment needs to be executable. 
        prot := bit_or(@as(i64) MapProt.Exec, bit_or(@as(i64) MapProt.Read, @as(i64) MapProt.Write));
        mprotect(u8.raw_from_ptr(mem.ptr), segment_size, prot);
    };
    
    // order matters!
    m.start_of_writable_code = mem.ptr;
    make_segment(mem, .Code, 0, segment_size);  // :CodeIsFirst
    make_segment(mem, .ConstantData, segment_size, segment_size*2);
    make_segment(mem, .MutableData, segment_size*2, segment_size*3);
    
    make_segment :: fn(mem: []u8, s: SegmentType, start: i64, end: i64) void => {
        m.segments&.insert(s, (mmapped = mem.slice(start, end), next = mem.index(start)));
    };
    
    m.initialized = true;
    m.goal = goal;
    m.exports = list(gpa);
    m.imports = list(gpa);
    m.local_needs_reloc = list(gpa);
    m.inlinable_but_referenced_later = list(gpa);
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    {   // TODO: this is kinda ass
        // :GotAtConstantDataBase
        next := m.segments&[.ConstantData].next&;
        m.got = (mmapped = (ptr = next[], len = MAX_GOT_SIZE), next = next[]);
        next[] = next[].offset(MAX_GOT_SIZE); 
        
        if goal.type == .JitOnly {
            range(0, MAX_GOT_SIZE / 8) { i |
                tried_to_call_uncompiled :: fn() void = {
                    scary_log("ICE: Tried to call an uncompiled function");
                    abort();
                };
                ptr_cast_unchecked(u8, @FnPtr() void, m.got.mmapped.ptr.offset(i * 8))[] = tried_to_call_uncompiled;
            };
        };
        
        next := m.segments&[.Code].next&;
        m.commands = (mmapped = (ptr = next[], len = COMMANDS_SIZE_GUESS), next = next[]);
        next[] = next[].offset(COMMANDS_SIZE_GUESS); 
        m.stubs = (mmapped = (ptr = next[], len = SIZE_OF_STUBS), next = next[]);
        next[] = next[].offset(SIZE_OF_STUBS); 
    };
    
    m.debug_out = fdopen(STD_ERR, "a".sym().c_str());
    assert(m.goal.os == .macos, "TODO: flip the switch on linux abi");
    m.types  = forever.box(@type m.types[]);
    
    //m.insb   = forever.box(@type m.insb[]); // :FUCKED i overflow the size becuase i figured 65k bytes aught to be enough for everyone. -- Oct 8  
    mem := forever.alloc(Qbe.Ins, Qbe.MaxInstructions);
    ::?*Array(Qbe.Ins, Qbe.MaxInstructions);
    m.insb = mem.as_array().unwrap();
    
    m.curi   = forever.box(@type m.curi[]); // TODO: megadumb if its the page allocaotr
    m.debug  = forever.box_zeroed(@type m.debug[]);
    m.target = forever.box(@type m.target[]);
    
    t := m.target;
    @match(m.goal.arch) {
        fn aarch64() => t.fill_target_arm(m.goal.os == .macos);
        fn x86_64()  => t.fill_target_amd(m.goal.os == .macos);
        fn wasm32()  => {
            t.finish_passes = finish_qbe_passes_wasm;
            t.abi0 = apple_extsb; // "Signed 8 and 16-bit scalars are sign-extended, and unsigned 8 and 16-bit scalars are zero-extended before passing or returning." 
            t.emit_fn = emit_func_wasm32;
        };
    };
    
    t.finish_module = @if_else {
        @if(m.goal.os == .macos)     => output_macho;
        @if(m.goal.arch == .wasm32)  => output_wasm_module;
        @if(m.goal.type == .JitOnly) => (fn(m: *QbeModule) [][]u8 = empty());
        @else => panic("TODO: we don't support this target");
    };
    
    m.types[] = m.new_long_life(0); // TODO: check the magic in QbeBuilder.init to make sure you remember to do this. 
    m.curi[] = m.insb.index_unchecked(Qbe.MaxInstructions);
    m.symbol_memmove = m.intern("memmove");
}

fn init_empty_for_template_only(m: *QbeModule, gpa: Alloc) void = {
    m[] = QbeModule.zeroed();
    m.initialized = true;
    m.forever = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    forever := m.forever&.borrow();
    @if(use_threads) {
        pthread_mutex_init(m.types_mutex&, 0).unwrap();
        pthread_mutex_init(m.icache_mutex&, 0).unwrap();
        pthread_mutex_init(m.intern_mutex&, 0).unwrap();
    };
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    //m.debug_out = fdopen(STD_ERR, "a".sym().c_str());
    m.types  = forever.box(@type m.types[]);
    mem := forever.alloc(Qbe.Ins, Qbe.MaxInstructions);
    ::?*Array(Qbe.Ins, Qbe.MaxInstructions);
    m.insb = mem.as_array().unwrap();
    m.curi   = forever.box(@type m.curi[]); // TODO: megadumb if its the page allocaotr
    m.debug  = forever.box_zeroed(@type m.debug[]);
    m.target = forever.box_zeroed(@type m.target[]);
    m.types[] = m.new_long_life(0); // TODO: check the magic in QbeBuilder.init to make sure you remember to do this. 
    m.curi[] = m.insb.index_unchecked(Qbe.MaxInstructions);
}

fn drop(m: *QbeModule) void = {
    fst := m.segments.data&[0].mmapped;
    page_allocator.dealloc(u8, (ptr = fst.ptr, len = fst.len * m.segments.data&.len()));
    @if(use_threads) {
        pthread_mutex_destroy(m.types_mutex&).unwrap(); 
        pthread_mutex_destroy(m.icache_mutex&).unwrap(); 
        pthread_mutex_destroy(m.intern_mutex&).unwrap(); 
    };
    m.forever&.deinit();
    m[] = zeroed(@type m[]);  // just to make sure you crash if you try to use it again. 
}

fn get_qbe_dylib_variable($T: Type, $name: Str) *T #generic = {
    lib := dlopen("./bindings/qbe/qbe_merged.dylib".sym().c_str(), DlFlag.Lazy);
    assert(!lib.lib.is_null(), "failed to open qbe dylib");
    ref := dlsym(lib, name.sym().c_str());
    if(ref.is_null(), => @panic("failed to get '%'", name));
    T.ptr_from_raw(ref)
}

::ptr_utils(*Qbe.Blk);

run_qbe_passes :: fn(fnn: *Qbe.Fn) void = {
    mark := mark_temporary_storage();
    m := fnn.globals;
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "function module not set");
    
    if fnn.globals.debug["P".char()] {
        out := fnn.globals.debug_out;
        write(out, "\n> After parsing:\n");
        printfn(fnn, out);
    };
    
    {m.target.abi0}(fnn);
    
    inlcalls(fnn);

    fillrpo(fnn);
    fillpreds(fnn);
    fill_use(fnn);
    promote(fnn); // optional
    fill_use(fnn);
    convert_to_ssa(fnn);
    fill_use(fnn);
    ssacheck(fnn); // debug
    fillalias(fnn); // !
    loadopt(fnn); // optional
    fill_use(fnn);
    fillalias(fnn); // for coalesce
    coalesce(fnn); // optional
    fill_use(fnn); 
    filldom(fnn);  // TODO: do we need this here? i dont think anything breaks it after convert_to_ssa does it
    ssacheck(fnn); // debug
    
    copy_elimination(fnn); // optional
    fill_use(fnn);
    fold_constants(fnn);
    if inlsave(fnn) {
        // all uses might be inlined so we suspend processing until we know you took a function pointer later. 
        if m.debug["G".char()] {
            @fmt_write(m.debug_out, "\nSuspending inlinable function: %\n", fnn.name());
        };
        return();
    };
    {m.target.finish_passes}(fnn);
    reset_temporary_storage(mark);
};
::[]u64;

fn finish_qbe_passes_native(fnn: *Qbe.Fn) void = {
    if fnn.mem.first.is_null() {
        fnn.mem = new(0);
    };
    m := fnn.globals;
    {m.target.abi1}(fnn);
    
    simplify(fnn);
    fillpreds(fnn);
    fill_use(fnn); 
    
    // :TodoFinishAmdSlots
    if fnn.globals.goal.arch == .aarch64 {
        // TODO: don't always need extra fillrpo? only if vararg or any_dead_blocks in fold_constants
        fillrpo(fnn);   // for fillalias. 
        fillalias(fnn); // for late slots mem opt, escaping slots are tracked at the beginning of isel. 
    };
    // arm isel inserts RMem (with different meaning from x64) for mul/store and RInt for load.
    {m.target.isel}(fnn);  // writes tmp.visit
                           // ^^^^^^^^^^^^^^^^
    elide_abi_slots(fnn);  // reads  tmp.visit
    
    fillrpo(fnn);
    fill_liveness(fnn);
    fillloop(fnn); // optional
    fillcost(fnn); // optional? includes filluse (counts only)
    spill(fnn);
    
    register_allocation(fnn);
    fillrpo(fnn);
    simplify_jump_chains(fnn); // optional
    fillpreds(fnn);
    fillrpo(fnn);
    
    // :LinkIsNowRpo
    set_link_from_rpo(fnn);
    
    emit_fn(fnn);
}

fn finish_qbe_passes_wasm(fnn: *Qbe.Fn) void = {
    if fnn.mem.first.is_null() {
        fnn.mem = new(0);
    };
    m := fnn.globals;
    wasm_abi(fnn);
    
    // TODO: wasm has memory.copy which we should use for blit instead of a bunch of ldr+str
    simplify(fnn);
    fillpreds(fnn);
    fill_use(fnn); 
    
    wasm_isel(fnn);
    // TODO: elide_abi_slots
    
    // TODO: maybe we still want liveness info even though we're not doing register allocation?
    
    fillrpo(fnn);
    simplify_jump_chains(fnn); // optional
    fillpreds(fnn);
    fillrpo(fnn);
    
    // TODO: convert to structured control flow as a seperate pass?
    
    // :LinkIsNowRpo
    set_link_from_rpo(fnn);
    
    emit_fn(fnn);
}

// debug. but it calls clsmerge which does mutate sometimes so maybe not.
fn fails_typecheck(f: *Qbe.Fn) ?Str = {
    fillpreds(f);
    pb  := init_bitset(f.nblk.zext());
    ppb := init_bitset(f.nblk.zext());
    for_blocks f { b | 
        for_phi b { p |
            if rtype(p.to) != .RTmp {
                return(Some = @tfmt("phi destination must be a tmp"));
            };
            f.get_temporary(p.to)[].cls = p.cls; // TODO: creepy. i want this to be optional debug check?
        };
        for_insts_forward b { i | 
            if rtype(i.to) == .RTmp {
                t := f.get_temporary(i.to);
                if clsmerge(t.cls&, i.cls()) {          // ^
                    return(Some = @tfmt("temporary %% is assigned with multiple types", "%", t.name()));
                };
            };
        };
    };
    for_blocks f { b |
        bszero(pb&);
        range(0, b.npred.zext()) { n |
            bsset(pb&, b.pred[n].id.zext());
        };
        for_phi b { p |
            bszero(ppb&);
            t := f.get_temporary(p.to);
            range(0, p.narg.zext()) { n |
                k := t.cls;
                if bshas(ppb&, p.blk[n].id.zext()) {
                    return(Some = @tfmt("multiple entries for @% in phi %%", p.blk[n].name(), "%", t.name()));
                };
                if !usecheck(p.arg[n], k, f) {
                    return(Some = @tfmt("invalid type (%) for operand %% in phi %%", k.raw(), "%", f.get_temporary(p.arg[n]).name(), "%", t.name()));
                };
                bsset(ppb&, p.blk[n].id.zext());
            };
            if !bsequal(pb&, ppb&) {
                return(Some = @tfmt("predecessors not matched in phi %%", "%", t.name()));
            };
        };
        for_insts_forward b { i |
            range(0, 2) { n |
                continue :: local_return;
                k := argcls(i, n);
                r := i.arg&[n];
                if k == .Ke {
                    return(Some = @tfmt("invalid instruction output type % in %", i.cls(), i.op()));
                };
                if(rtype(r) == .RType, => continue());
                word :: fn() => if(n == 1, => "second", => "first");
                if (rtype(r) != .RNull && k == .Kx) {
                    return(Some = @tfmt("no %s operand expected in %s", word(), i.op()));
                };
                if (rtype(r) == .RNull && k != .Kx) {
                    return(Some = @tfmt("missing % operand in %", word(), i.op()));
                };
                if !usecheck(r, k, f) {
                    return(Some = @tfmt("invalid type for % operand %", word(), i.op()));
                };
            };
        };
    
        r := b.jmp.arg;
        jump_error := false;
        if is_ret(b.jmp.type) {
            k: Qbe.Cls = @if(b.jmp.type == .retc, .Kl, @if(b.jmp.type.raw() >= Qbe.J.retsb.raw(), .Kw, @as(Qbe.Cls) @as(i32) b.jmp.type.raw() - Qbe.J.retw.raw()));
            jump_error = !usecheck(r, k, f);
        };
        if b.jmp.type == .jnz {
            jump_error = !usecheck(r, .Kw, f);
        };
        if jump_error {
            return(Some = @tfmt("invalid type for jump argument in block @%", b.name()));
        };
        
        if !b.s1.is_null() && b.s1.jmp.type == .Jxxx {   
            return(Some = @tfmt("block @% is used undefined", b.s1.name()));
        };
        if !b.s2.is_null() && b.s2.jmp.type == .Jxxx {
            return(Some = @tfmt("block @% is used undefined", b.s2.name()));
        };
    };
    .None
}

fn usecheck(r: Qbe.Ref, k: Qbe.Cls, f: *Qbe.Fn) bool = {
    rtype(r) == .RTmp || return(true);
    t := f.get_temporary(r);
    t.cls == k || (t.cls == .Kl && k == .Kw)
}

// all computers know these days is fill memory and branch
