#include_std("backend/ir.fr");
#include_std("backend/opt/simplify.fr");
#include_std("backend/opt/live.fr");
#include_std("backend/util.fr");
#include_std("backend/opt/copy.fr");
#include_std("backend/emit.fr");
#include_std("backend/opt/ssa.fr");
#include_std("backend/opt/cfg.fr");
#include_std("backend/opt/spill.fr");
#include_std("backend/opt/rega.fr");
#include_std("backend/meta/dump.fr");
#include_std("backend/opt/load.fr");
#include_std("backend/opt/alias.fr");
#include_std("backend/opt/inline.fr");
#include_std("backend/opt/mem.fr");
#include_std("backend/opt/fold.fr");
#use("@/backend/abi.fr");
#use("@/backend/meta/template.fr");
#include_std("backend/opt/slots.fr");
#use("@/lib/leb128.fr");
#use("@/lib/bit_fields.fr");
#use("@/lib/sys/sync/mutex.fr");

Arm  :: import("@/backend/arm64/bits.fr");
Amd  :: import("@/backend/amd64/bits.fr");
Wasm :: import("@/backend/wasm/bits.fr");

fn init_default_module_dyn(m: *QbeModule, vtable: *ImportVTable, goal: QbeTargetEnv) void = {  
    @assert(ptr_cast_unchecked(@type vtable.init_default_qbe_module, i64, vtable.init_default_qbe_module&)[] != 0, "init_default_qbe_module is not enabled");
    {vtable.init_default_qbe_module}(QbeModule.raw_from_ptr(m), QbeTargetEnv.raw_from_ptr(goal&));
}

// easy-ish (?) things to try to get used to making changes. 
// TODO: if they don't do it already: 
//       - track known truthyness per block (based on jumps in dominators)
//         to remove redundant checks (like bounds checks). that use kinda relies on inlining tho.
//       - jmp on the not (xor -1) of a cmp should just flip the cmp. 
// TODO: don't bother emitting nops (when copying with emiti() anyway)? is avoiding a branch worth your blocks using extra memory
// TODO: :Explain :UnacceptablePanic :SketchyIterTargets :TodoPort :SketchPlatformRegister :nullable
//       :MakeATestThatFails places where i don't know why qbe did something and all my current tests work if you change it. 
//                           so try to make a test case that requires qbe's behaviour or remove the extra code. 
//       :HardcodeAlloc 
//       :force_dep_on_memmove_hack
// TODO: fix tests/broken/todo.fr
// TODO: the trick of having a `BitSet w[1]` so it auto decays to a pointer is cute. it's kinda a shame i can't do that. 
// TODO: fix panic on overflow block name in rega
// TODO: don't use libc file structs all over the place. have my own non-painful buffered io. 
// TODO: look at the asm and make sure im not uxtb before every cmp #0 on a bool. have to fix the ir i generate.
//       do i need to be able to express that a function returns exactly 0 or 1?
//       once i expose w ops to the language i can just generate those instead of l for working with bools and then it should be fine?
// TODO: dead store elimination would make small examples generate tighter code. 
// TODO: don't do extra copies of arg/ret when just passing through to another call.
// TODO: arm isel can use ldp/stp for consecutive loads/stores and simplify:blit can do it in pairs to make that happen. 
//       i do a lot of copies so that might be an easy win for code size at least. 
// TODO: ir test that uses opaque types

QbeModule :: @rec @struct(
    // When we linked with qbe, these were pointers to it's externs so both sides could use them. TODO: some should be values now. 
    types: *QList(Qbe.Typ), // typ
    insb: *Array(Qbe.Ins, 1.shift_left(20)), // MaxInstructions
    curi: **Qbe.Ins,
    target: *Qbe.Target,
    debug: *Array(bool, "Z".char() + 1),
    
    number_of_types := 0,
    debug_out: *List(u8),
    symbols: []SymbolBucket,
    
    // stuff for emitting code.
    segments: EnumMap(SegmentType, SegmentCursor),
    fixups_locked := false,
    goal: QbeTargetEnv,
    start_of_writable_code: *u8,
    // these are special sections that live inside the main segments. 
    got: SegmentCursor,
    commands: SegmentCursor,
    stubs: SegmentCursor,
    
    exports: List(u32),
    imports: List(u32),
    local_needs_reloc: List(u32),  // OR for wasm: all local functions in order
    inlinable_but_referenced_later: List(u32),
    
    initialized: bool,
    
    // When this is set, forward references to symbols always emit an inline load from the global 
    // offset table instead of using an instruction with a direct offset that must be patched later.
    // this is useful for jitting without needing to go back and modify code. 
    got_indirection_instead_of_patches: bool,
    
    // set env FRANCA_DEBUG_THREADS to anything to enable. 
    convert_stores_to_atomic_for_debugging: bool,
    
    forever: ArenaAlloc,
    symbol_memmove: u32,
    types_mutex: Mutex,
    last_function_end: *u8,
    icache_mutex: Mutex,
    intern_mutex: Mutex,
    
    wasm_function_type_count: i32,
    wasm_symbol_stackbase: u32,
    wasm_symbol_env_parameter: u32,
    wasm_has_any_indirect_calls: bool,
    wasm_any_pare: bool,
    
    start_of_executable_stubs: *u8,
    
    debug_headers: List(u8),
    debug_data: List(u8),
    debug_source: []u8,
    debug_last_off_func: i64,
    debug_last_off_source: i64,
    
    llvm_global_value_buf: List(u8),
    llvm_global_type_buf: List(u8),
    llvm_global_code_buf: List(u8),
    debug_out_storage: List(u8),
    dis: DisassemblerFunc,
    need_write_protect: bool,
    use_map_jit_on_apple_silicon: bool,
    wasm_types_cache: HashMap([]u8, i32),
);

#use("@/lib/collections/map.fr");

DisassemblerFunc :: @FnPtr(out: *List(u8), code: []u8, arch: Arch) void;
    
QbeTargetEnv :: @struct(
    arch: Arch,
    // - should we use apple's abi changes?
    // - should we emit Mach-O or ELF?
    os: Os,
    type: QbeOutputPurpose,
);

// TODO: remove AsmText
QbeOutputPurpose :: @enum(i64) (AsmText, JitOnly, Exe, Relocatable, Dynamic);
::enum(QbeOutputPurpose);

MAX_GOT_SIZE :: 1.shift_left(15);
SIZE_OF_STUBS :: 1.shift_left(11); // TODO: not this :explain
COMMANDS_SIZE_GUESS :: 16384; // :explain

CHAINED_SIZE_GUESS :: 2000000;  // TODO: prescan to caluclate?
text_padding :: SIZE_OF_STUBS + COMMANDS_SIZE_GUESS;

fn init_default_module(m: *QbeModule, goal: QbeTargetEnv, expecting_rwx: bool) void = {
    dis: DisassemblerFunc = @if(is_linking_libc(), llvm_mc_dis, 
        fn(out: *List(u8), _: []u8, __: Arch) = out.push_all("no disassembler included\n"),
    );
    init(m, 1.shift_left(25), general_allocator(), goal, expecting_rwx, dis)
}

// 
// :JitMemProtect (use_map_jit_on_apple_silicon)
//
// They won't let you have RWX pages, but if you call pthread_jit_write_protect_np, you can have pages that 
// are writable on one thread but executable on another by passing MAP_JIT to mmap, which would be great, 
// except that you control the protection for ~every~ MAP_JIT page together per thread. So you can't 
// have a jitted program create a new MAP_JIT area and write code into it. A work around would be having the 
// jitted code write into other memory and then call a fully AOT function that toggles protection and copies 
// the into the MAP_JIT memory. This also isn't a problem if there's a clear boundery between when you're creating 
// code and executing it because you can just call mprotect at the end when you're done. 
//
// The main take away is the franca compiler wants to use this so comptime franca code cannot set expecting_rwx to true on aarch64 macos. 
//

// - the allocators need to not be temp() because we reset temp()
// - expecting_rwx only matters when jitting. if false, you have to call make_exec before trying to run the code. 
//   Regardless, be careful of cache coherency on aarch64, see comment on lib/sys/process.fr/clear_instruction_cache().
// - 'dis' is called for debug["D"]. the default one above just execs 'llvm-mc'. 
//   But i need to be able to override it when targetting blink to not pull in those libc functions. 
fn init(m: *QbeModule, segment_size: i64, gpa: Alloc, goal: QbeTargetEnv, expecting_rwx: bool, dis: DisassemblerFunc) void = {
    gpa = todo_allocator;
    m[] = QbeModule.zeroed();
    lock(m.intern_mutex&);  // surely this does nothing at this point but just for fun
    m.dis = dis;
    m.forever = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    forever := m.forever&.borrow();
    m.segments = EnumMap(SegmentType, SegmentCursor).zeroed();
    m.use_map_jit_on_apple_silicon = expecting_rwx && goal.type == .JitOnly && goal.arch == .aarch64 && goal.os == .macos;
    
    mem := if !m.use_map_jit_on_apple_silicon {
        page_allocator.alloc(u8, segment_size * 3)
    } else {
        F :: LibcType(.macos).MapFlag; // only apple makes us the MAP_JIT shenanigans. 
        fd: Fd = (fd = -1);
        flag := bit_or(@as(i64) F.Private, @as(i64) F.Anonymous); 
        
        // Reserve the whole contiguous area we want.  
        // It seems that MAP_JIT and then MAP_FIXED over it is fine, but the reverse is not. 
        prot_WX := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Write);
        flag_WX := bit_or(flag, @as(i64) F.Jit);  // :JitMemProtect .Jit means one thread will be able to write and one will be able to execute. 
        ptr := mmap(zeroed(rawptr), segment_size * 3, prot_WX, flag_WX, fd, 0);
        @assert_gt(ptr.int_from_rawptr(), 0, "mmap Jit failed");
        
        // Now remap over the data sections so they're not executable and both threads can write to them. 
        prot_RW := bit_or(@as(i64) MapProt.Read, @as(i64) MapProt.Write);
        flag_RW := bit_or(flag, @as(i64) F.Fixed); // .Fixed means the address hint will not be ignored. 
        ptr_rw := ptr.offset(segment_size);
        res := mmap(ptr_rw, segment_size * 2, prot_RW, flag_RW, fd, 0);
        @assert_eq(ptr_rw, res, "mmap Fixed failed");
        
        // So now we have a contigous chunk where the first third is MAP_JIT and the rest is normal.
        mem: []u8 = (ptr = u8.ptr_from_raw(ptr), len = segment_size * 3);
        // TODO: if we wanted to allow jit and then emit macho from the same module, you'd have to remap the `commands: SegmentCursor` as well. 
        mem
    };
    
    if !m.use_map_jit_on_apple_silicon && expecting_rwx {
        // Only the code segment needs to be executable. 
        prot := bit_or(@as(i64) MapProt.Exec, bit_or(@as(i64) MapProt.Read, @as(i64) MapProt.Write));
        mprotect(u8.raw_from_ptr(mem.ptr), segment_size, prot).unwrap();
    };
   
    // :SegmentsAreTheSameSize 
    // order matters!
    make_segment(mem, .Code, 0, segment_size);  // :CodeIsFirst
    make_segment(mem, .ConstantData, segment_size, segment_size*2);
    make_segment(mem, .MutableData, segment_size*2, segment_size*3);
    
    // No backing allocation. It's just for aot for now. 
    m.segments&[.ZeroInitData].mmapped.ptr = mem.ptr.offset(segment_size * 3);
    m.segments&[.ZeroInitData].next = m.segments&[.ZeroInitData].mmapped.ptr;
    
    make_segment :: fn(mem: []u8, s: SegmentType, start: i64, end: i64) void => {
        m.segments&.insert(s, (mmapped = mem.slice(start, end), next = mem.index(start)));
    };
    
    m.initialized = true;
    m.goal = goal;
    m.exports = list(gpa);
    m.imports = list(gpa);
    m.local_needs_reloc = list(gpa);
    m.inlinable_but_referenced_later = list(gpa);
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    m.debug_headers = list(gpa);
    m.debug_data = list(gpa);
    {   // TODO: this is kinda ass
        // :GotAtConstantDataBase
        next := m.segments&[.ConstantData].next&;
        m.got = (mmapped = (ptr = next[], len = MAX_GOT_SIZE), next = next[]);
        next[] = next[].offset(MAX_GOT_SIZE); 
        
        if goal.type == .JitOnly {
            range(0, MAX_GOT_SIZE / 8) { i |
                tried_to_call_uncompiled :: fn() void = {
                    panic("ICE: jitted code tried to call an uncompiled function through __GOT");
                };
                ptr_cast_unchecked(u8, @FnPtr() void, m.got.mmapped.ptr.offset(i * 8))[] = tried_to_call_uncompiled;
            };
        };
        
        next := m.segments&[.Code].next&;
        m.commands = (mmapped = (ptr = next[], len = COMMANDS_SIZE_GUESS), next = next[]);
        next[] = next[].offset(COMMANDS_SIZE_GUESS); 
        m.stubs = (mmapped = (ptr = next[], len = SIZE_OF_STUBS), next = next[]);
        m.start_of_executable_stubs = next[];
        next[] = next[].offset(SIZE_OF_STUBS); 
        m.start_of_writable_code = next[];
        
        if m.goal.arch == .wasm32 {
            // :WasmZeroPage Skip the low page so we don't live in crazy town where the null pointer is valid. 
            next := m.segments&[.MutableData].next&;
            next[] = next[].offset(wasm_page_size); 
        };
    };
    
    m.debug_out_storage = list(forever);
    m.debug_out = m.debug_out_storage&;
    m.types  = forever.box(@type m.types[]);
    
    m.insb   = forever.box(@type m.insb[]);
    
    m.curi   = forever.box(@type m.curi[]); // TODO: megadumb if its the page allocaotr
    m.debug  = forever.box_zeroed(@type m.debug[]);
    m.target = forever.box(@type m.target[]);
    
    debug := get_environment_variable("FRANCA_DEBUG_THREADS");  // TODO: remove this! make it more official
    m.convert_stores_to_atomic_for_debugging = debug.is_some() && m.goal.type != .JitOnly;
    
    EmitWasm :: import("@/backend/wasm/emit.fr");
    
    t := m.target;
    t.data = emit_data_to_segment;
    @match(m.goal.arch) {
        fn aarch64() => import("@/backend/arm64/target.fr")'fill_target_arm(t, m.goal.os == .macos);
        fn x86_64()  => import("@/backend/amd64/target.fr")'fill_target_amd(t, m.goal.os == .macos);
        fn wasm32()  => {
            t.finish_passes = finish_qbe_passes_wasm;
            t.abi0 = apple_extsb; // "Signed 8 and 16-bit scalars are sign-extended, and unsigned 8 and 16-bit scalars are zero-extended before passing or returning." 
            t.emit_fn = EmitWasm.emit_func_wasm32;
            t.fixup = EmitWasm.fixup_wasm32;
            m.wasm_symbol_stackbase = m.intern("__stack_base");
            m.wasm_symbol_env_parameter = m.intern("__env_parameter");
        }
    };
    t.finish_module = @if_else {
        @if(m.goal.arch == .wasm32)  => EmitWasm.output_wasm_module;
        @if(m.goal.type == .JitOnly) => (fn(m: *QbeModule) [][]u8 = empty());
        @if(m.goal.os == .macos)     => import("@/backend/macho/emit.fr").output_macho;
        @if(m.goal.os == .linux)     => import("@/backend/elf/emit.fr").output_elf;
        @else => panic("TODO: we don't support this target");
    };
    
    m.types[] = m.new_long_life(0); // TODO: check the magic in QbeBuilder.init to make sure you remember to do this. 
    m.curi[] = m.insb.index_unchecked(Qbe.MaxInstructions);
    if m.goal.arch != .wasm32 {
        m.symbol_memmove = m.intern("memmove");
    };
    
    m.llvm_global_value_buf = list(general_allocator());
    m.llvm_global_type_buf  = list(general_allocator());
    m.llvm_global_code_buf  = list(general_allocator());
    m.wasm_types_cache      = init(general_allocator());
    
    unlock(m.intern_mutex&);
}

fn init_empty_for_template_only(m: *QbeModule, gpa: Alloc) void = {
    m[] = QbeModule.zeroed();
    m.initialized = true;
    m.forever = init(gpa, size_of(Qbe.Ins) * 1.shift_left(20) + size_of(SymbolBucket) * IMask + 1.shift_left(10));
    forever := m.forever&.borrow();
    m.symbols = forever.alloc_zeroed(SymbolBucket, IMask + 1);
    m.types  = forever.box(@type m.types[]);
    mem := forever.alloc(Qbe.Ins, Qbe.MaxInstructions);
    ::?*Array(Qbe.Ins, Qbe.MaxInstructions);
    m.insb = mem.as_array().unwrap();
    m.curi   = forever.box(@type m.curi[]); // TODO: megadumb if its the page allocaotr
    m.debug  = forever.box_zeroed(@type m.debug[]);
    m.target = forever.box_zeroed(@type m.target[]);
    m.types[] = m.new_long_life(0); // TODO: check the magic in QbeBuilder.init to make sure you remember to do this. 
    m.curi[] = m.insb.index_unchecked(Qbe.MaxInstructions);
}

// TODO: this should go away eventually 
fn show_backend_stats() bool = 
    get_environment_variable("FRANCA_LOG_STATS").is_some();

fn drop(m: *QbeModule) void = {
    fst := m.segments.data&[0].mmapped;
    page_allocator.dealloc(u8, (ptr = fst.ptr, len = fst.len * 3)); // ZeroInitData has no allocation
    m.forever&.deinit();
    m[] = zeroed(@type m[]);  // just to make sure you crash if you try to use it again. 
}

::ptr_utils(*Qbe.Blk);

run_qbe_passes :: fn(f: *Qbe.Fn) void = {
    mark := mark_temporary_storage();
    m := f.globals;
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "function module not set");
    
    flush_debug(f.globals);
    if f.globals.debug["P".char()] {
        out := f.globals.debug_out;
        write(out, "\n> After parsing:\n");
        printfn(f, out);
    };
     
    replace_frontend_ops(f);
    
    if fails_typecheck(f) { err | 
        printfn(f, f.globals.debug_out);
        @panic("IR failed typecheck: %", err);
    };
    
    {m.target.abi0}(f);
    
    inlcalls(f);
    fillrpo(f);
    fillpreds(f);
    fill_use(f);
    promote(f); // optional
    fill_use(f);
    convert_to_ssa(f);
    fill_use(f);
    ssacheck(f); // debug
    fillalias(f); // !
    loadopt(f); // optional
    fill_use(f);
    fillalias(f); // for coalesce
    coalesce(f); // optional
    @if(f.globals.convert_stores_to_atomic_for_debugging) {
        fillalias(f);
        fill_use(f);
        slow_debug_threads(f);
    };
    fill_use(f); 
    filldom(f);  // TODO: do we need this here? i dont think anything breaks it after convert_to_ssa does it
    ssacheck(f); // debug
    
    copy_elimination(f); // optional
    fill_use(f);
    fold_constants(f);
    if inlsave(f) {
        // all uses might be inlined so we suspend processing until we know you took a function pointer later. 
        if m.debug["G".char()] {
            @fmt_write(m.debug_out, "\nSuspending inlinable function: %\n", f.name());
        };
        return();
    };
    {m.target.finish_passes}(f);
    reset_temporary_storage(mark);
};
::[]u64;

fn finish_qbe_passes_native(f: *Qbe.Fn) void = {
    if f.mem.first.is_null() {
        f.mem = new(0);
    };
    m := f.globals;
    with m.types_mutex& {
        {m.target.abi1}(f);
    };
    
    simplify(f);
    fillpreds(f);
    fill_use(f); 
    
    // TODO: don't always need extra fillrpo? only if vararg or any_dead_blocks in fold_constants
    fillrpo(f);   // for fillalias. 
    fillalias(f); // for late slots mem opt, escaping slots are tracked at the beginning of isel. 
        
    // arm isel inserts RMem (with different meaning from x64) for mul/store and RInt for load.
    {m.target.isel}(f);
    elide_abi_slots(f); 
    
    fillrpo(f);
    fill_liveness(f);
    fillloop(f); // optional
    fillcost(f); // optional? includes filluse (counts only)
    spill(f);
    
    register_allocation(f);
    fillrpo(f);
    simplify_jump_chains(f); // optional
    fillpreds(f);
    fillrpo(f);
    
    // :LinkIsNowRpo
    set_link_from_rpo(f);
    
    emit_fn(f);
}

fn finish_qbe_passes_wasm(f: *Qbe.Fn) void = {
    if f.mem.first.is_null() {
        f.mem = new(0);
    };
    m := f.globals;
    with m.types_mutex& {
        import("@/backend/wasm/abi.fr")'wasm_abi(f);
    };

    // TODO: wasm has memory.copy which we should use for blit instead of a bunch of ldr+str
    simplify(f);
    fillpreds(f);
    fill_use(f); 
    
    import("@/backend/wasm/isel.fr")'wasm_isel(f);
    
    // TODO: elide_abi_slots
    
    // TODO: maybe we still want liveness info even though we're not doing register allocation?
    
    // TODO: you want to do this but it breaks Blk.wasm_type. can it just go before isel? but still need to add something because split insts from jmp arg needs to be treated like phis. 
    //simplify_jump_chains(f); // optional
    
    
    fill_use(f); 
    
    emit_fn(f);
}

// TODO: detect non-unique block ids. 
fn fails_typecheck(f: *Qbe.Fn) ?Str = {
    // TODO: disabling this in -unsafe builds seems fine but it's sketchy that it calls clsmerge which does mutate sometimes. 
    @if(!::safety_check_enabled(.DebugAssertions)) return(.None);
    
    fillpreds(f);
    pb  := init_bitset(f.nblk.zext());
    ppb := init_bitset(f.nblk.zext());
    for_blocks f { b | 
        for_phi b { p |
            if rtype(p.to) != .RTmp {
                return(Some = @tfmt("phi destination must be a tmp"));
            };
            f.get_temporary(p.to)[].cls = p.cls; // TODO: creepy. i want this to be optional debug check?
        };
        for_insts_forward b { i | 
            if rtype(i.to) == .RTmp {
                t := f.get_temporary(i.to);
                if clsmerge(t.cls&, i.cls()) {          // ^
                    return(Some = @tfmt("temporary %% is assigned with multiple types (% vs %)", "%", t.name(), t.cls, i.cls()));
                };
            };
        };
    };
    for_blocks f { b |
        bszero(pb&);
        range(0, b.npred.zext()) { n |
            bsset(pb&, b.pred[n].id.zext());
        };
        for_phi b { p |
            bszero(ppb&);
            t := f.get_temporary(p.to);
            range(0, p.narg.zext()) { n |
                k := t.cls;
                if bshas(ppb&, p.blk[n].id.zext()) {
                    return(Some = @tfmt("multiple entries for @% in phi %%", p.blk[n].name(), "%", t.name()));
                };
                if !usecheck(p.arg[n], k, f) {
                    return(Some = @tfmt("invalid type (%) for operand %% in phi %%", k.raw(), "%", f.get_temporary(p.arg[n]).name(), "%", t.name()));
                };
                bsset(ppb&, p.blk[n].id.zext());
            };
            if !bsequal(pb&, ppb&) {
                return(Some = @tfmt("predecessors not matched in phi %%", "%", t.name()));
            };
        };
        for_insts_forward b { i |
            continue :: local_return;
            if i.op() == .syscall {
                i0 := b.find_first_arg(i);
                count := ptr_diff(i0, i);
                if(count > 6, => return(Some = "syscall has too many arguments"));
                if(i.arg&[1] != QbeNull, => return(Some = "syscall cannot have aggregate return value"));
                for(i0, i) { iarg |
                    if(iarg.op() != .arg, => return(Some = "syscalls cannot have aggragate, variadic, or env arguments"));
                };
                continue();
            };
            range(0, 2) { n |
                continue :: local_return;
                k := argcls(i, n);
                r := i.arg&[n];
                if k == .Ke {
                    return(Some = @tfmt("invalid instruction output type % in %", i.cls(), i.op()));
                };
                if(rtype(r) == .RType, => continue());
                word :: fn() => if(n == 1, => "second", => "first");
                if rtype(r) != .RNull && k == .Kx {
                    return(Some = @tfmt("no % operand expected in %", word(), i.op()));
                };
                if rtype(r) == .RNull && k != .Kx {
                    if i.op() == .copy && isreg(i.to) {
                        continue();  // :ExprLevelAsm
                    };
                    return(Some = @tfmt("missing % operand in %", word(), i.op()));
                };
                if !usecheck(r, k, f) {
                    return(Some = @tfmt("invalid type for % operand of % in block @%", word(), i.op(), b.name()));
                };
            };
            if is_store(i.op()) && i.to != QbeNull { 
                return(Some = @tfmt("store instruction must not have result"));
            };
        };
    
        r := b.jmp.arg;
        jump_error := false;
        if is_ret(b.jmp.type) {
            k: Qbe.Cls = @if(b.jmp.type == .retc, .Kl, @if(b.jmp.type.raw() >= Qbe.J.retsb.raw(), .Kw, @as(Qbe.Cls) @as(i32) b.jmp.type.raw() - Qbe.J.retw.raw()));
            jump_error = !usecheck(r, k, f);
        };
        if b.jmp.type == .jnz {
            jump_error = !usecheck(r, .Kw, f) || !@is(rtype(b.jmp.arg), .RTmp, .RCon);
        };
        if @is(b.jmp.type, .ret0, .hlt) {
            jump_error = b.jmp.arg != QbeNull;
        };
        if jump_error {
            return(Some = @tfmt("invalid type for jump argument in block @%", b.name()));
        };
        
        @debug_assert_ne(b.jmp.type, .switch, "B");
        if !b.s1.is_null() && b.s1.jmp.type == .Jxxx {   
            return(Some = @tfmt("block @% is used undefined", b.s1.name()));
        };
        if !b.s2.is_null() && b.s2.jmp.type == .Jxxx {
            return(Some = @tfmt("block @% is used undefined", b.s2.name()));
        };
    };
    .None
}

fn usecheck(r: Qbe.Ref, k: Qbe.Cls, f: *Qbe.Fn) bool = {
    rtype(r) == .RTmp || return(true);
    t := f.get_temporary(r);
    t.cls == k || (t.cls == .Kl && k == .Kw)
}

// 
// Some operations are provided for frontend convenience but don't actually have a target dependent implementation. 
// - J.switch -> jnz chain
// - O.assert -> jnz+hlt
//
fn replace_frontend_ops(f: *Qbe.Fn) void = {
    new_blocks_list := Qbe.Blk.ptr_from_int(0);
    last_block := f.start;
    changed := false;
    switches := f.switches.slice(0, f.switch_count.zext());
    fail_assertion: ?*Qbe.Blk = .None;
    
    xxx := 0;
    for_blocks f { b | 
        continue :: local_return;
        last_block = b;
        
        { // O.assert
            j := 0;
            while => j < b.nins.zext() {
                i := b.ins.index(j);
                j += 1;
                if i.op() == .assert {
                    r := f.newtmp("front", .Kw);
                    i[] = make_ins(.cnew, .Kw, r, i.arg&[0], QbeConZero);  // TODO: only looking at low 32 bits rn so it could just be jnz directly. but should really look at all 64. 
                    fail := or fail_assertion { 
                        b3 := add_block();
                        b3.jmp.type = .hlt;
                        fail_assertion = (Some = b3);
                        b3
                    };
                    b2 := add_block();
                    b2.s1 = b.s1; b2.s2 = b.s2; b2.jmp = b.jmp;
                    b2.nins = b.nins - j.trunc();
                    b2.ins = new(b2.nins.zext());
                    b2.ins.slice(0, b2.nins.zext()).copy_from(b.ins.slice(j, b.nins.zext()));
                    b.jmp = (type = .jnz, arg = r);
                    b.s1 = b2; b.s2 = fail;
                    b.nins = j.trunc();
                    // this matters if the frontend uses phis (or this is being called late because of slow_debug_threads)
                    for_jump_targets b2 { s |
                        for_phi s { p |  // paste from replacepreds
                            replacepred(p.blk, p.narg.zext(), b2, b);
                        };
                    };
                    b = b2;
                    j = 0;
                };
            };
        };
        
        if b.jmp.type != .switch {
            continue();
        };
        @debug_assert_eq(rtype(b.jmp.arg), .RInt, "J.switch expected RInt index into Fn.switches");
        payload := switches.index(b.jmp.arg.val());
        @debug_assert(@is(rtype(payload.inspect), .RCon, .RTmp), "invalid inspect value for switch");
        
        if payload.case_count == 0 {
            b.jmp = (type = .jmp, arg = QbeNull);
            b.s1 = payload.default;
            continue();
        };
        
        set_s1 :: fn(old_src: *Qbe.Blk, new_src: *Qbe.Blk, dest: *Qbe.Blk) void => {
            new_src.s1 = dest;
            for_phi dest { p | 
                n := index_in_phi(old_src, p);
                p.blk[n] = new_src;
            };
        };
        
        add_block :: fn() => {
            blk := newblk(); 
            f.nblk += 1;
            // build up a chain to add at the end
            blk.link = new_blocks_list;
            new_blocks_list = blk;
            blk
        };
        
        next_block := add_block();
        first_dispatch_block := next_block;
        cases := payload.cases.slice(0, payload.case_count);
        for cases { case |
            continue :: local_return;
            dest, value := case;
            if(dest.identical(payload.default), => continue());
            
            check := next_block;  
            next_block = add_block();
            
            matches := f.newtmp("switch", .Kw);
            check.nins = 1;
            check.ins = new(1);
            check.ins.first[] = make_ins(.ceql, .Kw, matches, payload.inspect, f.getcon(value));
            
            set_s1(b, check, dest);
            check.jmp = (type = .jnz, arg = matches);
            check.s2 = next_block;
        };
        
        set_s1(b, next_block, payload.default);
        next_block.jmp = (type = .jmp, arg = QbeNull);
        
        b.jmp = (type = .jmp, arg = QbeNull);
        b.s1 = first_dispatch_block;
    };
    @debug_assert(last_block.link.is_null());
    last_block.link = new_blocks_list;
}

// Convert every u32/u64 store to a cas that crashes the program if it fails. 
fn slow_debug_threads(f: *Qbe.Fn) void = {
    for_blocks f { b |
        changed := false;
        for_insts_rev b { i |
            i := i[];
            if i.op() == .storel && escapes(i.arg&[1], f) {
                f.move_end_of_block_to_scratch(b, i, changed&);
                @emit_instructions((f = f), (i.arg&[0], i.arg&[1]), """
                @foo
                    %old =l load %1
                    cas0 %1
                    %prev =l cas1 %old, %0
                    %success =l ceql %old, %prev
                    assert %success
                """);
                changed = true;
            } else {
                if changed {
                    f.emit(i[]);
                };
            }
        };
        @if(changed) f.copy_instructions_from_scratch(b); 
    };
    
    // for assert(). silly. just factor it out and do it directly? 
    replace_frontend_ops(f);
    fillpreds(f);
    fillrpo(f);
}

#use("@/lib/collections/enum_map.fr");
#use("@/lib/sort.fr");
#use("@/lib/sys/threads.fr");
#use("@/lib/sys/sync/atomics.fr");

wasm_page_size :: 65536;

// all computers know these days is fill memory and branch
