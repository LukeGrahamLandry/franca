// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

fn BIT(n: i64) u64 #inline = 1.shift_left(n);
fn BIT(n: u8) u64 #inline = 1.shift_left(n.zext());
fn SLOT(i: i64) Qbe.Ref #inline = ref(.RSlot, i.bit_and(0x1fffffff));
fn TMP(i: i64)  Qbe.Ref #inline = ref(.RTmp, i);
fn CALL(i: i64) Qbe.Ref #inline = ref(.RCall, i);
fn CON(i: i64) Qbe.Ref #inline = ref(.RCon, i);
fn INT(i: i64) Qbe.Ref #inline = ref(.RInt, i.bit_and(0x1fffffff));
fn MEM(i: i64) Qbe.Ref #inline = ref(.RMem, i);
fn TYPE(i: i64) Qbe.Ref #inline = ref(.RType, i);

fn TMP(i: i32) Qbe.Ref #inline = TMP(i.intcast());
fn SLOT(i: i32) Qbe.Ref #inline = SLOT(i.intcast());

fn isreg(r: Qbe.Ref) bool #inline = 
    rtype(r) == .RTmp && r.val() < Qbe.Tmp0;

// tmp[t].phi has which id was the .to of a phi with t as an arg. 
// this looks up a chain of phis to see where t is going to end up. 
// if its not used as a phi arg, that's just itself. 
fn phi_dest(t: i64, f: *Qbe.Fn) i64 = {
    t1 := f.tmp[t].phi.zext();
    if(t1 == 0, => return(t)); // t was not a phi arg
    t1 = phi_dest(t1, f);
    f.tmp[t].phi = t1.intcast();  // cache long chains
    t1
}

fn clsmerge(pk: *Qbe.Cls, k: Qbe.Cls) bool = {
    k1 := pk[];
    if k1 == .Kx {
        pk[] = k;
        return(false);
    };
    xx := (k1 == .Kw && k == .Kl); // TODO: compiler bug. fix overloading when you inline this. 
    if xx || (k1 == .Kl && k == .Kw) {
        pk[] = .Kw;
        return(false);
    };
    k1 != k
}

fn slice_pending_scratch(f: *Qbe.Fn) []Qbe.Ins = {
    n := Qbe.MaxInstructions;
    f.globals.insb.items().slice(n - f.len_scratch(), n)
}

fn copy_instructions_from_scratch(f: *Qbe.Fn, b: *Qbe.Blk) void = {
    @assert_ge(f.len_scratch(), 0, "too many instructions");
    idup(b, f.scratch_next().slice(f.len_scratch()));
    f.globals.curi = f.scratch_start(); 
}

// TODO: i should really just use a different version of emit() so this part could just be a memcpy
// hey i heard you like long function names...
fn copy_instructions_from_scratch_reversed_which_means_forwards(f: *Qbe.Fn, b: *Qbe.Blk) void = {
    n := f.len_scratch();
    @assert_ge(n, 0, "too many instructions");
    if n > b.ins.cap {
        // caller better not be trying to put this in the inlining cache
        b.ins.len = 0;
        b.ins&.reserve(n, temp());
    };
    b.ins.len = n;
    i := 0;
    for(f.scratch_next(), f.scratch_start()) { ins |
        b.ins[n - i - 1] = ins[];
        i += 1;
    };
    
	f.globals.curi = f.scratch_start();
}

fn move_end_of_block_to_scratch(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins, changed: *bool) void #inline = {
    if !changed[] {
        f.globals.curi = f.scratch_start();
        ni := ptr_diff(i.offset(1), b.ins.index_unchecked(b.ins.len));
        f.globals.curi = f.globals.curi.offset(-ni);
        icpy(f.globals.curi, i.offset(1).slice(ni));
        changed[] = true;
    }
}

fn scratch_start(f: *Qbe.Fn) *Qbe.Ins #inline =
    f.globals.insb.index_unchecked(Qbe.MaxInstructions);

fn scratch_next(f: *Qbe.Fn) *Qbe.Ins #inline =
    f.globals.curi;

fn reset_scratch(f: *Qbe.Fn) void #inline = {
    f.globals.curi = f.scratch_start(); 
}

fn len_scratch(f: *Qbe.Fn) i64 #inline = 
    f.scratch_next().ptr_diff(f.scratch_start()); 

// ONLY CALL ON THE BACKEND THREAD
// 
// it uses insb/curi as a buffer for emitting new instructions and then copies them out to a newly allocated array at the end of the block. 
// the parser starts at the beginning but the passes start at the end and `emit` in reverse. 
// so i think they scan block backwards. like isel will iterate backwards, adding some instructions, and copying out those it likes,
// and then at the end, copy from the buffer and update the block with that pointer. 
// so they're always in order in the blocks but you always process them backwards. 
//
// TODO: we rely on globals.curi pointing to the last inst we emitted (in isel) :LookAtLastInst
fn emit(f: *Qbe.Fn, op: Qbe.O, k: Qbe.Cls, to: Qbe.Ref, arg0: Qbe.Ref, arg1: Qbe.Ref) *Qbe.Ins = {
    i := f.globals.curi&;
    @debug_assert(!i[].identical(f.globals.insb.as_ptr()), "emit, too many instructions");
    i[] = i[].offset(-1);
    i[][] = make_ins(op, k, to, arg0, arg1);
    i[]
}

// ONLY CALL ON THE BACKEND THREAD
fn emit(f: *Qbe.Fn, i: Qbe.Ins) *Qbe.Ins #inline = 
    f.emit(i&.op(), i&.cls(), i.to, i.arg&[0], i.arg&[1]);

fn newtmp(f: *Qbe.Fn, debug_hint: Str, k: Qbe.Cls) Qbe.Ref = {
    t: i64 = f.ntmp.zext();
    f.ntmp += 1;
    f.tmp&.grow(t + 1);
    f.tmp.slice(t, t + 1).set_zeroed();
    tmp := f.tmp.index(t);
    tmp.name = debug_hint;
    tmp.cls = k;
    tmp.slot = -1;
    tmp.nuse = 1;
    tmp.ndef = 1;
    tmp.defining_block = -1;
    TMP(t)
}

CONMAP :: false;  // TODO: =true fails tests!
fn newcon(f: *Qbe.Fn, c0: *Qbe.Con) Qbe.Ref = {
    ::ptr_utils(@type f.consts[]);
    if CONMAP && !f.consts.is_null() {
        ::AutoHash(Qbe.Con, TrivialHasher);
        ::AutoHash(Qbe.Sym, TrivialHasher);
        ::AutoHash(Ty(u32, u32), TrivialHasher);
        ::AutoEq(Qbe.Con);
        ::AutoEq(Qbe.Sym);
        ::AutoEq(Ty(u32, u32));
        
        return f.consts.get_or_insert(c0[], => {
            f.newcon_unchecked(c0)
        })[];
    } else {
        // deduplicate
        // note: skip one for Qbe.Undef so that remains a marker you can see in the IR 
        //       (and won't show if you just use that bit pattern)
        range(1, f.ncon.zext()) { i |
            c1 := f.con.index(i);
            if c0.sym.id == c1.sym.id && c0.bits._0 == c1.bits._0 && c0.bits._1 == c1.bits._1 {
                return(CON(i));
            };
        };
        return f.newcon_unchecked(c0);
    };
    unreachable()
}

fn newcon_unchecked(f: *Qbe.Fn, c0: *Qbe.Con) Qbe.Ref = {
    push(f.con&, f.ncon&, c0[]);
    CON(f.ncon.zext() - 1)
}

fn getcon(f: *Qbe.Fn, val: i64) Qbe.Ref = 
    f.symcon((id = Qbe.no_symbol), val);

// TODO: call this from elsewhere
fn symcon(f: *Qbe.Fn, name: Str) Qbe.Ref = 
    f.symcon(f.globals.intern(name));

fn symcon(f: *Qbe.Fn, id: Qbe.Sym) Qbe.Ref = 
    f.symcon(id, 0);

fn symcon(f: *Qbe.Fn, id: Qbe.Sym, off: i64) Qbe.Ref = {
    con: Qbe.Con = con(id, off);
    f.newcon(con&)
}

fn new_mem(f: *Qbe.Fn, base: Qbe.Ref, index: Qbe.Ref, offset: i64, scale: i32) Qbe.Ref = {
    push(f.mem&, f.nmem&, (
        base   = base,
        index  = index,
        offset = con((id = Qbe.no_symbol), offset),
        scale  = scale,
    ));
    MEM(f.nmem.zext() - 1)
}

fn type(self: *Qbe.Con) Qbe.ConType = 
    @if(self.sym.id == Qbe.no_symbol, .CBits, .CAddr);

fn bits(self: *Qbe.Con) i64 = 
    self.bits._1.zext().shift_left(32).bit_or(self.bits._0.zext());

fn set_bits(self: *Qbe.Con, i: i64) void = {
    self.bits._1 = i.shift_right_logical(32).trunc();
    self.bits._0 = i.trunc();
}

// TODO: if this is inlined and you pass a fnptr to it, it copies Kw instead of Kl and the high bits are 0. 
//       bug with copy elimination? :CopyElimTruncSymbolBug
fn con(sym: Qbe.Sym, bits: i64) Qbe.Con #noinline = 
    (sym = sym, bits = (bits.trunc(), bits.shift_right_logical(32).trunc()));

// caller better not be trying to put this in the inlining cache
fn idup(b: *Qbe.Blk, src: []Qbe.Ins) void = {
    b.ins.len = 0;
    b.ins&.push_all(src, temp());
}

fn icpy(dest: *Qbe.Ins, src: []Qbe.Ins) void = {
    (@as([]Qbe.Ins)(ptr = dest, len = src.len)).copy_from(src);
}

fn salloc(rt: Qbe.Ref, rs: Qbe.Ref, f: *Qbe.Fn) bool = {
    /* we need to make sure
    * the stack remains aligned
    * (rsp = 0) mod 16
    */
    f.dynalloc = true;
    if rtype(rs) == .RCon {
        sz := f.con[rs.val()]&.bits();
        if sz < 0 || sz >= MAX_i32 - 15 {
            @panic("invalid alloc size %", sz);
        };
        sz = (sz + 15).bit_and(-16);
        f.emit(.salloc, .Kl, rt, f.getcon(sz), QbeNull);
        false
    } else {
        /* r0 = (r + 15) & -16 */
        r0 := f.newtmp("isel", .Kl);
        r1 := f.newtmp("isel", .Kl);
        f.emit(.salloc, .Kl, rt, r0, QbeNull);
        f.emit(.and, .Kl, r0, r1, f.getcon(-16));
        f.emit(.add, .Kl, r1, rs, f.getcon(15));
        if f.tmp[rs.val()].slot != -1 {
            @panic("unlikely alloc argument % for %", f.tmp[rs.val()]&.name(), f.tmp[rt.val()]&.name());
        };
        true
    }
}

// Inclusive!
fn between(o: Qbe.O, $fst: Qbe.O, $lst: Qbe.O) bool #inline = {
    ::assert(fst.raw() < lst.raw(), "between(Qbe.O) args are out of order");
    fst.raw() <= o.raw() && o.raw() <= lst.raw()
}

fn addins(pvins: *RawList(Qbe.Ins), i: Qbe.Ins) void #inline = 
    addins(pvins, i&);

fn addins(pvins: *RawList(Qbe.Ins), i: *Qbe.Ins) void #inline = {
    if i.op() != .nop {
        push(pvins, i[], temp());
    };
}

fn push(b: *Qbe.Blk, i: Qbe.Ins) void #inline = {
    push(b.ins&, i, temp());
}

fn push(b: *Qbe.Blk, op: Qbe.O, k: Qbe.Cls, to: Qbe.Ref, arg0: Qbe.Ref, arg1: Qbe.Ref) void = 
    push(b, make_ins(op, k, to, arg0, arg1));

fn append(dest: *RawList(Qbe.Ins), src_start: *Qbe.Ins, src_end: *Qbe.Ins) void = {
    @debug_assert(!dest.ptr.identical(src_start) || dest.ptr.is_null(), "append to self");
    count := src_start.ptr_diff(src_end);
    dest.reserve(count, temp());
    for(src_start, src_end) { i |
        if i.op() != .nop {
            dest.len += 1;
            dest[dest.len - 1] = i[];
        };
    };
    
    // it feels like this would be better but no, clearing out the nops is faster. -- Nov 20
    // it also feels too fragile that the below only works if you remove the nop check from addins.
    // same problem as :CreepyNop i suppose...
    //old_len := dest_len[];
    //dest_len[] += count;
    //dest.grow(dest_len[]);
    //dest: []Qbe.Ins = (ptr = dest.index(old_len), len = count);
    //src:  []Qbe.Ins = (ptr = src_start, len = count);
    //dest.copy_from(src);
}

fn addbins(b: *Qbe.Blk, pvins: *RawList(Qbe.Ins)) void = 
    append(pvins, b.ins.ptr, b.ins.index_unchecked(b.ins.len));

// we try to line up the opcode numbers so you can branchlessly convert between categories (ie. load<->ext, par<->ext)
fn rebase(value: Qbe.O, new_base: Qbe.O, old_base: Qbe.O) Qbe.O = 
    @as(Qbe.O) @as(i32) new_base.raw() + (value.raw() - old_base.raw());

///////////////////////
/// Condition Codes ///
///////////////////////

fn is_sel_flag(op: Qbe.O) bool =  
    op.between(.selieq, .selfuo);
    
fn iscmp(op: Qbe.O) bool =  // :CmpOrder
    op.between(.ceqw, .cuod);

// TODO: use multiple returns
// TODO: pc: *Qbe.Cmp
fn iscmp(op: Qbe.O, pk: *Qbe.Cls, pc: *i32) bool = { // :CmpOrder
    X :: fn($lo: Qbe.O, $hi: Qbe.O, $k: Qbe.Cls) => 
        if op.between(lo, hi) {
            pc[] = op.raw() - lo.raw() + @if(k.is_float(), Qbe.CmpICount.intcast(), 0);
            pk[] = k;
            return(true);
        };
    
    @if(!op.iscmp()) return(false);
    X(.ceqw, .cultw, .Kw);
    X(.ceql, .cultl, .Kl);
    X(.ceqs, .cuos, .Ks);
    X(.ceqd, .cuod, .Kd);
    unreachable()
}

fn default_init(f: *Qbe.Fn, m: *QbeModule) void = {
    f[] = Qbe.Fn.zeroed();
    f.ret_cls = .Kx;
    f.globals = m;
    f.tmp = new(Qbe.Tmp0);
    f.con = new(fixed_const_count);
    f.mem = new(0);
    ::assert_eq(Qbe.Ref);
    f.consts = temp().box(@type f.consts[]);
    f.consts[] = init(temp());
    push_fixed_consts(f);
    // Add temporaries representing physical registers.
    // For the comptime usecase, the is_float will be wrong for some targets but that's fine becuase register numbers will just be moved to the real function, 
    t := f.globals.target&;
    f.ntmp = Qbe.Tmp0;
    range(0, Qbe.Tmp0) { i |
        tmp := f.tmp.index(i);
        ::[]Qbe.Tmp;
        tmp.slice(1).set_zeroed();
   	    tmp.cls = @if(t.fpr.bit_is_set(i), .Kd, .Kl);  // spill() cares about this
        tmp.slot = -1;
        tmp.defining_block = -1;
    };
    f.track_ir_names = Qbe.TRACK_IR_NAMES;
}

fn push_fixed_consts(f: *Qbe.Fn) void = {
    ::assert_eq(Qbe.Ref);
    assert_eq(f.getcon(0xdeaddead), QbeUndef);
    assert_eq(f.getcon(0), QbeConZero);
    assert_eq(f.ncon.zext(), fixed_const_count);
}

fn is_symbol(f: *Qbe.Fn, r: Qbe.Ref) bool = 
    rtype(r) == .RCon && f.get_constant(r).type() == .CAddr;

fn get_type(m: *QbeModule, i: i64) *Qbe.Typ = {
    @debug_assert(m.types_mutex&.locked_by_me(), "tried to get_type while not holding types_mutex");
    m.types.index(i)
}

fn get_type(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Typ = {
    @debug_assert_eq(rtype(r), .RType);
    f.globals.get_type(r.val())
}

fn alloc_op(align_log2: i32) Qbe.O #inline = {
    /* specific to NAlign == 3 */
    al := if(align_log2 >= 2, => align_log2 - 2, => 0);
    @as(Qbe.O) @as(i32) Qbe.O.alloc4.raw() + al
}

// TODO: this is dumb. i should arrange the numbers so its just flipping a bit. 
//       don't forget to change the order of jmps/flags/ops, iscmp ranges. :CmpOrder
//       but have to wait until i'm not calling into qbe anymore.  -- Oct 8 


// !cc
fn cmpneg(cc: i32) i32 = {
    negate_cc :: @const_slice( // :CmpOrder
        // Cieq   Cine   Cisge   Cisgt   Cisle   Cislt   Ciuge   Ciugt   Ciule   Ciult
   Qbe.Cmp.Cine, .Cieq, .Cislt, .Cisle, .Cisgt, .Cisge, .Ciult, .Ciule, .Ciugt, .Ciuge,
        // Cfeq   Cfge   Cfgt   Cfle   Cflt   Cfne   Cfo   Cfuo
          .Cfne, .Cflt, .Cfle, .Cfgt, .Cfge, .Cfeq, .Cfuo, .Cfo
    );

    ::enum(Qbe.Cmp);
    @debug_assert(cc >= 0 && cc < Qbe.Cmp.enum_count().intcast(), "bad cc");
    negate_cc[cc.zext()].raw()
}

fn cmp_swap(cc: i32) i32 = {
    swap_cc :: @const_slice( // :CmpOrder
        // Cieq   Cine   Cisge   Cisgt   Cisle   Cislt   Ciuge   Ciugt   Ciule   Ciult
   Qbe.Cmp.Cieq, .Cine, .Cisle, .Cislt, .Cisge, .Cisgt, .Ciule, .Ciult, .Ciuge, .Ciugt,
        // Cfeq   Cfge   Cfgt   Cfle   Cflt   Cfne   Cfo   Cfuo
          .Cfeq, .Cfle, .Cflt, .Cfge, .Cfgt, .Cfne, .Cfo, .Cfuo
    );

    @debug_assert(cc >= 0 && cc < Qbe.Cmp.enum_count().intcast(), "bad cc");
    swap_cc[cc.zext()].raw()
}

////////////////
/// Bit Sets ///
////////////////
// TODO: use unsigned division can be shifts (if negative you truncate towards 0 not -inifinity which a shift gives you)

fn bsinit(bs: *Qbe.BSet, n: i64) void #inline = {
    n := (n + Qbe.NBit - 1) / Qbe.NBit;
    bs.nt = n.trunc();
    ::[]u64;
    bs.t = temp().alloc_zeroed(u64, n).as_ptr(); 
}

fn bsequal(a: *Qbe.BSet, b: *Qbe.BSet) bool = {
    @debug_assert(a.nt == b.nt, "BSet mismatch");
    range(0, a.nt.zext()) { i |
        if a.t.offset(i)[] != b.t.offset(i)[] {
            return(false);
        }
    };
    true
}

fn bszero(bs: *Qbe.BSet) void = {
    mem: []u64 = (ptr = bs.t, len = bs.nt.zext());
    mem.set_zeroed();
}

// it's sad how well this would vectorize
fn bscount(bs: *Qbe.BSet) u32 = {
    n: u32 = 0;
    range(0, bs.nt.zext()) { i |
        n += count_ones(bs.t.offset(i));
    };
    n
}

// TODO: make elt unsigned to the division is free
fn bshas(bs: *Qbe.BSet, elt: i64) bool #inline = {
    ::ptr_utils(u64);
    @debug_assert(elt < bs.nt.zext() * Qbe.NBit, "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    mask := BIT(j);
    slot[].bit_and(mask) != 0
}

fn bsset(bs: *Qbe.BSet, elt: i64) void #inline = {
    @debug_assert(elt < bsmax(bs), "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    slot[] = slot[].bit_or(BIT(j));
}

fn bsclr(bs: *Qbe.BSet, elt: i64) void #inline = {
    @debug_assert(elt < bsmax(bs), "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    slot[] = slot[].bit_and(bit_not(BIT(j)));
}

fn bsmax(bs: *Qbe.BSet) i64 #inline =
    bs.nt.zext() * Qbe.NBit;

fn init_bitset(size: i64) Qbe.BSet #inline = {
    // this puts it in the pool thing that freeall deals with. i can just stick an arena allocator somewhere i guess and clear it at the end of the function. 
    // is it worth resetting temp() between passes or should i just do it at the end and not deal with it? 
    s := Qbe.BSet.zeroed();
    bsinit(s&, size); 
    s
}

fn bscopy(dest: *Qbe.BSet, src: *Qbe.BSet) void = {
    copy_no_alias(u64.raw_from_ptr(dest.t), u64.raw_from_ptr(src.t), dest.nt.zext() * 8);
}

fn bsunion(a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_or(b));
fn bsinter(a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_and(b));
fn bsdiff( a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_and(b.bit_not()));

fn BSOP(a: *Qbe.BSet, b: *Qbe.BSet, $fuse: @Fn(a: u64, b: u64) u64) void #inline = {
    @debug_assert(a.nt == b.nt, "BSet mismatch");
    range(0, a.nt.zext()) { i |
        a := a.t.offset(i);
        a[] = fuse(a[], b.t.offset(i)[]);
    };
}

fn bs_setlow(bs: *Qbe.BSet, bits: u64) void = {
    bs.t[] = bs.t[].bit_or(bits);
}

fn bs_clrlow(bs: *Qbe.BSet, bits: u64) void = {
    bs.t[] = bs.t[].bit_and(bit_not(bits));
}

/////////////
/// Lists ///
/////////////
// TODO: use my allocators for this. 

VMagLong :: 0xcabba9e;  // random number chosen by fair die roll
VMagTemp :: 0xca88abe;

fn vnew(len: i64, esz: i64, where: i64, m: *QbeModule) rawptr = {
    ::ptr_utils(QListHeader);
    ::ptr_utils(*QListHeader);

    words := (len * esz + QListHeader.size_of() + 7) / 8;
    ::if([]i64);
    a := @if(where == VMagTemp, temp(), m.forever&.borrow());
    v := a.alloc_uninit(i64, words);
    v  := ptr_cast_unchecked(i64, QListHeader, v.ptr);
    v[] = (where = where, m = m, cap = len, esz = esz);
    QListHeader.raw_from_ptr(v.offset(1))
}

fn vgrow(vpr: rawptr, len: i64) void = {
    vp := ptr_from_raw(*QListHeader, vpr);
    ::ptr_utils(@type vp[][]);
    v := vp[].offset(-1);
    @debug_assert(!v.offset(1).is_null() && !v.is_null(), "vgrow on null QList");
    @debug_assert_ne(v.where, QLIST_DOUBLE_FREE, "QList grow after free ptr=% to len=% esz=%", vpr, len, v.esz);
    if(v.cap >= len, => return());
    len = max(len, v.cap * 2);
    v1 := vnew(len, v.esz, v.where, v.m);
    copy_no_alias(v1, QListHeader.raw_from_ptr(v.offset(1)), v.cap * v.esz);
    vfree(QListHeader.raw_from_ptr(v.offset(1)));
    vp[] = ptr_from_raw(QListHeader, v1);
}

fn debug_clear_owned_memory(v: *QListHeader) void = {
    len := v.cap * v.esz;
    p := ptr_cast_unchecked(@type v[], u8, v.offset(1));
    @if(SLOW_MEMORY_DEBUGGING) p.slice(len).set_bytes(SLOW_MEMORY_JUNK);
}

fn vfree(p: rawptr) void = @if(::safety_check_enabled(.DebugAssertions)) {
    v := QListHeader.ptr_from_raw(p).offset(-1);
    w := v.where;
    v.where = QLIST_DOUBLE_FREE;
    if w == VMagTemp {
        debug_clear_owned_memory(v);
        return();
    };
    @debug_assert_eq(w, VMagLong, "bad QList magic");
    debug_clear_owned_memory(v);
    // not a thing now that we're using module.forever. should profile to see which is better. 
    //a := general_allocator();
    //free(QListHeader.raw_from_ptr(v));
};

// This is a header that goes before the allocated memory for the array. 
// When a field is an QList it points to the first entry of one of these and you offset backwards to get the header. 
QListHeader :: @struct(
    where: i64,
    m: *QbeModule, // nullable
    esz: i64,
    cap: i64,
);

fn QList($T: Type) Type = {
    Self :: @struct(first: *T);
    ::ptr_utils(T);
    
    fn new(len: i64) Self = {
        (first = T.ptr_from_raw(vnew(len, T.size_of(), VMagTemp, QbeModule.ptr_from_int(0))))
    }
    
    fn new_long_life(m: *QbeModule, len: i64) Self = 
        (first = T.ptr_from_raw(vnew(len, T.size_of(), VMagLong, m)));
    
    fn new_copy(items: []T) Self = {
        self: Self = new(items.len);
        self.slice(0, items.len).copy_from(items);
        self
    }
    
    fn new_copy_long_life(m: *QbeModule, items: []T) Self = {
        self: Self = m.new_long_life(items.len);
        self.slice(0, items.len).copy_from(items);
        self
    }
    
    fn new_copy(self: Self, n: i32) Self = 
        new_copy(self.slice(0, n.intcast()));
    
    fn new_copy_long_life(m: *QbeModule, self: Self, n: i32) Self = 
        m.new_copy_long_life(self.slice(0, n.intcast()));
     
    // ensure capacity is >= len
    fn grow(s: *Self, len: i64) void #inline = {
        v := Self.raw_from_ptr(s);
        vgrow(v, len);
    }
    
    fn free(s: Self) void = {
        v := T.raw_from_ptr(s.first);
        vfree(v);
    }
    
    // TODO: bounds checking but its hard as long as i have to keep qbe's memory layout because the fields aren't together. 
    //       and anyway it would be sad to add alignment padding just for bounds checks. 
    
   fn header(s: Self) *QListHeader = {
        v := ptr_cast_unchecked(T, QListHeader, s.first);
        ::ptr_utils(QListHeader);
        v.offset(-1)
    };
    
    fn index(s: Self, i: i64) *T = {
        @safety(.Bounds) i.ult(s.header()[].cap);
        s.first.offset(i)
    }
    
    fn index_unchecked(s: Self, i: i64) *T = 
        s.first.offset(i);

    fn index(s: *Self, i: i64) *T #inline = 
        s[].index(i);
    
    fn slice(self: Self, start: i64, end: i64) []T #inline = {
        @if(::safety_check_enabled(.Bounds)) if(start == end, => return(empty()));  // HACK
        @debug_assert_ge(self.header()[].cap, end, "QList oob");
        (ptr = self.index(start), len = end - start)
    }
    
    fn push(self: *Self, count: *i64, t: T) void #inline = {
        self.grow(count[] + 1);
        self[count[]] = t;
        count[] += 1;
    }
    
    fn push(self: *Self, count: *u32, t: T) void #inline = {
        self.grow(count[].zext() + 1);
        self[count[].zext()] = t;
        count[] += 1;
    }
    
    fn push(self: *Self, count: *i32, t: T) void #inline = {
        self.grow(count[].intcast() + 1);
        self[count[].intcast()] = t;
        count[] += 1;
    }
    
    fn bake_relocatable_value(self: *Self) []BakedEntry =
        panic("bake QList as constant is not supported");
    
    Self
}

///////////////////
/// String Pool ///
///////////////////

IBits :: 12;
IMask :: 1.shift_left(IBits) - 1;

QLIST_DOUBLE_FREE :: 0x69696969;

fn intern(m: *QbeModule, s: Str) Qbe.Sym = {
    h := hash(s).bit_and(IMask.trunc());
    b := m.symbols.index(h.zext());
    n := m.lock_bucket(b);
    range(0, n.zext()) { i |
        if s == b.data[i].name {
            b.mutex&.unlock();
            return(id = h + i.trunc().shift_left(IBits));
        }
    };
    if b.mutex.count > 1 {
        // they call me refcell because im always refing cells
        @panic("can't intern new string '%' while bucket is locked elsewhere by this thread", s);
    };
    
    if n == 1.shift_left(32 - IBits) { 
        @panic("interning table overflow");
    };
    if n == 0 {
        b.data = m.new_long_life(1);
    } else {
        b.data&.grow(zext(n + 1));
    };
   
    mem := s.shallow_copy(m.forever&.borrow());
    data := b.data.index(n.zext());
    data[] = SymbolInfo.zeroed();
    data.name = mem;
    data.got_lookup_offset = -1;
    data.offset = -1;
    data.wasm_type_index = -1;
    data.alias = Qbe.no_symbol_S;
    id := h + n.shift_left(IBits);
    data.id = (id = id);
    
    b.n += 1;
    b.mutex&.unlock();
    data.id
}

LOCKED_BASE :: 0xF0000000;
    
// :ThreadSafety need to use cas or whatever
fn use_symbol(m: *QbeModule, id: Qbe.Sym, $body: @Fn(s: *SymbolInfo) void) void = {
    fence();
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "null module???");
    @debug_assert(m.initialized, "uninit module %", QbeModule.raw_from_ptr(m));
    
    bid: i64 = (@as(i64) id.id.zext()).bit_and(IMask);  // :UpdateBoot
    bucket := m.symbols[bid]&;
    old := m.lock_bucket(bucket);
    idx := id.id.shift_right_logical(IBits);
    @debug_assert_lt(idx, old, "invalid symbol id=%", id.id);
    symbol := bucket.data.index(idx.zext());
    @must_return body(symbol);
    @debug_assert_eq(bucket.n, old);
    bucket.mutex&.unlock();
}

fn lock_bucket(m: *QbeModule, bucket: *SymbolBucket) u32 = {
    bucket.mutex&.lock();
    bucket.n
}

// :ThreadSafety you can't call this one
fn get_symbol_info(m: *QbeModule, id: Qbe.Sym) *SymbolInfo = {
    @debug_assert(m.fixups_locked, "this is not threadsafe");
    bid: i64 = (@as(i64) id.id.zext()).bit_and(IMask);
    bucket := m.symbols[bid]&;
    idx: u32 = id.id.shift_right_logical(IBits);
    bucket.data.index(idx.zext())
}

fn str(m: *QbeModule, id: Qbe.Sym) Str = {
    name := "";
    use_symbol(m, id) { s |
        name = s.name;
    };
    @debug_assert(!name.ptr.is_null(), "null name for symbol %", id.id);
    name
}

fn hash(s: Str) u32 = {
    h: i64 = 1;
    for s { s |
        h = s.zext() + 17 * h; // prime number chosen by fair die roll 
    };
    h.trunc()
}

/////////////////
/// Iterators ///
/////////////////

fn tmps(f: *Qbe.Fn) []Qbe.Tmp #inline = 
    f.tmp.slice(0, f.ntmp.zext());

fn uses(t: *Qbe.Tmp) []Qbe.Use #inline = 
    t.use.slice(0, t.nuse.zext());

fn set_block_id(f: *Qbe.Fn) void = {
    i: i32 = 0;
    for_blocks f { b |
        b.id = i;
        i += 1;
    };
    @debug_assert_eq(i, f.nblk, "% lied about nblk", f.name());
};

fn for_blocks(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    b := f.start;
    ::ptr_utils(Qbe.Blk);
    while => !b.is_null() {
        body(b);
        b = b.link;
    };
}

fn for_dom(b: *Qbe.Blk, $body: @Fn(b: *Qbe.Blk) void) void = {
    b := b.dom;
    while => !b.is_null() {
        body(b);
        b = b.dlink;
    };
}

fn for_pred(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Blk) void) void = {
    range(0, b.npred.zext()) { p | 
        body(b.pred[p]);
    };
}

fn for_blocks_rpo_rev(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    ::ptr_utils(*Qbe.Blk);
    i: i64 = f.nblk.zext() - 1;
    while => i >= 0 {
        b := f.rpo[i];
        body(b);
        i -= 1;
    };
}

fn for_blocks_rpo_forward(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    i := 0;
    while => i < f.nblk.zext() {
        b := f.rpo[i];
        body(b);
        i += 1;
    };
}

// Not the same as just `for_rev(b.ins.slice(0, b.nins)`
// because the body is allowed to consume multiple instructions by offsetting the pointer. 
// This is needed because the ir uses a variable length encoding (for blit, call, args).
fn for_insts_rev(b: *Qbe.Blk, $body: @Fn(i: **Qbe.Ins) void) void = {
    i := b.ins.index_unchecked(b.ins.len);
    while => !i.identical(b.ins.ptr) {
        i = i.offset(-1);
        body(i&);
        // TODO: @debug_assert() `i` is still in range
    };
}

fn for_insts_forward(b: *Qbe.Blk, $body: @Fn(i: *Qbe.Ins) void) void = {
    ::enum(Qbe.O); ::enum(Qbe.Cls); ::enum(Qbe.J); ::if(Qbe.Ref); ::if(Qbe.Cls); ::enum(SymbolKind); ::if(*i64);
    each b.ins { i |
        body(i);
    };
}

fn for(bs: *Qbe.BSet, $body: @Fn(i: i64) void) void = {
    for(bs, 0, fn(i) => body(i))
}

fn for(bs: *Qbe.BSet, start: i64, $body: @Fn(i: i64) void) void = {
    t := start.shift_right_logical(6);
    range(t, bs.nt.zext()) { k |
        b := bs.t.offset(k)[];
        off := k * 64;
        for_bits(b, fn(i) => body(off + i));
    };
}

// iteration order is least significant first
fn for_bits(b: u64, $body: @Fn(i: i64) void) void = {
    b := bitcast b;
    while => b != 0 {
        body(trailing_zeros(b));
        bit := b.bit_and(-b);  // pick out the least significant bit
        b = b.bit_xor(bit);    // unset that bit
    };
}

// iteration order is MOST significant first
fn for_bits_rev(b: u64, $body: @Fn(i: i64) void) void = {
    b := bitcast b;
    while => b != 0 {
        i := 63 - leading_zeros(b);  // index of first set bit
        b = b.bit_xor(1.shift_left(i));  // unset that bit
        body(i);
    };
}

fn for_phi(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Phi) void) void = {
    p := b.phi;
    ::ptr_utils(Qbe.Phi);
    while => !p.is_null() {
        body(p);
        p = p.link;
    };
}

fn for_jump_targets(b: *Qbe.Blk, $body: @Fn(s: *Qbe.Blk) void) void = 
    for_jump_targets_mut(b, fn(b) => body(b[]));

fn for_jump_targets_mut(b: *Qbe.Blk, $body: @Fn(s: **Qbe.Blk) void) void = {
    if(b.s1.is_null(), => return());
    s := b.s1&;
    range(0, 2) { _ |
        body(s);
        if(b.s2.is_null() || identical(b.s1, b.s2), => return());
        s = b.s2&;
    };
}

fn for_pars(f: *Qbe.Fn, $body: @Fn(i: *Qbe.Ins) void) void = 
    for_insts_forward f.start { i | 
        continue :: local_return;
        ::enum(Qbe.O);
        if(@is(i.op(), .global_get, .pop, .flow, .nop), => continue()); // :HACK
        
        if(!is_par(i.op()), => return()); 
        body(i);
    };

fn get_int(f: *Qbe.Fn, r: Qbe.Ref) ?i64 #inline = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    ::enum(Qbe.ConType);
    if(c.type() != .CBits, => return(.None));
    (Some = c.bits())
}

// (id, offset)
fn get_sym(f: *Qbe.Fn, r: Qbe.Ref) ?Ty(Qbe.Sym, i64) #inline = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type() != .CAddr, => return(.None));
    (Some = (c.sym, c.bits()))
}

fn push(phi: *Qbe.Phi, blk: *Qbe.Blk, arg: Qbe.Ref) void = {
    // TODO: assert blk is unique
    phi.narg += 1;
    phi.arg&.grow(phi.narg.zext());
    phi.blk&.grow(phi.narg.zext());
    phi.arg[phi.narg.zext() - 1] = arg;
    phi.blk[phi.narg.zext() - 1] = blk;
}

fn add_padding(fields: *List(u32), current_size: *i64, target_size: i64) void #inline = {
    pad := target_size - current_size[];
    @debug_assert(pad >= 0, "trying to pad struct to smaller size");
    if pad > 0 {
        push(fields, pack(type = .FPad, len = pad.trunc()));
        current_size[] += pad;
    };
}

// since this might print the type, you shouldn't poke back in the module and mutate it later. 
fn new_type(m: *QbeModule, type: Qbe.Typ) i64 = {
    i := 0;
    with m.types_mutex& {
        a := m.forever&.borrow();
        ::AutoEq(Qbe.Typ); ::AutoHash(Qbe.Typ, TrivialHasher);
        ::AutoEq(Qbe.TypHeader); ::AutoHash(Qbe.TypHeader, TrivialHasher);
        ::HashEach([]u32, TrivialHasher);
        
        // TODO: this hashes the name as well!! which it shouldn't if the point is to help deduplication. 
        //       but it doesn't matter because emit_ir doesn't set the name to anything yet. 
        type.name = "";
        i = (m.type_lookup&.get_or_insert(type, a) {
            m.new_type_unchecked(type)
        })[]
    };
    i
}

fn new_type_unchecked(m: *QbeModule, type: Qbe.Typ) i64 = {
    @debug_assert(m.types_mutex&.locked_by_me());
    i := m.types.len;
    push(m.types&, type, m.forever&.borrow());
    when_debug(m, .Types) { out |
        print_type(m, i, out);
    };
    i
}

// TODO: call it from elsewhere
fn new_phi(f: *Qbe.Fn, b: *Qbe.Blk, k: Qbe.Cls, reserve_args: i64) *Qbe.Phi = 
    new_phi(b, k, reserve_args, f.newtmp("c", k));

fn new_phi(b: *Qbe.Blk, k: Qbe.Cls, reserve_args: i64, to: Qbe.Ref) *Qbe.Phi = {
    prev := b.phi;
    b.phi = temp().box(Qbe.Phi); 
    b.phi[] = (
        cls = k, 
        to = to,
        narg = 0,
        blk = new(reserve_args),
        arg = new(reserve_args),
        link = prev,
    );
    b.phi
}

fn argcls(i: *Qbe.Ins, argument_index: i64) Qbe.Cls = 
    argcls(i.op(), i.cls(), argument_index);

fn argcls(o: Qbe.O, k: Qbe.Cls, argument_index: i64) Qbe.Cls = {
    argcls := import("@/backend/meta/ops.fr").tables;
    it  := argcls[intcast @as(i32) o];
    ops_table_argscls(it, k, argument_index)
};

///////////////////////////////////////////////////////////////////////////////

fn pack_op_cls(op: Qbe.O, k: Qbe.Cls) u32 #inline = {
    ::enum(Qbe.Cls); ::enum(Qbe.O);
    (@as(i64) k.raw().zext()).shift_left(30).bit_or(op.raw().zext()).trunc()
}

// TODO: ugh. garbage setter, i need to add real bit fields to my language!
fn set_op(i: *Qbe.Ins, op: Qbe.O) void #inline = {
    i.op30_cls2 = pack_op_cls(op, i.cls());
}

fn set_cls(i: *Qbe.Ins, k: Qbe.Cls) void #inline = {
    i.op30_cls2 = pack_op_cls(i.op(), k);
}

fn set_nop(i: *Qbe.Ins) void #inline = {
    i[] = ::make_ins(.nop, .Kw, QbeNull, QbeNull, QbeNull);
}

fn op(i: *Qbe.Ins) Qbe.O #inline = {
    @as(Qbe.O) @as(i32) bitcast(@as(u32) i.op30_cls2.bit_and(1.shift_left(30) - 1))
}

fn cls(i: *Qbe.Ins) Qbe.Cls #inline = {
    x := i.op30_cls2.shift_right_logical(30).bit_and(0b11);
    @as(Qbe.Cls) @as(i32) x.bitcast()
}

// 0 for int, 1 for float. 
fn KBASE(k: Qbe.Cls) i64 #inline = {
    x: i64 = k.raw().shift_right_logical(1).zext();
    @debug_assert(x.bit_and(1) == x, "bad KBASE. Kx=% Ke=%", k == .Kx, k == .Ke);
    x
}

fn eq(a: Qbe.Ref, b: Qbe.Ref) bool #inline = 
    a.type3_val29 == b.type3_val29;
    
fn ne(a: Qbe.Ref, b: Qbe.Ref) bool #inline = 
    a.type3_val29 != b.type3_val29;

fn rtype(r: Qbe.Ref) Qbe.RegKind #inline = {
    r != QbeNull || return(.RNull);
    ::enum(Qbe.RegKind);
    @as(Qbe.RegKind) @as(u32) r.type3_val29.bit_and(1.shift_left(3) - 1)
}

fn get_constant(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Con #inline = {
    @debug_assert(rtype(r) == .RCon, "tried to get constant of non constant");
    i := r.val();
    @safety(.Bounds) i < f.ncon.zext();
    f.con.index(i)
}

fn get_memory(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Addr #inline = {
    @debug_assert(rtype(r) == .RMem, "tried to get memory of non memory");
    i := r.val();
    @safety(.Bounds) i < f.nmem.zext();
    f.mem.index(i)
}

fn get_temporary(f: *Qbe.Fn, i: i64) *Qbe.Tmp #inline = {
    @safety(.Bounds) i < f.ntmp.zext();
    f.tmp.index(i)
}

fn get_temporary(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Tmp #inline = {
    @debug_assert(rtype(r) == .RTmp, "tried to get tmp of non-tmp % in %", r, f.name());
    i := r.val();
    @safety(.Bounds) i.ult(f.ntmp.zext());
    f.tmp.index(i)
}

fn trunc(x: u32) i16 #redirect(u32, u16); // TODO
fn shift_right_logical(v: i16, shift_amount: i64) i16 = v.int().shift_right_logical(shift_amount).intcast().trunc();

MAX_REF_VAL: u32 : 1.shift_left(29) - 1;

fn ref(cls: Qbe.RegKind, id: i64) Qbe.Ref #inline = {
    ::enum(Qbe.RegKind);
    @debug_assert_ule(id, MAX_REF_VAL);
    (type3_val29 = cls.raw().bitcast().bit_or(id.shift_left(3).trunc()))
}

fn val(r: Qbe.Ref) i64 #inline = {
   x := @as(u32) r.type3_val29.shift_right_logical(3).bit_and(MAX_REF_VAL);
   x.zext()
}

fn rsval(r: Qbe.Ref) i32 #inline = {    
   intcast(r.val().bit_xor(0x10000000) - 0x10000000)
}

fn small_int_for_blit(x: i64) Qbe.Ref #inline = {
    ref(.RInt, x.bit_and(0x1fffffff))
}

fn get_jump_targets(b: *Qbe.Blk) []*Qbe.Blk = {
    ::enum(Qbe.J);

    @debug_assert_ne(b.jmp.type, .switch); 
    :: @assert_eq(offset_of(Qbe.Blk, Fields(Qbe.Blk).s1) + size_of(*Qbe.Blk), offset_of(Qbe.Blk, Fields(Qbe.Blk).s2), "field order matters for get_jump_targets SAFETY");
    (ptr = b.s1&, len = int(!b.s1.is_null()) + int(!b.s2.is_null()))
}

// these are carefully arranged to be packed in ir/Qbe.O,
// so ie. `@is(j, .alloc4, .alloc8, .alloc16)` is the same as manually writing `j.raw() >= Qbe.O.alloc4.raw() && j.raw() <= Qbe.O.alloc16.raw()`,
// I just think its easier to read when i can see all the values spelled out. 

fn is_ret(j: Qbe.J) bool #inline =
    j.raw() >= Qbe.J.retw.raw() && j.raw() <= Qbe.J.ret0.raw();

fn is_alloc(j: Qbe.O) bool #inline = {
    ::enum(Qbe.O);
    @is(j, .alloc4, .alloc8, .alloc16)
}

fn is_ext(j: Qbe.O) bool #inline = 
    @is(j, .extsb, .extub, .extsh, .extuh, .extsw, .extuw);

fn is_store(j: Qbe.O) bool #inline = 
    @is(j, .storeb, .storeh, .storew, .storel, .stores, .stored);
    
fn is_load(j: Qbe.O) bool #inline = 
    @is(j, .loadsb, .loadub, .loadsh, .loaduh, .loadsw, .loaduw, .load);

fn is_flag(j: Qbe.O) bool #inline = 
    j.raw() >= Qbe.O.flagieq.raw() && j.raw() <= Qbe.O.flagfuo.raw();

fn is_parbh(j: Qbe.O) bool #inline = 
    @is(j, .parsb, .parub, .parsh, .paruh);

fn is_par(j: Qbe.O) bool #inline = 
    between(j, .par, .pare);

fn is_arg(j: Qbe.O) bool #inline = 
    between(j, .arg, .argv);

fn is_retbh(j: Qbe.J) bool #inline = 
    @is(j, .retsb, .retub, .retsh, .retuh);
    
fn is_argbh(j: Qbe.O) bool #inline = 
    @is(j, .argsb, .argub, .argsh, .arguh);

// TODO: is it a mistake when amd64/isel calls this with .Kx? nowhere else does that. 
fn is_int(k: Qbe.Cls) bool #inline = {
    ::enum(Qbe.Cls);
    k != .Kx && KBASE(k) == 0
}

fn is_float(k: Qbe.Cls) bool = 
    KBASE(k) == 1;

fn is_wide(k: Qbe.Cls) bool #inline =
    (@as(i64) k.raw().zext()).bit_and(1) != 0;

fn no_result(o: Qbe.O) bool = @is(o, 
    .storeb, .storeh, .storew, .storel, .stores, .stored, .vastart, 
    .dbgloc, .nop, .blit0, .blit1, .xcmp, .xtest, .acmp, .acmn, .afcmp, 
    .reqz, .rnez, .arg, .argsb, .argub, .argsh, .arguh, .argc, .arge, 
    .argv, .asm, .push, .global_set, .sel0, .cas0, 
);

fixed_const_count :: 2;

// TODO: init array from tuple. and also real bit fields. so this doesn't suck as bad. 
fn make_ins(op: Qbe.O, k: Qbe.Cls, out: Qbe.Ref, in1: Qbe.Ref, in2: Qbe.Ref) Qbe.Ins #inline = 
    (op30_cls2 = pack_op_cls(op, k), arg = @array(in1, in2), to = out);
 
// so you can @assert_eq
fn display(self: Qbe.Ref, out: *List(u8)) void = {
    @fmt(out, "%:%", self.rtype(), self.val());
}

fn track_ir_names(f: *Qbe.Fn) bool #inline =
    @if(Qbe.TRACK_IR_NAMES, f.track_ir_names, false);

fn for_unions(t: *Qbe.Typ, i: *i64, $body: @Fn() void) void = {
    @debug_assert_eq(i[], 0);
    range(0, t.header.nunion.zext()) { n |
        body();
    };
    @debug_assert_eq(t.fields.len, i[]);
}

fn for_fields(t: *Qbe.Typ, i: *i64, $body: @Fn(tag: *Qbe.FieldType, len: u32) void) void = {
    f := unpack t.fields[i[]];  
    ::enum(@type f.type);
    while => f.type != .FEnd { 
        body(f.type, f.len);  // `body` may read `i` here
        i[] += 1;
        f = unpack t.fields[i[]];
    };
    i[] += 1;
}

fn cls(self: Qbe.FieldType) Qbe.Cls = {
    types :: @const_slice(Qbe.Cls.Ke, .Kw, .Kw, .Kw, .Kl, .Ks, .Kd, .Ke, .Ke);
    ::@assert_eq(types.len(), Qbe.FieldType.enum_count());
    types[zext self.raw()]
}

fn cls(self: Qbe.J) Qbe.Cls = 
    @as(Qbe.Cls) @as(i32) self.raw() - Qbe.J.retw.raw();

fn retk(k: Qbe.Cls) Qbe.J = 
    @as(Qbe.J) @as(i32) Qbe.J.retw.raw() + k.raw();

// TODO: check you don't overflow 16777216 types
fn pack(f: Qbe.Field) u32 = 
    bit_or(zext @as(u8) f.type, f.len.shift_left(8));
    
fn unpack(f: u32) Qbe.Field = 
    (type = @as(Qbe.FieldType) @as(u8) trunc f.bit_and(0xFF), len = f.shift_right_logical(8));

fn size_of_store(i: *Qbe.Ins) i32 = {
    i.maybe_store().expect_unchecked("valid op for size_of_store")
}

fn maybe_store(i: *Qbe.Ins) ?i32 = {
    o := i.op();
    sizes :: @const_slice(@as(u8) 1, 2, 4, 8, 4, 8);
    @if(o.is_store(), (Some = sizes[intcast(o.raw() - Qbe.O.storeb.raw())].zext()), .None)
}

fn maybe_load(i: *Qbe.Ins) ?i32 = {
    o := i.op();
    @if(!o.is_load()) return(.None);
    sizes :: @const_slice(@as(u8) 1, 1, 2, 2, 4, 4);
    (Some = @if(o == .load, 
        @if(i.cls().is_wide(), 8, 4), 
        sizes[intcast(o.raw() - Qbe.O.loadsb.raw())].zext(),
    ))
}

fn size_of_load(i: *Qbe.Ins) i32 = {
    i.maybe_load().expect_unchecked("valid op for size_of_load")
}

// TODO: silly
fn find_cas(i: *Qbe.Ins, b: *Qbe.Blk) Ty(*Qbe.Ins, *Qbe.Ins) = {
    // might not be directly the next instruction if we had to fixarg
    @match(i.op()) {
        fn cas0() => {
            icas1 := i;
            while => icas1.op() != .cas1 {  
                @debug_assert(!identical(icas1, b.ins.ptr.offset(b.ins.len)), "missing cas1");
                icas1 = icas1.offset(1);
            };
            (i, icas1)
        }
        fn cas1() => {
            icas0 := i;
            while => icas0.op() != .cas0 {  
                @debug_assert(!identical(icas0, b.ins.ptr), "missing cas0");
                icas0 = icas0.offset(-1);
            };
            (icas0, i)
        }
        @default => panic("expected cas");
    }
}

///////////////////////////////////////////////////////////////////////////////

fn ge(a: Qbe.Cls, b: Qbe.Cls) bool = a.raw().int() >= b.raw().int();
fn gt(a: Qbe.Cls, b: Qbe.Cls) bool = a.raw().int() > b.raw().int();

// 
// TODO: These are garbage garbage garbage and go away when i fix :BitFieldsCompileError (and then :UpdateBoot i guess)
//

fn is_dark(self: *Qbe.Typ) bool = {
    flags: Qbe.TypFlags = (repr = self.header.flags);
    flags.get(.is_dark) != 0
}

fn is_union(self: *Qbe.Typ) bool = {
    flags: Qbe.TypFlags = (repr = self.header.flags);
    flags.get(.is_union) != 0
}

fn set_is_union(self: *Qbe.Typ, c: bool) void = {
    flags: Qbe.TypFlags = (repr = self.header.flags);
    flags&.set(.is_union, int(c));
    self.header.flags = flags.repr;
}

fn set_is_dark(self: *Qbe.Typ, c: bool) void = {
    flags: Qbe.TypFlags = (repr = self.header.flags);
    flags&.set(.is_dark, int(c));
    self.header.flags = flags.repr;
}

fn eq(a: Qbe.Sym, b: Qbe.Sym) bool #redirect(Ty(u32, u32), bool);
fn ne(a: Qbe.Sym, b: Qbe.Sym) bool #redirect(Ty(u32, u32), bool);

fn ispow2(v: i64) bool = 
    v != 0 && v.bit_and(v - 1) == 0;
