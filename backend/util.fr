// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

// note: for now the c code still calls its own version of many of these so you can't change the layout of BSet/QList!

fn BIT(n: i64) u64 #inline = 1.shift_left(n);
fn SLOT(i: i64) Qbe.Ref #inline = ref(.RSlot, i.bit_and(0x1fffffff));
fn TMP(i: i64)  Qbe.Ref #inline = ref(.RTmp, i);
fn CALL(i: i64) Qbe.Ref #inline = ref(.RCall, i);
fn CON(i: i64) Qbe.Ref #inline = ref(.RCon, i);
fn INT(i: i64) Qbe.Ref #inline = ref(.RInt, i.bit_and(0x1fffffff));
fn MEM(i: i64) Qbe.Ref #inline = ref(.RMem, i);
fn TYPE(i: i64) Qbe.Ref #inline = ref(.RType, i);

fn TMP(i: i32) Qbe.Ref #inline = TMP(i.intcast());
fn SLOT(i: i32) Qbe.Ref #inline = SLOT(i.intcast());

fn isreg(r: Qbe.Ref) bool #inline = 
    rtype(r) == .RTmp && r.val() < Qbe.Tmp0;

// tmp[t].phi has which id was the .to of a phi with t as an arg. 
// this looks up a chain of phis to see where t is going to end up. 
// if its not used as a phi arg, that's just itself. 
fn phi_dest(t: i64, tmp: QList(Qbe.Tmp)) i64 = {
    t1 := tmp[t].phi.zext();
    if(t1 == 0, => return(t)); // t was not a phi arg
    t1 = phi_dest(t1, tmp);
    tmp[t].phi = t1.intcast();  // cache long chains
    t1
}

// note: typecheck still calls the c version of this
fn clsmerge(pk: *Qbe.Cls, k: Qbe.Cls) bool = {
    k1 := pk[];
    if k1 == .Kx {
        pk[] = k;
        return(false);
    };
    xx := (k1 == .Kw && k == .Kl); // TODO: compiler bug. fix overloading when you inline this. 
    if xx || (k1 == .Kl && k == .Kw) {
        pk[] = .Kw;
        return(false);
    };
    k1 != k
}

fn slice_pending_scratch(f: *Qbe.Fn) []Qbe.Ins = {
    n := Qbe.MaxInstructions;
    f.globals.insb.items().slice(n - f.len_scratch(), n)
}

fn copy_instructions_from_scratch(f: *Qbe.Fn, b: *Qbe.Blk) void = {
	idup(b, f.scratch_next(), f.len_scratch());
	f.globals.curi[] = f.scratch_start(); // TODO: make sure this is right. qbe didn't do it where i'd expect in simpl
}

fn scratch_start(f: *Qbe.Fn) *Qbe.Ins #inline =
    f.globals.insb.index_unchecked(Qbe.MaxInstructions);

fn scratch_next(f: *Qbe.Fn) *Qbe.Ins #inline =
    f.globals.curi[];

fn reset_scratch(f: *Qbe.Fn) void #inline = {
    f.globals.curi[] = f.scratch_start(); 
}

fn len_scratch(f: *Qbe.Fn) i64 #inline = 
    f.scratch_next().ptr_diff(f.scratch_start()); 

// it uses insb/curi as a buffer for emitting new instructions and then copies them out to a newly allocated array at the end of the block. 
// the parser starts at the beginning but the passes start at the end and `emit` in reverse. 
// so i think they scan block backwards. like isel will iterate backwards, adding some instructions, and copying out those it likes,
// and then at the end, copy from the buffer and update the block with that pointer. 
// so they're always in order in the blocks but you always process them backwards. 
//
// TODO: we rely on globals.curi pointing to the last inst we emitted (in isel) :LookAtLastInst
fn emit(f: *Qbe.Fn, op: Qbe.O, k: Qbe.Cls, to: Qbe.Ref, arg0: Qbe.Ref, arg1: Qbe.Ref) *Qbe.Ins = {
    i := f.globals.curi;
    if i[].identical(f.globals.insb.as_ptr()) {
        @panic("emit, too many instructions");
    };
    i[] = i[].offset(-1);
    i[][] = make_ins(op, k, to, arg0, arg1);
    i[]
}

fn emit(f: *Qbe.Fn, i: Qbe.Ins) *Qbe.Ins #inline = 
    f.emit(i&.op(), i&.cls(), i.to, i.arg&[0], i.arg&[1]);

fn newtmp(f: *Qbe.Fn, debug_hint: Str, k: Qbe.Cls) Qbe.Ref = {
    n := f.globals.next_tmp_id&;
    t: i64 = f.ntmp.zext();
    f.ntmp += 1;
    f.tmp&.grow(t + 1);
    f.tmp.slice(t, t + 1).set_zeroed();
    tmp := f.tmp.index(t);
    if debug_hint.len != 0 {
        l: List(u8) = (maybe_uninit = tmp.name&.items(), len = 0, gpa = panicking_allocator); // :UnacceptablePanic
        @fmt(l&, "%.%", debug_hint, n[]);
    };
    n[] += 1;
    tmp.cls = k;
    tmp.slot = -1;
    tmp.nuse = 1;
    tmp.ndef = 1;
    tmp.defining_block = -1;
    TMP(t)
}

:: {
    fn eq(a: Qbe.Sym, b: Qbe.Sym) bool #inline = 
        a.type == b.type && a.id == b.id;
    fn ne(a: Qbe.Sym, b: Qbe.Sym) bool #inline = !(a == b);
};

fn newcon(f: *Qbe.Fn, c0: *Qbe.Con) Qbe.Ref = {
    // deduplicate
    range(1, f.ncon.zext()) { i |
        c1 := f.con.index(i);
        if c0.type == c1.type && c0.sym == c1.sym && c0.bits.i == c1.bits.i {
            return(CON(i));
        };
    };
    
    // push
    f.con&.grow(f.ncon.zext() + 1);
    f.con[f.ncon.zext()] = c0[];
    f.ncon += 1;
    CON(f.ncon.zext() - 1)
}

// TODO: is it worth the copy paste instead of calling newcon to avoid comparing .sym everytime?
fn getcon(f: *Qbe.Fn, val: i64) Qbe.Ref #inline = {
    // deduplicate
    range(1, f.ncon.zext()) { c |
        cc := f.con[c]&;
        if cc.type == .CBits && cc.bits.i == val {
            return(CON(c));
        };
    };
    // push
    c := f.ncon.zext();
    f.ncon += 1;
    f.con&.grow(f.ncon.zext());
    f.con[c] = (type = .CBits, bits = (i = val));
    CON(c)
}

fn new_mem(f: *Qbe.Fn, base: Qbe.Ref, index: Qbe.Ref, offset: i64, scale: i32) Qbe.Ref = {
    m := f.nmem.zext();
    f.nmem += 1;
    f.mem&.grow(m + 1);
    f.mem[m].base = base;
    f.mem[m].index = index;
    f.mem[m].offset.type = @if(scale == 0, .CUndef, .CBits);
    f.mem[m].offset.bits.i = offset;
    f.mem[m].scale = scale;
    MEM(m)
}


// TODO: use slices 

fn idup(b: *Qbe.Blk, src: *Qbe.Ins, n: i64) void = {
    b.nins = n.trunc();
    if(n == 0, => return());
    if b.ins.first.is_null() {
        b.ins = new(n);  // caller better not be trying to put this in the inlining cache
    } else {
        @debug_assert(!src.identical(b.ins.first));
        b.ins&.grow(n);
    };
    mem := b.ins.slice(0, n);
    mem.copy_from(ptr = src, len = n);
}

fn icpy(dest: *Qbe.Ins, src: *Qbe.Ins, n: i64) *Qbe.Ins = {
    if(n == 0, => return(dest));
    (@as([]Qbe.Ins)(ptr = dest, len = n)).copy_from(ptr = src, len = n); // TODO: ugly
    dest.offset(n)
}

fn salloc(rt: Qbe.Ref, rs: Qbe.Ref, f: *Qbe.Fn) void = {
    /* we need to make sure
    * the stack remains aligned
    * (rsp = 0) mod 16
    */
    f.dynalloc = true;
    if rtype(rs) == .RCon {
        sz := f.con[rs.val()].bits.i;
        if sz < 0 || sz >= MAX_i32 - 15 {
            @panic("invalid alloc size %", sz);
        };
        sz = (sz + 15).bit_and(-16);
        f.emit(.salloc, .Kl, rt, f.getcon(sz), QbeNull);
    } else {
        /* r0 = (r + 15) & -16 */
        r0 := f.newtmp("isel", .Kl);
        r1 := f.newtmp("isel", .Kl);
        f.emit(.salloc, .Kl, rt, r0, QbeNull);
        f.emit(.and, .Kl, r0, r1, f.getcon(-16));
        f.emit(.add, .Kl, r1, rs, f.getcon(15));
        if f.tmp[rs.val()].slot != -1 {
            @panic("unlikely alloc argument % for %", f.tmp[rs.val()]&.name(), f.tmp[rt.val()]&.name());
        }
    }
}

fn between(o: Qbe.O, $fst: Qbe.O, $lst: Qbe.O) bool #inline = {
    ::assert(fst.raw() < lst.raw(), "between(Qbe.O) args are out of order");
    fst.raw() <= o.raw() && o.raw() <= lst.raw()
}

fn addins(pvins: *QList(Qbe.Ins), pnins: *i64, i: *Qbe.Ins) void = {
    if i.op() != .nop {
        pnins[] += 1;
        pvins.grow(pnins[]);
        pvins[][pnins[] - 1] = i[];
    };
}


fn addbins(b: *Qbe.Blk, pvins: *QList(Qbe.Ins), pnins: *i64) void = {
    for(b.ins.first, b.ins.index(b.nins.zext())) { i | 
        addins(pvins, pnins, i);
    };
}

// we try to line up the opcode numbers so you can branchlessly convert between categories (ie. load<->ext, par<->ext)
fn rebase(value: Qbe.O, new_base: Qbe.O, old_base: Qbe.O) Qbe.O = 
    @as(Qbe.O) @as(i32) new_base.raw() + (value.raw() - old_base.raw());

///////////////////////
/// Condition Codes ///
///////////////////////

// TODO: use multiple returns
fn iscmp(op: Qbe.O, pk: *Qbe.Cls, pc: *i32) bool = { // :CmpOrder
    if op.between(.ceqw, .cultw) {
        pc[] = op.raw() - Qbe.O.ceqw.raw();
        pk[] = .Kw;
        return(true);
    };
    if op.between(.ceql, .cultl) {
        pc[] = op.raw() - Qbe.O.ceql.raw();
        pk[] = .Kl;
        return(true);
    };
    
    ::@assert_eq(Qbe.Cmp.enum_count(), Qbe.CmpICount + Qbe.CmpFCount, "cmp counts don't add up");
    if op.between(.ceqs, .cuos) {
        pc[] = Qbe.CmpICount.intcast() + op.raw() - Qbe.O.ceqs.raw();
        pk[] = .Ks;
        return(true);
    }; 
    if op.between(.ceqd, .cuod) {
        pc[] = Qbe.CmpICount.intcast() + op.raw() - Qbe.O.ceqd.raw();
        pk[] = .Kd;
        return(true);
    };
    
    false
}

// TODO: this is dumb. i should arrange the numbers so its just flipping a bit. 
//       don't forget to change the order of jmps/flags/ops, iscmp ranges. :CmpOrder
//       but have to wait until i'm not calling into qbe anymore.  -- Oct 8 

::List(Qbe.Cmp);
::enum_basic(Qbe.Cmp);

fn cmpneg(cc: i32) i32 = {
    negate_cc :: @const_slice( // :CmpOrder
        // Cieq   Cine   Cisge   Cisgt   Cisle   Cislt   Ciuge   Ciugt   Ciule   Ciult
   Qbe.Cmp.Cine, .Cieq, .Cislt, .Cisle, .Cisgt, .Cisge, .Ciult, .Ciule, .Ciugt, .Ciuge,
        // Cfeq   Cfge   Cfgt   Cfle   Cflt   Cfne   Cfo   Cfuo
          .Cfne, .Cflt, .Cfle, .Cfgt, .Cfge, .Cfeq, .Cfuo, .Cfo
    );

    @debug_assert(cc >= 0 && cc < Qbe.Cmp.enum_count().intcast(), "bad cc");
    negate_cc[cc.zext()].raw()
}

fn cmpop(cc: i32) i32 = {
    swap_cc :: @const_slice( // :CmpOrder
        // Cieq   Cine   Cisge   Cisgt   Cisle   Cislt   Ciuge   Ciugt   Ciule   Ciult
   Qbe.Cmp.Cieq, .Cine, .Cisle, .Cislt, .Cisge, .Cisgt, .Ciule, .Ciult, .Ciuge, .Ciugt,
        // Cfeq   Cfge   Cfgt   Cfle   Cflt   Cfne   Cfo   Cfuo
          .Cfeq, .Cfle, .Cflt, .Cfge, .Cfgt, .Cfne, .Cfo, .Cfuo
    );

    @debug_assert(cc >= 0 && cc < Qbe.Cmp.enum_count().intcast(), "bad cc");
    swap_cc[cc.zext()].raw()
}

////////////////
/// Bit Sets ///
////////////////
// TODO: use unsigned division can be shifts (if negative you truncate towards 0 not -inifinity which a shift gives you)

fn bsinit(bs: *Qbe.BSet, n: i64) void = {
    n := (n + Qbe.NBit - 1) / Qbe.NBit;
    bs.nt = n.trunc();
    bs.t = temp().alloc_zeroed(u64, n).as_ptr(); 
}

// use `for` which calls this. 
fn bsiter(bs: *Qbe.BSet, elt: *i32) bool = {
    t, j := elt[].zext().div_mod(Qbe.NBit);
    if(t >= bs.nt.zext(), => return(false));
    b := bs.t.offset(t)[];
    b = b.bit_and(bit_not(BIT(j) - 1));
    while => b == 0 {
        t += 1;
        if(t >= bs.nt.zext(), => return(false));
        b = bs.t.offset(t)[];
    };
    elt[] = intcast(Qbe.NBit * t + trailing_zeros(b));
    true
}

fn bsequal(a: *Qbe.BSet, b: *Qbe.BSet) bool = {
    @debug_assert(a.nt == b.nt, "BSet mismatch");
    range(0, a.nt.zext()) { i |
        if a.t.offset(i)[] != b.t.offset(i)[] {
            return(false);
        }
    };
    true
}

fn bszero(bs: *Qbe.BSet) void = {
    mem: []u64 = (ptr = bs.t, len = bs.nt.zext());
    mem.set_zeroed();
}

fn bscount(bs: *Qbe.BSet) u32 = {
    n: u32 = 0;
    range(0, bs.nt.zext()) { i |
        n += count_ones(bs.t.offset(i)[]);
    };
    n
}

// TODO: make elt unsigned to the division is free
fn bshas(bs: *Qbe.BSet, elt: i64) bool #inline = {
    @debug_assert(elt < bs.nt.zext() * Qbe.NBit, "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    mask := BIT(j);
    slot[].bit_and(mask) != 0
}

fn bsset(bs: *Qbe.BSet, elt: i64) void #inline = {
    @debug_assert(elt < bsmax(bs), "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    slot[] = slot[].bit_or(BIT(j));
}

fn bsclr(bs: *Qbe.BSet, elt: i64) void #inline = {
    @debug_assert(elt < bsmax(bs), "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    slot[] = slot[].bit_and(bit_not(BIT(j)));
}

fn bsmax(bs: *Qbe.BSet) i64 #inline =
    bs.nt.zext() * Qbe.NBit;

fn init_bitset(size: i64) Qbe.BSet #inline = {
    // this puts it in the pool thing that freeall deals with. i can just stick an arena allocator somewhere i guess and clear it at the end of the function. 
    // is it worth resetting temp() between passes or should i just do it at the end and not deal with it? 
    s := Qbe.BSet.zeroed();
    bsinit(s&, size); 
    s
}

fn bscopy( a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => b);
fn bsunion(a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_or(b));
fn bsinter(a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_and(b));
fn bsdiff( a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_and(b.bit_not()));

fn BSOP(a: *Qbe.BSet, b: *Qbe.BSet, $fuse: @Fn(a: u64, b: u64) u64) void #inline = {
    @debug_assert(a.nt == b.nt, "BSet mismatch");
    range(0, a.nt.zext()) { i |
        a.t.offset(i)[] = fuse(a.t.offset(i)[], b.t.offset(i)[]);
    };
}

/////////////
/// Lists ///
/////////////
// TODO: use my allocators for this. 

VMag :: 0xcabba9e;  // random number chosen by fair die roll

::ptr_utils(QListHeader);
::ptr_utils(*QListHeader);

fn calloc(nitems: i64, size: i64) rawptr #libc;

fn vnew(len: i64, esz: i64, pool: Qbe.Pool) rawptr = {
    // qbe had .PFn for auto free when done all passes and .PHeap for manually free. 
    
    cap := 2;
    while => cap < len {
        cap *= 2;
    };
    a := @if(pool == .PLong, libc_allocator, temp());
    v := a.alloc_zeroed(i64, (cap * esz + QListHeader.size_of() + 7) / 8);  // :HardcodeAlloc
    @debug_assert(!v.ptr.is_null(), "alloc failed?????");
    v := ptr_cast_unchecked(i64, QListHeader, v.ptr);
    v.mag = VMag;
    v.cap = cap;
    v.esz = esz;
    v.pool = pool;
    QListHeader.raw_from_ptr(v.offset(1))
}

fn vgrow(vp: rawptr, len: i64) void = {
    vp := ptr_from_raw(*QListHeader, vp);
    v := vp[].offset(-1);
    xxx := !v.offset(1).is_null(); // TODO: compiler bug
    @debug_assert(xxx, "ugh");
    @debug_assert(v.mag == VMag, "bad magic");
    if(v.cap >= len, => return());
    v1 := vnew(len, v.esz, v.pool);
    memcpy(v1, QListHeader.raw_from_ptr(v.offset(1)), v.cap * v.esz);
    vfree(QListHeader.raw_from_ptr(v.offset(1)));
    vp[] = ptr_from_raw(QListHeader, v1);
}

::enum(Qbe.Pool);

fn vfree(p: rawptr) void = {
    v := QListHeader.ptr_from_raw(p).offset(-1);
    @debug_assert(v.mag == VMag);
    if v.pool == .PLong {
        v.mag = 0;
        a := libc_allocator;
        free(QListHeader.raw_from_ptr(v));
    };
}

// This is a header that goes before the allocated memory for the array. 
// When a field is an QList it points to the first entry of one of these and you offset backwards to get the header. 
QListHeader :: @struct(
    mag: u64,
    pool: Qbe.Pool,
    esz: i64,
    cap: i64,
    // I don't have unsized types. lets hope thats not a big deal
    // union { long long ll; long double ld; void *ptr; } align[];
);

fn header();
fn grow();
fn new_copy();
fn new_long_life();

fn QList($T: Type) Type = {
    Self :: @struct(first: *T);
    ::ptr_utils(T);
    
    fn new(len: i64) Self = {
        (first = T.ptr_from_raw(vnew(len, T.size_of(), .PHeap)))
    }
    
    fn new_long_life(len: i64) Self = 
        (first = T.ptr_from_raw(vnew(len, T.size_of(), .PLong)));
    
    fn new_copy(items: []T) Self = {
        self: Self = new(items.len);
        self.slice(0, items.len).copy_from(items);
        self
    }
    
    // ensure capacity is >= len
    fn grow(s: *Self, len: i64) void = {
        v := Self.raw_from_ptr(s);
        vgrow(v, len);
    }
    
    fn free(s: Self) void = {
        v := T.raw_from_ptr(s.first);
        vfree(v);
    }
    
    // TODO: bounds checking but its hard as long as i have to keep qbe's memory layout because the fields aren't together. 
    //       and anyway it would be sad to add alignment padding just for bounds checks. 
    
    fn index(s: Self, i: i64) *T #inline = 
        s.first.offset(i);
    
    fn index(s: *Self, i: i64) *T #inline = 
        s.first.offset(i);
    
    fn slice(self: Self, start: i64, end: i64) []T #inline = {
        (ptr = self.first.offset(start), len = end - start)
    }
    
    Self
}

///////////////////
/// String Pool ///
///////////////////
// TODO: less messing with c strings but for now i want to provide exactly the same interface as real qbe. 

IBits :: 12;
IMask :: 1.shift_left(IBits) - 1;

fn intern(m: *QbeModule, s: CStr) u32 = {
    id := m.my_intern(s);
    @if(LINK_QBE_C) {
        // TODO: this is a waste of time that lets me easily use thier parser. 
        intern :: fn(_0: CStr) u32 #import("qbe");
        return(intern(s));
    };
    
    id
}

fn my_intern(m: *QbeModule, s: CStr) u32 = {
    h := hash(s).bit_and(IMask.trunc());
    b := m.symbols.index(h.zext());
    n := b.n;
    strcmp :: fn(a: CStr, b: CStr) i64 #libc;
    range(0, n.zext()) { i |
        if strcmp(s, b.data[i].name) == 0 {
            return(h + i.trunc().shift_left(IBits));
        }
    };
    
    if n == 1.shift_left(32 - IBits) { 
        @panic("interning table overflow");
    };
    if n == 0 {
        b.data = new_long_life(1);
    } else {
        if n.bit_and(n - 1) == 0 {
            b.data&.grow(zext(n + n));
        }
    };
    
    string   := s.str();
    mem := m.forever&.borrow().alloc(u8, string.len + 1);
    mem.slice(0, string.len).copy_from(string);
    mem[string.len] = 0;
    data := b.data.index(n.zext());
    data[] = SymbolInfo.zeroed();
    data.name = (ptr = mem.ptr);
    data.got_lookup_offset = -1;
    data.offset = -1;
    
    b.n = n + 1;
    id := h + n.shift_left(IBits);
    id
}

// unstable!
fn get_symbol_info(m: *QbeModule, id: u32) *SymbolInfo = {
    @if(LINK_QBE_C) {
        // TODO: this is a waste of time that lets me easily use thier parser. 
        str :: fn(_0: u32) CStr #import("qbe");
        s := str(id);
        id = m.my_intern(s);
    };
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "null module???");
    bid: i64 = id.bit_and(IMask).zext();
    bucket := m.symbols[bid];
    idx := id.shift_right_logical(IBits);
    bucket.data.index(idx.zext())
}

fn str(m: *QbeModule, id: u32) CStr = {
    s := m.get_symbol_info(id);
    @debug_assert(!s.name.ptr.is_null(), "null name for symbol %", id);
    s.name
}

fn hash(s: CStr) u32 = {
    h: u32 = 0;
    for s { s |
        h = s.zext() + 17 * h; // prime number chosen by fair die roll 
    };
    h
}

/////////////////
/// Iterators ///
/////////////////

fn tmps(f: *Qbe.Fn) []Qbe.Tmp #inline = 
    f.tmp.slice(0, f.ntmp.zext());

fn uses(t: *Qbe.Tmp) []Qbe.Use #inline = 
    t.use.slice(0, t.nuse.zext());
    
fn for_blocks(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    b := f.start;
    while => !b.is_null() {
        body(b);
        b = b.link;
    };
}

fn for_dom(b: *Qbe.Blk, $body: @Fn(b: *Qbe.Blk) void) void = {
    b := b.dom;
    while => !b.is_null() {
        body(b);
        b = b.dlink;
    };
}

fn for_pred(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Blk) void) void = {
    range(0, b.npred.zext()) { p | 
        body(b.pred[p]);
    };
}

::ptr_utils(*Qbe.Blk);
fn for_blocks_rpo_rev(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    i: i64 = f.nblk.zext() - 1;
    while => i >= 0 {
        b := f.rpo[i];
        body(b);
        i -= 1;
    };
}

fn for_blocks_rpo_forward(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    i := 0;
    while => i < f.nblk.zext() {
        b := f.rpo[i];
        body(b);
        i += 1;
    };
}

// Not the same as just `for_rev(b.ins.slice(0, b.nins)`
// because the body is allowed to consume multiple instructions by offsetting the pointer. 
// This is needed because the ir uses a vairable length encoding (for blit, call, args).
fn for_insts_rev(b: *Qbe.Blk, $body: @Fn(i: **Qbe.Ins) void) void = {
    i := b.ins.index(b.nins.zext());
    while => !i.identical(b.ins.first) {
        i = i.offset(-1);
        body(i&);
        // TODO: @debug_assert() `i` is still in range
    };
}

fn for_insts_forward(b: *Qbe.Blk, $body: @Fn(i: *Qbe.Ins) void) void = {
    i := b.ins.first;
    while => !i.identical(b.ins.index(b.nins.zext())) {
        body(i);
        i = i.offset(1);
    };
}

// :TodoPort bsiter. 
fn for(bs: *Qbe.BSet, $body: @Fn(i: i64) void) void = {
    for(bs, 0, fn(i) => body(i))
}

fn for(bs: *Qbe.BSet, start: i64, $body: @Fn(i: i64) void) void = {
    i: i32 = start.intcast();
    while => bsiter(bs, i&) {
        body(i.zext());
        i += 1;
    };
}

::ptr_utils(Qbe.Phi);
fn for_phi(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Phi) void) void = {
    p := b.phi;
    while => !p.is_null() {
        body(p);
        p = p.link;
    };
}

fn for_jump_targets(b: *Qbe.Blk, $body: @Fn(s: *Qbe.Blk) void) void = 
    for_jump_targets_mut(b, fn(b) => body(b[]));

fn for_jump_targets_mut(b: *Qbe.Blk, $body: @Fn(s: **Qbe.Blk) void) void = {
    // TODO: this pastes the code twice which isn't really what you want probably. 
    if !b.s1.is_null() {
        body(b.s1&);
    };
    // :SketchyIterTargets i use this some places that qbe doesn't have the !identical check (sometimes they do have it) so might need to split into two functions. 
    if !b.s2.is_null() && !b.s1.identical(b.s2) {
        body(b.s2&);
    };
}

//////////////////
/// Ugly Table ///
//////////////////

fn argcls(i: *Qbe.Ins, argument_index: i64) Qbe.Cls = {
    argcls_index :: fn(o: Qbe.O, k: Qbe.Cls, i: i64) i64 => 
        o.raw().int() * 8 + k.raw().int() * 2 + i;

    // TODO: can store 2 per byte instead of 0.5 per byte. 
    //       but even like this its replacing 4KB of code with 2KB of data.
    // argcls_eval looks super branchy so we generate a table of all the options at comptime. 
    argcls_table :: @static(Array(Qbe.Cls, Qbe.O.enum_count() * 4 * 2));
    :: {
        for_enum Qbe.O { o | 
            for (@slice(Qbe.Cls.Kw, .Kl, .Ks, .Kd)) { k | 
                range(0, 2) { i | 
                    argcls_table[argcls_index(o, k, i)] = argcls_eval(o, k, i);
                };
            };
        };
    };
    
    // at runtime its just a lookup
    argcls_table[argcls_index(i.op(), i.cls(), argument_index)]
}

// TODO: fix @is so it can be used here. 
// I'm not totally sure this is a better way to represet this information than just pasting the table. but i really hate pasting tables. so here we are. 
fn argcls_eval(o: Qbe.O, k: Qbe.Cls, argument_index: i64) Qbe.Cls = {
    n := argument_index;
    int := k.is_int();
    
    if(o.between(.ceqw, .cultw), => return(if(int, => .Kw, => .Ke)));
    if(o.between(.ceql, .cultl), => return(if(int, => .Kl, => .Ke)));
    if(o.between(.ceqs, .cuos),  => return(if(int, => .Ks, => .Ke)));
    if(o.between(.ceqd, .cuod),  => return(if(int, => .Kd, => .Ke)));
    
    if(o.between(.flagieq, .flagfuo), => return(if(int, => .Kx, => .Ke)));
    if o == .neg || o == .copy || o == .arg {
        return(if(n == 0, => k, => .Kx));
    };
    
    if o == .add || o == .sub || o == .div || o == .mul || o == .xcmp || o == .swap {
        return(k);
    };
    if (@is(o, .sar, .shr, .shl)) {
        if(!int, => return(.Ke));  
        return(if(n == 0, => k, => .Kw));
    };
    if o == .xtest || o == .acmp || o == .acmn || o == .udiv || o == .urem || o == .or || o == .and || o == .xor || o == .rem {
        return(if(int, => k, => .Ke));  
    };
    if o.between(.par, .paruh) || o == .nop || o == .argv {
        return(.Kx);
    };
    if o == .load || o == .vaarg || o == .call {
        return(if(n == 0, => .Kl, => .Kx));
    };
    if o.between(.alloc4, .alloc16) {
        if(k != .Kl, => return(.Ke));
        return(if(n == 0, => .Kl, => .Kx));
    };
    if (@is(o, .sign, .xidiv, .xdiv, .reqz, .rnez)) {
        if(!int, => return(.Ke));
        return(if(n == 0, => k, => .Kx));
    };
    if o.between(.argsb, .arguh) {
        if(n == 1, => return(.Kx));
        return(if(k == .Kw, => k, => .Ke));
    };
    if o.between(.loadsb, .loaduw) || o == .addr {
        if(!int, => return(.Ke));
        return(if(n == 0, => .Kl, => .Kx));
    };
    if o.between(.extsb, .extuh) {
        if(!int, => return(.Ke));
        return(if(n == 0, => .Kw, => .Kx));
    };
    
    int_c :: fn(kk: Qbe.Cls) => return(if(int, => if(n == 0, => kk, => .Kx), => .Ke));
    if(o == .stosi || o == .stoui, => int_c(.Ks));
    if(o == .dtosi || o == .dtoui, => int_c(.Kd));
    float_c :: fn(kk: Qbe.Cls) => return(if(!int, => if(n == 0, => kk, => .Kx), => .Ke));
    if(o == .swtof || o == .uwtof, => float_c(.Kw));
   
    if(o == .sltof || o == .ultof, => float_c(.Kl));
    if o.between(.storeb, .stored) {
        //if(k != .Kw, => return(.Ke)); // TODO: why do we care?
        if(n == 1, => return(.Kl));
        w: i64 = Qbe.O.storew.raw().zext();
        k := o.raw().zext().max(w) - w; // :StoreOrder
        return(@as(Qbe.Cls) @as(i16) k.intcast().trunc());
    };
    
    want_w :: fn(fst: Qbe.Cls, snd: Qbe.Cls) => return(if(k == .Kw, => if(n == 0, => fst, => snd), => .Ke));
    if(o == .vastart, => want_w(.Kl, .Kx));
    if(o == .dbgloc,  => want_w(.Kw, .Kw));
    if(o == .blit0,   => want_w(.Kl, .Kl));
    if(o == .blit1,   => want_w(.Kw, .Kx));
    want_l :: fn(fst: Qbe.Cls, snd: Qbe.Cls) => return(if(k == .Kl, => if(n == 0, => fst, => snd), => .Ke));
    if(o == .extsw || o == .extuw, => want_l(.Kw, .Kx));
    if(o == .parc || o == .pare, => want_l(.Kx, .Kx));
    if(o == .argc, => want_l(.Kx, .Kl));
    if(o == .arge || o == .salloc, => want_l(.Kl, .Kx));
    if(o == .exts, => return(if(k == .Kd, => if(n == 0, => .Ks, => .Kx), => .Ke)));
    if(o == .truncd, => return(if(k == .Ks, => if(n == 0, => .Kd, => .Kx), => .Ke)));
    if(o == .afcmp, => return(if(!int, => k, => .Ke)));
    if o == .cast {
        if(n == 1, => return(.Kx));
        k: i64 = k.raw().zext().bit_xor(2);
        return(@as(Qbe.Cls) @as(i16) k.intcast().trunc());
    };
    if o == .Oxxx {
        return(.Ke); // unreachable. this is just so it can go in the table. 
    };

    names :: Qbe.O.get_enum_names();
    @panic("invalid op number for argcls %", names[o.raw().zext()])
}
