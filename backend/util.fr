// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

// note: for now the c code still calls its own version of many of these so you can't change the layout of BSet/QList!

fn BIT(n: i64) u64 #inline = 1.shift_left(n);
fn SLOT(i: i64) Qbe.Ref #inline = ref(.RSlot, i.bit_and(0x1fffffff));
fn TMP(i: i64)  Qbe.Ref #inline = ref(.RTmp, i);
fn CALL(i: i64) Qbe.Ref #inline = ref(.RCall, i);
fn CON(i: i64) Qbe.Ref #inline = ref(.RCon, i);
fn INT(i: i64) Qbe.Ref #inline = ref(.RInt, i.bit_and(0x1fffffff));
fn MEM(i: i64) Qbe.Ref #inline = ref(.RMem, i);
fn TYPE(i: i64) Qbe.Ref #inline = ref(.RType, i);

fn TMP(i: i32) Qbe.Ref #inline = TMP(i.intcast());
fn SLOT(i: i32) Qbe.Ref #inline = SLOT(i.intcast());

fn isreg(r: Qbe.Ref) bool #inline = 
    rtype(r) == .RTmp && r.val() < Qbe.Tmp0;

// tmp[t].phi has which id was the .to of a phi with t as an arg. 
// this looks up a chain of phis to see where t is going to end up. 
// if its not used as a phi arg, that's just itself. 
fn phi_dest(t: i64, tmp: QList(Qbe.Tmp)) i64 = {
    t1 := tmp[t].phi.zext();
    if(t1 == 0, => return(t)); // t was not a phi arg
    t1 = phi_dest(t1, tmp);
    tmp[t].phi = t1.intcast();  // cache long chains
    t1
}

fn clsmerge(pk: *Qbe.Cls, k: Qbe.Cls) bool = {
    k1 := pk[];
    if k1 == .Kx {
        pk[] = k;
        return(false);
    };
    xx := (k1 == .Kw && k == .Kl); // TODO: compiler bug. fix overloading when you inline this. 
    if xx || (k1 == .Kl && k == .Kw) {
        pk[] = .Kw;
        return(false);
    };
    k1 != k
}

fn slice_pending_scratch(f: *Qbe.Fn) []Qbe.Ins = {
    n := Qbe.MaxInstructions;
    f.globals.insb.items().slice(n - f.len_scratch(), n)
}

fn copy_instructions_from_scratch(f: *Qbe.Fn, b: *Qbe.Blk) void = {
	idup(b, f.scratch_next(), f.len_scratch());
	f.globals.curi[] = f.scratch_start(); 
}

// TODO: i should really just use a different version of emit() so this part could just be a memcpy
// hey i heard you like long function names...
fn copy_instructions_from_scratch_reversed_which_means_forwards(f: *Qbe.Fn, b: *Qbe.Blk) void = {
    n := f.len_scratch();
    b.nins = n.trunc();
    if(n == 0, => return());
    if b.ins.first.is_null() {
        b.ins = new(n);  // caller better not be trying to put this in the inlining cache
    } else {
        b.ins&.grow(n);
    };
    i := 0;
    for(f.scratch_next(), f.scratch_start()) { ins |
        b.ins[n - i - 1] = ins[];
        i += 1;
    };
    
	f.globals.curi[] = f.scratch_start();
}

fn scratch_start(f: *Qbe.Fn) *Qbe.Ins #inline =
    f.globals.insb.index_unchecked(Qbe.MaxInstructions);

fn scratch_next(f: *Qbe.Fn) *Qbe.Ins #inline =
    f.globals.curi[];

fn reset_scratch(f: *Qbe.Fn) void #inline = {
    f.globals.curi[] = f.scratch_start(); 
}

fn len_scratch(f: *Qbe.Fn) i64 #inline = 
    f.scratch_next().ptr_diff(f.scratch_start()); 

// it uses insb/curi as a buffer for emitting new instructions and then copies them out to a newly allocated array at the end of the block. 
// the parser starts at the beginning but the passes start at the end and `emit` in reverse. 
// so i think they scan block backwards. like isel will iterate backwards, adding some instructions, and copying out those it likes,
// and then at the end, copy from the buffer and update the block with that pointer. 
// so they're always in order in the blocks but you always process them backwards. 
//
// TODO: we rely on globals.curi pointing to the last inst we emitted (in isel) :LookAtLastInst
fn emit(f: *Qbe.Fn, op: Qbe.O, k: Qbe.Cls, to: Qbe.Ref, arg0: Qbe.Ref, arg1: Qbe.Ref) *Qbe.Ins = {
    i := f.globals.curi;
    if i[].identical(f.globals.insb.as_ptr()) {
        @panic("emit, too many instructions");
    };
    i[] = i[].offset(-1);
    i[][] = make_ins(op, k, to, arg0, arg1);
    i[]
}

fn emit(f: *Qbe.Fn, i: Qbe.Ins) *Qbe.Ins #inline = 
    f.emit(i&.op(), i&.cls(), i.to, i.arg&[0], i.arg&[1]);

fn newtmp(f: *Qbe.Fn, debug_hint: Str, k: Qbe.Cls) Qbe.Ref = {
    t: i64 = f.ntmp.zext();
    f.ntmp += 1;
    f.tmp&.grow(t + 1);
    f.tmp.slice(t, t + 1).set_zeroed();
    tmp := f.tmp.index(t);
    @if(TRACK_IR_NAMES) {
        l: List(u8) = (maybe_uninit = tmp.name&.items(), len = 0, gpa = panicking_allocator); // :UnacceptablePanic
        @fmt(l&, "%.%", debug_hint, t);
    };
    tmp.cls = k;
    tmp.slot = -1;
    tmp.nuse = 1;
    tmp.ndef = 1;
    tmp.defining_block = -1;
    TMP(t)
}

:: {
    fn eq(a: Qbe.Sym, b: Qbe.Sym) bool #inline = 
        a.id == b.id;
    fn ne(a: Qbe.Sym, b: Qbe.Sym) bool #inline = !(a == b);
};

fn newcon(f: *Qbe.Fn, c0: *Qbe.Con) Qbe.Ref = {
    // deduplicate
    range(1, f.ncon.zext()) { i |
        c1 := f.con.index(i);
        if c0.type == c1.type && c0.bits.i == c1.bits.i && c0.sym == c1.sym {
            return(CON(i));
        };
    };
    
    // push
    f.con&.grow(f.ncon.zext() + 1);
    f.con[f.ncon.zext()] = c0[];
    f.ncon += 1;
    CON(f.ncon.zext() - 1)
}

// TODO: is it worth the copy paste instead of calling newcon to avoid comparing .sym everytime?
fn getcon(f: *Qbe.Fn, val: i64) Qbe.Ref = {
    // deduplicate
    range(1, f.ncon.zext()) { c |
        cc := f.con[c]&;
        if cc.type == .CBits && cc.bits.i == val {
            return(CON(c));
        };
    };
    // push
    c := f.ncon.zext();
    f.ncon += 1;
    f.con&.grow(f.ncon.zext());
    f.con[c] = (type = .CBits, bits = (i = val));
    CON(c)
}

// TODO: call this from elsewhere
fn symcon(f: *Qbe.Fn, name: Str) Qbe.Ref = 
    f.symcon(f.globals.intern(name));

fn symcon(f: *Qbe.Fn, id: u32) Qbe.Ref = {
    con: Qbe.Con = (type = .CAddr, sym = (id = id), bits = (i = 0));
    f.newcon(con&)
}

fn unique_named_con(f: *Qbe.Fn, sym: u32, off: i64) Qbe.Ref = {
    c := f.ncon.zext();
    f.ncon += 1;
    f.con&.grow(f.ncon.zext());
    f.con[c] = (type = .CAddr, bits = (i = off), sym = (id = sym));
    CON(c)
}

fn new_mem(f: *Qbe.Fn, base: Qbe.Ref, index: Qbe.Ref, offset: i64, scale: i32) Qbe.Ref = {
    m := f.nmem.zext();
    f.nmem += 1;
    f.mem&.grow(m + 1);
    f.mem[m].base = base;
    f.mem[m].index = index;
    f.mem[m].offset.type = @if(scale == 0, .CUndef, .CBits);
    f.mem[m].offset.bits.i = offset;
    f.mem[m].scale = scale;
    MEM(m)
}

// TODO: use slices 

fn idup(b: *Qbe.Blk, src: *Qbe.Ins, n: i64) void = {
    b.nins = n.trunc();
    //if n == 0, => return()); // TODO
    if b.ins.first.is_null() {
        b.ins = new(n);  // caller better not be trying to put this in the inlining cache
    } else {
        @debug_assert(!src.identical(b.ins.first));
        b.ins&.grow(n);
    };
    mem := b.ins.slice(0, n);
    mem.copy_from(ptr = src, len = n);
}

fn icpy(dest: *Qbe.Ins, src: *Qbe.Ins, n: i64) *Qbe.Ins = {
    if(n == 0, => return(dest));
    (@as([]Qbe.Ins)(ptr = dest, len = n)).copy_from(ptr = src, len = n); // TODO: ugly
    dest.offset(n)
}

fn salloc(rt: Qbe.Ref, rs: Qbe.Ref, f: *Qbe.Fn) void = {
    /* we need to make sure
    * the stack remains aligned
    * (rsp = 0) mod 16
    */
    f.dynalloc = true;
    if rtype(rs) == .RCon {
        sz := f.con[rs.val()].bits.i;
        if sz < 0 || sz >= MAX_i32 - 15 {
            @panic("invalid alloc size %", sz);
        };
        sz = (sz + 15).bit_and(-16);
        f.emit(.salloc, .Kl, rt, f.getcon(sz), QbeNull);
    } else {
        /* r0 = (r + 15) & -16 */
        r0 := f.newtmp("isel", .Kl);
        r1 := f.newtmp("isel", .Kl);
        f.emit(.salloc, .Kl, rt, r0, QbeNull);
        f.emit(.and, .Kl, r0, r1, f.getcon(-16));
        f.emit(.add, .Kl, r1, rs, f.getcon(15));
        if f.tmp[rs.val()].slot != -1 {
            @panic("unlikely alloc argument % for %", f.tmp[rs.val()]&.name(), f.tmp[rt.val()]&.name());
        }
    }
}

fn between(o: Qbe.O, $fst: Qbe.O, $lst: Qbe.O) bool #inline = {
    ::assert(fst.raw() < lst.raw(), "between(Qbe.O) args are out of order");
    fst.raw() <= o.raw() && o.raw() <= lst.raw()
}

fn addins(pvins: *QList(Qbe.Ins), pnins: *i64, i: Qbe.Ins) void #inline = 
    addins(pvins, pnins, i&);

fn addins(pvins: *QList(Qbe.Ins), pnins: *i64, i: *Qbe.Ins) void #inline = {
    if i.op() != .nop {
        pnins[] += 1;
        pvins.grow(pnins[]);
        pvins[][pnins[] - 1] = i[];
    };
}

fn push(b: *Qbe.Blk, i: Qbe.Ins) void #inline = {
    b.ins&.grow(b.nins.zext() + 1);
    b.ins[b.nins.zext()] = i;
    b.nins += 1;
}

fn append(dest: *QList(Qbe.Ins), dest_len: *i64, src_start: *Qbe.Ins, src_end: *Qbe.Ins) void = {
    @debug_assert(!dest.first.identical(src_start));
    count := src_start.ptr_diff(src_end);
    dest.grow(dest_len[] + count);
    for(src_start, src_end) { i |
        if i.op() != .nop {
            dest_len[] += 1;
            dest[][dest_len[] - 1] = i[];
        };
    };
    
    // it feels like this would be better but no, clearing out the nops is faster. -- Nov 20
    // it also feels too fragile that the below only works if you remove the nop check from addins.
    //old_len := dest_len[];
    //dest_len[] += count;
    //dest.grow(dest_len[]);
    //dest: []Qbe.Ins = (ptr = dest.index(old_len), len = count);
    //src:  []Qbe.Ins = (ptr = src_start, len = count);
    //dest.copy_from(src);
}

fn addbins(b: *Qbe.Blk, pvins: *QList(Qbe.Ins), pnins: *i64) void = 
    append(pvins, pnins, b.ins.first, b.ins.index(b.nins.zext()));

// we try to line up the opcode numbers so you can branchlessly convert between categories (ie. load<->ext, par<->ext)
fn rebase(value: Qbe.O, new_base: Qbe.O, old_base: Qbe.O) Qbe.O = 
    @as(Qbe.O) @as(i32) new_base.raw() + (value.raw() - old_base.raw());

///////////////////////
/// Condition Codes ///
///////////////////////

fn iscmp(op: Qbe.O) bool =  // :CmpOrder
    op.between(.ceqw, .cuod);

// TODO: use multiple returns
fn iscmp(op: Qbe.O, pk: *Qbe.Cls, pc: *i32) bool = { // :CmpOrder
    if !op.iscmp() {
        return(false);
    };
    
    if op.between(.ceqw, .cultw) {
        pc[] = op.raw() - Qbe.O.ceqw.raw();
        pk[] = .Kw;
        return(true);
    };
    if op.between(.ceql, .cultl) {
        pc[] = op.raw() - Qbe.O.ceql.raw();
        pk[] = .Kl;
        return(true);
    };
    
    ::@assert_eq(Qbe.Cmp.enum_count(), Qbe.CmpICount + Qbe.CmpFCount, "cmp counts don't add up");
    if op.between(.ceqs, .cuos) {
        pc[] = Qbe.CmpICount.intcast() + op.raw() - Qbe.O.ceqs.raw();
        pk[] = .Ks;
        return(true);
    }; 
    if op.between(.ceqd, .cuod) {
        pc[] = Qbe.CmpICount.intcast() + op.raw() - Qbe.O.ceqd.raw();
        pk[] = .Kd;
        return(true);
    };
    
    false
}

// TODO: this is dumb. i should arrange the numbers so its just flipping a bit. 
//       don't forget to change the order of jmps/flags/ops, iscmp ranges. :CmpOrder
//       but have to wait until i'm not calling into qbe anymore.  -- Oct 8 

::List(Qbe.Cmp);
::enum_basic(Qbe.Cmp);

fn cmpneg(cc: i32) i32 = {
    negate_cc :: @const_slice( // :CmpOrder
        // Cieq   Cine   Cisge   Cisgt   Cisle   Cislt   Ciuge   Ciugt   Ciule   Ciult
   Qbe.Cmp.Cine, .Cieq, .Cislt, .Cisle, .Cisgt, .Cisge, .Ciult, .Ciule, .Ciugt, .Ciuge,
        // Cfeq   Cfge   Cfgt   Cfle   Cflt   Cfne   Cfo   Cfuo
          .Cfne, .Cflt, .Cfle, .Cfgt, .Cfge, .Cfeq, .Cfuo, .Cfo
    );

    @debug_assert(cc >= 0 && cc < Qbe.Cmp.enum_count().intcast(), "bad cc");
    negate_cc[cc.zext()].raw()
}

fn cmpop(cc: i32) i32 = {
    swap_cc :: @const_slice( // :CmpOrder
        // Cieq   Cine   Cisge   Cisgt   Cisle   Cislt   Ciuge   Ciugt   Ciule   Ciult
   Qbe.Cmp.Cieq, .Cine, .Cisle, .Cislt, .Cisge, .Cisgt, .Ciule, .Ciult, .Ciuge, .Ciugt,
        // Cfeq   Cfge   Cfgt   Cfle   Cflt   Cfne   Cfo   Cfuo
          .Cfeq, .Cfle, .Cflt, .Cfge, .Cfgt, .Cfne, .Cfo, .Cfuo
    );

    @debug_assert(cc >= 0 && cc < Qbe.Cmp.enum_count().intcast(), "bad cc");
    swap_cc[cc.zext()].raw()
}

////////////////
/// Bit Sets ///
////////////////
// TODO: use unsigned division can be shifts (if negative you truncate towards 0 not -inifinity which a shift gives you)

fn bsinit(bs: *Qbe.BSet, n: i64) void #inline = {
    n := (n + Qbe.NBit - 1) / Qbe.NBit;
    bs.nt = n.trunc();
    bs.t = temp().alloc_zeroed(u64, n).as_ptr(); 
}

fn bsequal(a: *Qbe.BSet, b: *Qbe.BSet) bool = {
    @debug_assert(a.nt == b.nt, "BSet mismatch");
    range(0, a.nt.zext()) { i |
        if a.t.offset(i)[] != b.t.offset(i)[] {
            return(false);
        }
    };
    true
}

fn bszero(bs: *Qbe.BSet) void = {
    mem: []u64 = (ptr = bs.t, len = bs.nt.zext());
    mem.set_zeroed();
}

// it's sad how well this would vectorize
fn bscount(bs: *Qbe.BSet) u32 = {
    n: u32 = 0;
    range(0, bs.nt.zext()) { i |
        n += count_ones(bs.t.offset(i));
    };
    n
}

// TODO: make elt unsigned to the division is free
fn bshas(bs: *Qbe.BSet, elt: i64) bool #inline = {
    @debug_assert(elt < bs.nt.zext() * Qbe.NBit, "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    mask := BIT(j);
    slot[].bit_and(mask) != 0
}

fn bsset(bs: *Qbe.BSet, elt: i64) void #inline = {
    @debug_assert(elt < bsmax(bs), "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    slot[] = slot[].bit_or(BIT(j));
}

fn bsclr(bs: *Qbe.BSet, elt: i64) void #inline = {
    @debug_assert(elt < bsmax(bs), "bs oob");
    i, j := elt.div_mod(Qbe.NBit);
    slot := bs.t.offset(i);
    slot[] = slot[].bit_and(bit_not(BIT(j)));
}

fn bsmax(bs: *Qbe.BSet) i64 #inline =
    bs.nt.zext() * Qbe.NBit;

fn init_bitset(size: i64) Qbe.BSet #inline = {
    // this puts it in the pool thing that freeall deals with. i can just stick an arena allocator somewhere i guess and clear it at the end of the function. 
    // is it worth resetting temp() between passes or should i just do it at the end and not deal with it? 
    s := Qbe.BSet.zeroed();
    bsinit(s&, size); 
    s
}

fn bscopy(a: *Qbe.BSet, b: *Qbe.BSet) void = 
    copy_no_alias(u64.raw_from_ptr(a.t), u64.raw_from_ptr(b.t), a.nt.zext() * 8);

fn bsunion(a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_or(b));
fn bsinter(a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_and(b));
fn bsdiff( a: *Qbe.BSet, b: *Qbe.BSet) void = BSOP(a, b, fn(a, b) => a.bit_and(b.bit_not()));

fn BSOP(a: *Qbe.BSet, b: *Qbe.BSet, $fuse: @Fn(a: u64, b: u64) u64) void #inline = {
    @debug_assert(a.nt == b.nt, "BSet mismatch");
    range(0, a.nt.zext()) { i |
        a := a.t.offset(i);
        a[] = fuse(a[], b.t.offset(i)[]);
    };
}

/////////////
/// Lists ///
/////////////
// TODO: use my allocators for this. 

VMagLong :: 0xcabba9e;  // random number chosen by fair die roll
VMagTemp :: 0xca88abe;

::ptr_utils(QListHeader);
::ptr_utils(*QListHeader);

fn calloc(nitems: i64, size: i64) rawptr #libc;

fn vnew(len: i64, esz: i64, where: i64, m: *QbeModule) rawptr #inline = {
    cap := 2;
    while => cap < len {
        cap *= 2;
    };
    
    words := (cap * esz + QListHeader.size_of() + 7) / 8;
    ::if([]i64);
    v := if where == VMagTemp {
        temp().alloc(i64, words)
    } else {
        @if(use_threads) pthread_mutex_lock(m.intern_mutex&).unwrap();
        mem := m.forever&.borrow().alloc(i64, words);
        @if(use_threads) pthread_mutex_unlock(m.intern_mutex&).unwrap();
        mem
    };
    v := ptr_cast_unchecked(i64, QListHeader, v.ptr);
    v.where = where;
    v.m = m;
    v.cap = cap;
    v.esz = esz;
    QListHeader.raw_from_ptr(v.offset(1))
}

fn vgrow(vpr: rawptr, len: i64) void = {
    vp := ptr_from_raw(*QListHeader, vpr);
    v := vp[].offset(-1);
    @debug_assert(!v.offset(1).is_null() && !v.is_null(), "vgrow on null QList");
    @debug_assert_ne(v.where, QLIST_DOUBLE_FREE, "QList bad magic growing % to %", vpr, len);
    if(v.cap >= len, => return());
    v1 := vnew(len, v.esz, v.where, v.m);
    copy_no_alias(v1, QListHeader.raw_from_ptr(v.offset(1)), v.cap * v.esz);
    vfree(QListHeader.raw_from_ptr(v.offset(1)));
    vp[] = ptr_from_raw(QListHeader, v1);
}

::enum(Qbe.Pool);

fn debug_zero_owned_memory(v: *QListHeader) void = {
    len := v.cap * v.esz;
    p := ptr_cast_unchecked(@type v, u8, v.offset(1));
    //memset(u8.raw_from_ptr(p), 0, len);
}

fn vfree(p: rawptr) void = {
    v := QListHeader.ptr_from_raw(p).offset(-1);
    w := v.where;
    v.where = QLIST_DOUBLE_FREE;
    if w == VMagTemp {
        debug_zero_owned_memory(v);
        return();
    };
    @debug_assert_eq(w, VMagLong, "bad QList magic");
    debug_zero_owned_memory(v);
    // not a thing now that we're using module.forever. should profile to see which is better. 
    //a := libc_allocator;
    //free(QListHeader.raw_from_ptr(v));
}

// This is a header that goes before the allocated memory for the array. 
// When a field is an QList it points to the first entry of one of these and you offset backwards to get the header. 
QListHeader :: @struct(
    where: i64,
    m: *QbeModule, // nullable
    esz: i64,
    cap: i64,
);

fn header();
fn grow();
fn new_copy();
fn new_copy_long_life();
fn new_long_life();
fn new();

fn QList($T: Type) Type = {
    Self :: @struct(first: *T);
    ::ptr_utils(T);
    
    fn new(len: i64) Self = {
        (first = T.ptr_from_raw(vnew(len, T.size_of(), VMagTemp, QbeModule.ptr_from_int(0))))
    }
    
    fn new_long_life(m: *QbeModule, len: i64) Self = 
        (first = T.ptr_from_raw(vnew(len, T.size_of(), VMagLong, m)));
    
    fn new_copy(items: []T) Self = {
        self: Self = new(items.len);
        self.slice(0, items.len).copy_from(items);
        self
    }
    
    fn new_copy_long_life(m: *QbeModule, items: []T) Self = {
        self: Self = m.new_long_life(items.len);
        self.slice(0, items.len).copy_from(items);
        self
    }
    
    // ensure capacity is >= len
    fn grow(s: *Self, len: i64) void = {
        v := Self.raw_from_ptr(s);
        vgrow(v, len);
    }
    
    fn free(s: Self) void = {
        v := T.raw_from_ptr(s.first);
        vfree(v);
    }
    
    // TODO: bounds checking but its hard as long as i have to keep qbe's memory layout because the fields aren't together. 
    //       and anyway it would be sad to add alignment padding just for bounds checks. 
    
    fn index(s: Self, i: i64) *T #inline = 
        s.first.offset(i);
    
    fn index(s: *Self, i: i64) *T #inline = 
        s.first.offset(i);
    
    fn slice(self: Self, start: i64, end: i64) []T #inline = {
        (ptr = self.first.offset(start), len = end - start)
    }
    
    Self
}

///////////////////
/// String Pool ///
///////////////////

IBits :: 12;
IMask :: 1.shift_left(IBits) - 1;

QLIST_DOUBLE_FREE :: 0x69696969;

fn intern(m: *QbeModule, s: Str) u32 = {
    h := hash(s).bit_and(IMask.trunc());
    b := m.symbols.index(h.zext());
    // :ThreadSafety
    n := b.n;
    @if(use_threads) {
        locked_by_me := LOCKED_BASE.bit_or(context(DefaultContext)[].thread_index.trunc());
        @debug_assert(n != locked_by_me, "already locked by this thread");
        while => n.bit_and(LOCKED_BASE) != 0 {
            // If this happened a lot you'd want to use something less dumb.
            // But you get here ~3 times when self compiling so it doesn't matter yet -- Dec 25
            usleep(1);
            n = b.n;
        };
        b.n = locked_by_me;
    };
    range(0, n.zext()) { i |
        if s == b.data[i].name {
            b.n = n;
            return(h + i.trunc().shift_left(IBits));
        }
    };
    
    if n == 1.shift_left(32 - IBits) { 
        @panic("interning table overflow");
    };
    if n == 0 {
        b.data = m.new_long_life(1);
    } else {
        if n.bit_and(n - 1) == 0 {
            b.data&.grow(zext(n + n));
        }
    };
   
    // TODO: have ArenaAlloc use atomics
    @if(use_threads) pthread_mutex_lock(m.intern_mutex&).unwrap();
    mem := m.forever&.borrow().alloc(u8, s.len);
    @if(use_threads) pthread_mutex_unlock(m.intern_mutex&).unwrap();
    
    mem.copy_from(s);
    data := b.data.index(n.zext());
    data[] = SymbolInfo.zeroed();
    data.name = mem;
    data.got_lookup_offset = -1;
    data.offset = -1;
    data.wasm_type_index = -1;
    
    b.n = n + 1;
    id := h + n.shift_left(IBits);
    id
}

LOCKED_BASE :: 0xF0000000;
    
// TODO: this is garbage i need a way to defer something to run if body early exits. 
// :ThreadSafety need to use cas or whatever
fn use_symbol(m: *QbeModule, id: u32, $body: @Fn(s: *SymbolInfo) void) void = {
    ::ptr_utils(QbeModule);
    @debug_assert(!m.is_null(), "null module???");
    bid: i64 = id.bit_and(IMask).zext();
    bucket := m.symbols[bid]&;
    
    old := bucket.n;
    @if(use_threads) {
        // :ThreadSafety
        thread := context(DefaultContext)[].thread_index;
        locked_by_me := LOCKED_BASE.bit_or(thread.trunc());
        @debug_assert(old != locked_by_me, "use_symbol but already locked by this thread id=%, thread=%", id, thread);
        while => old.bit_and(LOCKED_BASE) != 0 {
            usleep(1);
            old = bucket.n;
        };
        bucket.n = locked_by_me;
    };
    idx := id.shift_right_logical(IBits);
    symbol := bucket.data.index(idx.zext());
    body(symbol);
    bucket.n = old;
}

// :ThreadSafety you can't call this one
fn get_symbol_info(m: *QbeModule, id: u32) *SymbolInfo = {
    bid: i64 = id.bit_and(IMask).zext();
    bucket := m.symbols[bid]&;
    idx := id.shift_right_logical(IBits);
    bucket.data.index(idx.zext())
}

fn str(m: *QbeModule, id: u32) Str = {
    name := "";
    use_symbol(m, id) { s |
        name = s.name;
    };
    @debug_assert(!name.ptr.is_null(), "null name for symbol %", id);
    name
}

fn hash(s: Str) u32 = {
    h: i64 = 1;
    for s { s |
        h = s.zext() + 17 * h; // prime number chosen by fair die roll 
    };
    h.trunc()
}

/////////////////
/// Iterators ///
/////////////////

fn tmps(f: *Qbe.Fn) []Qbe.Tmp #inline = 
    f.tmp.slice(0, f.ntmp.zext());

fn uses(t: *Qbe.Tmp) []Qbe.Use #inline = 
    t.use.slice(0, t.nuse.zext());
    
fn for_blocks(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    b := f.start;
    while => !b.is_null() {
        body(b);
        b = b.link;
    };
}

fn for_dom(b: *Qbe.Blk, $body: @Fn(b: *Qbe.Blk) void) void = {
    b := b.dom;
    while => !b.is_null() {
        body(b);
        b = b.dlink;
    };
}

fn for_pred(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Blk) void) void = {
    range(0, b.npred.zext()) { p | 
        body(b.pred[p]);
    };
}

::ptr_utils(*Qbe.Blk);
fn for_blocks_rpo_rev(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    i: i64 = f.nblk.zext() - 1;
    while => i >= 0 {
        b := f.rpo[i];
        body(b);
        i -= 1;
    };
}

fn for_blocks_rpo_forward(f: *Qbe.Fn, $body: @Fn(b: *Qbe.Blk) void) void = {
    i := 0;
    while => i < f.nblk.zext() {
        b := f.rpo[i];
        body(b);
        i += 1;
    };
}

// Not the same as just `for_rev(b.ins.slice(0, b.nins)`
// because the body is allowed to consume multiple instructions by offsetting the pointer. 
// This is needed because the ir uses a vairable length encoding (for blit, call, args).
fn for_insts_rev(b: *Qbe.Blk, $body: @Fn(i: **Qbe.Ins) void) void = {
    i := b.ins.index(b.nins.zext());
    while => !i.identical(b.ins.first) {
        i = i.offset(-1);
        body(i&);
        // TODO: @debug_assert() `i` is still in range
    };
}

fn for_insts_forward(b: *Qbe.Blk, $body: @Fn(i: *Qbe.Ins) void) void = {
    i := b.ins.first;
    while => !i.identical(b.ins.index(b.nins.zext())) {
        body(i);
        i = i.offset(1);
    };
}

fn for(bs: *Qbe.BSet, $body: @Fn(i: i64) void) void = {
    for(bs, 0, fn(i) => body(i))
}

fn for(bs: *Qbe.BSet, start: i64, $body: @Fn(i: i64) void) void = {
    t := start.shift_right_logical(6);
    range(t, bs.nt.zext()) { k |
        b := bs.t.offset(k)[].bitcast();
        while => b != 0 {
            body(k * 64 + trailing_zeros(b));
            bit := b.bit_and(-b);  // pick out the least significant bit
            b = b.bit_xor(bit);    // unset that bit
        };
    };
}

::ptr_utils(Qbe.Phi);
fn for_phi(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Phi) void) void = {
    p := b.phi;
    while => !p.is_null() {
        body(p);
        p = p.link;
    };
}

fn for_jump_targets(b: *Qbe.Blk, $body: @Fn(s: *Qbe.Blk) void) void = 
    for_jump_targets_mut(b, fn(b) => body(b[]));

fn for_jump_targets_mut(b: *Qbe.Blk, $body: @Fn(s: **Qbe.Blk) void) void = {
    targets := get_jump_targets(b);
    enumerate targets { i, s |
        continue :: local_return;
        // :SketchyIterTargets i use this some places that qbe doesn't have the !identical check (sometimes they do have it) so might need to split into two functions. 
        for targets.slice(0, i) { s2 | // :SLOW 
            if(s[].identical(s2), => continue());
        };
        
        body(s);
    };
}

fn for_pars(f: *Qbe.Fn, $body: @Fn(i: *Qbe.Ins) void) void = 
    for_insts_forward f.start { i | 
        continue :: local_return;
        if(@is(i.op(), .global_get, .pop), => continue()); // :HACK
        
        if(!is_par(i.op()), => return()); 
        body(i);
    };

fn get_int(f: *Qbe.Fn, r: Qbe.Ref) ?i64 #inline = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type != .CBits, => return(.None));
    (Some = c.bits.i)
}

// (id, offset)
fn get_sym(f: *Qbe.Fn, r: Qbe.Ref) ?Ty(u32, i64) #inline = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type != .CAddr, => return(.None));
    (Some = (c.sym.id, c.bits.i))
}

fn push(phi: *Qbe.Phi, blk: *Qbe.Blk, arg: Qbe.Ref) void = {
    // TODO: assert blk is unique
    phi.narg += 1;
    phi.arg&.grow(phi.narg.zext());
    phi.blk&.grow(phi.narg.zext());
    phi.arg[phi.narg.zext() - 1] = arg;
    phi.blk[phi.narg.zext() - 1] = blk;
}

//////////////////
/// Ugly Table ///
//////////////////

fn argcls(i: *Qbe.Ins, argument_index: i64) Qbe.Cls = {
    argcls_index :: fn(o: Qbe.O, k: Qbe.Cls, i: i64) i64 => 
        o.raw().int() * 8 + k.raw().int() * 2 + i;

    // TODO: can store 2 per byte instead of 0.5 per byte. 
    //       but even like this its replacing 4KB of code with 2KB of data.
    // argcls_eval looks super branchy so we generate a table of all the options at comptime. 
    argcls_table :: @static(Array(Qbe.Cls, Qbe.O.enum_count() * 4 * 2));
    :: {
        for_enum Qbe.O { o | 
            K :: @const_slice(Qbe.Cls.Kw, .Kl, .Ks, .Kd);
            for K { k | 
                range(0, 2) { i | 
                    argcls_table[argcls_index(o, k, i)] = argcls_eval(o, k, i);
                };
            };
        };
    };
    
    // at runtime its just a lookup
    argcls_table[argcls_index(i.op(), i.cls(), argument_index)]
}

// TODO: fix @is so it can be used here. 
// I'm not totally sure this is a better way to represet this information than just pasting the table. but i really hate pasting tables. so here we are. 
fn argcls_eval(o: Qbe.O, k: Qbe.Cls, argument_index: i64) Qbe.Cls = {
    n := argument_index;
    int := k.is_int();
    
    if(o.between(.ceqw, .cultw), => return(if(int, => .Kw, => .Ke)));
    if(o.between(.ceql, .cultl), => return(if(int, => .Kl, => .Ke)));
    if(o.between(.ceqs, .cuos),  => return(if(int, => .Ks, => .Ke)));
    if(o.between(.ceqd, .cuod),  => return(if(int, => .Kd, => .Ke)));
    
    if(o.between(.flagieq, .flagfuo), => return(if(int, => .Kx, => .Ke)));
    if (@is(o, .neg, .copy, .arg, .push, .switch0)) {
        return(if(n == 0, => k, => .Kx));
    };
    
    if (@is(o, .add, .sub, .div, .mul, .xcmp, .swap)) {
        return(k);
    };
    if (@is(o, .sar, .shr, .shl)) {
        if(!int, => return(.Ke));  
        return(if(n == 0, => k, => .Kw));
    };
    if (@is(o, .xtest, .acmp, .acmn, .udiv, .urem, .or, .and, .xor, .rem)) {
        return(if(int, => k, => .Ke));  
    };
    if o.between(.par, .paruh) || (@is(o, .nop, .argv, .pop, .global_get)) {
        return(.Kx);
    };
    
    if (@is(o, .load, .vaarg, .call, .truncl)) {
        return(if(n == 0, => .Kl, => .Kx));
    };
    if o.between(.alloc4, .alloc16) {
        if(k != .Kl, => return(.Ke));
        return(if(n == 0, => .Kl, => .Kx));
    };
    if (@is(o, .sign, .xidiv, .xdiv, .reqz, .rnez)) {
        if(!int, => return(.Ke));
        return(if(n == 0, => k, => .Kx));
    };
    if o.between(.argsb, .arguh) {
        if(n == 1, => return(.Kx));
        return(if(k == .Kw, => k, => .Ke));
    };
    if o.between(.loadsb, .loaduw) || o == .addr {
        if(!int, => return(.Ke));
        return(if(n == 0, => .Kl, => .Kx));
    };
    if o.between(.extsb, .extuh) {
        if(!int, => return(.Ke));
        return(if(n == 0, => .Kw, => .Kx));
    };
    
    int_c :: fn(kk: Qbe.Cls) => return(if(int, => if(n == 0, => kk, => .Kx), => .Ke));
    if(o == .stosi || o == .stoui, => int_c(.Ks));
    if(o == .dtosi || o == .dtoui, => int_c(.Kd));
    float_c :: fn(kk: Qbe.Cls) => return(if(!int, => if(n == 0, => kk, => .Kx), => .Ke));
    if(o == .swtof || o == .uwtof, => float_c(.Kw));
   
    if(o == .sltof || o == .ultof, => float_c(.Kl));
    if o.between(.storeb, .stored) {
        //if(k != .Kw, => return(.Ke)); // TODO: why do we care?
        if(n == 1, => return(.Kl));
        w: i64 = Qbe.O.storew.raw().zext();
        k := o.raw().zext().max(w) - w; // :StoreOrder
        return(@as(Qbe.Cls) @as(i32) k.intcast());
    };
    
    want_w :: fn(fst: Qbe.Cls, snd: Qbe.Cls) => return(if(k == .Kw, => if(n == 0, => fst, => snd), => .Ke));
    if(o == .vastart, => want_w(.Kl, .Kx));
    if(o == .dbgloc,  => want_w(.Kw, .Kw));
    if(o == .blit0,   => want_w(.Kl, .Kl));
    if(o == .blit1,   => want_w(.Kw, .Kx));
    want_l :: fn(fst: Qbe.Cls, snd: Qbe.Cls) => return(if(k == .Kl, => if(n == 0, => fst, => snd), => .Ke));
    if(o == .extsw || o == .extuw, => want_l(.Kw, .Kx));
    if(o == .parc || o == .pare, => want_l(.Kx, .Kx));
    if(o == .argc, => want_l(.Kx, .Kl));
    if(o == .arge || o == .salloc, => want_l(.Kl, .Kx));
    if(o == .exts, => return(if(k == .Kd, => if(n == 0, => .Ks, => .Kx), => .Ke)));
    if(o == .truncd, => return(if(k == .Ks, => if(n == 0, => .Kd, => .Kx), => .Ke)));
    if(o == .afcmp, => return(if(!int, => k, => .Ke)));
    if o == .cast {
        if(n == 1, => return(.Kx));
        k: i64 = k.raw().zext().bit_xor(2);
        return(@as(Qbe.Cls) @as(i32) k.intcast());
    };
    if o == .Oxxx {
        return(.Ke); // unreachable. this is just so it can go in the table. 
    };
    if (@is(o, .asm , .syscall)) {
        return(if(n == 0, => .Kw, => .Kx));
    };

    if (@is(o, .call_indirect)) {
        return(if(n == 0, => .Kw, => .Ke));
    };
    if (@is(o, .global_set)) {
        return(if(n == 0, => .Kl, => k));
    };
    
    names :: Qbe.O.get_enum_names();
    @panic("invalid op number for argcls %", names[o.raw().zext()])
}

///////////////////////////////////////////////////////////////////////////////

fn pack_op_cls(op: Qbe.O, k: Qbe.Cls) u32 #inline = {
    (@as(i64) k.raw().zext()).shift_left(30).bit_or(op.raw().zext()).trunc()
}

// TODO: ugh. garbage setter, i need to add real bit fields to my language!
fn set_op(i: *Qbe.Ins, op: Qbe.O) void #inline = {
    i.op30_cls2 = pack_op_cls(op, i.cls());
}

fn set_cls(i: *Qbe.Ins, k: Qbe.Cls) void #inline = {
    i.op30_cls2 = pack_op_cls(i.op(), k);
}

fn set_nop(i: *Qbe.Ins) void #inline = {
    i[] = Qbe.Ins.zeroed();
    i.set_op(.nop);
}

fn op(i: *Qbe.Ins) Qbe.O #inline = {
    @as(Qbe.O) @as(i32) bitcast(@as(u32) i.op30_cls2.bit_and(1.shift_left(30) - 1))
}

fn cls(i: *Qbe.Ins) Qbe.Cls #inline = {
    x := i.op30_cls2.shift_right_logical(30).bit_and(0b11);
    @as(Qbe.Cls) @as(i32) x.bitcast()
}

// 0 for int, 1 for float. 
fn KBASE(k: Qbe.Cls) i64 #inline = {
    x: i64 = k.raw().shift_right_logical(1).zext();
    @debug_assert(x.bit_and(1) == x, "bad KBASE. Kx=% Ke=%", k == .Kx, k == .Ke);
    x
}

// TODO: compiler bug. this shouldn't need to be in a block. this just delays adding it to the overload set so it doesn't get confused.
:: {
    fn eq(a: Qbe.Ref, b: Qbe.Ref) bool #inline = 
        a.type3_val29 == b.type3_val29;
        
    fn ne(a: Qbe.Ref, b: Qbe.Ref) bool #inline = 
        a.type3_val29 != b.type3_val29;
};

fn rtype(r: Qbe.Ref) Qbe.RegKind #inline = {
    r != QbeNull || return(.RNull);
    @as(Qbe.RegKind) @as(u32) r.type3_val29.bit_and(1.shift_left(3) - 1)
}

fn get_constant(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Con #inline = {
    @debug_assert(rtype(r) == .RCon, "tried to get constant of non constant");
    i := r.val();
    //@safety(.Bounds) i < f.ncon.zext();
    @debug_assert(i < f.ncon.zext(), "con oob");
    f.con.index(i)
}

fn get_memory(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Addr #inline = {
    @debug_assert(rtype(r) == .RMem, "tried to get memory of non memory");
    i := r.val();
    //@safety(.Bounds) i < f.nmem.zext();
    @debug_assert(i < f.nmem.zext(), "mem oob");
    f.mem.index(i)
}

fn get_temporary(f: *Qbe.Fn, i: i64) *Qbe.Tmp #inline = {
    //@safety(.Bounds) i < f.ntmp.zext();
    @debug_assert(i < f.ntmp.zext(), "tmp oob");
    f.tmp.index(i)
}

fn get_temporary(f: *Qbe.Fn, r: Qbe.Ref) *Qbe.Tmp #inline = {
    @debug_assert(rtype(r) == .RTmp, "tried to get tmp of non-tmp");
    i := r.val();
    @safety(.Bounds) i < f.ntmp.zext();
    f.tmp.index(i)
}

fn trunc(x: u32) i16 #redirect(u32, u16); // TODO
fn shift_right_logical(v: i16, shift_amount: i64) i16 = v.int().shift_right_logical(shift_amount).intcast().trunc();

fn ref(cls: Qbe.RegKind, id: i64) Qbe.Ref #inline = {
    (type3_val29 = cls.raw().bitcast().bit_or(id.shift_left(3).trunc()))
}

fn val(r: Qbe.Ref) i64 #inline = {
   x := @as(u32) r.type3_val29.shift_right_logical(3).bit_and(1.shift_left(29) - 1);
   x.zext()
}

fn rsval(r: Qbe.Ref) i32 #inline = {    
   intcast(r.val().bit_xor(0x10000000) - 0x10000000)
}

// TODO: can we use this instead of making a constant for small integer literals? 
//       qbe only uses this for blit but CBits for all constants.
fn small_int_for_blit(x: i64) Qbe.Ref #inline = {
    ref(.RInt, x.bit_and(0x1fffffff))
}

fn set_switch_targets(b: *Qbe.Blk, targets: []*Qbe.Blk) void = {
    b.s1 = ptr_cast_unchecked(*Qbe.Blk, Qbe.Blk, targets.ptr);
    b.s2 = ptr_cast_unchecked(*Qbe.Blk, Qbe.Blk, targets.index_unchecked(targets.len));
}

fn get_switch_targets(b: *Qbe.Blk) ?[]*Qbe.Blk #inline = {
    if b.jmp.type != .switch {
        return(.None);
    };
    s1 := ptr_cast_unchecked(Qbe.Blk, *Qbe.Blk, b.s1);
    s2 := ptr_cast_unchecked(Qbe.Blk, *Qbe.Blk, b.s2);
    (Some = (ptr = s1, len = ptr_diff(s1, s2)))
}

fn get_jump_targets(b: *Qbe.Blk) []*Qbe.Blk = {
    // TODO: boot can't do this because it didn't have require_layout_ready? :UpdateBoot
    //:: @assert_eq(offset_of(Qbe.Blk, Fields(Qbe.Blk).s1) + size_of(*Qbe.Blk), offset_of(Qbe.Blk, Fields(Qbe.Blk).s2), "field order matters for get_jump_targets SAFETY");
    if(get_switch_targets(b), fn(s) => return(s));
    (ptr = b.s1&, len = int(!b.s1.is_null()) + int(!b.s2.is_null()))
}

// these are carefully arranged to be packed in ir/Qbe.O,
// so ie. `@is(j, .alloc4, .alloc8, .alloc16)` is the same as manually writing `j.raw() >= Qbe.O.alloc4.raw() && j.raw() <= Qbe.O.alloc16.raw()`,
// I just think its easier to read when i can see all the values spelled out. 

fn is_ret(j: Qbe.J) bool =  // TODO: tried to call uncompiled if #inline :FUCKED
    j.raw() >= Qbe.J.retw.raw() && j.raw() <= Qbe.J.ret0.raw();

fn is_alloc(j: Qbe.O) bool #inline = 
    @is(j, .alloc4, .alloc8, .alloc16);

fn is_ext(j: Qbe.O) bool #inline = 
    @is(j, .extsb, .extub, .extsh, .extuh, .extsw, .extuw);

fn is_store(j: Qbe.O) bool #inline = 
    @is(j, .storeb, .storeh, .storew, .storel, .stores, .stored);
    
fn is_load(j: Qbe.O) bool #inline = 
    @is(j, .loadsb, .loadub, .loadsh, .loaduh, .loadsw, .loaduw, .load);

fn is_flag(j: Qbe.O) bool #inline = 
    j.raw() >= Qbe.O.flagieq.raw() && j.raw() <= Qbe.O.flagfuo.raw();

fn is_parbh(j: Qbe.O) bool #inline = 
    @is(j, .parsb, .parub, .parsh, .paruh);

fn is_par(j: Qbe.O) bool #inline = 
    j.raw() >= Qbe.O.par.raw() && j.raw() <= Qbe.O.pare.raw();

fn is_arg(j: Qbe.O) bool #inline = 
    j.raw() >= Qbe.O.arg.raw() && j.raw() <= Qbe.O.argv.raw();

fn is_retbh(j: Qbe.J) bool #inline = 
    j.raw() >= Qbe.J.retsb.raw() && j.raw() <= Qbe.J.retuh.raw();
    
fn is_argbh(j: Qbe.O) bool #inline = 
    @is(j, .argsb, .argub, .argsh, .arguh);

// TODO: is it a mistake when amd64/isel calls this with .Kx? nowhere else does that. 
fn is_int(k: Qbe.Cls) bool #inline =
    k != .Kx && KBASE(k) == 0;

fn is_wide(k: Qbe.Cls) bool #inline =
    (@as(i64) k.raw().zext()).bit_and(1) != 0;
   
FIXED_CONST_COUNT :: 2;

// TODO: init array from tuple. and also real bit fields. so this doesn't suck as bad. 
fn make_ins(op: Qbe.O, k: Qbe.Cls, out: Qbe.Ref, in1: Qbe.Ref, in2: Qbe.Ref) Qbe.Ins #inline = 
    (op30_cls2 = pack_op_cls(op, k), arg = init(@slice(in1, in2)), to = out);
 
// so you can @assert_eq
fn display(self: Qbe.Ref, out: *List(u8)) void = {
    out.push_all("TODO:Qbe.Ref");
}

///////////////////////////////////////////////////////////////////////////////

::enum_basic(Qbe.ConType);

::enum_basic(Qbe.Cls);
::enum_basic(Qbe.J);
::enum_basic(Qbe.RegKind);
::if(Qbe.Cls);
::if(Qbe.O);
::enum_basic(Qbe.O);
::ptr_utils(Qbe.Blk);

::if(i32);


fn ge(a: Qbe.Cls, b: Qbe.Cls) bool = a.raw().int() >= b.raw().int();
fn gt(a: Qbe.Cls, b: Qbe.Cls) bool = a.raw().int() > b.raw().int();
