//! Unlike Qbe, we don't rely on a seperate assembler. We just put the bytes in memory and jump there. 
//! This means we also have to handle updating forward references to functions that haven't been compiled yet. 

// TODO: don't setup stack frame for leaf like amd64?
// TODO: make sure i have .ssa tests for every instruction. rn there are some that just fail the franca tests. 
//       addr slot >4095, sltof, loadaddr with offset, 
// TODO: a function you can call to emit forwarding shims for uncompiled functions. 
//       so it in emit with the arch independent ir. i guess that means the front end could just do it. 
// TODO: (when i want to do my own aot) have the addr lookup table in a seperate segment before the functions and then don't include it in the aot binrary, just patch instead. 
// TODO: take out the internal jump distance checks because we check at the end that the function fits in the segment (jsut also check that its within i19 because why not)
// TODO: use ldp/stp for callee saved registers

// TODO: need to replace all the .goal checks when we want to support ELF

ArmState :: @struct(
    m: *QbeModule,
    f: *Qbe.Fn,
    frame: i64,
    padding: i64,
    start_of_function: i64,
    buf: List(u8),
);

fn emit_func_arm64(m: *QbeModule, f: *Qbe.Fn, text: *FILE) void = {
    @debug_assert_eq(m.goal.arch, .aarch64, "how did we get here?");
    // :TempAsmText :SLOW
    name: CStr = (ptr = f.name&.as_ptr());
    if m.want_text_asm {
        s := items(@format(".text\n.balign 4\n.globl _%\n_%:\n", name, name) temp());
        write(text, s);
    };
    
    // We know we're emitting the function right here so we can do old fixups now. 
    name_id := m.intern(@as(CStr) (ptr = f.name&.as_ptr()));
    
    m.align_to(.Code, 4);  // should never be unaligned anyway but we live in fear
    start_offset := m.current_offset(.Code);
    self: ArmState = (m = m, f = f, frame = 0, padding = 0, start_of_function = start_offset, buf = list(temp()));
    
    maybe_add_export(m, name_id, f.lnk.export);
    m.do_jit_fixups(name_id, .Code, start_offset);
    
    self&.emit_the_code();
    write(text, self.buf.items());
    total_code_so_far := start_offset + self&.next_inst_offset();
    code := m.segments&[.Code]&;
    @assert(total_code_so_far < code.mmapped.len, "too much code (% bytes)", total_code_so_far);
    code_bytes := code.mmapped.slice(start_offset, total_code_so_far);
    if f.globals.debug["D".char()] {
        write(self.f.globals.debug_out, items(@format("# disassembly of '%' (% bytes)\n", name, code_bytes.len) temp()));
        asm := code_bytes;
        hex: List(u8) = list(asm.len * 5, temp());
        for asm { byte |
            hex&.push_prefixed_hex_byte(byte);
            hex&.push_all(" ");
        };
        arch := "--arch=aarch64";
        out := open_temp_file();
        out.fd&.write(hex.items());
        // TODO: have the caller pass in anything like this, 
        //       but this is just for debugging the compiler so its not a big deal. :driver_io
        // varient 1 means intel syntax. ignored when doing arm. 
        success := run_cmd_blocking("llvm-mc", @slice(arch, "--disassemble", "-output-asm-variant=1", "--show-encoding", out&.s_name()));
        if(!success, => eprintln("ICE(debugging): couldn't disassemble"));
        out.remove();
    };
}

fn segment_address(self: *QbeModule, segment: SegmentType, offset_in_segment: i64) *u8 = {
    data := self.segments&[segment]&;
    data.mmapped.ptr.offset(offset_in_segment)
}

// TODO: when doing aot we just have to ask for our segments to have the same spacing as they did now and then relative addressing will just work out. 
//       but for data constants with pointers we'll need to remember what relocations to ask the loader to do at runtime. 
//       would be great if my language had nice relative pointers and then you wouldn't have to deal with that as much
//
fn do_jit_fixups(self: *QbeModule, id: u32, segment: SegmentType, final_offset: i64) void = {
    @assert(final_offset >= 0 && final_offset < self.segments&[segment].mmapped.len, "thats not in the segment");
    symbol := self.get_symbol_info(id);
    @assert(symbol.kind != .Local, "Redeclared symbol %", symbol.name);
    symbol.segment = segment;
    symbol.offset = final_offset;
    symbol.kind = .Local;
    dest := self.segment_address(symbol.segment, symbol.offset);
    fixups := symbol.fixups&;
    self.do_fixups(dest, symbol);
    if self.goal.type == .Exe { //  || symbol.segment == .Code // :canireallynothavecodeberelative
        // Since it's a local symbol, we don't care about the relocations anymore. The linker never needs to know. 
        // TODO: actually thats also true for imports becuase calls go through __got. 
        unordered_retain(fixups, fn(it) => it.type&.is(.DataAbsolute));
        if(fixups.len == 0, => fixups.drop(self.gpa));
    } else {
        // you still care because the static linker wants to move things around. :track_got_reloc
        if !symbol.local_but_marked_for_relocation {
            self.local_needs_reloc&.push(id);
            symbol.local_but_marked_for_relocation = true;
        };
    }; 
}
::tagged(FixupType);

fn do_fixups(self: *QbeModule, address: *u8, symbol: *SymbolInfo) void = {
    fixups := symbol.fixups.items();
    local := symbol.kind == .Local;
    assert(!self.fixups_locked, "you cannot add more fixups after emitting for aot");
    new_got_reloc: ?Fixup = .None;
    each fixups { fixup | 
        @match(fixup.type) {
            fn Call() => {
                if self.debug["T".char()] {
                    @fmt_write(self.debug_out, "# patch call % calls %\n", u8.int_from_ptr(fixup.patch_at), symbol.name);
                };
                distance := ptr_diff(fixup.patch_at, address) / 4;
                in_range := distance.abs() < 1.shift_left(26);
                assert(!local || in_range, "local symbol too far away");
                if !local {
                    // For AOT we'll tell the loader to put the right offset in the __got for us. 
                    // For jitting we could reference directly but this indirection makes it consistant. 
                    // bl from the caller to the trampoline, then the trampoline loads from __got and calls that. 
                    // means you always reserve one instruction for a call before knowing if its local. 
                    // > my understanding is that in the old days you could have a shim 
                    // > that checked the return address and patched it to not need the extra indirection after the first call, 
                    // > but i we care too much about security to do that any more. 
                    if symbol.offset == -1 {
                        // create a new trampoline in the __stubs section. 
                        code := self.segments&.index(.Code);
                        symbol.offset = self.stubs&.len() + COMMANDS_SIZE_GUESS;
                        trampoline := self.stubs&.reserve(Array(u32, 3));
                        reg := Arm64Reg.IP0.raw() - 1; // x16
                        new_got_reloc = load_from_got(self, symbol, reg, trampoline.items().slice(0, 2), address).or_else(=> new_got_reloc);
                        trampoline[2] = br(reg, 0);
                    };
                    stub_trampoline := self.segment_address(.Code, symbol.offset);
                    distance = ptr_diff(fixup.patch_at, stub_trampoline) / 4;
                };
                assert(distance.abs() < 1.shift_left(26), "can't even jump to trampoline, thats bad.");
                inst := ptr_cast_unchecked(u8, u32, fixup.patch_at);
                inst[] = b(s_trunc(distance, 26), 1);
            }
            fn InReg(it) => {
                if self.debug["T".char()] {
                    @fmt_write(self.debug_out, "# patch addr % refs %\n", u8.int_from_ptr(fixup.patch_at), u8.int_from_ptr(address));
                };
                assert(it.increment == 0, "TODO: offset symbols");
                inst: []u32 = (ptr = ptr_cast_unchecked(u8, u32, fixup.patch_at), len = 2);
                if local {
                    i_page, i_off := encode_adrp_add(address, fixup.patch_at, @as(u5) it.r);
                    inst[0] = i_page;
                    inst[1] = i_off;
                } else {
                    // TODO: untested
                    new_got_reloc = load_from_got(self, symbol, it.r, inst, address).or_else(=> new_got_reloc);
                };
            }
            fn DataAbsolute() => {
                reloc := ptr_cast_unchecked(u8, *u8, fixup.patch_at);
                reloc[] = address;
            }
        }
    };
    if new_got_reloc { it | // avoided reallocating while iterating. rustc would be proud.
        symbol.fixups&.push(it, self.gpa);
    };
}

// do not push to symbol.fixups here!
fn load_from_got(self: *QbeModule, symbol: *SymbolInfo, reg_encoding: i64, insts: []u32, jit_address: *u8) ?Fixup = {
    @debug_assert(!self.fixups_locked);
    assert(!self.fixups_locked, "got_lookup_offset is stomped by emitting for aot");
    new_got_reloc: ?Fixup = .None;
    if symbol.got_lookup_offset == -1 {
        // need to reserve a new __got entry.  :GotAtConstantDataBase 
        symbol.got_lookup_offset = self.got&.len();
        got := self.got&.reserve(*u8);  // :TodoErrorMessage
        if self.debug["T".char()] {
            @fmt_write(self.debug_out, "# reserve __got[%] = %\n", symbol.got_lookup_offset / 8, symbol.name);
        };
        new_got_reloc = (Some = (patch_at = ptr_cast_unchecked(*u8, u8, got), type = .DataAbsolute));
        got[] = jit_address; // because we don't loop again to handle `new_got_reloc`
    };
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    @debug_assert_eq(ptr_cast_unchecked(u8, i64, got)[], u8.int_from_ptr(jit_address));
    
    i_page, i_off := encode_adrp_load(got, ptr_cast_unchecked(u32, u8, insts.ptr), @as(u5) reg_encoding);
    insts[0] = i_page;
    insts[1] = i_off; 
    new_got_reloc
}

// TODO: is this better than brk? 
tried_to_call_uncompiled :: @as(rawptr) fn() void = {
    @panic("TODO: backend jitted code tried to call uncompiled function");
};

fn emit_the_code(self: *ArmState) void #once = {
    // TODO: what is this? is it the control flow integrety shit? 
    // ("\thint\t#34\n");
    self.inst(0xd503245f);
    
    self.framelayout();
    
    if self.f.vararg && !self.f.globals.target.apple {
        panic("TODO: non-apple arm varargs.");
        //for (n=7; n>=0; n--)
        //    fprintf(e->f, "\tstr\tq%d, [sp, -16]!\n", n);
        //for (n=7; n>=0; n-=2)
        //    fprintf(e->f, "\tstp\tx%d, x%d, [sp, -16]!\n", n-1, n);
    };
    self.prelude_grow_stack();
    self.inst(add_im(Bits.X64, fp, sp, 0, 0)); // Note: normal mov encoding can't use sp :SpEncoding
    
    offset_to_callee_saved := (self.frame - self.padding) / 4;
    for arm64_rclob { r |
        if self.f.reg.bit_and(BIT(r.raw())) != 0 {
            offset_to_callee_saved -= 2;
            op := if(r.is_float(), => Qbe.O.stored, => Qbe.O.storel);
            i := make_ins(op, .Kw, QbeNull, TMP(r.raw()), SLOT(offset_to_callee_saved));
            self.emitins(i&);
        };
    };
    
    // TODO: make sure b.id is sequential
    local_labels := temp().alloc_zeroed(i64, self.f.nblk.zext());
    Patch :: @struct(offset_from_start: i64, cond: ?Cond, target_bid: i64, text_offset: i64);
    patches: List(Patch) = list(temp());
    
    branch :: b;
    // :LinkIsNowRpo TODO: why are we iterating to set links instead of just iterating the rpo array. (don't forget to update the comparisons to .link below). 
    for_blocks self.f { b |
        @debug_assert(local_labels[b.id.zext()] == 0, "block ids must be unique");
        local_labels[b.id.zext()] = self.next_inst_offset();
        for_insts_forward b { i |
            self.emitins(i);
        };
        @match(b.jmp.type) {
            fn Jhlt() => self.inst(brk(1000));
            fn Jret0() => {
                // if you run the simplify_jump_chains pass, you'll only need to emit this epilogue once. 
                offset_to_callee_saved = (self.frame - self.padding) / 4;
                for arm64_rclob { r |
                    if self.f.reg.bit_and(BIT(r.raw())) != 0 {
                        offset_to_callee_saved -= 2;
                        cls := if(r.is_float(), => Qbe.Cls.Kd, => Qbe.Cls.Kl);
                        i := make_ins(.load, cls, TMP(r.raw()), SLOT(offset_to_callee_saved), QbeNull);
                        self.emitins(i&);
                    };
                };
                if self.f.dynalloc {
                    // someone did an alloca so we don't statically know the size of our stack frame. 
                    self.inst(add_im(Bits.X64, sp, fp, 0, 0)); // :SpEncoding
                };
                self.epilogue_shrink_stack();
                self.inst(ret());
            }
            fn Jjmp() => {
                // :GotoJmp
                if !b.s1.identical(b.link) {
                    target := local_labels[b.s1.id.zext()];
                    here := self.next_inst_offset();
                    if target != 0 {
                        distance := (target - here) / 4;
                        assert(distance.abs() < 1.shift_left(26), "can't jump that far");
                        self.inst(branch(s_trunc(distance, 26), 0));
                    } else {
                        // we haven't made this block yet.
                        patches&.push(offset_from_start = here, cond = .None, target_bid = b.s1.id.zext(), text_offset = self.buf.len);
                        self.inst(brk(0xabba));
                        @if(self.m.want_text_asm) @fmt(self.buf&, "             "); // :TempAsmText HACK
                    };
                }; // else we can just fall through becuase we're about to emit the target block
            }
            @default => {
                Jjf :: Qbe.J.Jjfieq; // TODO: use this everywhere
                c: i32 = zext(b.jmp.type.raw() - Jjf.raw());
                if c < 0 || c > Qbe.NCmp {
                    panic("unhandled jump") // TODO: " %d", b.jmp.type);
                };
                
                if b.link.identical(b.s2) {
                    t := b.s1;
                    b.s1 = b.s2;
                    b.s2 = t;
                } else {
                    // we jump to s2 when condition c is true, so flip the condition. 
                    c = cmpneg(c);
                };
                cond := arm_condition_codes[c.zext()];
                here := self.next_inst_offset();
                target := local_labels[b.s2.id.zext()]; // TODO: wrong index if you forget .zext() but it still typechecks. :FUCKED
                if target != 0 {
                    distance := (target - here) / 4;
                    assert(distance.abs() < 1.shift_left(19), "can't jump that far");
                    // TODO: bounds check
                    self.inst(b_cond(s_trunc(distance, 19), cond));
                } else {
                    // we haven't made this block yet.
                    patches&.push(offset_from_start = here, cond = (Some = cond), target_bid = b.s2.id.zext(), text_offset = self.buf.len);
                    self.inst(brk(0xabba)); // so the offsets for the below work out
                    @if(self.m.want_text_asm) @fmt(self.buf&, "             "); // :TempAsmText HACK
                };
                // :GotoJmp
                if !b.s1.identical(b.link) {
                    target := local_labels[b.s1.id.zext()];
                    here := self.next_inst_offset();
                    if target != 0 {
                        distance := (target - here) / 4 - 1;
                        @assert(distance.abs() < 1.shift_left(28) - 1, "how is your function so big");
                        self.inst(branch(s_trunc(distance, 26), 0));
                    } else {
                        // we haven't made this block yet.
                        patches&.push(offset_from_start = here, cond = .None, target_bid = b.s1.id.zext(), text_offset = self.buf.len);
                        self.inst(brk(0xabba));
                        @if(self.m.want_text_asm) @fmt(self.buf&, "             "); // :TempAsmText HACK
                    };
                }; // else we can just fall through becuase we're about to emit the target block
            };
        };
    };
    each patches& { p |
        target := local_labels[p.target_bid];
        @debug_assert(target != 0, "we should have emitted the block");
        distance := (target - p.offset_from_start) / 4;
        @debug_assert(distance > 0, "dont need to patch backward jumps");
        code := self.m.segments&[.Code]&;
        patch := code.mmapped.ptr.offset(self.start_of_function + p.offset_from_start);
        patch := ptr_cast_unchecked(u8, u32, patch);
        @debug_assert_eq(brk(0xabba), patch[], "not expecting patch");
        if distance == 0 {
            @debug_assert(false, "this shouldn't happen because we'd know to fall through without waiting for a patch");
            patch[] = arm_nop;
        } else {
            if p.cond { cc | 
                assert(distance.abs() < 1.shift_left(19), "cant jump that far");
                patch[] = b_cond(s_trunc(distance, 19), cc);
            } else {
                assert(distance.abs() < 1.shift_left(26), "cant jump that far");
                patch[] = branch(s_trunc(distance, 26), 0);
            };
        };
        @if(self.m.want_text_asm) {
            // :TempAsmText HACK
            patch_buf: List(u8) = (maybe_uninit = self.buf.items().slice(p.text_offset, p.text_offset + 25), len = 0, gpa = panicking_allocator);
            @fmt(patch_buf&, "\t.word %;     \n", patch[]);
        } 
    };
}

fn framelayout(self: *ArmState) void = {
    o := 0;
    for arm64_rclob { r |
        o += 1.bit_and(self.f.reg.bitcast().shift_right_logical(r.raw()));
    };
    f := self.f.slot.intcast();
    f = (f + 3).bit_and(-4);
    o += o.bit_and(1);
    self.padding = 4*(f - self.f.slot.intcast());
    self.frame = 4*f + 8*o;
}

// has to deal with limited size of immediates. stp encodes them as imm/8 in an i7
fn prelude_grow_stack(self: *ArmState) void = {
    if self.frame + 16 <= 512 {
        encoded := s_trunc(-(self.frame + 16) / 8, 7);
        self.inst(stp_pre(Bits.X64, fp, lr, sp, encoded)); 
        return();
    };
    negative_16 := s_trunc(-2, 7);
    if self.frame <= 4095 {
        self.inst(sub_im(Bits.X64, sp, sp, @as(u12) self.frame, 0));
        self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
        return();
    };
    if self.frame <= 65535 {
        self.inst(movz(Bits.X64, x16, self.frame.trunc(), .Left0));
        self.inst(sub_er(Bits.X64, sp, sp, x16));
        self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
        return();
    };
    
    top := self.frame.shift_right_logical(16);
    bottom := self.frame.bit_and(0xFFFF);    
    self.inst(movz(Bits.X64, x16, bottom.trunc(), .Left0));
    self.inst(movk(Bits.X64, x16, top.trunc(), .Left16));
    self.inst(sub_er(Bits.X64, sp, sp, x16));
    self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
}

// has to deal with limited size of immediates. 
fn epilogue_shrink_stack(self: *ArmState) void = {
    o := self.frame + 16;
    if self.f.vararg && !self.f.globals.target.apple {
        o += 192;
    };
    // TODO: why isn't this also 512?
    if o <= 504 { 
        encoded_offset := o / 8; 
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) encoded_offset)); 
        return();
    };
    if o - 16 <= 4095 {
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
        self.inst(add_im(Bits.X64, sp, sp, @as(u12) @as(i64) o - 16, 0));
        return();
    };
    if o - 16 <= 65535 {
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
        self.inst(movz(Bits.X64, x16, o.trunc() - 16, .Left0));
        self.inst(add_er(Bits.X64, sp, sp, x16));
        return();
    };
    
    top: u16 = (o - 16).bit_and(0xFFFF).trunc();
    bottom: u16 = (o - 16).shift_right_logical(16).trunc();
    self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
    self.inst(movz(Bits.X64, x16, bottom, .Left0));
    self.inst(movk(Bits.X64, x16, top, .Left16));
    self.inst(add_er(Bits.X64, sp, sp, x16));
}

// can remove this if i decide to fully give up emitting text. 
fn rname(r: i64, k: Qbe.Cls) Str = {
    @debug_assert(k == .Kl, "I only use this for loading symbol addresses. float/32 bit doesn't make sense. %", @as(i64) k.raw().zext());
    @debug_assert(r != Arm64Reg.SP.raw(), "stomp sp?");
    items(@format("x%", r - Arm64Reg.R0.raw()) temp())
}

fn slot(self: *ArmState, r: Qbe.Ref) i64 = {
    s := rsval(r).intcast();
    if s == -1 {
        return(16 + self.frame);
    };
    if s < 0 {
        if self.f.vararg && !self.f.globals.target.apple {
            return(16 + self.frame + 192 - (s+2));
        };
        return(16 + self.frame - (s+2));
    };
    16 + self.padding + 4 * s
}

// This will get more complicated when i can have multiple of the same segment. 
fn distance_between(m: *QbeModule, start_segment: SegmentType, start_offset: i64, end_segment: SegmentType, end_offset: i64) i64 = {
    ptr_diff(m.segment_address(start_segment, start_offset), m.segment_address(end_segment, end_offset))
}

fn current_offset(m: *QbeModule, segment: SegmentType) i64 = {
    ptr_diff(m.segments&[segment].mmapped.ptr, m.segments&[segment].next)
}

fn align_to(m: *QbeModule, segment: SegmentType, align: i64) void = {
    m.segments&.index(segment).align_to(align);
}

// TODO: rename this. 
fn loadaddr_bits(self: *ArmState, c: *Qbe.Con, dest_reg: i64, thread_local: bool) void = {
    symbol := self.m.get_symbol_info(c.sym.id);
    patch_addr := self.m.segment_address(.Code, self.m.current_offset(.Code));
    
    patch: Fixup = (patch_at = patch_addr, type = (InReg = (r = dest_reg, increment = 0)));
    // i think i don't need a different relocation type for thread locals? 
    // TODO: but makes sture that's still true for relocatable objects.
    @match(symbol.kind) {
        fn DynamicPatched() => panic("TODO: handle predeclared import");
        fn Local() => {
            // :canireallynothavecodeberelative
            // TODO: is there some way to have it not move around my functions so i can use relative calls within this one segment?
            if self.m.goal.type == .Relocatable  { // && symbol.segment != .Code 
                // we're creating input for a static linker and we want it to be able to move shit around. 
                symbol.fixups&.push(patch, self.m.gpa);
            };
            
            // we've already compiled it so just compute its address directly.
            distance := self.m.distance_between(.Code, self.m.current_offset(.Code), symbol.segment, symbol.offset);
            if self.f.globals.debug["T".char()] {
                write(self.m.debug_out, items(@format("# direct offset % for '%' %[%]\n", distance, self.m.str(c.sym.id), symbol.segment, symbol.offset) temp()));
            };
            xxx := symbol.segment != .Code;
            @debug_assert(xxx || distance < 0, "we must be jumping backwards");
            //i_page, i_off := @if(thread_local, 
            //    encode_adrp_load(self.m.segment_address(symbol.segment, symbol.offset), patch_addr, @as(u5) dest_reg), 
            //    encode_adrp_add(self.m.segment_address(symbol.segment, symbol.offset), patch_addr, @as(u5) dest_reg), 
            //);
            i_page, i_off := encode_adrp_add(self.m.segment_address(symbol.segment, symbol.offset), patch_addr, @as(u5) dest_reg);
            self.inst_bytes_only(i_page);
            self.inst_bytes_only(i_off);
        }
        fn Pending() => {
            symbol.fixups&.push(patch, self.m.gpa);
            self.inst_bytes_only(brk(0x0123));
            self.inst_bytes_only(brk(0x0456));
        }
    };
}

fn loadaddr(self: *ArmState, c: *Qbe.Con, dest: i64, reg_name: Str, do_text: bool) void = {
    symbol_name := self.m.str(c.sym.id);

    //p = l[0] == '"' ? "" : T.assym;
    
    self.loadaddr_bits(c, dest, c.sym.type == .SThr);
    apple := self.f.globals.target.apple;
    if do_text && self.m.want_text_asm {
        @match(c.sym.type) {
            fn SGlo() => {
                // :TempAsmText
                // when outputting text we just let the linker deal with it. 
                if apple {
                    @fmt(self.buf&, "\tadrp\t%, _%@page\n\tadd\t%, %, _%@pageoff\n", reg_name, symbol_name, reg_name, reg_name, symbol_name);
                } else {
                    @fmt(self.buf&, "\tadrp\t%, %\n\tadd\t%, %, #:lo12:%\n", reg_name, symbol_name, reg_name, reg_name, symbol_name);
                };
            }
            fn SThr() => {
                // :TempAsmText
                if apple {
                    @fmt(self.buf&, "\tadrp\t%, _%@tlvppage\n\tldr\t%, [%, _%@tlvppageoff]\n", reg_name, symbol_name, reg_name, reg_name, symbol_name);
                } else {
                    assert(do_text && self.m.want_text_asm, "TODO: non-apple thread locals");
                    self.inst_bytes_only(brk(1111)); // TODO: patch
                    @fmt(self.buf&, "\tmrs\t%, tpidr_el0\n\tadd\t%, %, #:tprel_hi12:%, lsl #12\n\tadd\t%, %, #:tprel_lo12_nc:%\n", reg_name, reg_name, reg_name, symbol_name, reg_name, reg_name, symbol_name);
                };
            };
        };
    };
    if c.bits.i != 0 {
        // TODO: you can do this as part of the thing above. 
        @assert(c.bits.i < 4096 && c.bits.i > 0, "TODO: loadaddr of % with offset %", symbol_name, c.bits.i);
        self.inst(add_im(Bits.X64, @as(u5) dest, @as(u5) dest, @as(u12) c.bits.i, 0));
    };
}

fn loadcon(self: *ArmState, c: *Qbe.Con, r: i64, k: Qbe.Cls, do_text: bool) void = {
    w := is_wide(k);
    n := c.bits.i;
    if (c.type == .CAddr) {
        @debug_assert(w, "can't have 32 bit addr: %", self.m.str(c.sym.id));
        rn := rname(r + 1, k); // HACK
        self.loadaddr(c, r, rn, do_text);
        return();
    };
    @debug_assert(c.type == .CBits, "bad constant");
    s := Bits.X64;
    if !w {
        s = .W32;
        // TODO: is this the same?
        //n = (int32_t)n;
        n = n.bit_and(1.shift_left(32) - 1);
    };
    
    if n.bit_and(0xffff) == n { 
        self.inst(movz(s, r, n.trunc(), .Left0));
        return();
    };
    // TODO: some have better encoding
    //if n.bit_or(0xffff) == -1 || arm64_logimm(n, k) {
    //    fprintf(e.f, "\tmov\t%s, #%\n", rn, n);
    //} else {
        self.inst(movz(s, r, n.bit_and(0xffff).trunc(), .Left0));
        shift_encode := 1;
        n = n.shift_right_logical(16);
        while => n != 0 {
            part: u16 = n.bit_and(0xffff).trunc();
            if part != 0 {
                self.inst(movk(s, @as(u5) r, part, @as(Hw) shift_encode));
            };
            shift_encode += 1;
            n = n.shift_right_logical(16);
        };
    //};
}

fn fixarg(self: *ArmState, pr: *Qbe.Ref, sz: i64) void = {
    r := pr[];
    if (rtype(r) == .RSlot) {
        s := self.slot(r);
        // you're actually allowed unaligned accesses, you just can't encode them in the immediate. 
        // qbe's stack slot allocating doesn't care about ensuring 8 byte slots are aligned,
        // because it never folds offsets into the immediate space (ie use RSlot in .load directly)
        // except for those inserted by reload_spilled. 
        // maybe a better fix for this would be adding a padding slot between the 4 byte and 8 byte slot groups in isel. 
        // but you hit the bad case exactly twice when self compiling so meh -- Oct 31. 
        unaligned := s.mod(sz) != 0;  
        out_of_range := s > sz * 4095;  // TODO: is this the same: s > sz * 4095
        if out_of_range || unaligned {
            i := make_ins(.addr, .Kl, TMP(Arm64Reg.IP0.raw()), r, QbeNull);
            self.emitins(i&);
            pr[] = TMP(Arm64Reg.IP0.raw());
        }
    }
}

// TODO: fix non-i64 fn ordinal
fn get_name(o: Qbe.O) Str = {
    names :: Qbe.O.get_enum_names();
    idx := @as(i64) intcast(@as(i32) o.raw());
    names[idx]
}

fn emitins(self: *ArmState, i: *Qbe.Ins) void = {
    ::if(Bits);
    ::if(u32);
    ::if(FType);
    F :: @Fn(s: FType, dest: u5, a: u5, b: u5) u32;
    I :: @Fn(s: Bits, dest: u5, a: u5, b: u5) u32;
    inst_name := i.op().get_name();
    si := if(i.cls().is_wide(), => Bits.X64, => Bits.W32);
    sf := if(i.cls().is_wide(), => FType.D64, => FType.S32);
    
    bin :: fn($if_float: F, $if_int: I) => {
        @assert(rtype(i.arg&[0]) == .RTmp && rtype(i.arg&[1]) == .RTmp, "TODO: args need reg. op=%", inst_name);
        op := if i.cls().is_int() {
            if_int(si, i.to.int_reg(), i.arg&[0].int_reg(), i.arg&[1].int_reg())
        } else {
            if_float(sf, i.to.float_reg(), i.arg&[0].float_reg(), i.arg&[1].float_reg())
        };
        self.inst(op);
    };
    int_bin :: fn($if_int: I) => {
        @assert(rtype(i.arg&[0]) == .RTmp && rtype(i.arg&[1]) == .RTmp, "TODO: args need reg %", inst_name);
        op := if_int(si, i.to.int_reg(), i.arg&[0].int_reg(), i.arg&[1].int_reg());
        self.inst(op);
    };
    int_un :: fn($int: @Fn(s: Bits, dest: u5, src: u5) u32) => {
        @assert(rtype(i.arg&[0]) == .RTmp, "TODO: args need reg %", inst_name);
        op := int(si, i.to.int_reg(), i.arg&[0].int_reg());
        self.inst(op);
    };

    @match(i.op()) {
        fn nop()    => ();
        fn dbgloc() => (); // we don't do debug info yet.
        fn copy() => {
            if (i.to == i.arg&[0], => return()); 
            if (rtype(i.to) == .RSlot) {
                r := i.to;
                if !isreg(i.arg&[0]) {
                    i.to = TMP(Arm64Reg.R18.raw()); // :SketchPlatformRegister
                    self.emitins(i);
                    i.arg&[0] = i.to;
                };
                op := @as(Qbe.O) @as(i32) Qbe.O.storew.raw() + i.cls().raw().zext();
                i[] = make_ins(op, .Kw, i.to, i.arg&[0], r);
                self.emitins(i);
                return();
            };
            @debug_assert(isreg(i.to), "can only copy to a register");
            @match(rtype(i.arg&[0])) {
                fn RCon() => {
                    c := self.f.get_constant(i.arg&[0]);
                    self.loadcon(c, i.to.int_reg(), i.cls(), true);
                }
                fn RSlot() => {
                    i.set_op(.load);
                    self.emitins(i);
                }
                @default => {
                    @debug_assert(i.to.val() != Arm64Reg.R18.raw(), "please do not the platform register. thank you.");
                    if i.cls().is_int() {
                        // 10101010   0001 |0011| 0000 |0011 11101000|
                        //    @bits(sf, 0b0101010, Shift.LSL, 0b0, 0b11111, 0b000000, src, dest);
                        self.inst(mov(si, i.to.int_reg(), i.arg&[0].int_reg())); // they can't refer to SP so its fine
                    } else {
                        self.inst(fmov(sf, i.to.float_reg(), i.arg&[0].float_reg()));  
                    };
                };
            }
        }
        fn addr() => {
            @debug_assert(rtype(i.arg&[0]) == .RSlot, "can only addr of a stack slot");
            reg := i.to.int_reg();
            s := self.slot(i.arg&[0]);
            @debug_assert(s >= 0, "slot must be positive");
            if (s <= 4095) {
                self.inst(add_im(Bits.X64, reg, fp, @as(u12) s, 0));
            } else {
                if (s <= 65535) {
                    self.inst(movz(Bits.X64, reg, s.trunc(), .Left0));
                    self.inst(add_er(Bits.X64, reg, fp, reg));
                } else {
                    panic("TODO: addr");
                    fprintf(e.f,
                        "\tmov\t%s, #%\n",
                        "\tmovk\t%s, #%, lsl #16\n",
                        "\tadd\t%s, x29, %s\n",
                        rn, s.bit_and(0xFFFF), rn, s.shift_right_logical(16), rn, rn
                    );
                }
            };
        }
        fn call() => {
            callee := i.arg&[0];
            if rtype(callee) == .RCon {
                // TODO: to start with, we always call through a register so i can deal with fewer cases. 
                //       for better code gen should bring back use of bl instruction. 
                c := self.f.get_constant(i.arg&[0]);
                if c.type != .CAddr || c.sym.type != .SGlo || c.bits.i != 0 {
                    panic("invalid call argument");
                };
                
                symbol := self.m.get_symbol_info(c.sym.id);
                patch_at := self.m.segments&[.Code].next;
                @match(symbol.kind) {
                    fn DynamicPatched() => panic("TODO: handle predeclared import");
                    fn Local() => {
                        // already compiled it so branch there directly
                        distance := self.m.distance_between(.Code, self.m.current_offset(.Code), symbol.segment, symbol.offset) / 4;
                        assert(distance.abs() < 1.shift_left(26), "can't call that far but I don't let modules be that big yet.");
                        self.inst_bytes_only(b(s_trunc(distance, 26), 1));
                        // :canireallynothavecodeberelative
                        // TODO: I don't have a test that requires this 
                        //       but i think you do because you need the other ones?
                        if self.m.goal.type == .Relocatable { 
                            symbol.fixups&.push((patch_at = patch_at, type = .Call), self.m.gpa);
                        }; 
                    };
                    fn Pending() => {
                        // haven't compiled it yet so will need a patch. 
                        symbol.fixups&.push((patch_at = patch_at, type = .Call), self.m.gpa);
                        self.inst_bytes_only(brk(0x0789));
                        if self.m.debug["T".char()] {
                            @fmt_write(self.m.debug_out, "# pending call % calls %\n", u8.int_from_ptr(patch_at), symbol.name);
                        };
                    }
                };
                if self.m.want_text_asm {
                    // dynamic link things it wont let you adrp to? you can only bl
                    l := self.m.str(c.sym.id);
                    p := self.f.globals.target.assym&.as_ptr();
                    p: CStr = (ptr = p);
                    p := str(p);
                    p := if(l.ptr[] == "\"".ascii(), => "", => p);
                    @fmt(self.buf&, "\tbl %%\n", p, l);
                };
            } else {
                self.inst(br(callee.int_reg(), 1));
            };
        }
        fn salloc() => {
            xxx := i.arg&[0].int_reg();
            self.inst(sub_er(Bits.X64, sp, sp, xxx));
            if i.to != QbeNull {
                self.inst(sub_im(Bits.X64, i.to.int_reg(), sp, 0, 0)); // :SpEncoding
            }
        }
        // TODO: these could kinda be data, the encodings are the same for many of them.
        fn add() => {
            if rtype(i.arg&[1]) == .RCon {
                // isel enforces that it's an int and in range for this encoding. 
                c := self.f.get_constant(i.arg&[1]);
                self.inst(add_im(si, i.to.int_reg(), i.arg&[0].int_reg(), @as(u12) c.bits.i, 0));
                return();
            };
            bin(@as(F) fadd, fn(s, d, a, b) => if(a == sp, => add_er(s, d, a, b), => add_sr(s, d, a, b, .LSL, 0))); // could always use *_er but then the disassembly looks dumb
        }
        fn sub()  => bin(@as(F) fsub, fn(s, d, a, b) => if(a == sp, => sub_er(s, d, a, b), => sub_sr(s, d, a, b, .LSL, 0))); // ^
        fn mul()  => bin(@as(F) fmul, fn(s, d, a, b) => madd(s, d, a, b, xzr));
        fn div()  => bin(@as(F) fdiv, fn(s, d, a, b) => sdiv(s, d, a, b)); // TODO: if you pass it here you force all calls to be inline :FUCKED
        fn udiv() => int_bin(fn(s, d, a, b) => udiv(s, d, a, b));
        fn sar()  => int_bin(@as(I) asrv);
        fn shr()  => int_bin(@as(I) lsrv);
        fn shl()  => int_bin(@as(I) lslv);
        fn or()   => int_bin(fn(s, d, a, b) => orr(s, d, a, b, .LSL, 0));
        fn xor()  => int_bin(fn(s, d, a, b) => eor(s, d, a, b, .LSL, 0));
        fn and()  => int_bin(fn(s, d, a, b) => and_sr(s, d, a, b, .LSL, 0));
        fn rem() => {
            // TODO: qbe uses x18 for this but i feel like you're not allowed to do that. :SketchPlatformRegister
            a, b := (i.arg&[0].int_reg(), i.arg&[1].int_reg());
            self.inst(sdiv(si, x18, a, b));
            self.inst(msub(si, i.to.int_reg(), x18, b, a));
        };
        fn urem() => {
            // TODO: qbe uses x18 for this but i feel like you're not allowed to do that. 
            a, b := (i.arg&[0].int_reg(), i.arg&[1].int_reg());
            self.inst(udiv(si, x18, a, b));
            self.inst(msub(si, i.to.int_reg(), x18, b, a));
        };
        fn extsb()  => int_un(fn(s, d, a) => bmf(s, 0b0, 0b000000, 0b000111, d, a)); // sxtb
        fn extub()  => int_un(fn(s, d, a) => bmf(s, 0b1, 0b000000, 0b000111, d, a)); // uxtb
        fn extsh()  => int_un(fn(s, d, a) => bmf(s, 0b0, 0b000000, 0b001111, d, a)); // sxth
        fn extuh()  => int_un(fn(s, d, a) => bmf(s, 0b1, 0b000000, 0b001111, d, a)); // uxth
        fn extsw()  => int_un(fn(s, d, a) => bmf(s, 0b0, 0b000000, 0b011111, d, a)); // sxtw
        // note: mov w0, w0 is not a nop! it zeros the high bits. 
        fn extuw()  => int_un(fn(s, d, a) => mov(.W32, d, a));
        fn exts()   => self.inst(fcnv(.S32, .D64, i.to.float_reg(), i.arg&[0].float_reg()));
        fn truncd() => self.inst(fcnv(.D64, .S32, i.to.float_reg(), i.arg&[0].float_reg()));
        fn swap() => {
            if i.cls().is_int() {
                // TODO: qbe uses x18 for this but i feel like you're not allowed to do that. :SketchPlatformRegister
                a, b := (i.arg&[0].int_reg(), i.arg&[1].int_reg());
                self.inst(mov(si, x18, a)); 
                self.inst(mov(si, a, b));
                self.inst(mov(si, b, x18));
            } else {
                a, b := (i.arg&[0].float_reg(), i.arg&[1].float_reg());
                // we can't refer to v31 because we waste a slot on 0 so this will never stomp anything // :V31
                self.inst(fmov(sf, 31, a));
                self.inst(fmov(sf, a, b));
                self.inst(fmov(sf, b, 31));
            };
        }
        fn acmp() => {
            a, b := (i.arg&[0], i.arg&[1]);
            @debug_assert(rtype(a) == .RTmp, "cmp fst needs reg");
            if rtype(b) == .RTmp {
                self.inst(cmp(si, a.int_reg(), b.int_reg())); 
            } else {
                c := self.f.get_constant(b);
                @debug_assert(c.type == .CBits, "cmp to addr");
                if c.bits.i.bit_and(1.shift_left(12) - 1) == c.bits.i {
                    self.inst(cmp_im(si, a.int_reg(), @as(u12) c.bits.i, 0));
                } else {
                    encoded := c.bits.i.shift_right_logical(12);
                    //@debug_assert_eq(encoded.bit_and(1.shift_left(12) - 1).shift_left(12), c.bits.i, "bad cmp const");
                    self.inst(cmp_im(si, a.int_reg(), @as(u12) encoded, 1));
                };
            }
        }
        // TODO: encoding for this is just one bit different from ^
        fn acmn() => {
            a, b := (i.arg&[0], i.arg&[1]);
            @debug_assert(rtype(a) == .RTmp, "cmp fst needs reg");
            if rtype(b) == .RTmp {
                self.inst(cmn(si, a.int_reg(), b.int_reg())); 
            } else {
                c := self.f.get_constant(b);
                @debug_assert(c.type == .CBits, "cmp to addr");
                // TODO: write a test that fails if you put `c.bits.i < 1.shift_left(12) - 1` here
                if c.bits.i.bit_and(1.shift_left(12) - 1) == c.bits.i {
                    self.inst(cmn_im(si, a.int_reg(), @as(u12) c.bits.i, 0));
                } else {
                    encoded := c.bits.i.shift_right_logical(12);
                    //@debug_assert_eq(encoded.bit_and(1.shift_left(12) - 1).shift_left(12), c.bits.i, "bad cmp const");
                    self.inst(cmn_im(si, a.int_reg(), @as(u12) encoded, 1));
                };
            }
        }
        fn afcmp() => {
            a, b := (i.arg&[0], i.arg&[1]);
            @debug_assert(rtype(a) == .RTmp && rtype(b) == .RTmp, "fcmp needs reg");
            self.inst(fcmpe(sf, a.float_reg(), b.float_reg()));
        }
        @default => self.emitins2(i);
    } // early returns
}

// FUCK not enough bits to refer to all slots. if only this backend worked already and i could kill my old one. 
fn emitins2(self: *ArmState, i: *Qbe.Ins) void = {
    names :: Qbe.O.get_enum_names();
    idx := @as(i64) intcast(@as(i32) i.op().raw());
    inst_name := names[idx];
    si := if(i.cls().is_wide(), => Bits.X64, => Bits.W32);
    sf := if(i.cls().is_wide(), => FType.D64, => FType.S32);
    
    if is_load(i.op()) {
        self.fixarg(i.arg&.index(0), size_of_load(i).intcast());
        offset := 0;
        size := size_of_load(i);
        addr := if rtype(i.arg&[0]) == .RSlot {
            offset += self.slot(i.arg&[0]);
            @debug_assert(i.arg&[1] == QbeNull, "i dont think i mix offset and slot in ldr");
            fp
        } else {
            if rtype(i.arg&[1]) == .RInt {
                offset += zext(rsval(i.arg&[1]));
            };
            i.arg&[0].int_reg()
        };
        
        @debug_assert_eq(offset.mod(size.zext()), 0);
        offset /= size.zext();
        @debug_assert_eq(offset.bit_and(1.shift_left(12) - 1), offset, "can't encode offset");
        // TODO: u/s h/b
        if i.cls().is_int() { // TODO: is this wrong?
            s := firstbit(size.zext().bitcast());
            si_inv := if(i.cls().is_wide(), => Bits.W32, => Bits.X64);
            @match(i.op()) {
                fn loadsw() => {
                    if i.cls() == .Kl {
                        self.inst(@bits(0b10, 0b11100110, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                    } else {
                        self.inst(@bits(@as(u2) s, 0b11100101, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                    };
                }
                fn loadsh() => self.inst(@bits(0b01, 0b1110011, si_inv, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                fn loadsb() => self.inst(@bits(0b00, 0b1110011, si_inv, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                @default => self.inst(@bits(@as(u2) s, 0b11100101, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
            };
        } else {
            self.inst(f_ldr_uo(si /* not sf*/, i.to.float_reg(), addr, @as(u12) offset));
        };
        return();
    };
    if is_store(i.op()) {
        self.fixarg(i.arg&.index(1), size_of_store(i).intcast());
        offset := 0;
        size := size_of_store(i);
        addr := @match(rtype(i.arg&[1])) {
            fn RSlot() => {
                offset = self.slot(i.arg&[1]) / size.zext();
                fp
            }
            fn RTmp() => i.arg&[1].int_reg();
            fn RMem() => {
                m := self.f.get_memory(i.arg&[1]);
                offset = m.offset.bits.i / size.zext();
                m.base.int_reg()
            }
            @default => panic("bad str addr type");
        };
        if i.op() == .stores {
            self.inst(f_str_uo(Bits.W32, @as(u5) i.arg&[0].float_reg(), addr, @as(u12) offset));
            return();
        };
        if i.op() == .stored {
            self.inst(f_str_uo(Bits.X64, @as(u5) i.arg&[0].float_reg(), addr, @as(u12) offset));
            return();
        };
        s := firstbit(size.zext().bitcast());
        self.inst(@bits(@as(u2) s, 0b11100100, @as(u12) offset, @as(u5) addr, @as(u5) i.arg&[0].int_reg()));
        return();
    };
    if is_flag(i.op()) {
        flag_base :: Qbe.O.flagieq;  // TODO: use this everywhere
        c: i32 = i.op().raw() - flag_base.raw();
        if c < 0 || c > Qbe.NCmp {
            panic("unhandled flag")
        };
        c = cmpneg(c); // cset is an alias for CSINC so encoding is backwards from what you'd type in an assembler. 
        cond := arm_condition_codes[c.zext()];
        self.inst(cset(si, i.to.int_reg(), cond));
        return();
    };
    @match(i.op()) {
        fn cast() => {
            @match(i.cls()) {
                fn Kw() => {
                    self.inst(@bits(Bits.W32, 0b0011110, FType.S32, 0b10, 0b0, 0b11, 0b0, 0b000000, @as(u5) i.arg&[0].float_reg(), @as(u5) i.to.int_reg()));
                }
                fn Kl() => {
                    self.inst(@bits(Bits.X64, 0b0011110, FType.D64, 0b10, 0b0, 0b11, 0b0, 0b000000, @as(u5) i.arg&[0].float_reg(), @as(u5) i.to.int_reg()));
                }
                fn Ks() => {
                    self.inst(@bits(Bits.W32, 0b0011110, FType.S32, 0b10, 0b0, 0b11, 0b1, 0b000000, @as(u5) i.arg&[0].int_reg(), @as(u5) i.to.float_reg()));
                }
                fn Kd() => {
                    self.inst(@bits(Bits.X64, 0b0011110, FType.D64, 0b10, 0b0, 0b11, 0b1, 0b000000, @as(u5) i.arg&[0].int_reg(), @as(u5) i.to.float_reg()));
                }
                @default => unreachable();
            };
            return();
        }
        fn stosi() => self.inst(fcvtzs(si, FType.S32, i.to.int_reg(), i.arg&[0].float_reg()));
        fn dtosi() => self.inst(fcvtzs(si, FType.D64, i.to.int_reg(), i.arg&[0].float_reg()));
        fn swtof() => self.inst(scvtf(Bits.W32, sf, i.to.float_reg(), i.arg&[0].int_reg()));
        fn uwtof() => self.inst(@bits(Bits.W32, 0b0011110, sf, 0b100011000000, i.arg&[0].int_reg(), i.to.float_reg())); // ucvtf
        fn ultof() => self.inst(@bits(Bits.X64, 0b0011110, sf, 0b100011000000, i.arg&[0].int_reg(), i.to.float_reg())); // ucvtf
        fn stoui() => self.inst(@bits(si,  0b0011110, FType.S32, 0b111001000000, i.arg&[0].float_reg(), i.to.int_reg())); // fcvtzu
        fn dtoui() => self.inst(@bits(si,  0b0011110, FType.D64, 0b111001000000, i.arg&[0].float_reg(), i.to.int_reg())); // fcvtzu
        fn sltof() => self.inst(scvtf(Bits.X64, sf, i.to.float_reg(), i.arg&[0].int_reg()));
        @default => @panic("TODO: op encoding %", inst_name);
    };
}

/*

  Stack-frame layout:

  +=============+
  | varargs     |
  |  save area  |
  +-------------+
  | callee-save |  ^
  |  registers  |  |
  +-------------+  |
  |    ...      |  |
  | spill slots |  |
  |    ...      |  | e->frame
  +-------------+  |
  |    ...      |  |
  |   locals    |  |
  |    ...      |  |
  +-------------+  |
  | e->padding  |  v
  +-------------+
  |  saved x29  |
  |  saved x30  |
  +=============+ <- x29

*/

fn inst_bytes_only(self: *ArmState, opcode: u32) void = {
    code := self.m.segments&[.Code]&;
    ptr_cast_unchecked(u8, u32, code.next)[] = opcode;  // SAFETY: aligned becuase we're always emitting arm instructions
    code.next = code.next.offset(4);
}

fn inst(self: *ArmState, opcode: u32) void = {
    code := self.m.segments&[.Code]&;
    ptr_cast_unchecked(u8, u32, code.next)[] = opcode;  // SAFETY: aligned becuase we're always emitting arm instructions
    code.next = code.next.offset(4);
    // :TempAsmText
    @if(self.m.want_text_asm) {
        @fmt(self.buf&, "\t.word %\n", opcode);
    };
}

fn next_inst_offset(self: *ArmState) i64 = {
    code := self.m.segments&[.Code]&;
    ptr_diff(code.mmapped.ptr, code.next) - self.start_of_function
}

fn encode_adr(distance: i64, dest_reg: u5) u32 = {
    @assert(distance.abs() < 1.shift_left(21), "cant reference that far % (TODO: can just use more insts)", distance);
    high := distance.shift_right_arithmetic(2).bit_and(1.shift_left(19) - 1);
    low := distance.bit_and(0b11);
    adr(high, low, dest_reg)
}

fn encode_adrp_add(dest: *u8, src: *u8, reg: u5) Ty(u32, u32) = {
    dest := u8.int_from_ptr(dest);
    src := u8.int_from_ptr(src);
    
    distance := dest - src;
    adr_range :: 1.shift_left(21);
    if dest.abs() < adr_range {  // TODO: untested because i have such big segments
        // if its an import, you need the extra nop so you take up the right amount of space for the relocation when doing AOT. 
        return(encode_adr(distance, reg), arm_nop); 
    };
    
    page_offset := dest.shift_right_logical(12) - src.shift_right_logical(12);
    high := page_offset.shift_right_logical(2).bit_and(1.shift_left(19) - 1);
    low := page_offset.bit_and(0b11);
    bottom12 := @as(u12) @as(i64) dest.bit_and(1.shift_left(12) - 1);
    (adrp(high, low, reg), add_im(Bits.X64, reg, reg, bottom12, 0))
}

fn encode_adrp_load(dest: *u8, src: *u8, reg: u5) Ty(u32, u32) = {
    dest := u8.int_from_ptr(dest);
    src := u8.int_from_ptr(src);
    
    page_offset := dest.shift_right_logical(12) - src.shift_right_logical(12);
    high := page_offset.shift_right_logical(2).bit_and(1.shift_left(19) - 1);
    low := page_offset.bit_and(0b11);
    bottom12 := @as(u12) @as(i64) dest.bit_and(1.shift_left(12) - 1) / 8;
    (adrp(high, low, reg), ldr_uo(Bits.X64, reg, reg, bottom12))
}

fn sub_er(sf: Bits, dest: RegO, a: RegI, b: RegI) u32 =
    @bits(sf, 0b1001011001, b, 0b011 /*option*/, 0b000 /*imm3*/, a, dest);
    
fn add_er(sf: Bits, dest: RegO, a: RegI, b: RegI) u32 =
    @bits(sf, 0b0001011001, b, 0b011 /*option*/, 0b000 /*imm3*/, a, dest);
