//! Unlike Qbe, we don't rely on a seperate assembler. We just put the bytes in memory and jump there. 
//! This means we also have to handle updating forward references to functions that haven't been compiled yet. 

// TODO: don't setup stack frame for leaf like amd64?
// TODO: make sure i have .ssa tests for every instruction. rn there are some that just fail the franca tests. 
//       addr slot >4095, sltof, loadaddr with offset,
// TODO: a function you can call to emit forwarding shims for uncompiled functions. 
//       so it in emit with the arch independent ir. i guess that means the front end could just do it. 
// TODO: (when i want to do my own aot) have the addr lookup table in a seperate segment before the functions and then don't include it in the aot binrary, just patch instead. 
// TODO: take out the internal jump distance checks because we check at the end that the function fits in the segment (jsut also check that its within i19 because why not)
// TODO: use ldp/stp for callee saved registers

// TODO: need to replace all the .goal checks when we want to support ELF

ArmState :: @struct(
    m: *QbeModule,
    f: *Qbe.Fn,
    frame: i64,
    padding: i64,
    start_of_function: i64,
);

// TODO: remove the *FILE param, it was for when we were matching qbe's interface and outputting text. 
fn emit_func_arm64(m: *QbeModule, f: *Qbe.Fn, _: *FILE) void = {
    @debug_assert_eq(m.goal.arch, .aarch64, "how did we get here?");
    @assert(!m.want_text_asm, "we don't output text anymore. pass -e or whatever");
    name: CStr = (ptr = f.name&.as_ptr());
    
    // We know we're emitting the function right here so we can do old fixups now. 
    name_id := m.intern(name);
    
    m.align_to(.Code, 4);  // should never be unaligned anyway but we live in fear
    start_offset := m.current_offset(.Code);
    self: ArmState = (m = m, f = f, frame = 0, padding = 0, start_of_function = start_offset);
    
    maybe_add_export(m, name_id, f.lnk.export);
    m.do_jit_fixups(name_id, .Code, start_offset);
    
    self&.emit_the_code();
    total_code_so_far := start_offset + self&.next_inst_offset();
    code := m.segments&[.Code]&;
    @assert(total_code_so_far < code.mmapped.len, "too much code (% bytes)", total_code_so_far);
    code_bytes := code.mmapped.slice(start_offset, total_code_so_far);
    // :FuncSizeLimit 
    // b.cond gives you 19 bits, but encoded/4, but signed so that's 20 bits.  
    // you can remove this limit on the function but then have to add the check when emitting each branch. 
    @assert_lt(code_bytes.len, 1.shift_left(20), "we don't allow a function larger than one instruction can jump");
    maybe_llvm_dis(f, code_bytes, "--arch=aarch64");
}

fn maybe_llvm_dis(f: *Qbe.Fn, code_bytes: []u8, arch_name: Str) void = {
    if f.globals.debug["D".char()] {
        @fmt_write(f.globals.debug_out, "# disassembly of '%' (% bytes)\n", f.name(), code_bytes.len);
        asm := code_bytes;
        hex: List(u8) = list(asm.len * 5, temp());
        for asm { byte |
            hex&.push_prefixed_hex_byte(byte);
            hex&.push_all(" ");
        };
        out := open_temp_file();
        out.fd&.write(hex.items());
        DIS :: "llvm-mc";
        // TODO: have the caller pass in anything like this, 
        //       but this is just for debugging the compiler so its not a big deal. :driver_io
        // varient 1 means intel syntax. ignored when doing arm. 
        success := run_cmd_blocking(DIS, @slice(arch_name, "--disassemble", "-output-asm-variant=1", "--show-encoding", out&.s_name()));
        if(!success, => eprintln("ICE(debugging): couldn't disassemble"));
        out.remove();
    };
}

fn segment_address(self: *QbeModule, segment: SegmentType, offset_in_segment: i64) *u8 = {
    data := self.segments&[segment]&;
    data.mmapped.ptr.offset(offset_in_segment)
}

fn do_fixups_arm64(self: *QbeModule, address: *u8, symbol: *SymbolInfo) void = {
    fixups := symbol.fixups.items();
    local := symbol.kind == .Local;
    new_got_reloc: ?Fixup = .None;
    each fixups { fixup | 
        @match(fixup.type) {
            fn Call() => {
                if self.debug["T".char()] {
                    @fmt_write(self.debug_out, "# patch call % calls %\n", u8.int_from_ptr(fixup.patch_at), symbol.name);
                };
                distance := ptr_diff(fixup.patch_at, address) / 4;
                in_range := distance.abs() < 1.shift_left(26);
                assert(!local || in_range, "local symbol too far away");
                if !local {
                    // For AOT we'll tell the loader to put the right offset in the __got for us. 
                    // For jitting we could reference directly but this indirection makes it consistant. 
                    // bl from the caller to the trampoline, then the trampoline loads from __got and calls that. 
                    // means you always reserve one instruction for a call before knowing if its local. 
                    // > my understanding is that in the old days you could have a shim 
                    // > that checked the return address and patched it to not need the extra indirection after the first call, 
                    // > but i we care too much about security to do that any more. 
                    if symbol.offset == -1 {
                        // create a new trampoline in the __stubs section. 
                        code := self.segments&.index(.Code);
                        symbol.offset = self.stubs&.len() + COMMANDS_SIZE_GUESS;
                        trampoline := self.stubs&.reserve(Array(u32, 3));
                        reg := Arm64Reg.IP0.raw() - 1; // x16
                        new_got_reloc = load_from_got(self, symbol, reg, trampoline.items().slice(0, 2), address).or_else(=> new_got_reloc);
                        trampoline[2] = br(reg, 0);
                    };
                    stub_trampoline := self.segment_address(.Code, symbol.offset);
                    distance = ptr_diff(fixup.patch_at, stub_trampoline) / 4;
                };
                assert(distance.abs() < 1.shift_left(26), "can't even jump to trampoline, thats bad.");
                inst := ptr_cast_unchecked(u8, u32, fixup.patch_at);
                inst[] = b(s_trunc(distance, 26), 1);
            }
            fn InReg(it) => {
                if self.debug["T".char()] {
                    @fmt_write(self.debug_out, "# patch addr % refs %=%\n", u8.int_from_ptr(fixup.patch_at), u8.int_from_ptr(address), symbol.name);
                };
                assert(it.increment == 0, "TODO: offset symbols");
                inst: []u32 = (ptr = ptr_cast_unchecked(u8, u32, fixup.patch_at), len = 2);
                if local {
                    i_page, i_off := encode_adrp_add(address, fixup.patch_at, @as(u5) it.r);
                    inst[0] = i_page;
                    inst[1] = i_off;
                } else {
                    // TODO: untested
                    new_got_reloc = load_from_got(self, symbol, it.r, inst, address).or_else(=> new_got_reloc);
                };
            }
            fn DataAbsolute() => {
                reloc := ptr_cast_unchecked(u8, *u8, fixup.patch_at);
                reloc[] = address;
            }
            @default => panic("arm64 unexpected fixup type");
        }
    };
    if new_got_reloc { it | // avoided reallocating while iterating. rustc would be proud.
        symbol.fixups&.push(it, self.gpa);
    };
}

// do not push to symbol.fixups here!
fn load_from_got(self: *QbeModule, symbol: *SymbolInfo, reg_encoding: i64, insts: []u32, jit_address: *u8) ?Fixup = {
    new_got_reloc := ensure_got_slot(self, symbol, jit_address);
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    @debug_assert_eq(ptr_cast_unchecked(u8, i64, got)[], u8.int_from_ptr(jit_address));
    
    i_page, i_off := encode_adrp_load(got, ptr_cast_unchecked(u32, u8, insts.ptr), @as(u5) reg_encoding);
    insts[0] = i_page;
    insts[1] = i_off; 
    new_got_reloc
}

// TODO: is this better than brk? 
tried_to_call_uncompiled :: @as(rawptr) fn() void = {
    @panic("TODO: backend jitted code tried to call uncompiled function");
};

Patch :: @struct(offset_from_start: i64, cond: ?i64, target_bid: i64);

fn emit_the_code(self: *ArmState) void #once = {
    // TODO: what is this? is it the control flow integrety shit? 
    // ("\thint\t#34\n");
    self.inst(0xd503245f);
    
    self.framelayout();
    
    if self.f.vararg && !self.f.globals.target.apple {
        panic("TODO: non-apple arm varargs.");
        //for (n=7; n>=0; n--)
        //    fprintf(e->f, "\tstr\tq%d, [sp, -16]!\n", n);
        //for (n=7; n>=0; n-=2)
        //    fprintf(e->f, "\tstp\tx%d, x%d, [sp, -16]!\n", n-1, n);
    };
    self.prelude_grow_stack();
    self.inst(add_im(Bits.X64, fp, sp, 0, 0)); // Note: normal mov encoding can't use sp :SpEncoding
    
    offset_to_callee_saved := (self.frame - self.padding) / 4;
    for arm64_rclob { r |
        if self.f.reg.bit_and(BIT(r.raw())) != 0 {
            offset_to_callee_saved -= 2;
            op := if(r.is_float(), => Qbe.O.stored, => Qbe.O.storel);
            i := make_ins(op, .Kw, QbeNull, TMP(r.raw()), SLOT(offset_to_callee_saved));
            self.emitins(i&);
        };
    };
    
    // TODO: make sure b.id is sequential
    local_labels := temp().alloc_zeroed(i64, self.f.nblk.zext());
    patches: List(Patch) = list(temp());
    
    branch :: b;
    // :LinkIsNowRpo TODO: why are we iterating to set links instead of just iterating the rpo array. (don't forget to update the comparisons to .link below). 
    for_blocks self.f { b |
        @debug_assert(local_labels[b.id.zext()] == 0, "block ids must be unique");
        local_labels[b.id.zext()] = self.next_inst_offset();
        for_insts_forward b { i |
            self.emitins(i);
        };
        @match(b.jmp.type) {
            fn hlt() => self.inst(brk(1000));
            fn ret0() => {
                // if you run the simplify_jump_chains pass, you'll only need to emit this epilogue once. 
                offset_to_callee_saved = (self.frame - self.padding) / 4;
                for arm64_rclob { r |
                    if self.f.reg.bit_and(BIT(r.raw())) != 0 {
                        offset_to_callee_saved -= 2;
                        cls := if(r.is_float(), => Qbe.Cls.Kd, => Qbe.Cls.Kl);
                        i := make_ins(.load, cls, TMP(r.raw()), SLOT(offset_to_callee_saved), QbeNull);
                        self.emitins(i&);
                    };
                };
                if self.f.dynalloc {
                    // someone did an alloca so we don't statically know the size of our stack frame. 
                    self.inst(add_im(Bits.X64, sp, fp, 0, 0)); // :SpEncoding
                };
                self.epilogue_shrink_stack();
                self.inst(ret());
            }
            fn jmp() => {
                // :GotoJmp
                if !b.s1.identical(b.link) {
                    target := local_labels[b.s1.id.zext()];
                    here := self.next_inst_offset();
                    if target != 0 {
                        distance := (target - here) / 4;
                        assert(distance.abs() < 1.shift_left(26), "can't jump that far");
                        self.inst(branch(s_trunc(distance, 26), 0));
                    } else {
                        // we haven't made this block yet.
                        patches&.push(offset_from_start = here, cond = .None, target_bid = b.s1.id.zext());
                        self.inst(brk(0xabba));
                    };
                }; // else we can just fall through becuase we're about to emit the target block
            }
            @default => {
                Jjf :: Qbe.J.jfieq; // TODO: use this everywhere
                c: i32 = zext(b.jmp.type.raw() - Jjf.raw());
                @debug_assert(!(c < 0 || c > Qbe.NCmp), "unhandled jump");
                
                if b.link.identical(b.s2) {
                    t := b.s1;
                    b.s1 = b.s2;
                    b.s2 = t;
                } else {
                    // we jump to s2 when condition c is true, so flip the condition. 
                    c = cmpneg(c);
                };
                here := self.next_inst_offset();
                target := local_labels[b.s2.id.zext()];
                if target != 0 {
                    cond := arm_condition_codes[c.zext()];
                    distance := (target - here) / 4;
                    self.inst(b_cond(s_trunc(distance, 19), cond));   // :FuncSizeLimit
                } else {
                    // we haven't made this block yet.
                    patches&.push(offset_from_start = here, cond = (Some = c.zext()), target_bid = b.s2.id.zext());
                    self.inst(brk(0xabba)); // so the offsets for the below work out
                };
                // :GotoJmp
                if !b.s1.identical(b.link) {
                    target := local_labels[b.s1.id.zext()];
                    here := self.next_inst_offset();
                    if target != 0 {
                        distance := (target - here) / 4 - 1;
                        self.inst(branch(s_trunc(distance, 26), 0));  // :FuncSizeLimit
                    } else {
                        // we haven't made this block yet.
                        patches&.push(offset_from_start = here, cond = .None, target_bid = b.s1.id.zext());
                        self.inst(brk(0xabba));
                    };
                }; // else we can just fall through becuase we're about to emit the target block
            };
        };
    };
    each patches& { p |
        target := local_labels[p.target_bid];
        // You never jump to the start block and theres normally a prelude anyway so 0 works as a sentinal. 
        @debug_assert(target != 0, "we should have emitted the block");  
        distance := (target - p.offset_from_start) / 4;
        @debug_assert(distance > 0, "dont need to patch backward jumps");
        code  := self.m.segments&[.Code]&;
        patch := code.mmapped.ptr.offset(self.start_of_function + p.offset_from_start);
        patch := ptr_cast_unchecked(u8, u32, patch);
        @debug_assert_eq(brk(0xabba), patch[], "not expecting patch");
        if p.cond { cc | 
            cc := arm_condition_codes[cc];
            patch[] = b_cond(s_trunc(distance, 19), cc);  // :FuncSizeLimit
        } else {
            patch[] = branch(s_trunc(distance, 26), 0);   // :FuncSizeLimit
        };
    };
}

fn framelayout(self: *ArmState) void = {
    o := 0;
    for arm64_rclob { r |
        o += 1.bit_and(self.f.reg.bitcast().shift_right_logical(r.raw()));
    };
    f := self.f.slot.intcast();
    f = (f + 3).bit_and(-4);
    o += o.bit_and(1);
    self.padding = 4*(f - self.f.slot.intcast());
    self.frame = 4*f + 8*o;
}

// has to deal with limited size of immediates. stp encodes them as imm/8 in an i7
fn prelude_grow_stack(self: *ArmState) void = {
    if self.frame + 16 <= 512 {
        encoded := s_trunc(-(self.frame + 16) / 8, 7);
        self.inst(stp_pre(Bits.X64, fp, lr, sp, encoded)); 
        return();
    };
    negative_16 := s_trunc(-2, 7);
    if self.frame <= 4095 {
        self.inst(sub_im(Bits.X64, sp, sp, @as(u12) self.frame, 0));
        self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
        return();
    };
    if self.frame <= 65535 {
        self.inst(movz(Bits.X64, x16, self.frame.trunc(), .Left0));
        self.inst(sub_er(Bits.X64, sp, sp, x16));
        self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
        return();
    };
    
    // :StackSizeLimit
    // could remove this limit and just add another case with more movks.
    // but whatever allocated your stack for you isn't gonna give you this much either
    @assert_lt(self.frame, 1.shift_left(32), "we don't allow >4GB stack frame"); 
    
    top := self.frame.shift_right_logical(16);
    bottom := self.frame.bit_and(0xFFFF);    
    self.inst(movz(Bits.X64, x16, bottom.trunc(), .Left0));
    self.inst(movk(Bits.X64, x16, top.trunc(), .Left16));
    self.inst(sub_er(Bits.X64, sp, sp, x16));
    self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
}

// has to deal with limited size of immediates. 
fn epilogue_shrink_stack(self: *ArmState) void = {
    o := self.frame + 16;
    if self.f.vararg && !self.f.globals.target.apple {
        o += 192;
    };
    // TODO: why isn't this also 512?
    if o <= 504 { 
        encoded_offset := o / 8; 
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) encoded_offset)); 
        return();
    };
    if o - 16 <= 4095 {
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
        self.inst(add_im(Bits.X64, sp, sp, @as(u12) @as(i64) o - 16, 0));
        return();
    };
    if o - 16 <= 65535 {
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
        self.inst(movz(Bits.X64, x16, o.trunc() - 16, .Left0));
        self.inst(add_er(Bits.X64, sp, sp, x16));
        return();
    };
    
    // :StackSizeLimit
    top: u16 = (o - 16).bit_and(0xFFFF).trunc();
    bottom: u16 = (o - 16).shift_right_logical(16).trunc();
    self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
    self.inst(movz(Bits.X64, x16, bottom, .Left0));
    self.inst(movk(Bits.X64, x16, top, .Left16));
    self.inst(add_er(Bits.X64, sp, sp, x16));
}

fn slot(self: *ArmState, r: Qbe.Ref) i64 = {
    s := rsval(r).intcast();
    if s == -1 {
        return(16 + self.frame);
    };
    if s < 0 {
        if self.f.vararg && !self.f.globals.target.apple {
            return(16 + self.frame + 192 - (s+2));
        };
        return(16 + self.frame - (s+2));
    };
    16 + self.padding + 4 * s
}

// This will get more complicated when i can have multiple of the same segment. 
fn distance_between(m: *QbeModule, start_segment: SegmentType, start_offset: i64, end_segment: SegmentType, end_offset: i64) i64 = {
    ptr_diff(m.segment_address(start_segment, start_offset), m.segment_address(end_segment, end_offset))
}

fn current_offset(m: *QbeModule, segment: SegmentType) i64 = {
    ptr_diff(m.segments&[segment].mmapped.ptr, m.segments&[segment].next)
}

fn align_to(m: *QbeModule, segment: SegmentType, align: i64) void = {
    m.segments&.index(segment).align_to(align);
}

// TODO: rename this. 
fn loadaddr_bits(self: *ArmState, c: *Qbe.Con, dest_reg: i64, thread_local: bool) void = {
    symbol := self.m.get_symbol_info(c.sym.id);
    patch_addr := self.m.segment_address(.Code, self.m.current_offset(.Code));
    
    patch: Fixup = (patch_at = patch_addr, type = (InReg = (r = dest_reg, increment = 0)));
    // i think i don't need a different relocation type for thread locals? 
    // TODO: but makes sture that's still true for relocatable objects.
    @match(symbol.kind) {
        fn DynamicPatched() => panic("TODO: handle predeclared import");
        fn Local() => {
            // :canireallynothavecodeberelative
            // TODO: is there some way to have it not move around my functions so i can use relative calls within this one segment?
            if self.m.goal.type == .Relocatable  { // && symbol.segment != .Code 
                // we're creating input for a static linker and we want it to be able to move shit around. 
                symbol.fixups&.push(patch, self.m.gpa);
            };
            
            // we've already compiled it so just compute its address directly.
            distance := self.m.distance_between(.Code, self.m.current_offset(.Code), symbol.segment, symbol.offset);
            if self.f.globals.debug["T".char()] {
                write(self.m.debug_out, items(@format("# direct offset % for '%' %[%]\n", distance, self.m.str(c.sym.id), symbol.segment, symbol.offset) temp()));
            };
            xxx := symbol.segment != .Code;
            @debug_assert(xxx || distance < 0, "we must be jumping backwards");
            //i_page, i_off := @if(thread_local, 
            //    encode_adrp_load(self.m.segment_address(symbol.segment, symbol.offset), patch_addr, @as(u5) dest_reg), 
            //    encode_adrp_add(self.m.segment_address(symbol.segment, symbol.offset), patch_addr, @as(u5) dest_reg), 
            //);
            i_page, i_off := encode_adrp_add(self.m.segment_address(symbol.segment, symbol.offset), patch_addr, @as(u5) dest_reg);
            self.inst(i_page);
            self.inst(i_off);
        }
        fn Pending() => {
            symbol.fixups&.push(patch, self.m.gpa);
            self.inst(brk(0x0123));
            self.inst(brk(0x0456));
        }
    };
}

fn loadcon(self: *ArmState, c: *Qbe.Con, r: i64, k: Qbe.Cls) void = {
    w := is_wide(k);
    n := c.bits.i;
    if c.type == .CAddr {
        self.loadaddr_bits(c, r, c.sym.type == .SThr);
        if c.bits.i != 0 {
            // TODO: you can do this as part of the thing above. 
            @assert(c.bits.i < 4096 && c.bits.i > 0, "TODO: loadaddr of % with offset %", self.m.str(c.sym.id), c.bits.i);
            self.inst(add_im(Bits.X64, @as(u5) r, @as(u5) r, @as(u12) c.bits.i, 0));
        };
        if !w {
            @eprintln("# Taking the low 32 bits of the symbol '%', you can do that but it's an odd choice so perhaps was a mistake.", self.m.str(c.sym.id));
            // we could warn on this if we were feeling friendly. 
            i := make_ins(.extuw, .Kw, TMP(r+1), TMP(r+1), QbeNull);
            self.emitins(i&);
        };
        return();
    };
    @debug_assert(c.type == .CBits, "bad constant");
    s := Bits.X64;
    if !w {
        s = .W32;
        n = n.bit_and(1.shift_left(32) - 1);
    };
   
    // fast path for small positive
    if n.bit_and(0xffff) == n { 
        self.inst(movz(s, r, n.trunc(), .Left0));
        return();
    };
    // fast path for small negative
    if n.bit_or(0xffff) == -1 { // high bits are all ones
        self.inst(movn(s, r, n.bit_not().trunc(), .Left0));
        return();
    };
    // TODO: qbe uses arm64_logimm to get more cases (but we need more instruction encoding logic for that).
    // TODO: choose best instruction to start with movz/movn instead of assuming movz

    // this uses movz for the first chunk and then movk for the rest. 
    first := true;
    for_enum Hw { shift_encode | 
        part: u16 = n.bit_and(0xffff).trunc();
        n = n.shift_right_logical(16);
        if part != 0 {
            op := if first {
                first = false;
                movz(s, r, part, shift_encode)
            } else {
                movk(s, r, part, shift_encode)
            };
            self.inst(op);
        };
    };
}

fn fixarg(self: *ArmState, pr: *Qbe.Ref, sz: i64) void = {
    r := pr[];
    if rtype(r) == .RSlot {
        s := self.slot(r);
        // you're actually allowed unaligned accesses, you just can't encode them in the immediate. 
        // qbe's stack slot allocating doesn't care about ensuring 8 byte slots are aligned,
        // because it never folds offsets into the immediate space (ie use RSlot in .load directly)
        // except for those inserted by reload_spilled. 
        // maybe a better fix for this would be adding a padding slot between the 4 byte and 8 byte slot groups in isel. 
        // but you hit the bad case exactly twice when self compiling so meh -- Oct 31. 
        unaligned := s.mod(sz) != 0;  
        out_of_range := s > sz * 4095;
        if out_of_range || unaligned {
            i := make_ins(.addr, .Kl, TMP(Arm64Reg.IP0.raw()), r, QbeNull);
            self.emitins(i&);
            pr[] = TMP(Arm64Reg.IP0.raw());
        }
    }
}

// TODO: fix non-i64 fn ordinal
fn get_name(o: Qbe.O) Str = {
    names :: Qbe.O.get_enum_names();
    idx := @as(i64) intcast(@as(i32) o.raw());
    names[idx]
}

fn check_args(i: *Qbe.Ins) void = {
    @debug_assert(rtype(i.arg&[0]) == .RTmp && rtype(i.arg&[1]) == .RTmp, "TODO: args need reg. op=%", i.op().get_name());
}

//
// PLEASE DO NOT THE FUCKING PLATFORM REGISTER!
// "It should not be assumed that treating the register as callee-saved will be sufficient to satisfy the requirements of the platform."
// It gets stomped on context switch. Which can happen between arbitrary instructions. 
// i.e. `mov x18, XX; mov XX, YY; mov YY, x18;` is not atomic.
// This is not a theoretical problem. It happens frequently on m1 macos 14.6.1 at the very least.
// So instead of using r18 we use r17. 
// Which is an intra-procedure-call temporary register for the linker to use so its fine as a temporary. 
//
arm_scratch_int :: Arm64Reg.IP1;

fn emitins(self: *ArmState, i: *Qbe.Ins) void = {
    ::if(u32);
    F :: @Fn(s: FType, dest: u5, a: u5, b: u5) u32;
    I :: @Fn(s: Bits, dest: u5, a: u5, b: u5) u32;
    inst_name := i.op().get_name();
    w := i.cls().is_wide();
    si := @if(w, Bits.X64,  Bits.W32);
    sf := @if(w, FType.D64,  FType.S32);
    
    bin :: fn($if_float: F, $if_int: I) => {
        i.check_args();
        op := if i.cls().is_int() {
            if_int(si, i.to.int_reg(), i.arg&[0].int_reg(), i.arg&[1].int_reg())
        } else {
            if_float(sf, i.to.float_reg(), i.arg&[0].float_reg(), i.arg&[1].float_reg())
        };
        self.inst(op);
    };
    int_bin :: fn($if_int: I) => {
        i.check_args();
        op := if_int(si, i.to.int_reg(), i.arg&[0].int_reg(), i.arg&[1].int_reg());
        self.inst(op);
    };
    int_un :: fn($int: @Fn(s: Bits, dest: u5, src: u5) u32) => {
        @debug_assert(rtype(i.arg&[0]) == .RTmp, "TODO: args need reg %", inst_name);
        op := int(si, i.to.int_reg(), i.arg&[0].int_reg());
        self.inst(op);
    };

    @match(i.op()) {
        fn nop()    => ();
        fn dbgloc() => (); // we don't do debug info yet.
        fn copy() => {
            if (i.to == i.arg&[0], => return()); 
            // this is kinda clunky, we let rega insert copies that reference stack slots and then lower them here. 
            if (rtype(i.to) == .RSlot) {
                r := i.to;
                if !isreg(i.arg&[0]) {
                    i.to = TMP(arm_scratch_int);
                    self.emitins(i);
                    i.arg&[0] = i.to;
                };
                op := @as(Qbe.O) @as(i32) Qbe.O.storew.raw() + i.cls().raw().zext();
                i[] = make_ins(op, .Kw, i.to, i.arg&[0], r);
                self.emitins(i);
                return();
            };
            @debug_assert(isreg(i.to), "can only copy to a register");
            @match(rtype(i.arg&[0])) {
                fn RCon() => {
                    c := self.f.get_constant(i.arg&[0]);
                    self.loadcon(c, i.to.int_reg(), i.cls());
                }
                fn RSlot() => {
                    i.set_op(.load);
                    self.emitins(i);
                }
                @default => {
                    @debug_assert(i.to.val() != Arm64Reg.PLA.raw(), "please do not the platform register. thank you.");
                    if i.cls().is_int() {
                        // 10101010   0001 |0011| 0000 |0011 11101000|
                        //    @bits(sf, 0b0101010, Shift.LSL, 0b0, 0b11111, 0b000000, src, dest);
                        self.inst(mov(si, i.to.int_reg(), i.arg&[0].int_reg())); // they can't refer to SP so its fine
                    } else {
                        self.inst(fmov(sf, i.to.float_reg(), i.arg&[0].float_reg()));  
                    };
                };
            }
        }
        fn addr() => {
            @debug_assert(rtype(i.arg&[0]) == .RSlot, "can only addr of a stack slot");
            reg := i.to.int_reg();
            s := self.slot(i.arg&[0]);
            @debug_assert(s >= 0, "slot must be positive");
            if (s <= 4095) {
                self.inst(add_im(Bits.X64, reg, fp, @as(u12) s, 0));
            } else {
                if (s <= 65535) {
                    self.inst(movz(Bits.X64, reg, s.trunc(), .Left0));
                    self.inst(add_er(Bits.X64, reg, fp, reg));
                } else {
                    // :StackSizeLimit
                    self.inst(movz(Bits.X64, reg, s.trunc(), .Left0));
                    self.inst(movk(Bits.X64, reg, s.shift_right_logical(16).trunc(), .Left0));
                    self.inst(add_er(Bits.X64, reg, fp, reg));
                };
            };
        }
        fn call() => {
            callee := i.arg&[0];
            if rtype(callee) == .RCon {
                // TODO: to start with, we always call through a register so i can deal with fewer cases. 
                //       for better code gen should bring back use of bl instruction. 
                c := self.f.get_constant(i.arg&[0]);
                if c.type != .CAddr || c.sym.type != .SGlo || c.bits.i != 0 {
                    panic("invalid call argument");
                };
                
                symbol := self.m.get_symbol_info(c.sym.id);
                patch_at := self.m.segments&[.Code].next;
                @match(symbol.kind) {
                    fn DynamicPatched() => panic("TODO: handle predeclared import");
                    fn Local() => {
                        // already compiled it so branch there directly
                        distance := self.m.distance_between(.Code, self.m.current_offset(.Code), symbol.segment, symbol.offset) / 4;
                        assert(distance.abs() < 1.shift_left(26), "can't call that far but I don't let modules be that big yet.");
                        self.inst(b(s_trunc(distance, 26), 1));
                        // :canireallynothavecodeberelative
                        // TODO: I don't have a test that requires this 
                        //       but i think you do because you need the other ones?
                        if self.m.goal.type == .Relocatable { 
                            symbol.fixups&.push((patch_at = patch_at, type = .Call), self.m.gpa);
                        }; 
                    };
                    fn Pending() => {
                        // haven't compiled it yet so will need a patch. 
                        symbol.fixups&.push((patch_at = patch_at, type = .Call), self.m.gpa);
                        self.inst(brk(0x0789));
                        if self.m.debug["T".char()] {
                            @fmt_write(self.m.debug_out, "# pending call % calls %\n", u8.int_from_ptr(patch_at), symbol.name);
                        };
                    }
                };
            } else {
                self.inst(br(callee.int_reg(), 1));
            };
        }
        fn salloc() => {
            self.inst(sub_er(Bits.X64, sp, sp, i.arg&[0].int_reg()));
            if i.to != QbeNull {
                self.inst(sub_im(Bits.X64, i.to.int_reg(), sp, 0, 0)); // :SpEncoding
            }
        }
        // TODO: these could kinda be data, the encodings are the same for many of them.
        fn add() => {
            if self.f.get_int(i.arg&[1]) { imm |
                // isel enforces that it's an int and in range for this encoding. 
                self.inst(add_im(si, i.to.int_reg(), i.arg&[0].int_reg(), @as(u12) imm, 0));
                return();
            };
            bin(@as(F) fadd, fn(s, d, a, b) => if(a == sp, => add_er(s, d, a, b), => add_sr(s, d, a, b, .LSL, 0))); // could always use *_er but then the disassembly looks dumb
        }
        fn sub()  => {
            if self.f.get_int(i.arg&[1]) { imm |
                // isel enforces that it's an int and in range for this encoding. 
                self.inst(sub_im(si, i.to.int_reg(), i.arg&[0].int_reg(), @as(u12) imm, 0));
                return();
            };
            bin(@as(F) fsub, fn(s, d, a, b) => if(a == sp, => sub_er(s, d, a, b), => sub_sr(s, d, a, b, .LSL, 0))); // ^
        }
        fn mul()  => {
            if rtype(i.arg&[1]) == .RMem {
                m := self.f.get_memory(i.arg&[1]);
                self.inst(madd(si, i.to.int_reg(), m.base.int_reg(), m.index.int_reg(), i.arg&[0].int_reg()));
                return();
            };
            bin(@as(F) fmul, fn(s, d, a, b) => madd(s, d, a, b, xzr));
        }
        fn div()  => bin(@as(F) fdiv, fn(s, d, a, b) => sdiv(s, d, a, b)); // TODO: if you pass it here you force all calls to be inline :FUCKED
        fn udiv() => int_bin(fn(s, d, a, b) => udiv(s, d, a, b));
        fn sar()  => {
            if self.f.get_int(i.arg&[1]) { imm |
                mask := 1.shift_left(5 + w.int()) - 1;
                imm := imm.bit_and(mask);
                d, a := (i.to.int_reg(), i.arg&[0].int_reg());
                if imm == 0 {
                    self.inst(mov(si, d, a)); 
                } else {
                    self.inst(bfm(si, 0b0, imm, mask, d, a));
                };
            } else {
                int_bin(@as(I) asrv);
            };
        }
        fn shr()  => {
            if self.f.get_int(i.arg&[1]) { imm |
                mask := 1.shift_left(5 + w.int()) - 1;
                imm := imm.bit_and(mask);
                d, a := (i.to.int_reg(), i.arg&[0].int_reg());
                if imm == 0 {
                    self.inst(mov(si, d, a)); 
                } else {
                    self.inst(bfm(si, 0b1, imm, mask, d, a));
                };
            } else {
                int_bin(@as(I) lsrv);
            };
        }
        fn shl()  => {
            if self.f.get_int(i.arg&[1]) { imm |
                mask := 1.shift_left(5 + w.int()) - 1;
                imm := imm.bit_and(mask);
                d, a := (i.to.int_reg(), i.arg&[0].int_reg());
                if imm == 0 {
                    self.inst(mov(si, d, a)); 
                } else {
                    imms := mask - imm;
                    immr := mask + 1 - imm;
                    self.inst(bfm(si, 0b1, immr, imms, d, a));
                };
            } else {
                int_bin(@as(I) lslv);
            };
        }
        fn or()   => int_bin(fn(s, d, a, b) => orr(s, d, a, b, .LSL, 0));
        fn xor()  => int_bin(fn(s, d, a, b) => eor(s, d, a, b, .LSL, 0));
        fn and()  => int_bin(fn(s, d, a, b) => and_sr(s, d, a, b, .LSL, 0));
        fn rem() => {
            a, b := (i.arg&[0].int_reg(), i.arg&[1].int_reg());
            r := arm_scratch_int.int_id();
            self.inst(sdiv(si, r, a, b));
            self.inst(msub(si, i.to.int_reg(), r, b, a));
        };
        fn urem() => {
            a, b := (i.arg&[0].int_reg(), i.arg&[1].int_reg());
            r := arm_scratch_int.int_id();
            self.inst(udiv(si, r, a, b));
            self.inst(msub(si, i.to.int_reg(), r, b, a));
        };
        fn extsb()  => int_un(fn(s, d, a) => bfm(s, 0b0, 0b000000, 0b000111, d, a)); // sxtb
        fn extub()  => int_un(fn(s, d, a) => bfm(s, 0b1, 0b000000, 0b000111, d, a)); // uxtb
        fn extsh()  => int_un(fn(s, d, a) => bfm(s, 0b0, 0b000000, 0b001111, d, a)); // sxth
        fn extuh()  => int_un(fn(s, d, a) => bfm(s, 0b1, 0b000000, 0b001111, d, a)); // uxth
        fn extsw()  => int_un(fn(s, d, a) => bfm(s, 0b0, 0b000000, 0b011111, d, a)); // sxtw
        // note: mov w0, w0 is not a nop! it zeros the high bits. 
        fn extuw()  => int_un(fn(s, d, a) => mov(.W32, d, a));
        fn exts()   => self.inst(fcnv(.S32, .D64, i.to.float_reg(), i.arg&[0].float_reg()));
        fn truncd() => self.inst(fcnv(.D64, .S32, i.to.float_reg(), i.arg&[0].float_reg()));
        fn swap() => {
            if i.cls().is_int() {
                a, b := (i.arg&[0].int_reg(), i.arg&[1].int_reg());
                r := arm_scratch_int.int_id();
                self.inst(mov(si, r, a)); 
                self.inst(mov(si, a, b));
                self.inst(mov(si, b, r));
            } else {
                a, b := (i.arg&[0].float_reg(), i.arg&[1].float_reg());
                // we can't refer to v31 because we waste a slot on 0 so this will never stomp anything // :V31
                self.inst(fmov(sf, 31, a));
                self.inst(fmov(sf, a, b));
                self.inst(fmov(sf, b, 31));
            };
        }
        fn acmp() => {
            a, b := (i.arg&[0], i.arg&[1]);
            @debug_assert(rtype(a) == .RTmp, "cmp fst needs reg");
            if rtype(b) == .RTmp {
                self.inst(cmp(si, a.int_reg(), b.int_reg())); 
            } else {
                c := self.f.get_constant(b);
                @debug_assert(c.type == .CBits, "cmp to addr");
                if c.bits.i.bit_and(1.shift_left(12) - 1) == c.bits.i {
                    self.inst(cmp_im(si, a.int_reg(), @as(u12) c.bits.i, 0));
                } else {
                    encoded := c.bits.i.shift_right_logical(12);
                    //@debug_assert_eq(encoded.bit_and(1.shift_left(12) - 1).shift_left(12), c.bits.i, "bad cmp const");
                    self.inst(cmp_im(si, a.int_reg(), @as(u12) encoded, 1));
                };
            }
        }
        // TODO: encoding for this is just one bit different from ^
        fn acmn() => {
            a, b := (i.arg&[0], i.arg&[1]);
            @debug_assert(rtype(a) == .RTmp, "cmp fst needs reg");
            if rtype(b) == .RTmp {
                self.inst(cmn(si, a.int_reg(), b.int_reg())); 
            } else {
                c := self.f.get_constant(b);
                @debug_assert(c.type == .CBits, "cmp to addr");
                // TODO: write a test that fails if you put `c.bits.i < 1.shift_left(12) - 1` here
                if c.bits.i.bit_and(1.shift_left(12) - 1) == c.bits.i {
                    self.inst(cmn_im(si, a.int_reg(), @as(u12) c.bits.i, 0));
                } else {
                    encoded := c.bits.i.shift_right_logical(12);
                    //@debug_assert_eq(encoded.bit_and(1.shift_left(12) - 1).shift_left(12), c.bits.i, "bad cmp const");
                    self.inst(cmn_im(si, a.int_reg(), @as(u12) encoded, 1));
                };
            }
        }
        fn afcmp() => {
            a, b := (i.arg&[0], i.arg&[1]);
            @debug_assert(rtype(a) == .RTmp && rtype(b) == .RTmp, "fcmp needs reg");
            self.inst(fcmpe(sf, a.float_reg(), b.float_reg()));
        }
        @default => self.emitins2(i);
    } // early returns
}

// FUCK not enough bits to refer to all slots. if only this backend worked already and i could kill my old one. 
fn emitins2(self: *ArmState, i: *Qbe.Ins) void = {
    names :: Qbe.O.get_enum_names();
    idx: i64 = intcast(i.op().raw());
    inst_name := names[idx];
    si := @if(i.cls().is_wide(), Bits.X64, Bits.W32);
    sf := @if(i.cls().is_wide(), FType.D64, FType.S32);
    
    if maybe_load(i) { size |
        self.fixarg(i.arg&.index(0), size.intcast());
        offset := 0;
        addr := if rtype(i.arg&[0]) == .RSlot {
            offset += self.slot(i.arg&[0]);
            @debug_assert(i.arg&[1] == QbeNull, "i dont think i mix offset and slot in ldr");
            fp
        } else {
            if rtype(i.arg&[1]) == .RInt {   // TODO: could easily get rid of this branch. is that worth always setting it to RInt(0)?
                offset += zext(rsval(i.arg&[1]));
            };
            i.arg&[0].int_reg()
        };
        
        @debug_assert_eq(offset.mod(size.zext()), 0);
        offset /= size.zext();
        @debug_assert_eq(offset.bit_and(1.shift_left(12) - 1), offset, "can't encode offset");
        // TODO: u/s h/b
        if i.cls().is_int() { // TODO: is this wrong?
            s := trailing_zeros(size.zext().bitcast());
            si_inv := @if(i.cls().is_wide(), Bits.W32, Bits.X64);
            @match(i.op()) {
                fn loadsw() => {
                    if i.cls() == .Kl {
                        self.inst(@bits(0b10, 0b11100110, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                    } else {
                        self.inst(@bits(@as(u2) s, 0b11100101, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                    };
                }
                fn loadsh() => self.inst(@bits(0b01, 0b1110011, si_inv, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                fn loadsb() => self.inst(@bits(0b00, 0b1110011, si_inv, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
                @default => self.inst(@bits(@as(u2) s, 0b11100101, @as(u12) offset, @as(u5) addr, i.to.int_reg()));
            };
        } else {
            self.inst(f_ldr_uo(si /* not sf*/, i.to.float_reg(), addr, @as(u12) offset));
        };
        return();
    };
    if maybe_store(i) { size |
        self.fixarg(i.arg&.index(1), size.intcast());
        offset := 0;
        addr := @match(rtype(i.arg&[1])) {
            fn RSlot() => {
                offset = self.slot(i.arg&[1]) / size.zext();
                fp
            }
            fn RTmp() => i.arg&[1].int_reg();
            fn RMem() => {
                m := self.f.get_memory(i.arg&[1]);
                offset = m.offset.bits.i / size.zext();
                m.base.int_reg()
            }
            @default => panic("bad str addr type");
        };
        if i.op() == .stores {
            self.inst(f_str_uo(Bits.W32, @as(u5) i.arg&[0].float_reg(), addr, @as(u12) offset));
            return();
        };
        if i.op() == .stored {
            self.inst(f_str_uo(Bits.X64, @as(u5) i.arg&[0].float_reg(), addr, @as(u12) offset));
            return();
        };
        s := trailing_zeros(size.zext().bitcast());
        self.inst(@bits(@as(u2) s, 0b11100100, @as(u12) offset, @as(u5) addr, @as(u5) i.arg&[0].int_reg()));
        return();
    };
    if is_flag(i.op()) {
        flag_base :: Qbe.O.flagieq;  // TODO: use this everywhere
        c: i32 = i.op().raw() - flag_base.raw();
        if c < 0 || c > Qbe.NCmp {
            panic("unhandled flag")
        };
        c = cmpneg(c); // cset is an alias for CSINC so encoding is backwards from what you'd type in an assembler. 
        cond := arm_condition_codes[c.zext()];
        self.inst(cset(si, i.to.int_reg(), cond));
        return();
    };
    @match(i.op()) {
        fn cast() => {
            @match(i.cls()) {
                fn Kw() => {
                    self.inst(@bits(Bits.W32, 0b0011110, FType.S32, 0b10, 0b0, 0b11, 0b0, 0b000000, @as(u5) i.arg&[0].float_reg(), @as(u5) i.to.int_reg()));
                }
                fn Kl() => {
                    self.inst(@bits(Bits.X64, 0b0011110, FType.D64, 0b10, 0b0, 0b11, 0b0, 0b000000, @as(u5) i.arg&[0].float_reg(), @as(u5) i.to.int_reg()));
                }
                fn Ks() => {
                    self.inst(@bits(Bits.W32, 0b0011110, FType.S32, 0b10, 0b0, 0b11, 0b1, 0b000000, @as(u5) i.arg&[0].int_reg(), @as(u5) i.to.float_reg()));
                }
                fn Kd() => {
                    self.inst(@bits(Bits.X64, 0b0011110, FType.D64, 0b10, 0b0, 0b11, 0b1, 0b000000, @as(u5) i.arg&[0].int_reg(), @as(u5) i.to.float_reg()));
                }
                @default => unreachable();
            };
            return();
        }
        fn stosi() => self.inst(fcvtzs(si, FType.S32, i.to.int_reg(), i.arg&[0].float_reg()));
        fn dtosi() => self.inst(fcvtzs(si, FType.D64, i.to.int_reg(), i.arg&[0].float_reg()));
        fn swtof() => self.inst(scvtf(Bits.W32, sf, i.to.float_reg(), i.arg&[0].int_reg()));
        fn uwtof() => self.inst(@bits(Bits.W32, 0b0011110, sf, 0b100011000000, i.arg&[0].int_reg(), i.to.float_reg())); // ucvtf
        fn ultof() => self.inst(@bits(Bits.X64, 0b0011110, sf, 0b100011000000, i.arg&[0].int_reg(), i.to.float_reg())); // ucvtf
        fn stoui() => self.inst(@bits(si,  0b0011110, FType.S32, 0b111001000000, i.arg&[0].float_reg(), i.to.int_reg())); // fcvtzu
        fn dtoui() => self.inst(@bits(si,  0b0011110, FType.D64, 0b111001000000, i.arg&[0].float_reg(), i.to.int_reg())); // fcvtzu
        fn sltof() => self.inst(scvtf(Bits.X64, sf, i.to.float_reg(), i.arg&[0].int_reg()));
        @default => @panic("TODO: op encoding %", inst_name);
    };
}

// same as qbe
/*

  Stack-frame layout:

  +=============+
  | varargs     |
  |  save area  |
  +-------------+
  | callee-save |  ^
  |  registers  |  |
  +-------------+  |
  |    ...      |  |
  | spill slots |  |
  |    ...      |  | e->frame
  +-------------+  |
  |    ...      |  |
  |   locals    |  |
  |    ...      |  |
  +-------------+  |
  | e->padding  |  v
  +-------------+
  |  saved x29  |
  |  saved x30  |
  +=============+ <- x29

*/

fn inst(self: *ArmState, opcode: u32) void = {
    code := self.m.segments&[.Code]&;
    ptr_cast_unchecked(u8, u32, code.next)[] = opcode;  // SAFETY: aligned becuase we're always emitting arm instructions
    code.next = code.next.offset(4);
}

fn next_inst_offset(self: *ArmState) i64 = {
    code := self.m.segments&[.Code]&;
    ptr_diff(code.mmapped.ptr, code.next) - self.start_of_function
}

fn encode_adrp_add(dest: *u8, src: *u8, reg: u5) Ty(u32, u32) = {
    dest := u8.int_from_ptr(dest);
    src := u8.int_from_ptr(src);
    
    distance := dest - src;
    adr_range :: 1.shift_left(21);
    if dest.abs() < adr_range {  // TODO: untested because i have such big segments
        // if its an import, you need the extra nop so you take up the right amount of space for the relocation when doing AOT. 
        high := distance.shift_right_arithmetic(2).bit_and(1.shift_left(19) - 1);
        low := distance.bit_and(0b11);
        return(adr(high, low, reg), arm_nop); 
    };
    
    page_offset := dest.shift_right_logical(12) - src.shift_right_logical(12);
    high := page_offset.shift_right_logical(2).bit_and(1.shift_left(19) - 1);
    low := page_offset.bit_and(0b11);
    bottom12 := @as(u12) @as(i64) dest.bit_and(1.shift_left(12) - 1);
    (adrp(high, low, reg), add_im(Bits.X64, reg, reg, bottom12, 0))
}

fn encode_adrp_load(dest: *u8, src: *u8, reg: u5) Ty(u32, u32) = {
    dest := u8.int_from_ptr(dest);
    src := u8.int_from_ptr(src);
    
    page_offset := dest.shift_right_logical(12) - src.shift_right_logical(12);
    high := page_offset.shift_right_logical(2).bit_and(1.shift_left(19) - 1);
    low := page_offset.bit_and(0b11);
    bottom12 := @as(u12) @as(i64) dest.bit_and(1.shift_left(12) - 1) / 8;
    (adrp(high, low, reg), ldr_uo(Bits.X64, reg, reg, bottom12))
}

fn sub_er(sf: Bits, dest: RegO, a: RegI, b: RegI) u32 =
    @bits(sf, 0b1001011001, b, 0b011 /*option*/, 0b000 /*imm3*/, a, dest);
    
fn add_er(sf: Bits, dest: RegO, a: RegI, b: RegI) u32 =
    @bits(sf, 0b0001011001, b, 0b011 /*option*/, 0b000 /*imm3*/, a, dest);
