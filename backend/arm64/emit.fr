//! Unlike Qbe, we don't rely on a seperate assembler. We just put the bytes in memory and jump there. 
//! This means we also have to handle updating forward references to functions that haven't been compiled yet. 

// TODO: don't setup stack frame for leaf like amd64?
// TODO: make sure i have .ssa tests for every instruction. rn there are some that just fail the franca tests. 
//       addr slot >4095, sltof, loadaddr with offset,
// TODO: a function you can call to emit forwarding shims for uncompiled functions. 
//       so it in emit with the arch independent ir. i guess that means the front end could just do it. 
// TODO: (when i want to do my own aot) have the addr lookup table in a seperate segment before the functions and then don't include it in the aot binrary, just patch instead. 
// TODO: take out the internal jump distance checks because we check at the end that the function fits in the segment (jsut also check that its within i19 because why not)
// TODO: use ldp/stp for callee saved registers
// TODO: relocation to fold symbol offsets into the adrp+add instead of wasting an extra instruction :SLOW
// TODO: when jitting, check if constant ints are close enough to load with adr instead of several movk

// TODO: need to replace all the .goal checks when we want to support ELF

ArmState :: @struct(
    m: *QbeModule,
    f: *Qbe.Fn,
    frame: i64,
    padding: i64,
    start_of_function: i64,
    patches: List(Patch),
    needs_frame := true,
);
b_jmp :: b;
fn emit_func_arm64(f: *Qbe.Fn) void = {
    m := f.globals;
    @debug_assert_eq(m.goal.arch, .aarch64, "how did we get here?");
    self: ArmState = (m = m, f = f, frame = 0, padding = 0, start_of_function = m.current_offset(.Code), patches = list(f.nblk.zext(), temp()));
    self&.emit_the_code();
}

fn segment_address(self: *QbeModule, segment: SegmentType, offset_in_segment: i64) *u8 = {
    data := self.segments&[segment]&;
    data.mmapped.ptr.offset(offset_in_segment)
}

fn fixup_arm64(self: *QbeModule, symbol: *SymbolInfo, fixup: *Fixup, new_got_reloc: *?Fixup) void = {
    address := symbol.jit_addr;
    local  := symbol.kind == .Local;
    @match(fixup.type) {
        fn Call() => {
            if self.debug["T".char()] {
                @fmt_write(self.debug_out, "# patch call % calls %\n", u8.int_from_ptr(fixup.patch_at), symbol.name);
            };
            distance := ptr_diff(fixup.patch_at, u8.ptr_from_raw(address)) / 4;
            in_range := distance.abs() < 1.shift_left(26);
            assert(!local || in_range, "local symbol too far away");
            if !local {
                // For AOT we'll tell the loader to put the right offset in the __got for us. 
                // For jitting we could reference directly but this indirection makes it consistant. 
                // bl from the caller to the trampoline, then the trampoline loads from __got and calls that. 
                // means you always reserve one instruction for a call before knowing if its local. 
                // > my understanding is that in the old days you could have a shim 
                // > that checked the return address and patched it to not need the extra indirection after the first call, 
                // > but i we care too much about security to do that any more. 
                if symbol.offset == -1 {
                    insane_caches := use_threads && self.goal.type == .JitOnly && self.goal.os == .macos;
                    
                    // create a new trampoline in the __stubs section. 
                    @if(insane_caches) pthread_mutex_lock(self.icache_mutex&).unwrap();
                    symbol.offset = self.stubs&.len() + COMMANDS_SIZE_GUESS;
                    trampoline := self.stubs&.reserve(Array(u32, 3));
                    @if(insane_caches) pthread_mutex_unlock(self.icache_mutex&).unwrap();
                    reg := Arm64Reg.IP0.raw() - 1; // x16
                    new_got_reloc[] = load_from_got(self, symbol, reg, trampoline.items().slice(0, 2), address).or_else(=> new_got_reloc[]);
                    trampoline[2] = br(reg, 0);
                    
                    if insane_caches {
                        code := self.segments&.index(.Code);
                        // sys_dcache_flush
                        clear_instruction_cache(u8.raw_from_ptr(code.mmapped.ptr).offset(symbol.offset), size_of(@type trampoline[]));
                        fence();
                    };
                };
                stub_trampoline := self.segment_address(.Code, symbol.offset);
                distance = ptr_diff(fixup.patch_at, stub_trampoline) / 4;
            };
            @assert_lt(distance.abs(), 1.shift_left(26), "can't even jump to trampoline, thats bad.");
            inst := ptr_cast_unchecked(u8, u32, fixup.patch_at);
            inst[] = b_jmp(s_trunc(distance, 26), 1);
        }
        fn InReg(it) => {
            if self.debug["T".char()] {
                @fmt_write(self.debug_out, "# patch addr % refs %=%\n", u8.int_from_ptr(fixup.patch_at), address, symbol.name);
            };
            inst: []u32 = (ptr = ptr_cast_unchecked(u8, u32, fixup.patch_at), len = 2);
            if local {
                i_page, i_off := encode_adrp_add(address.offset(it.increment.zext()), fixup.patch_at, @as(u5) it.r);
                inst[0] = i_page;
                inst[1] = i_off;
            } else {
                // TODO: untested
                new_got_reloc[] = load_from_got(self, symbol, it.r, inst, address).or_else(=> new_got_reloc[]);
                it.got_load = true;
            };
        }
        fn DataAbsolute(it) => {
            reloc := ptr_cast_unchecked(u8, rawptr, fixup.patch_at);
            reloc[] = address.offset(it.increment);
            m := self;
            @debug_assert(!(m.got_indirection_instead_of_patches && m.goal.type == .JitOnly && address.is_null()), "you cant do that bro");
        }
        @default => panic("arm64 unexpected fixup type");
    }
}

// do not push to symbol.fixups here!
fn load_from_got(self: *QbeModule, symbol: *SymbolInfo, reg_encoding: i64, insts: []u32, jit_address: rawptr) ?Fixup = {
    new_got_reloc := ensure_got_slot(self, symbol, jit_address);
    @debug_assert(symbol.got_lookup_offset >= 0 && symbol.got_lookup_offset.mod(8) == 0);
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    //@debug_assert_eq(ptr_cast_unchecked(u8, rawptr, got)[], jit_address); // TODO: this should be true except for got_indirection_instead_of_patches 
    
    i_page, i_off := encode_adrp_load(got, ptr_cast_unchecked(u32, u8, insts.ptr), @as(u5) reg_encoding);
    insts[0] = i_page;
    insts[1] = i_off; 
    new_got_reloc
}

Patch :: @struct(offset_from_start: i64, cond: i32, target_bid: i32);

fn emit_the_code(self: *ArmState) void #once = {
    // TODO: what is this? is it the control flow integrety shit? 
    // ("\thint\t#34\n");
    self.inst(0xd503245f);
    
    self.needs_frame = bit_and(self.f.reg, arm64_clob_mask) != 0 || !self.f.leaf || self.f.vararg || self.f.slot > 0 || self.f.dynalloc;
    if self.needs_frame {
        self.framelayout();
        
        if self.f.vararg && !self.f.globals.target.apple {
            panic("TODO: non-apple arm varargs.");
            //for (n=7; n>=0; n--)
            //    fprintf(e->f, "\tstr\tq%d, [sp, -16]!\n", n);
            //for (n=7; n>=0; n-=2)
            //    fprintf(e->f, "\tstp\tx%d, x%d, [sp, -16]!\n", n-1, n);
        };
        
        self.prelude_grow_stack();
        self.inst(add_im(Bits.X64, fp, sp, 0, 0)); // Note: normal mov encoding can't use sp :SpEncoding
        
        offset_to_callee_saved := (self.frame - self.padding);
        for arm64_rclob { r |
            if self.f.reg.bit_and(BIT(r.raw())) != 0 {
                offset_to_callee_saved -= 2 * 4;
                op := if(r.is_float(), => Qbe.O.stored, => Qbe.O.storel);
                i := make_ins(op, .Kw, QbeNull, TMP(r.raw()), SLOT(offset_to_callee_saved));
                self.emitins(i&, zeroed(*Qbe.Blk));
            };
        };
    } else {
        // This matters because we might have args passed on the stack (negative RSlot),
        // and normally `fn slot` will offset by 16 to skip over our saved fp/lr, so we undo that here. 
        self.frame = -16;
    };
    
    local_labels := temp().alloc_zeroed(i64, self.f.nblk.zext()); // TODO: use u32 and offset from start of function 
    
    branch :: b;
    // :LinkIsNowRpo TODO: why are we iterating to set links instead of just iterating the rpo array. (don't forget to update the comparisons to .link below). 
    for_blocks self.f { b |
        @debug_assert_lt(b.id, self.f.nblk, "invalid block id!");
        @debug_assert(local_labels[b.id.zext()] == 0, "block ids must be unique");
        local_labels[b.id.zext()] = self.next_inst_offset();
        for_insts_forward b { i |
            self.emitins(i, b);
        };
        @match(b.jmp.type) {
            fn hlt() => self.inst(brk(1000));
            fn ret0() => {
                if self.needs_frame {
                    // if you run the simplify_jump_chains pass, you'll only need to emit this epilogue once. 
                    offset_to_callee_saved := (self.frame - self.padding);
                    for arm64_rclob { r |
                        if self.f.reg.bit_and(BIT(r.raw())) != 0 {
                            offset_to_callee_saved -= 2 * 4;
                            cls := if(r.is_float(), => Qbe.Cls.Kd, => Qbe.Cls.Kl);
                            i := make_ins(.load, cls, TMP(r.raw()), SLOT(offset_to_callee_saved), QbeNull);
                            self.emitins(i&, b);
                        };
                    };
                    if self.f.dynalloc {
                        // someone did an alloca so we don't statically know the size of our stack frame. 
                        self.inst(add_im(Bits.X64, sp, fp, 0, 0)); // :SpEncoding
                    };
                    self.epilogue_shrink_stack();
                };
                self.inst(ret());
            }
            fn jmp() => if !b.s1.identical(b.link) {
                self.jmp(b.s1, -1);
            };  // else fall through; we're about to emit the target block
            fn switch() => unreachable();
            @default => {
                Jjf :: Qbe.J.jfieq; // TODO: use this everywhere
                c: i32 = b.jmp.type.raw() - Jjf.raw();
                @debug_assert(!(c < 0 || c > Qbe.NCmp), "unhandled jump");
                
                if b.link.identical(b.s2) {
                    t := b.s1;
                    b.s1 = b.s2;
                    b.s2 = t;
                } else {
                    // we jump to s2 when condition c is true, so flip the condition. 
                    c = cmpneg(c);
                };
                self.jmp(b.s2, c);
                if !b.s1.identical(b.link) {
                    self.jmp(b.s1, -1);
                }; // else fall through; we're about to emit the target block
            };
        };
    };
    
    each self.patches& { p |
        target := local_labels[p.target_bid.zext()];
        // You never jump to the start block and theres normally a prelude anyway so 0 works as a sentinal. 
        @debug_assert(target != 0, "we should have emitted the block");  
        distance := (target - p.offset_from_start) / 4;
        code  := self.m.segments&[.Code]&;
        patch := code.mmapped.ptr.offset(self.start_of_function + p.offset_from_start);
        patch := ptr_cast_unchecked(u8, u32, patch);
        @debug_assert_eq(brk(0xabba), patch[], "not expecting patch");
        if p.cond >= 0 {
            cc := arm_condition_codes[p.cond.zext()];
            patch[] = b_cond(s_trunc(distance, 19), cc);  // :FuncSizeLimit
        } else {
            patch[] = branch(s_trunc(distance, 26), 0);   // :FuncSizeLimit
        };
    };
}

// cond: -1 == Always
fn jmp(self: *ArmState, s: *Qbe.Blk, cond: i32) void #inline = {
    self.patches&.push(offset_from_start = self.next_inst_offset(), cond = cond, target_bid = s.id);
    self.inst(brk(0xabba));
}

fn framelayout(self: *ArmState) void = {
    o := 0;
    for arm64_rclob { r |
        o += 1.bit_and(self.f.reg.bitcast().shift_right_logical(r.raw()));
    };
    f := self.f.slot.intcast() / 4;
    f = (f + 3).bit_and(-4);
    o += o.bit_and(1);
    self.padding = 4*(f - self.f.slot.intcast() / 4);
    self.frame = 4*f + 8*o;
}

// has to deal with limited size of immediates. stp encodes them as imm/8 in an i7
fn prelude_grow_stack(self: *ArmState) void = {
    if self.frame + 16 <= 512 {
        encoded := s_trunc(-(self.frame + 16) / 8, 7);
        self.inst(stp_pre(Bits.X64, fp, lr, sp, encoded)); 
        return();
    };
    negative_16 := s_trunc(-2, 7);
    if self.frame <= 4095 {
        self.inst(sub_im(Bits.X64, sp, sp, @as(u12) self.frame, 0));
        self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
        return();
    };
    if self.frame <= 65535 {
        self.inst(movz(Bits.X64, x16, self.frame.trunc(), .Left0));
        self.inst(sub_er(Bits.X64, sp, sp, x16));
        self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
        return();
    };
    
    // :StackSizeLimit
    // could remove this limit and just add another case with more movks.
    // but whatever allocated your stack for you isn't gonna give you this much either
    @assert_lt(self.frame, 1.shift_left(32), "we don't allow >4GB stack frame"); 
    
    top := self.frame.shift_right_logical(16);
    bottom := self.frame.bit_and(0xFFFF);    
    self.inst(movz(Bits.X64, x16, bottom.trunc(), .Left0));
    self.inst(movk(Bits.X64, x16, top.trunc(), .Left16));
    self.inst(sub_er(Bits.X64, sp, sp, x16));
    self.inst(stp_pre(Bits.X64, fp, lr, sp, negative_16)); 
}

// has to deal with limited size of immediates. 
fn epilogue_shrink_stack(self: *ArmState) void = {
    o := self.frame + 16;
    if self.f.vararg && !self.f.globals.target.apple {
        o += 192;
    };
    if o <= 504 { 
        encoded_offset := o / 8; 
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) encoded_offset)); 
        return();
    };
    if o - 16 <= 4095 {
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
        self.inst(add_im(Bits.X64, sp, sp, @as(u12) @as(i64) o - 16, 0));
        return();
    };
    if o - 16 <= 65535 {
        self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
        self.inst(movz(Bits.X64, x16, o.trunc() - 16, .Left0));
        self.inst(add_er(Bits.X64, sp, sp, x16));
        return();
    };
    
    // :StackSizeLimit
    top:    u16 = (o - 16).shift_right_logical(16).trunc();
    bottom: u16 = (o - 16).bit_and(0xFFFF).trunc();
    self.inst(ldp_post(Bits.X64, fp, lr, sp, @as(i7) 2)); // 16 encoded as imm7/8
    self.inst(movz(Bits.X64, x16, bottom, .Left0));
    self.inst(movk(Bits.X64, x16, top, .Left16));
    self.inst(add_er(Bits.X64, sp, sp, x16));
}

fn slot(self: *ArmState, r: Qbe.Ref) i64 = {
    s := rsval(r).intcast();
    if s == -1 {
        return(16 + self.frame);
    };
    if s < 0 {
        if self.f.vararg && !self.f.globals.target.apple {
            return(16 + self.frame + 192 - (s+2));
        };
        return(16 + self.frame - (s+2));
    };
    16 + self.padding + s
}

// This will get more complicated when i can have multiple of the same segment. 
fn distance_between(m: *QbeModule, start_segment: SegmentType, start_offset: i64, end_segment: SegmentType, end_offset: i64) i64 = {
    @debug_assert(end_offset >= 0 && start_offset >= 0);
    ptr_diff(m.segment_address(start_segment, start_offset), m.segment_address(end_segment, end_offset))
}

fn current_offset(m: *QbeModule, segment: SegmentType) i64 = {
    ptr_diff(m.segments&[segment].mmapped.ptr, m.segments&[segment].next)
}

fn align_to(m: *QbeModule, segment: SegmentType, align: i64) void = {
    m.segments&.index(segment).align_to(align);
}

// TODO: rename this. 
fn loadaddr_bits(self: *ArmState, c: *Qbe.Con, dest_reg: i64) void = {
    use_symbol(self.m, c.sym.id) { symbol |
        patch_addr := self.m.segment_address(.Code, self.m.current_offset(.Code));
        
        @debug_assert(c.bits.i >= 0 && c.bits.i < MAX_i32);
        // :ArmAddendReloc 
        // I'm too lazy to add a new relocation type right now (its a whole seperate one for the addend i think? theres no slot on pageoff for it)
        // So for now i just generate a few extra instructions when making a relinkable object
        //
        inc := if(self.m.goal.type == .Relocatable, => 0, => c.bits.i);
        patch: Fixup = (patch_at = patch_addr, type = (InReg = (r = dest_reg, increment = inc.trunc())));
        @match(symbol.kind) {
            fn DynamicPatched() => self.immediate_fixup_got(dest_reg, inc, symbol);
            fn Local() => {
                // we've already compiled it so just compute its address directly.
                distance := self.m.distance_between(.Code, self.m.current_offset(.Code), symbol.segment, symbol.offset + inc);
                if self.f.globals.debug["T".char()] {
                    write(self.m.debug_out, items(@format("# direct offset % for '%' %[%]\n", distance, self.m.str(c.sym.id), symbol.segment, symbol.offset) temp()));
                };
                xxx := symbol.segment != .Code;
                @debug_assert(xxx || distance < 0, "we must be jumping backwards");
                dest := self.m.segment_address(symbol.segment, symbol.offset + inc);
                i_page, i_off := encode_adrp_add(u8.raw_from_ptr(dest), patch_addr, @as(u5) dest_reg);
                self.inst(i_page);
                self.inst(i_off);
                
                // :canireallynothavecodeberelative
                // TODO: is there some way to have it not move around my functions so i can use relative calls within this one segment?
                if self.m.goal.type == .Relocatable  { // && symbol.segment != .Code 
                    // we're creating input for a static linker and we want it to be able to move shit around. 
                    push_fixup(self.m, symbol, patch);
                };
            }
            fn Pending() => {
                if self.m.got_indirection_instead_of_patches || self.m.goal.type == .Relocatable {
                    self.immediate_fixup_got(dest_reg, inc, symbol);
                } else {
                    push_fixup(self.m, symbol, patch);
                    self.inst(brk(0x0123));
                    self.inst(brk(0x0456));
                }
            }
        };
    };
    if self.m.goal.type == .Relocatable && c.bits.i != 0 { // :ArmAddendReloc
        @assert(c.bits.i < 4096 && c.bits.i > 0, "TODO: loadaddr of % with offset %", self.m.str(c.sym.id), c.bits.i);
        self.inst(add_im(Bits.X64, @as(u5) dest_reg, @as(u5) dest_reg, @as(u12) c.bits.i, 0));
    };
}

fn immediate_fixup_got(self: *ArmState, dest_reg: i64, inc: i64, symbol: *SymbolInfo) void = {
    patch_addr := self.m.segment_address(.Code, self.m.current_offset(.Code));
    patch: Fixup = (patch_at = patch_addr, type = (InReg = (r = dest_reg, increment = inc.trunc(), got_load = true)));
    if self.m.goal.type != .JitOnly { 
        push_fixup(self.m, symbol, patch); 
    };
    // this is correct for got_indirection_instead_of_patches too, InReg never references a stub. 
    self.inst(brk(0x111));
    self.inst(brk(0x222));
    new_got_slot: ?Fixup = .None;
    fixup_arm64(self.m, symbol, patch&, new_got_slot&);
    if new_got_slot { it |
        push_fixup(self.m, symbol, it);
    };
}

fn loadcon(self: *ArmState, c: *Qbe.Con, r: i64, k: Qbe.Cls) void #once = {
    w := is_wide(k);
    n := c.bits.i;
    if c.type == .CAddr {
        self.loadaddr_bits(c, r);
        if !w {
            @eprintln("# Taking the low 32 bits of the symbol '%', you can do that but it's an odd choice so perhaps was a mistake $%", self.m.str(c.sym.id), self.f.name());
            // we could warn on this if we were feeling friendly. 
            i := make_ins(.extuw, .Kw, TMP(r+1), TMP(r+1), QbeNull);
            self.emitins(i&, zeroed(*Qbe.Blk));
        };
        return();
    };
    @debug_assert(c.type == .CBits, "bad constant");
    s := Bits.X64;
    if !w {
        s = .W32;
        n = n.bit_and(1.shift_left(32) - 1);
    };
   
    // fast path for small positive
    if n.bit_and(0xffff) == n { 
        self.inst(movz(s, r, n.trunc(), .Left0));
        return();
    };
    // fast path for small negative
    if n.bit_or(0xffff) == -1 { // high bits are all ones
        self.inst(movn(s, r, n.bit_not().trunc(), .Left0));
        return();
    };
    // TODO: qbe uses arm64_logimm to get more cases (but we need more instruction encoding logic for that).
    // TODO: choose best instruction to start with movz/movn instead of assuming movz

    // this uses movz for the first chunk and then movk for the rest. 
    first := true;
    for_enum Hw { shift_encode | 
        part: u16 = n.bit_and(0xffff).trunc();
        n = n.shift_right_logical(16);
        if part != 0 {
            op := if first {
                first = false;
                movz(s, r, part, shift_encode)
            } else {
                movk(s, r, part, shift_encode)
            };
            self.inst(op);
        };
    };
}

fn fixarg(self: *ArmState, pr: *Qbe.Ref, sz: i64) void = {
    r := pr[];
    if rtype(r) == .RSlot {
        s := self.slot(r);
        // you're actually allowed unaligned accesses, you just can't encode them in the immediate. 
        // qbe's stack slot allocating doesn't care about ensuring 8 byte slots are aligned,
        // because it never folds offsets into the immediate space (ie use RSlot in .load directly)
        // except for those inserted by reload_spilled. 
        // maybe a better fix for this would be adding a padding slot between the 4 byte and 8 byte slot groups in isel. 
        // but you hit the bad case exactly twice when self compiling so meh -- Oct 31. 
        unaligned := s.mod(sz) != 0;  
        out_of_range := s > sz * 4095;
        if out_of_range || unaligned {
            i := make_ins(.addr, .Kl, TMP(Arm64Reg.IP0.raw()), r, QbeNull);
            self.emitins(i&, zeroed(*Qbe.Blk));
            pr[] = TMP(Arm64Reg.IP0.raw());
        }
    }
}

// TODO: fix non-i64 fn ordinal
fn get_name(o: Qbe.O) Str = {
    names :: Qbe.O.get_enum_names();
    idx := @as(i64) intcast(@as(i32) o.raw());
    names[idx]
}

fn check_args(i: *Qbe.Ins) void = {
    @debug_assert(rtype(i.arg&[0]) == .RTmp && rtype(i.arg&[1]) == .RTmp, "TODO: args need reg. op=%", i.op().get_name());
}

//
// PLEASE DO NOT THE FUCKING PLATFORM REGISTER!
// "It should not be assumed that treating the register as callee-saved will be sufficient to satisfy the requirements of the platform."
// It gets stomped on context switch. Which can happen between arbitrary instructions. 
// i.e. `mov x18, XX; mov XX, YY; mov YY, x18;` is not atomic.
// This is not a theoretical problem. It happens frequently on m1 macos 14.6.1 at the very least.
// So instead of using r18 we use r17. 
// Which is an intra-procedure-call temporary register for the linker to use so its fine as a temporary. 
//
arm_scratch_int :: Arm64Reg.IP1;

fn emitins(self: *ArmState, i: *Qbe.Ins, b: *Qbe.Blk) void = {
    ::if(u32);
    F :: @Fn(s: FType, dest: u5, a: u5, b: u5) u32;
    I :: @Fn(s: Bits, dest: u5, a: u5, b: u5) u32;
    inst_name := i.op().get_name();
    w := i.cls().is_wide();
    si := @if(w, Bits.X64,  Bits.W32);
    sf := @if(w, FType.D64,  FType.S32);
    a0, a1 := (i.arg&[0], i.arg&[1]);
    
    bin :: fn($if_float: F, $if_int: I) => {
        i.check_args();
        op := if i.cls().is_int() {
            if_int(si, i.to.int_reg(), a0.int_reg(), a1.int_reg())
        } else {
            if_float(sf, i.to.float_reg(), a0.float_reg(), a1.float_reg())
        };
        self.inst(op);
    };
    int_bin :: fn($if_int: I) => {
        i.check_args();
        op := if_int(si, i.to.int_reg(), a0.int_reg(), a1.int_reg());
        self.inst(op);
    };
    int_un :: fn($int: @Fn(s: Bits, dest: u5, src: u5) u32) => {
        @debug_assert(rtype(a0) == .RTmp, "TODO: args need reg %", inst_name);
        op := int(si, i.to.int_reg(), a0.int_reg());
        self.inst(op);
    };

    if maybe_load(i) { size |
        self.fixarg(a0&, size.intcast());
        offset := 0;
        addr := if rtype(a0) == .RSlot {
            offset += self.slot(a0);
            @debug_assert(a1 == QbeNull, "i dont think i mix offset and slot in ldr");
            // !needs_frame can have RSlot for arguments
            @if(self.needs_frame, fp, sp)
        } else {
            if rtype(a1) == .RInt {   // TODO: could easily get rid of this branch. is that worth always setting it to RInt(0)?
                offset += zext(rsval(a1));
            };
            a0.int_reg()
        };
        @debug_assert_eq(offset.mod(size.zext()), 0, "load offset % not multiple of size % for %", offset, size, i.op());
        offset /= size.zext();
        @debug_assert_eq(offset.bit_and(1.shift_left(12) - 1), offset, "can't encode offset in $%", self.f.name());
        // TODO: u/s h/b
        if i.cls().is_int() { // TODO: is this wrong?
            s := trailing_zeros(size.zext());
            o := encode_int_load(i.cls().is_wide(), i.op(), s, i.to.int_reg(), @as(u12) offset, @as(u5) addr);
            self.inst(o);
        } else {
            self.inst(f_ldr_uo(si /* not sf*/, i.to.float_reg(), addr, @as(u12) offset));
        };
        return();
    };
    if maybe_store(i) { size |
        self.fixarg(a1&, size.intcast());
        offset := 0;
        addr := @match(rtype(a1)) {
            fn RSlot() => {
                offset = self.slot(a1) / size.zext();
                // !needs_frame can have RSlot for arguments
                @if(self.needs_frame, fp, sp)
            }
            fn RTmp() => a1.int_reg();
            fn RMem() => {
                m := self.f.get_memory(a1);
                offset = m.offset.bits.i / size.zext();
                @debug_assert_eq(offset * size.zext(), m.offset.bits.i, "store offset not a multiple of size");
                m.base.int_reg()
            }
            @default => panic("bad address type for store");
        };
        @debug_assert_eq(offset.bit_and(1.shift_left(12) - 1), offset, "can't encode offset in $%", self.f.name());
        if i.op() == .stores {
            self.inst(f_str_uo(Bits.W32, @as(u5) a0.float_reg(), addr, @as(u12) offset));
            return();
        };
        if i.op() == .stored {
            self.inst(f_str_uo(Bits.X64, @as(u5) a0.float_reg(), addr, @as(u12) offset));
            return();
        };
        s := trailing_zeros(size.zext());
        self.inst(@bits(@as(u2) s, 0b11100100, @as(u12) offset, @as(u5) addr, @as(u5) a0.int_reg()));
        return();
    };
    if is_flag(i.op()) {
        flag_base :: Qbe.O.flagieq;  // TODO: use this everywhere
        c: i32 = i.op().raw() - flag_base.raw();
        if c < 0 || c > Qbe.NCmp {
            panic("unhandled flag")
        };
        c = cmpneg(c); // cset is an alias for CSINC so encoding is backwards from what you'd type in an assembler. 
        cond := arm_condition_codes[c.zext()];
        self.inst(cset(si, i.to.int_reg(), cond));
        return();
    };
    if i.op().is_sel_flag() {
        if(i.to == QbeNull, => return()); // TODO: notice this earlier
        c: i32 = i.op().raw() - Qbe.O.selieq.raw();
        cond := arm_condition_codes[c.zext()];
        @debug_assert(rtype(i.to) == .RTmp && rtype(a0) == .RTmp && rtype(a0) == .RTmp, "selCC needs int reg % % %", i.to, a0, a1);
        d, t, f := (i.to.int_reg(), a0.int_reg(), a1.int_reg());
        self.inst(csel(si, d, t, f, cond));
        return();
    };
    
    @match(i.op()) {
        fn nop()    => ();
        fn dbgloc() => self.m.add_debug_info(i, self.next_inst_offset());
        fn copy() => {
            if (i.to == a0, => return()); 
            if (i.to == QbeNull || a0 == QbeNull, => return());  // :ExprLevelAsm clobbers are represented as empty copies
            // this is kinda clunky, we let rega insert copies that reference stack slots and then lower them here. 
            if rtype(i.to) == .RSlot {
                r := i.to;
                if !isreg(a0) {
                    i.to = TMP(arm_scratch_int);
                    self.emitins(i, b);
                    a0 = i.to;
                };
                op := @as(Qbe.O) @as(i32) Qbe.O.storew.raw() + i.cls().raw();
                i[] = make_ins(op, .Kw, i.to, a0, r);
                self.emitins(i, b);
                return();
            };
            @debug_assert(isreg(i.to), "can only copy to a register");
            @match(rtype(a0)) {
                fn RCon() => {
                    c := self.f.get_constant(a0);
                    self.loadcon(c, i.to.int_reg(), i.cls());
                }
                fn RSlot() => {
                    i.set_op(.load);
                    self.emitins(i, b);
                }
                @default => {
                    @debug_assert(i.to.val() != Arm64Reg.PLA.raw(), "please do not the platform register. thank you.");
                    if i.cls().is_int() {
                        self.inst(mov(si, i.to.int_reg(), a0.int_reg())); // they can't refer to SP so its fine
                    } else {
                        self.inst(fmov(sf, i.to.float_reg(), a0.float_reg()));  
                    };
                };
            }
        }
        fn addr() => {
            @debug_assert(rtype(a0) == .RSlot, "can only addr of a stack slot");
            reg := i.to.int_reg();
            s := self.slot(a0);
            @debug_assert(s >= 0, "slot must be positive");
            // !needs_frame can have RSlot for arguments
            base := @if(self.needs_frame, fp, sp);
            if (s <= 4095) {
                self.inst(add_im(Bits.X64, reg, base, @as(u12) s, 0));
            } else {
                if (s <= 65535) {
                    self.inst(movz(Bits.X64, reg, s.trunc(), .Left0));
                    self.inst(add_er(Bits.X64, reg, base, reg));
                } else {
                    // :StackSizeLimit
                    self.inst(movz(Bits.X64, reg, s.trunc(), .Left0));
                    self.inst(movk(Bits.X64, reg, s.shift_right_logical(16).trunc(), .Left16));
                    self.inst(add_er(Bits.X64, reg, base, reg));
                };
            };
        }
        fn call() => {
            callee := a0;
            if rtype(callee) == .RCon {
                c := self.f.get_constant(a0);
                if c.type != .CAddr || c.bits.i != 0 {
                    panic("invalid call argument");
                };
                
                use_symbol(self.m, c.sym.id) { symbol |
                    patch_at := self.m.segments&[.Code].next;
                    @match(symbol.kind) {
                        fn DynamicPatched() => {
                            new_got_slot: ?Fixup = .None;
                            patch: Fixup = (patch_at = patch_at, type = .Call);
                            self.inst(brk(0x987));
                            if self.m.goal.type != .JitOnly { 
                                push_fixup(self.m, symbol, patch); 
                            };
                            fixup_arm64(self.m, symbol, patch&, new_got_slot&);
                            if new_got_slot { it |
                                push_fixup(self.m, symbol, it);
                            };
                        }
                        fn Local() => {
                            // already compiled it so branch there directly
                            distance := self.m.distance_between(.Code, self.m.current_offset(.Code), symbol.segment, symbol.offset) / 4;
                            assert(distance.abs() < 1.shift_left(26), "can't call that far but I don't let modules be that big yet.");
                            self.inst(b_jmp(s_trunc(distance, 26), 1));
                            // :canireallynothavecodeberelative
                            // TODO: I don't have a test that requires this 
                            //       but i think you do because you need the other ones?
                            if self.m.goal.type == .Relocatable { 
                                push_fixup(self.m, symbol, (patch_at = patch_at, type = .Call));
                            }; 
                        };
                        fn Pending() => {
                            if self.m.got_indirection_instead_of_patches {
                                dest_reg := TMP(arm_scratch_int).int_reg();
                                self.immediate_fixup_got(dest_reg, 0, symbol);
                                self.inst(br(dest_reg, 1));
                            } else {
                                // haven't compiled it yet so will need a patch. 
                                push_fixup(self.m, symbol, (patch_at = patch_at, type = .Call));
                                self.inst(brk(0x789));
                                if self.m.debug["T".char()] {
                                    @fmt_write(self.m.debug_out, "# pending call % calls %\n", u8.int_from_ptr(patch_at), symbol.name);
                                };
                            };
                        }
                    };
                };
            } else {
                self.inst(br(callee.int_reg(), 1));
            };
        }
        fn salloc() => {
            @debug_assert(self.needs_frame);
            self.inst(sub_er(Bits.X64, sp, sp, a0.int_reg()));
            if i.to != QbeNull {
                self.inst(sub_im(Bits.X64, i.to.int_reg(), sp, 0, 0)); // :SpEncoding
            }
        }
        // TODO: these could kinda be data, the encodings are the same for many of them.
        fn add() => @match(rtype(a1)) {
            fn RTmp() => {
                bin(@as(F) fadd, fn(s, d, a, b) => if(a == sp, => add_er(s, d, a, b), => add_sr(s, d, a, b, .LSL, 0))); // could always use *_er but then the disassembly looks dumb
            }
            fn RCon() => {
                c := self.f.get_constant(a1);
                @debug_assert(is_int(i.cls()) && c.bits.i.bit_and(1.shift_left(12) - 1) == c.bits.i && c.type == .CBits);
                self.inst(add_im(si, i.to.int_reg(), a0.int_reg(), @as(u12) c.bits.i, 0));
            }
            fn RMem() => {
                @debug_assert(is_int(i.cls()));
                m := self.f.get_memory(a1);
                self.inst(madd(si, i.to.int_reg(), m.base.int_reg(), m.index.int_reg(), a0.int_reg()));
            }
            @default => panic("bad add operand");
        };
        fn sub() => {
            if self.f.get_int(a1) { imm |
                @debug_assert(is_int(i.cls()) && imm.bit_and(1.shift_left(12) - 1) == imm);
                self.inst(sub_im(si, i.to.int_reg(), a0.int_reg(), @as(u12) imm, 0));
                return();
            };
            bin(@as(F) fsub, fn(s, d, a, b) => if(a == sp, => sub_er(s, d, a, b), => sub_sr(s, d, a, b, .LSL, 0))); // ^
        }
        fn mul()  => bin(@as(F) fmul, fn(s, d, a, b) => madd(s, d, a, b, xzr));
        fn div()  => bin(@as(F) fdiv, fn(s, d, a, b) => sdiv(s, d, a, b)); // TODO: if you pass it here you force all calls to be inline :FUCKED
        fn udiv() => int_bin(fn(s, d, a, b) => udiv(s, d, a, b));
        fn sar()  => encode_shift(self, i, 0b0, 0b10, fn(imm, mask) => (imm, mask));
        fn shr()  => encode_shift(self, i, 0b1, 0b01, fn(imm, mask) => (imm, mask));
        fn shl()  => encode_shift(self, i, 0b1, 0b00, fn(imm, mask) => (mask + 1 - imm, mask - imm));
        fn or()   => int_bin(fn(s, d, a, b) => orr(s, d, a, b, .LSL, 0));
        fn xor()  => int_bin(fn(s, d, a, b) => eor(s, d, a, b, .LSL, 0));
        fn and()  => int_bin(fn(s, d, a, b) => and_sr(s, d, a, b, .LSL, 0));
        fn rem()  => encode_rem(self, i, 0b1);
        fn urem() => encode_rem(self, i, 0b0);
        fn extsb()  => int_un(fn(s, d, a) => bfm(s, 0b0, 0b000000, 0b000111, d, a)); // sxtb
        fn extub()  => int_un(fn(s, d, a) => bfm(s, 0b1, 0b000000, 0b000111, d, a)); // uxtb
        fn extsh()  => int_un(fn(s, d, a) => bfm(s, 0b0, 0b000000, 0b001111, d, a)); // sxth
        fn extuh()  => int_un(fn(s, d, a) => bfm(s, 0b1, 0b000000, 0b001111, d, a)); // uxth
        fn extsw()  => int_un(fn(s, d, a) => bfm(s, 0b0, 0b000000, 0b011111, d, a)); // sxtw
        // note: mov w0, w0 is not a nop! it zeros the high bits. 
        fn extuw()  => int_un(fn(s, d, a) => mov(.W32, d, a));
        fn exts()   => self.inst(fcnv(.S32, .D64, i.to.float_reg(), a0.float_reg()));
        fn truncd() => self.inst(fcnv(.D64, .S32, i.to.float_reg(), a0.float_reg()));
        fn swap() => {
            if i.cls().is_int() {
                a, b := (a0.int_reg(), a1.int_reg());
                r := arm_scratch_int.int_id();
                @debug_assert_ne(@as(i64) r, @as(i64) a);
                @debug_assert_ne(@as(i64) r, @as(i64) b);
                self.inst(mov(si, r, a)); 
                self.inst(mov(si, a, b));
                self.inst(mov(si, b, r));
            } else {
                a, b := (a0.float_reg(), a1.float_reg());
                // we can't refer to v31 because we waste a slot on 0 so this will never stomp anything // :V31
                self.inst(fmov(sf, 31, a));
                self.inst(fmov(sf, a, b));
                self.inst(fmov(sf, b, 31));
            };
        }
        fn acmp()  => encode_cmp(self, i, 0b1);
        fn acmn()  => encode_cmp(self, i, 0b0);
        fn afcmp() => {
            @debug_assert(rtype(a0) == .RTmp && rtype(a1) == .RTmp, "fcmp needs reg");
            self.inst(fcmpe(sf, a0.float_reg(), a1.float_reg()));
        }
        fn cast() => {
            src, dest, bit := @if(i.cls().is_int(),
                (a0.float_reg(), i.to.int_reg(), 0b0),
                (a0.int_reg(), i.to.float_reg(), 0b1),
            );
            si, sf := @if(i.cls().is_wide(),
                (Bits.X64, FType.D64),
                (Bits.W32, FType.S32),
            );
            self.inst(@bits(si, 0b0011110, sf, 0b10, 0b0, 0b11, bit, 0b000000, @as(u5) src, @as(u5) dest));
            return();
        }
        fn stosi() => self.inst(fcvtzs(si, FType.S32, i.to.int_reg(), a0.float_reg()));
        fn dtosi() => self.inst(fcvtzs(si, FType.D64, i.to.int_reg(), a0.float_reg()));
        fn swtof() => self.inst(scvtf(Bits.W32, sf, i.to.float_reg(), a0.int_reg()));
        fn uwtof() => self.inst(@bits(Bits.W32, 0b0011110, sf, 0b100011000000, a0.int_reg(), i.to.float_reg())); // ucvtf
        fn ultof() => self.inst(@bits(Bits.X64, 0b0011110, sf, 0b100011000000, a0.int_reg(), i.to.float_reg())); // ucvtf
        fn stoui() => self.inst(@bits(si,  0b0011110, FType.S32, 0b111001000000, a0.float_reg(), i.to.int_reg())); // fcvtzu
        fn dtoui() => self.inst(@bits(si,  0b0011110, FType.D64, 0b111001000000, a0.float_reg(), i.to.int_reg())); // fcvtzu
        fn sltof() => self.inst(scvtf(Bits.X64, sf, i.to.float_reg(), a0.int_reg()));
        fn neg() => {
            if i.cls().is_int() {
                self.inst(@bits(si, 0b1001011000, a0.int_reg(), 0b000000, 0b11111, i.to.int_reg()));
            } else {
                self.inst(@bits(0b00011110, sf, 0b100001010000, a0.float_reg(), i.to.float_reg()));  // fneg
            };
        }
        fn asm() => {
            imm := self.f.get_int(a0).expect("op 'asm' arg is constant int");
            self.inst(imm.trunc());
        }
        fn syscall()  => {
            // HACK TODO: do this in abi but its super painful
            if self.f.globals.target.apple {
                i[] = make_ins(.copy, .Kl, TMP(Arm64Reg.IP0), TMP(Arm64Reg.R8), QbeNull); self.emitins(i, b); 
            };
            self.inst(@bits(0b11010100, 0b000, @as(u16) 0, 0b000, 0b01));  // SVC
        }
        fn byteswap() => int_un(fn(s, d, a) => @bits(s, 0b10110101100000000001, s, a, d)); // REV
        fn bitswap()  => int_un(fn(s, d, a) => @bits(s, 0b101101011000000000000, a, d)); // rbit
        fn clz()      => int_un(fn(s, d, a) => @bits(s, 0b101101011000000000100, a, d));
        fn rotr() => {
            w  := i.cls().is_wide();
            if self.f.get_int(a1) { imm |
                mask := 1.shift_left(5 + w.int()) - 1;
                imm  := imm.bit_and(mask);
                d, a := (i.to.int_reg(), a0.int_reg());
                self.inst(@bits(si, 0b00100111, si, 0b0, @as(u5) a, @as(u6) imm, @as(u5) a, @as(u5) d));  // EXTR
            } else {
                self.inst(@bits(si, 0b0011010110, @as(u5) a1.int_reg(), 0b001011, @as(u5) a0.int_reg(), @as(u5) i.to.int_reg())); 
            };
        }
        fn ones() => {
            // count the ones in each byte and then sum the results 
            self.inst(@bits(0b0, 0b0, 0b0, 0b01110, 0b00, 0b10000, 0b00101, 0b10, @as(u5) a0.float_reg(), @as(u5) i.to.float_reg())); // CNT
            self.inst(@bits(0b0, 0b0, 0b1, 0b01110, FType.S32, 0b11000, 0b00011, 0b10, @as(u5) i.to.float_reg(), @as(u5) i.to.float_reg())); // UADDLV
        }
        fn cas1() => {
            // This is painful because the instruction reads and writes to the same register. 
            // I don't know a sane way of expressing that constraint for rega().
            // what you want to do is `if(old_in != res) copy(res <- old_in);` like you do for x64, 
            // but it's hard because there's a bunch of arguments. so what if p/new is also in res?
            m := self.f.get_memory(i.arg&[0]);
            k := i.cls();
            r17 := TMP(arm_scratch_int);
            if i.to == QbeNull {
                i.to = r17;
            };
            p, new, old_in, res := (i.arg&[1], m.index, m.base, i.to);
            real_res := res;
            if res != old_in {
                if res == new || res == p {
                    res = r17;
                };
                i[] = make_ins(.copy, k, res, old_in, QbeNull); self.emitins(i, b); 
            };
            rs, rt, rn := (@as(u5) res.int_reg(), @as(u5) new.int_reg(), @as(u5) p.int_reg());
            self.inst(@bits(0b1, si, 0b0010001, 0b0, 0b1, rs, 0b0, 0b11111, rn, rt)); 
            if res != real_res {
                i[] = make_ins(.copy, k, real_res, res, QbeNull); self.emitins(i, b); 
            };
        }
        fn assert() => {
            // TODO: change the direction of the jump and only have one fail place. 
            i[] = make_ins(.acmp, .Kw, QbeNull, a0, QbeConZero);
            self.emitins(i, b);
            self.inst(b_cond(s_trunc(2, 19), .NE));
            self.inst(brk(0x9801));
        }
        fn trace_start() => {
            i[] = make_ins(.copy, .Kl, i.to, TMP(Arm64Reg.FP), QbeNull);
            self.emitins(i, b);
        }
        @default => {
            printfn(self.f, self.m.debug_out);
            @panic("TODO: op encoding %", inst_name);
        };
    } // early returns
}

// TODO: silly
fn find_cas(i: *Qbe.Ins, b: *Qbe.Blk) Ty(*Qbe.Ins, *Qbe.Ins) = {
    // might not be directly the next instruction if we had to fixarg
    @match(i.op()) {
        fn cas0() => {
            icas1 := i;
            while => icas1.op() != .cas1 {  
                @debug_assert(!identical(icas1, b.ins.first.offset(b.nins.zext())), "missing cas1");
                icas1 = icas1.offset(1);
            };
            (i, icas1)
        }
        fn cas1() => {
            icas0 := i;
            while => icas0.op() != .cas0 {  
                @debug_assert(!identical(icas0, b.ins.first), "missing cas0");
                icas0 = icas0.offset(-1);
            };
            (icas0, i)
        }
        @default => panic("expected cas");
    }
}

fn encode_int_load(wide: bool, o: Qbe.O, log2_size: i64, dest: u5, offset: u12, addr: u5) u32 = {
    si_inv := @if(wide, Bits.W32, Bits.X64);
    @match(o) {
        fn loadsw() => {
            ::if(u32);
            if wide {
                @bits(0b10, 0b11100110, offset, addr, dest)
            } else {
                @bits(@as(u2) log2_size, 0b11100101, offset, addr, dest)
            }
        }
        fn loadsh() => @bits(0b01, 0b1110011, si_inv, offset, addr, dest);
        fn loadsb() => @bits(0b00, 0b1110011, si_inv, offset, addr, dest);
        @default => @bits(@as(u2) log2_size, 0b11100101, offset, addr, dest);
    }
}

fn encode_cmp(self: *ArmState, i: *Qbe.Ins, $no_negate: u1) void = {
    si: Bits = @if(i.cls().is_wide(), .X64, .W32);
    a0, a1 := (i.arg&[0], i.arg&[1]);
    @debug_assert(rtype(a0) == .RTmp, "cmp fst needs reg");
    if rtype(a1) == .RTmp {
        a, b := (a0.int_reg(), a1.int_reg());
        self.inst(@bits(si, no_negate, 0b101011, Shift.LSL, 0b0, b, 0b000000, a, 0b11111)); 
    } else {
        c := self.f.get_constant(a1);
        @debug_assert(c.type == .CBits, "cmp to addr");
        no_shift := c.bits.i.bit_and(1.shift_left(12) - 1) == c.bits.i;
        encoded  := if(no_shift, => c.bits.i, => c.bits.i.shift_right_logical(12));
        @debug_assert_eq(encoded.bit_and(1.shift_left(12) - 1).shift_left(int(!no_shift) * 12), c.bits.i, "failed encode cmp");
        self.inst(@bits(si, no_negate, 0b1100010, @if(no_shift, 0b0, 0b1), @as(u12) encoded, a0.int_reg(), 0b11111));
    }
}

fn encode_shift(self: *ArmState, i: *Qbe.Ins, $unsigned: u1, $opcode: u2, $immr_imms: @Fn(imm: i64, mask: i64) Ty(u6, u6)) void = {
    w  := i.cls().is_wide();
    si: Bits = @if(w, .X64, .W32);
    a0, a1 := (i.arg&[0], i.arg&[1]);
    if self.f.get_int(a1) { imm |
        mask := 1.shift_left(5 + w.int()) - 1;
        imm := imm.bit_and(mask);
        d, a := (i.to.int_reg(), a0.int_reg());
        if imm == 0 {
            self.inst(mov(si, d, a)); 
        } else {
            immr, imms := immr_imms(imm, mask);
            self.inst(bfm(si, unsigned, immr, imms, d, a));
        };
    } else {
        i.check_args();
        dest, val, shift := (i.to.int_reg(), a0.int_reg(), a1.int_reg());
        self.inst(@bits(si, 0b0011010110, shift, 0b0010, opcode, val, dest));
    };
}

fn encode_rem(self: *ArmState, i: *Qbe.Ins, $signed: u1) void = {
    si: Bits = @if(i.cls().is_wide(), .X64, .W32);
    dest, dividend, divisor, scratch := (i.to.int_reg(), i.arg&[0].int_reg(), i.arg&[1].int_reg(), arm_scratch_int.int_id());
    self.inst(@bits(si, 0b0011010110, divisor, 0b00001, signed, dividend, scratch));  // scratch = dividend / divisor;
    self.inst(msub(si, dest, scratch, divisor, dividend));                            // dest = dividend - scratch * divisor;
}

// same as qbe
/*

  Stack-frame layout:

  +=============+
  | varargs     |
  |  save area  |
  +-------------+
  | callee-save |  ^
  |  registers  |  |
  +-------------+  |
  |    ...      |  |
  | spill slots |  |
  |    ...      |  | e->frame
  +-------------+  |
  |    ...      |  |
  |   locals    |  |
  |    ...      |  |
  +-------------+  |
  | e->padding  |  v
  +-------------+
  |  saved x29  |
  |  saved x30  |
  +=============+ <- x29

*/

fn inst(self: *ArmState, opcode: u32) void = {
    code := self.m.segments&[.Code]&;
    ptr_cast_unchecked(u8, u32, code.next)[] = opcode;  // SAFETY: aligned becuase we're always emitting arm instructions
    code.next = code.next.offset(4);
}

fn next_inst_offset(self: *ArmState) i64 = {
    code := self.m.segments&[.Code]&;
    ptr_diff(code.mmapped.ptr, code.next) - self.start_of_function
}

fn encode_adrp_add(dest: rawptr, src: *u8, reg: u5) Ty(u32, u32) = {
    dest := int_from_rawptr(dest);
    src := u8.int_from_ptr(src);
    
    distance := dest - src;
    adr_range :: 1.shift_left(21);
    if dest.abs() < adr_range {  // TODO: untested because i have such big segments
        // if its an import, you need the extra nop so you take up the right amount of space for the relocation when doing AOT. 
        return(encode_adr(distance, reg), arm_nop); 
    };
    
    page_offset := dest.shift_right_logical(12) - src.shift_right_logical(12);
    high := page_offset.shift_right_logical(2).bit_and(1.shift_left(19) - 1);
    low := page_offset.bit_and(0b11);
    bottom12 := @as(u12) @as(i64) dest.bit_and(1.shift_left(12) - 1);
    (adrp(high, low, reg), add_im(Bits.X64, reg, reg, bottom12, 0))
}

fn encode_adr(distance: i64, reg: u5) u32 = {
    high := distance.shift_right_arithmetic(2).bit_and(1.shift_left(19) - 1);
    low := distance.bit_and(0b11);
    adr(high, low, reg)
}

fn encode_adrp_load(dest: *u8, src: *u8, reg: u5) Ty(u32, u32) #once = {
    dest := u8.int_from_ptr(dest);
    src := u8.int_from_ptr(src);
    
    page_offset := dest.shift_right_logical(12) - src.shift_right_logical(12);
    high := page_offset.shift_right_logical(2).bit_and(1.shift_left(19) - 1);
    low := page_offset.bit_and(0b11);
    bottom12 := @as(u12) @as(i64) dest.bit_and(1.shift_left(12) - 1) / 8;
    (adrp(high, low, reg), ldr_uo(Bits.X64, reg, reg, bottom12))
}

fn sub_er(sf: Bits, dest: RegO, a: RegI, b: RegI) u32 =
    @bits(sf, 0b1001011001, b, 0b011 /*option*/, 0b000 /*imm3*/, a, dest);
    
fn add_er(sf: Bits, dest: RegO, a: RegI, b: RegI) u32 =
    @bits(sf, 0b0001011001, b, 0b011 /*option*/, 0b000 /*imm3*/, a, dest);
