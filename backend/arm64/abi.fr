// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! Here we lower the par/arg instructions to be copies from specific registers dictated by the target calling convention. 
//! In the input ir structs are passed as pointers. Some of them get split into registers. 
//! These are the hoops everyone agrees to jump through when you declare something extern C.
//! - https://github.com/ARM-software/abi-aa/releases
//! Also just for fun apple has slightly different rules. 
//! - https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms
//! It's crazy to me that this is more code than register allocation. 
// TODO: i haven't tested the non-apple versions. 
// :Explain but this stuff isn't super subtle, it's just tedius. 

ArgClass :: @enum(u8) (
    CNone = 0,
    Cstk  = 1, /* pass on the stack */
    Cptr  = 2, /* replaced by a pointer */
    CBoth = 3,
);

Class :: @struct(
    class: ArgClass,
    ishfa: bool, // Homogeneous Floating-point Aggregate: <= 4 fields, all the same float type. 
    hfa: @struct(base: Qbe.Cls, size: u8),
    size: i32,
    align: i32,
    t: Qbe.TypHeader,
    nreg: u8,
    ngp: u8,
    nfp: u8,
    reg: Array(Arm64Reg, 4),
    cls: Array(Qbe.Cls, 4), 
);

Params :: @struct(ngp: i32, nfp: i32, stk: i32);

::List(Qbe.O);

// Parameters
gpreg     :: @const_slice(Arm64Reg.R0, .R1, .R2, .R3, .R4, .R5, .R6, .R7);
fpreg     :: @const_slice(Arm64Reg.V0, .V1, .V2, .V3, .V4, .V5, .V6, .V7);

RCallArm :: @bit_fields(
    gp_ret   := 2,
    fp_ret   := 3,
    gp_arg   := 4,
    fp_arg   := 4,
    indirect := 1,  // indirect result register x8 used   (alternatively used to pass the syscall number)
    env      := 1,  // env pointer passed in x9
    _        := 14,
    tag      := 3,
);

fn isfloatv(t: *Qbe.Typ, cls: *Qbe.Cls, count: *u8, globals: *QbeModule) bool = {
    ::enum_basic(Qbe.FieldType);
    ::ptr_utils(Qbe.Typ);
    i := 0;
    biggest := 0;
    for_unions(t, i&) { 
        case: u8 = 0;
        for_fields(t, i&) { type, len |
            @match(type) {
                fn Fs() => {
                    if(cls[] == .Kd, => return(false));
                    cls[] = .Ks;
                    case += 1;
                }
                fn Fd() => {
                    if(cls[] == .Ks, => return(false));
                    cls[] = .Kd;
                    case += 1;
                }
                fn FTyp() => {
                    type := globals.get_type(len.zext());
                    if !isfloatv(type, cls, case&, globals) {
                        return(false);
                    }
                }
                @default => return(false);
            };
        };
        biggest = biggest.max(case.zext());
    };
    count[] += biggest.trunc();  // TODO: overflow
    count[] <= 4
}

fn typclass(c: *Class, t: *Qbe.Typ, gp: []Arm64Reg, fp: []Arm64Reg, globals: *QbeModule) void = {
    sz := align_to(t.header.size, 8);
    c.t = t.header;
    c.class = .CNone;
    c.ngp = 0;
    c.nfp = 0;
    c.align = 8;

    @assert(t.header.align_log2 <= 3, "alignments larger than 8 are not supported");

    c.hfa.base = .Kx;
    c.hfa.size = 0;
    unsized := t.is_dark() || sz == 0;
    c.ishfa = !unsized && isfloatv(t, c.hfa.base&, c.hfa.size&, globals);
    
    if !c.ishfa && (unsized || sz > 16) {
        /* large structs are replaced by a
         * pointer to some caller-allocated
         * memory */
        c.hfa.size = 0;
        c.class&.with(.Cptr);
        c.size = 8;
        c.ngp = 1;
        c.reg&[0] = gp.index_unchecked(0)[];
        c.cls&[0] = .Kl;
        return();
    };

    c.size = bitcast sz;

    if c.ishfa {
        if ((@as(i64) c.hfa.size.zext()) != t.header.size.zext() / if(is_wide(c.hfa.base), => 8, => 4)) {
            type_index := ptr_diff(globals.types.ptr, t);
            print_type(globals, type_index, globals.debug_out);
            @panic("hfa size wrong! % % %", c.hfa.size, t.header.size, if(is_wide(c.hfa.base), => 8, => 4));
        };
        range(0, c.hfa.size.zext()) { n |
            c.reg&[n] = fp.index_unchecked(0)[];
            fp = fp.slice_pop_first();
            c.cls&[n] = c.hfa.base;
            c.nfp += 1;
        };
        c.nreg = c.nfp;
    } else {
        c.hfa.size = 0;
        range(0, sz.zext() / 8) { n |
            c.reg&[n] = gp.index_unchecked(0)[];
            gp = gp.slice_pop_first();
            c.cls&[n] = .Kl;
            c.ngp += 1;
        };
        c.nreg = c.ngp;
    };
}

fn sttmps(tmp: *Qbe.Ref, cls: []Qbe.Cls, nreg: u32, mem: Qbe.Ref, f: *Qbe.Fn) void = {
    @debug_assert(nreg <= 4, "too many reg");
    off := 0;
    range(0, nreg.zext()) { n | 
        tmp.offset(n)[] = f.newtmp("abi", cls[n]);
        r := f.newtmp("abi", .Kl);
        o := cls[n].store_op();
        if off == 0 { 
            f.emit(o, .Kw, QbeNull, tmp.offset(n)[], mem);
        } else {
            f.emit(o, .Kw, QbeNull, tmp.offset(n)[], r);
            f.emit(.add, .Kl, r, mem, f.getcon(off));
        };
        off += if(is_wide(cls[n]), => 8, => 4);
    };
}

/* todo, may read out of bounds: 
 * If the size of the struct is not a multiple of 8, the actual struct
 * size may be different from the size reserved on the stack.
*/
fn ldregs(reg: *Arm64Reg, cls: *Qbe.Cls, n: u8, mem: Qbe.Ref, f: *Qbe.Fn) void = {
    off := 0;
    range(0, n.zext()) { i |
        if off == 0 { 
            f.emit(.load, cls.offset(i)[], TMP(reg.offset(i)[]), mem, QbeNull);
        } else {
            r := f.newtmp("abi", .Kl);
            f.emit(.load, cls.offset(i)[], TMP(reg.offset(i)[]), r, QbeNull);
            f.emit(.add, .Kl, r, mem, f.getcon(off));
        };
        off += if(is_wide(cls.offset(i)[]), => 8, => 4);
    }
}

fn selret(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    j := b.jmp.type;
    if(!is_ret(j) || j == .ret0, => return());

    r := b.jmp.arg;
    b.jmp.type = .ret0;
    
    ::if(Ty(i64, i64));
    gp, fp := if j == .retc {
        @debug_assert_ne(f.retty, QbeNull, "retc without type");
        m := f.globals;
        type := f.get_type(f.retty);
        cr := Class.zeroed();
        typclass(cr&, type, gpreg, fpreg, f.globals);
        if cr.class.has(.Cptr) {
            @debug_assert(rtype(f.retr) == .RTmp, "see // :SetRetr");
            f.emit(.blit1, .Kw, QbeNull, INT(zext cr.t.size), QbeNull);
            f.emit(.blit0, .Kw, QbeNull, r, f.retr);
            (0, 0)
        } else {
            ldregs(cr.reg&.as_ptr(), cr.cls&.as_ptr(), cr.nreg, r, f);
            (cr.ngp.zext(), cr.nfp.zext())
        }
    } else {
        k := j.cls();
        R := if(is_int(k), => TMP(Arm64Reg.R0), => TMP(Arm64Reg.V0));
        f.emit(.copy, k, R, r, QbeNull);
        (int(is_int(k)), int(!is_int(k)))
    };

    cty := RCallArm.zeroed();
    set(cty&, .gp_ret, gp);
    set(cty&, .fp_ret, fp);
    b.jmp.arg = CALL(cty);
}

fn argsclass(i0: *Qbe.Ins, i1: *Qbe.Ins, carg: *Class, f: *Qbe.Fn) RCallArm = {
    va   := false;
    envc := 0;
    gp   := gpreg;
    fp   := fpreg;
    ngp  := 8;
    nfp  := 8;
    c := carg.offset(-1);
    for(i0, i1) { i |
        next :: local_return;
        c = c.offset(1);
        
        o := i.op();
        @debug_assert(!is_parbh(o) && !is_argbh(o), "abi0");
        is_argpar := @is(o, .par, .arg);
        if is_argpar {
            c.size = 8;
            if f.globals.target.apple && !is_wide(i.cls()) {
                c.size = 4;
            };
            
            c.align = c.size;
            c.cls&[0] = i.cls();
            if va {
                c.class&.with(.Cstk);
                c.size = 8;
                c.align = 8;
                next();
            };
            if KBASE(i.cls()) == 0 && ngp > 0 {
                ngp -= 1;
                c.reg&[0] = gp[0];
                gp = gp.slice_pop_first();
                next();
            };
            if KBASE(i.cls()) == 1 && nfp > 0 {
                nfp -= 1;
                c.reg&[0] = fp[0];
                fp = fp.slice_pop_first();
                next();
            };
            c.class&.with(.Cstk);
            next();
        };
        if o == .parc || o == .argc {
            typclass(c, f.get_type(i.arg&[0]), gp, fp, f.globals);
            if c.ngp.zext() <= ngp {
                if c.nfp.zext() <= nfp {
                    ngp -= c.ngp.zext();
                    nfp -= c.nfp.zext();
                    gp = gp.slice_pop_from_start(c.ngp.zext());
                    fp = fp.slice_pop_from_start(c.nfp.zext());
                    next();
                } else { 
                    nfp = 0;
                }
            } else {
                ngp = 0;
            };
            c.class&.with(.Cstk);
            next();
        };
        
        if o == .pare || o == .arge {
            c.reg&[0] = .R9;
            c.cls&[0] = .Kl;
            envc = 1;
            next();
        };
        if o == .argv {
            va = f.globals.target.apple;
            next();
        };
        @panic("bad inst for argsclass")
    };
    
    // note: gp.len not the same as ngp becuase we clamped to 0. 
    r := RCallArm.zeroed();
    set(r&, .gp_arg, 8-gp.len);
    set(r&, .fp_arg, 8-fp.len);
    set(r&, .env, envc);
    r
}

fn slice_pop_from_start(s: []Arm64Reg, n: i64) []Arm64Reg = {
    s.ptr = s.ptr.offset(n);
    s.len -= n;
    s
}

fn arm64_retregs(r: Qbe.Ref, p: *Array(i32, 2)) u64 = {
    r := to_rcall(r, RCallArm);
    ngp := get(r, .gp_ret);
    nfp := get(r, .fp_ret);
    ::ptr_utils(@type p[]);
    if !p.is_null() {
        p[0] = ngp.intcast();
        p[1] = nfp.intcast();
    };
    ngp_mask := (1.shift_left(ngp) - 1).shift_left(zext Arm64Reg.R0.raw());
    nfp_mask := (1.shift_left(nfp) - 1).shift_left(zext Arm64Reg.V0.raw());
    bit_or(ngp_mask, nfp_mask).bitcast()
}

fn arm64_argregs(r: Qbe.Ref, p: *Array(i32, 2)) u64 = {
    ::ptr_utils(Array(i32, 2));
    r := to_rcall(r, RCallArm);
    ngp := get(r, .gp_arg);
    nfp := get(r, .fp_arg);
    x8  := get(r, .indirect);
    x9  := get(r, .env);
    if !p.is_null() {
        p[0] = intcast(ngp + x8 + x9);
        p[1] = nfp.intcast();
    };
    ngp_mask := (1.shift_left(ngp) - 1).shift_left(zext Arm64Reg.R0.raw());
    nfp_mask := (1.shift_left(nfp) - 1).shift_left(zext Arm64Reg.V0.raw());
    b := bit_or(ngp_mask, nfp_mask);
    b  = b.bit_or(x8.shift_left(zext Arm64Reg.R8.raw()));
    b  = b.bit_or(x9.shift_left(zext Arm64Reg.R9.raw()));
    b.bitcast()
}

/*
what about this?

b := BitSet(Arm64Reg).zeroed();
set_many(b&, .R0, ngp);
set_many(b&, .V0, nfp);
set(b&, .R8, x8 == 1);
set(b&, .R9, x9 == 1);
b
*/

fn stkblob(r: Qbe.Ref, c: *Class, f: *Qbe.Fn, il: *List(Qbe.Ins)) void = {
    @debug_assert(r != QbeNull, "stkblob null dest");
    al := (@as(i32) c.t.align_log2.zext()) - 2; /* NAlign == 3 */
    if al < 0 {
        al = 0;
    };
    // If we're pulling the struct out of registers, use the rounded size computed in typclass. 
    sz := if(c.class.has(.Cptr), => c.t.size, => c.size.bitcast());
    il.push(make_ins(@as(Qbe.O) @as(i32) Qbe.O.alloc4.raw() + al, .Kl, r, f.getcon(zext sz), QbeNull));
}

fn align(x: i32, al: i32) i32 #inline = 
    (x + al - 1).bit_and(-al);

// This makes it different from Qbe but works with the way compiler/emit_i.fr generates code. 
ASSUME_NO_ALIAS_ARGS :: true;

selcall :: fn(f: *Qbe.Fn, i0: *Qbe.Ins, i1: *Qbe.Ins, ilp: *List(Qbe.Ins)) void = {
    tmp := Array(Qbe.Ref, 4).zeroed();
    ca := temp().alloc_zeroed(Class, ptr_diff(i0, i1));
    m := f.globals;
    cty := argsclass(i0, i1, ca.ptr, f);

    stk: i32 = 0;
    c := ca.ptr;
    for(i0, i1) { i |
        if c.class.has(.Cptr) {
            if ASSUME_NO_ALIAS_ARGS {
                i.arg&[0] = i.arg&[1];
            } else {
                i.arg&[0] = f.newtmp("abi", .Kl);
                stkblob(i.arg&[0], c, f, ilp);
            };
            i.set_op(.arg);
        };
        if c.class.has(.Cstk) {
            stk = align(stk, c.align);
            stk += c.size;
        };
        c = c.offset(1);
    };
    stk = align(stk, 16);
    rstk := f.getcon(stk.intcast());
    if stk != 0 {
        f.emit(.add, .Kl, TMP(Arm64Reg.SP), TMP(Arm64Reg.SP), rstk);
    };

    if i1.arg&[1] != QbeNull {
        type := f.get_type(i1.arg&[1]);
        cr := Class.zeroed();
        typclass(cr&, type, gpreg, fpreg, f.globals);
        @debug_assert(i1.to != QbeNull, "call returning aggregate must have a destination.");
        stkblob(i1.to, cr&, f, ilp);
        set(cty&, .gp_ret, cr.ngp.zext());
        set(cty&, .fp_ret, cr.nfp.zext());
        if cr.class.has(.Cptr) {
            /* spill & rega expect calls to be
             * followed by copies from regs,
             * so we emit a dummy
             */
            set(cty&, .indirect, 1);
            f.emit(.copy, .Kw, QbeNull, TMP(Arm64Reg.R0), QbeNull);
        } else {
            sttmps(tmp&.as_ptr(), cr.cls&.items(), cr.nreg.zext(), i1.to, f);
            range(0, cr.nreg.zext()) { n |
                r := TMP(cr.reg&[n]);
                f.emit(.copy, cr.cls&[n], tmp&[n], r, QbeNull);
            };
        };
    } else {
        if is_int(i1.cls()) {
            f.emit(.copy, i1.cls(), i1.to, TMP(Arm64Reg.R0), QbeNull);
            set(cty&, .gp_ret, 1);
        } else {
            f.emit(.copy, i1.cls(), i1.to, TMP(Arm64Reg.V0), QbeNull);
            set(cty&, .fp_ret, 1);
        };
    };

    f.emit(.call, .Kw, QbeNull, i1.arg&[0], CALL(cty));

    if get(cty, .indirect) != 0 {
        /* struct return argument */
        f.emit(.copy, .Kl, TMP(Arm64Reg.R8), i1.to, QbeNull);
    };

    c := ca.ptr;
    for(i0, i1) { i | 
        if !c.class.has(.Cstk) {
            if i.op() == .arg || i.op() == .arge {
                f.emit(.copy, c.cls&[0], TMP(c.reg&[0]), i.arg&[0], QbeNull);
            };
            if i.op() == .argc {
                ldregs(c.reg&.as_ptr(), c.cls&.as_ptr(), c.nreg, i.arg&[1], f);
            };
        };
        c = c.offset(1);
    };

    /* populate the stack */
    off: i32 = 0;
    c := ca.ptr;
    for(i0, i1) { i | 
        if c.class.has(.Cstk) {
            off = align(off, c.align);
            r := f.newtmp("abi", .Kl);
            if i.op() == .arg {
                o := @switch(c.size) {
                    @case(4) => c.cls&[0].store_op();
                    @case(8) => c.cls&[0].store_op();
                    @default => panic("unreachable stack size");
                };
                f.emit(o, .Kw, QbeNull, i.arg&[0], r);
            } else {
                @debug_assert(i.op() == .argc, "bad arg op");
                f.emit(.blit1, .Kw, QbeNull, INT(c.size.intcast()), QbeNull);
                f.emit(.blit0, .Kw, QbeNull, i.arg&[1], r);
            };
            f.emit(.add, .Kl, r, TMP(Arm64Reg.SP), f.getcon(off.intcast()));
            off += c.size;
        };
        c = c.offset(1);
    };
    
    if stk != 0 {
        f.emit(.sub, .Kl, TMP(Arm64Reg.SP), TMP(Arm64Reg.SP), rstk);
    };

    c := ca.ptr;
    for(i0, i1) { i | 
        if c.class.has(.Cptr) && !ASSUME_NO_ALIAS_ARGS {
            f.emit(.blit1, .Kw, QbeNull, INT(zext c.t.size), QbeNull);
            f.emit(.blit0, .Kw, QbeNull, i.arg&[1], i.arg&[0]);
        };
        c = c.offset(1);
    }
};

fn selpar(f: *Qbe.Fn, i0: *Qbe.Ins, i1: *Qbe.Ins) Params = {
    tmp := Array(Qbe.Ref, 16).zeroed();
    ca := temp().alloc_zeroed(Class, ptr_diff(i0, i1));
    f.reset_scratch();

    cty := argsclass(i0, i1, ca.as_ptr(), f);
    discard := Array(i32, 2).ptr_from_int(0);
    f.reg = arm64_argregs(CALL(cty), discard);

    il := Qbe.Ins.list(temp());
    t := tmp&.as_ptr();
    c := ca.as_ptr();
    for(i0, i1) { i |
        if i.op() == .parc && c.class == .CNone {
            sttmps(t, c.cls&.items(), c.nreg.zext(), i.to, f);
            stkblob(i.to, c, f, il&);
            t = t.offset(c.nreg.zext());
        };
        c = c.offset(1);
    };
    for_rev il { it |
        f.emit(it);
    };
    
    if f.retty != QbeNull {
        cr := Class.zeroed(); 
        type := f.get_type(f.retty);
        typclass(cr&, type, gpreg, fpreg, f.globals);
        if cr.class.has(.Cptr) {
            f.retr = f.newtmp("abi", .Kl);  // :SetRetr
            r8: i64 = zext Arm64Reg.R8.raw();
            f.emit(.copy, .Kl, f.retr, TMP(r8), QbeNull);
            f.reg = f.reg.bit_or(BIT(r8));
        };
    };

    t := tmp&.as_ptr();
    off: i32 = 0;
    c := ca.as_ptr();
    for(i0, i1) { i | 
        if i.op() == .parc && !c.class.has(.Cptr) {
            if c.class.has(.Cstk) {
                off = align(off, c.align);
                f.tmp[i.to.val()].slot = -(off+2);
                off += c.size;
            } else {
                range(0, c.nreg.zext()) { n |
                    r := TMP(c.reg&[n]);
                    f.emit(.copy, c.cls&[n], t[], r, QbeNull);
                    t = t.offset(1);
                };
            };
        } else {
            if c.class.has(.Cstk) {
                // convert to a load from the stack slot
                off = align(off, c.align);
                @debug_assert(!is_parbh(i.op()), "abi0");
                f.emit(.load, c.cls&[0], i.to, SLOT(-(off+2)), QbeNull);
                off += c.size;
            } else {
                f.emit(.copy, c.cls&[0], i.to, TMP(c.reg&[0]), QbeNull);
            }
        };
        c = c.offset(1);
    };
    
    return(
        stk = align(off, 8),
        ngp = get(cty, .gp_arg).intcast(),
        nfp = get(cty, .fp_arg).intcast(),
    );
}

fn apple_selvaarg(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = 
    @emit_instructions((f = f), (i.to, i.arg&[0], i.cls()), """
    @start
        %stk =l load %1
        %0 =2 load %stk
        %stk8 =l add %stk, 8
        storel %stk8, %1
    """);

// this is very similar to the sysv one but of course not exactly the same. 
fn arm64_selvaarg(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    isint     := is_int(i.cls());
    if_24_28  := f.getcon(if(isint, => 24, => 28));
    if_8_16   := f.getcon(if(isint, => 8, => 16));
    
    @emit_instructions((f = f, b = b), (i.arg&[0], if_24_28, if_8_16, i.to, i.cls()), """
    @b
        %r0a =l add %0, %1
        %nr =l loadsw %r0a
        %r1a =w csltw %nr, 0
        jnz %r1a, @breg, @bstk
    @breg
        %r0b =l add %0, %2
        %r1b =l loadl %r0b
        %lreg =l add %r1b, %nr
        %r0c =w add %nr, %2
        %r1c =l add %0, %1
        storew %r0c, %r1c
        jmp @b0
    @bstk
        %lstk =l loadl %0
        %r1d =l add %lstk, 8
        storel %r1d, %0
        jmp @b0
    @b0
        %loc =l phi @breg %lreg, @bstk %lstk
        %3 =4 load %loc
    """);
}

fn apple_selvastart(f: *Qbe.Fn, p: Params, ap: Qbe.Ref) void = 
    @emit_instructions((f = f), (f.getcon(p.stk.zext()), ap), """
    @start
        %stk =l addr S-1
        %arg =l add %stk, %0
        storel %arg, %1
    """);

fn arm64_selvastart(f: *Qbe.Fn, p: Params, ap: Qbe.Ref) void = 
    @emit_instructions((f = f), (f.getcon((p.nfp.zext()-8)*16), f.getcon((p.ngp.zext()-8)*8), f.getcon(p.stk.zext() + 192), ap), """
    @start
        %ap28 =l add %3, 28
        storew %0, %ap28
        %ap24 =l add %3, 24
        storew %1, %ap24
        %ap16 =l add %3, 16
        %rsave =l addr S-1
        %save192 =l add %rsave, 192
        storel %save192, %ap16
        %ap8 =l add %3, 8
        %save64 =l add %rsave, 64
        storel %save64, %ap8
        %r0 =l add %rsave, %2
        storel %r0, %3
    """);

    
selvastart :: fn(f: *Qbe.Fn, p: Params, ap: Qbe.Ref) void = if f.globals.target.apple {
    apple_selvastart(f, p, ap);
} else {
    arm64_selvastart(f, p, ap);
};

selvaarg :: fn(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = if f.globals.target.apple {
    apple_selvaarg(f, b, i);
} else {
    arm64_selvaarg(f, b, i);
};

fn arm64_abi(f: *Qbe.Fn) void = 
    f.native_abi(Params, selpar, selcall, sel_syscall_arm64, selvastart, selvaarg, selret);

fn has(a: ArgClass, b: ArgClass) bool #inline = {
    ::enum_basic(ArgClass);
    (@as(i64) a.raw().zext()).bit_and(b.raw().zext()) != 0
}

fn with(a: *ArgClass, b: ArgClass) void #inline = {
    a[] = @as(ArgClass) @as(u8) (@as(i64) a[].raw().zext()).bit_or(b.raw().zext()).trunc();
}

fn sel_syscall_arm64(f: *Qbe.Fn, i0: *Qbe.Ins, i1: *Qbe.Ins) void = {
    count := ptr_diff(i0, i1);
    ca := RCallArm.zeroed();
    set(ca&, .indirect, 1);
    set(ca&, .gp_arg, count);
    set(ca&, .gp_ret, 1);
    
    f.emit(.copy, i1.cls(), i1.to, TMP(Arm64Reg.R0), QbeNull);  // result returned in R0 as normal. 
    f.emit(.syscall, i1.cls(), QbeNull, QbeNull, CALL(ca)); 
    f.emit(.copy, .Kl, TMP(Arm64Reg.R8), i1.arg&[0], QbeNull);  // syscall number 
    
    ni := 0;
    for(i0, i1) { i |
        f.emit(.copy, i.cls(), TMP(gpreg[ni]), i.arg&[0], QbeNull);
        ni += 1;
    };
}

fn CALL(r: RCallArm) Qbe.Ref = CALL(@as(i64) r.repr.zext());

#use("@/backend/arm64/target.fr");
#use("@/backend/lib.fr");
#use("@/backend/abi.fr");
