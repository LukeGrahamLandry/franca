// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! Here we lower the par/arg instructions to be copies from specific registers dictated by the target calling convention. 
//! In the input ir structs are passed as pointers. Some of them get split into registers. 
//! These are the hoops everyone agrees to jump through when you declare something extern C.
//! - https://github.com/ARM-software/abi-aa/releases
//! Also just for fun apple has slightly different rules. 
//! - https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms
//! It's crazy to me that this is more code than register allocation. 
// TODO: i haven't tested the non-apple versions. 
// :Explain but this stuff isn't super subtle, it's just tedius. 

ArgClass :: @enum(u8) (
    CNone = 0,
    Cstk  = 1, /* pass on the stack */
    Cptr  = 2, /* replaced by a pointer */
    CBoth = 3,
);

Class :: @struct(
    class: ArgClass,
    ishfa: bool, // Homogeneous Floating-point Aggregate: <= 4 fields, all the same float type. 
    hfa: @struct(base: Qbe.Cls, size: u8),
    size: i32,
    align: i32,
    t: *Qbe.Typ,
    nreg: u8,
    ngp: u8,
    nfp: u8,
    reg: Array(Arm64Reg, 4),
    cls: Array(Qbe.Cls, 4), 
);

Insl   :: @rec @struct(i: Qbe.Ins, link: *Insl);
Params :: @struct(ngp: i32, nfp: i32, stk: i32);

::List(Qbe.O);
gpreg     :: @const_slice(Arm64Reg.R0, .R1, .R2, .R3, .R4, .R5, .R6, .R7);
fpreg     :: @const_slice(Arm64Reg.V0, .V1, .V2, .V3, .V4, .V5, .V6, .V7);
store_ops :: @const_slice(Qbe.O.storew, .storel, .stores, .stored); // :QbeClsi16 order matters

/* layout of call's second argument (RCall)
 *
 *         13
 *  29   14 |    9    5   2  0
 *  |0.00|x|x|xxxx|xxxx|xxx|xx|                  range
 *        | |    |    |   |  ` gp regs returned (0..2)
 *        | |    |    |   ` fp regs returned    (0..4)
 *        | |    |    ` gp regs passed          (0..8)
 *        | |     ` fp regs passed              (0..8)
 *        | ` indirect result register x8 used  (0..1)
 *        ` env pointer passed in x9            (0..1)
 */
 // TODO: go through and use named constants for the magic offsets.
 //       or better would be implement real bit fields and just cast the int whenever its used.  

::enum_basic(Qbe.FieldType);
::ptr_utils(Qbe.Typ);

fn isfloatv(t: *Qbe.Typ, cls: *Qbe.Cls, count: *u8, globals: *QbeModule) bool = {
    f := t.fields[0]&;  
    biggest := 0;
    range(0, t.nunion.zext()) { n |
        case: u8 = 0;
        while => f.type != .FEnd { 
            @match(f.type) {
                fn Fs() => {
                    if(cls[] == .Kd, => return(false));
                    cls[] = .Ks;
                    case += 1;
                }
                fn Fd() => {
                    if(cls[] == .Ks, => return(false));
                    cls[] = .Kd;
                    case += 1;
                }
                fn FTyp() => {
                    type := globals.types[].index(f.len.zext());
                    if !isfloatv(type, cls, case&, globals) {
                        return(false);
                    }
                }
                @default => return(false);
            };
            f = f.offset(1);
        };
        biggest = biggest.max(case.zext());
        f = f.offset(1);
    };
    count[] += biggest.trunc();  // TODO: overflow
    count[] <= 4
}

fn typclass(c: *Class, t: *Qbe.Typ, gp: []Arm64Reg, fp: []Arm64Reg, globals: *QbeModule) void = {
    sz := (t.size + 7).bit_and(-8);  // round up to a 8 byte alignment
    c.t = t;
    c.class = .CNone;
    c.ngp = 0;
    c.nfp = 0;
    c.align = 8;

    @assert(t.align_log2 <= 3, "alignments larger than 8 are not supported");

    c.hfa.base = .Kx;
    c.hfa.size = 0;
    unsized := t.is_dark || sz == 0;
    c.ishfa = !unsized && isfloatv(t, c.hfa.base&, c.hfa.size&, globals);
    
    if !c.ishfa && (unsized || sz > 16) {
        /* large structs are replaced by a
         * pointer to some caller-allocated
         * memory */
        c.hfa.size = 0;
        c.class&.with(.Cptr);
        c.size = 8;
        c.ngp = 1;
        c.reg&[0] = gp.index_unchecked(0)[];
        c.cls&[0] = .Kl;
        return();
    };

    c.size = sz.intcast();

    if c.ishfa {
        if ((@as(i64) c.hfa.size.zext()) != t.size / if(is_wide(c.hfa.base), => 8, => 4)) {
            type_index := ptr_diff(globals.types.first, t);
            print_type(globals, type_index, globals.debug_out);
            @panic("hfa size wrong! % % %", c.hfa.size, t.size, if(is_wide(c.hfa.base), => 8, => 4));
        };
        range(0, c.hfa.size.zext()) { n |
            c.reg&[n] = fp.index_unchecked(0)[];
            fp = fp.slice_pop_first();
            c.cls&[n] = c.hfa.base;
            c.nfp += 1;
        };
        c.nreg = c.nfp;
    } else {
        c.hfa.size = 0;
        range(0, sz / 8) { n |
            c.reg&[n] = gp.index_unchecked(0)[];
            gp = gp.slice_pop_first();
            c.cls&[n] = .Kl;
            c.ngp += 1;
        };
        c.nreg = c.ngp;
    };
}

fn sttmps(tmp: *Qbe.Ref, cls: []Qbe.Cls, nreg: u32, mem: Qbe.Ref, f: *Qbe.Fn) void = {
    @debug_assert(nreg <= 4, "too many reg");
    off := 0;
    range(0, nreg.zext()) { n | 
        tmp.offset(n)[] = f.newtmp("abi", cls[n]);
        r := f.newtmp("abi", .Kl);
        store_idx: i64 = cls[n].raw().zext();
        xx := store_ops[store_idx];
        if off == 0 { 
            f.emit(xx, .Kl, QbeNull, tmp.offset(n)[], mem);
        } else {
            f.emit(xx, .Kl, QbeNull, tmp.offset(n)[], r);
            f.emit(.add, .Kl, r, mem, f.getcon(off));
        };
        off += if(is_wide(cls[n]), => 8, => 4);
    };
}

/* todo, may read out of bounds: 
 * If the size of the struct is not a multiple of 8, the actual struct
 * size may be different from the size reserved on the stack.
*/
fn ldregs(reg: *Arm64Reg, cls: *Qbe.Cls, n: u8, mem: Qbe.Ref, f: *Qbe.Fn) void = {
    off := 0;
    range(0, n.zext()) { i |
        if off == 0 { 
            f.emit(.load, cls.offset(i)[], TMP(reg.offset(i)[].raw()), mem, QbeNull);
        } else {
            r := f.newtmp("abi", .Kl);
            f.emit(.load, cls.offset(i)[], TMP(reg.offset(i)[].raw()), r, QbeNull);
            f.emit(.add, .Kl, r, mem, f.getcon(off));
        };
        off += if(is_wide(cls.offset(i)[]), => 8, => 4);
    }
}

fn selret(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    j := b.jmp.type;

    if !is_ret(j) || j == .ret0 {
        return();
    };

    r := b.jmp.arg;
    b.jmp.type = .ret0;

    cty := if j == .retc {
        @debug_assert(f.retty >= 0, "retc without type");
        m := f.globals;
        type := f.globals.types[][f.retty.zext()]&;
        cr := Class.zeroed();
        typclass(cr&, type, gpreg, fpreg, f.globals);
        if cr.class.has(.Cptr) {
            @debug_assert(rtype(f.retr) == .RTmp, "see // :SetRetr");
            f.emit(.blit1, .Kw, QbeNull, INT(cr.t.size), QbeNull);
            f.emit(.blit0, .Kw, QbeNull, r, f.retr);
            0
        } else {
            ldregs(cr.reg&.as_ptr(), cr.cls&.as_ptr(), cr.nreg, r, f);
            cr.nfp.zext().shift_left(2).bit_or(cr.ngp.zext())
        }
    } else {
        k := @as(Qbe.Cls) @as(i32) (j.raw() - Qbe.J.retw.raw());
        if (KBASE(k) == 0) {
            f.emit(.copy, k, TMP(Arm64Reg.R0), r, QbeNull);
            1
        } else {
            f.emit(.copy, k, TMP(Arm64Reg.V0), r, QbeNull);
            1.shift_left(2)
        }
    };

    b.jmp.arg = CALL(cty);
}

fn argsclass(i0: *Qbe.Ins, i1: *Qbe.Ins, carg: *Class, globals: *QbeModule) i32 = {
    va   := false;
    envc := 0;
    gp   := gpreg;
    fp   := fpreg;
    ngp  := 8;
    nfp  := 8;
    c := carg.offset(-1);
    for(i0, i1) { i |
        next :: local_return;
        c = c.offset(1);
        
        o := i.op();
        is_byte   := @is(o, .argsb, .argub, .parsb, .parub);
        is_half   := @is(o, .argsh, .arguh, .parsh, .paruh); 
        is_argpar := @is(o, .par, .arg);
        if is_byte { 
            c.size = 1;
        };
        if is_half { 
            c.size = 2;
        };
        if is_argpar {
            c.size = 8;
            if globals.target.apple && !is_wide(i.cls()) {
                c.size = 4;
            };
        };
        if is_byte || is_half || is_argpar { 
            c.align = c.size;
            c.cls&[0] = i.cls();
            if va {
                c.class&.with(.Cstk);
                c.size = 8;
                c.align = 8;
                next();
            };
            if KBASE(i.cls()) == 0 && ngp > 0 {
                ngp -= 1;
                c.reg&[0] = gp[0];
                gp = gp.slice_pop_first();
                next();
            };
            if KBASE(i.cls()) == 1 && nfp > 0 {
                nfp -= 1;
                c.reg&[0] = fp[0];
                fp = fp.slice_pop_first();
                next();
            };
            c.class&.with(.Cstk);
            next();
        };
        if o == .parc || o == .argc {
            typclass(c, globals.types.index(i.arg&[0].val()), gp, fp, globals);
            if c.ngp.zext() <= ngp {
                if c.nfp.zext() <= nfp {
                    ngp -= c.ngp.zext();
                    nfp -= c.nfp.zext();
                    gp = gp.slice_pop_from_start(c.ngp.zext());
                    fp = fp.slice_pop_from_start(c.nfp.zext());
                    next();
                } else { 
                    nfp = 0;
                }
            } else {
                ngp = 0;
            };
            c.class&.with(.Cstk);
            next();
        };
        
        if o == .pare || o == .arge {
            c.reg&[0] = .R9;
            c.cls&[0] = .Kl;
            envc = 1;
            next();
        };
        if o == .argv {
            va = globals.target.apple;
            next();
        };
        @panic("bad inst for argsclass")
    };
    
    // note: gp.len not the same as ngp becuase we clamped to 0. 
    ints := (8-gp.len).shift_left(5);
    floats := (8-fp.len).shift_left(9);
    xxx := envc.shift_left(14).bit_or(ints).bit_or(floats);
    xxx: i32 = xxx.intcast();
    xxx
}

fn slice_pop_from_start(s: []Arm64Reg, n: i64) []Arm64Reg = {
    s.ptr = s.ptr.offset(n);
    s.len -= n;
    s
}

fn arm64_retregs(r: Qbe.Ref, p: *Array(i32, 2)) u64 = {
    @debug_assert(rtype(r) == .RCall, "can't decode non call");
    ngp := r.val().bit_and(3);
    nfp := r.val().shift_right_logical(2).bit_and(7);
    if !p.is_null() {
        p[0] = ngp.intcast();
        p[1] = nfp.intcast();
    };
    b: u64 = 0;
    range(0, ngp) { n | 
        b = b.bit_or(BIT(Arm64Reg.R0.raw() + n));
    };
    range(0, nfp) { n | 
        b = b.bit_or(BIT(Arm64Reg.V0.raw() + n));
    };
    b
}

::ptr_utils(Array(i32, 2));
fn arm64_argregs(r: Qbe.Ref, p: *Array(i32, 2)) u64 = {
    @debug_assert(rtype(r) == .RCall, "can't decode non-call");
    r := r.val();
    ngp := r.shift_right_logical(5).bit_and(15);
    nfp := r.shift_right_logical(9).bit_and(15);
    x8 := r.shift_right_logical(13).bit_and(1);
    x9 := r.shift_right_logical(14).bit_and(1);
    if !p.is_null() {
        p[0] = intcast(ngp + x8 + x9);
        p[1] = nfp.intcast();
    };
    b: u64 = 0;
    range(0, ngp) { n |
        b = b.bit_or(BIT(Arm64Reg.R0.raw() + n));
    };
    range(0, nfp) { n |
        b = b.bit_or(BIT(Arm64Reg.V0.raw() + n));
    };
    b = b.bit_or(x8.shift_left(Arm64Reg.R8.raw()).bitcast());
    b = b.bit_or(x9.shift_left(Arm64Reg.R9.raw()).bitcast());
    b
}

fn stkblob(r: Qbe.Ref, c: *Class, f: *Qbe.Fn, ilp: **Insl) void = {
    @debug_assert(r != QbeNull, "stkblob null dest");
    il := temp().box(Insl); 
    al := c.t.align_log2 - 2; /* NAlign == 3 */
    if al < 0 {
        al = 0;
    };
    // If we're pulling the struct out of registers, use the rounded size computed in typclass. 
    sz := if(c.class.has(.Cptr), => c.t.size, => c.size.intcast());
    
    il.i = make_ins(@as(Qbe.O) @as(i32) Qbe.O.alloc4.raw() + al, .Kl, r, f.getcon(sz), QbeNull);
    il.link = ilp[];
    ilp[] = il;
}

fn align(x: i32, al: i32) i32 #inline = 
    (x + al - 1).bit_and(-al);

fn selcall(f: *Qbe.Fn, i0: *Qbe.Ins, i1: *Qbe.Ins, ilp: **Insl) void = {
    tmp := Array(Qbe.Ref, 4).zeroed();
    ca := temp().alloc_zeroed(Class, ptr_diff(i0, i1));
    m := f.globals;
    cty := argsclass(i0, i1, ca.ptr, f.globals);

    stk: i32 = 0;
    c := ca.ptr;
    for(i0, i1) { i |
        if c.class.has(.Cptr) {
            i.arg&[0] = f.newtmp("abi", .Kl);
            stkblob(i.arg&[0], c, f, ilp);
            i.set_op(.arg);
        };
        if c.class.has(.Cstk) {
            stk = align(stk, c.align);
            stk += c.size;
        };
        c = c.offset(1);
    };
    stk = align(stk, 16);
    rstk := f.getcon(stk.intcast());
    if stk != 0 {
        f.emit(.add, .Kl, TMP(Arm64Reg.SP), TMP(Arm64Reg.SP), rstk);
    };

    if i1.arg&[1] != QbeNull {
        type := f.globals.types[].index(i1.arg&[1].val());
        cr := Class.zeroed();
        typclass(cr&, type, gpreg, fpreg, f.globals);
        @debug_assert(i1.to != QbeNull, "call returning aggregate must have a destination.");
        stkblob(i1.to, cr&, f, ilp);
        cty = cty.bit_or(cr.nfp.zext().shift_left(2)).bit_or(cr.ngp.zext());
        if cr.class.has(.Cptr) {
            /* spill & rega expect calls to be
             * followed by copies from regs,
             * so we emit a dummy
             */
            cty = cty.bit_or(1.shift_left(13)).bit_or(1);
            f.emit(.copy, .Kw, QbeNull, TMP(Arm64Reg.R0), QbeNull);
        } else {
            sttmps(tmp&.as_ptr(), cr.cls&.items(), cr.nreg.zext(), i1.to, f);
            range(0, cr.nreg.zext()) { n |
                r := TMP(cr.reg&[n].raw());
                f.emit(.copy, cr.cls&[n], tmp&[n], r, QbeNull);
            };
        };
    } else {
        if KBASE(i1.cls()) == 0 {
            f.emit(.copy, i1.cls(), i1.to, TMP(Arm64Reg.R0), QbeNull);
            cty = cty.bit_or(1);
        } else {
            f.emit(.copy, i1.cls(), i1.to, TMP(Arm64Reg.V0), QbeNull);
            cty = cty.bit_or(1.shift_left(2));
        };
    };

    f.emit(.call, .Kw, QbeNull, i1.arg&[0], CALL(cty.zext()));

    if cty.bit_and(1.shift_left(13)) != 0 {
        /* struct return argument */
        f.emit(.copy, .Kl, TMP(Arm64Reg.R8), i1.to, QbeNull);
    };

    c := ca.ptr;
    for(i0, i1) { i | 
        if !c.class.has(.Cstk) {
            if i.op() == .arg || i.op() == .arge {
                f.emit(.copy, c.cls&[0], TMP(c.reg&[0]), i.arg&[0], QbeNull);
            };
            if i.op() == .argc {
                ldregs(c.reg&.as_ptr(), c.cls&.as_ptr(), c.nreg, i.arg&[1], f);
            };
        };
        c = c.offset(1);
    };

    /* populate the stack */
    off: i32 = 0;
    c := ca.ptr;
    for(i0, i1) { i | 
        if c.class.has(.Cstk) {
            off = align(off, c.align);
            r := f.newtmp("abi", .Kl);
            if i.op() == .arg || is_argbh(i.op()) {
                o := @switch(c.size) {
                    @case(1) => Qbe.O.storeb;
                    @case(2) => Qbe.O.storeh;
                    @case(4) => store_ops[c.cls&[0].raw().zext()];
                    @case(8) => store_ops[c.cls&[0].raw().zext()];
                    @default => panic("unreachable stack size");
                };
                f.emit(o, .Kw, QbeNull, i.arg&[0], r);
            } else {
                @debug_assert(i.op() == .argc, "bad arg op");
                f.emit(.blit1, .Kw, QbeNull, INT(c.size.intcast()), QbeNull);
                f.emit(.blit0, .Kw, QbeNull, i.arg&[1], r);
            };
            f.emit(.add, .Kl, r, TMP(Arm64Reg.SP), f.getcon(off.intcast()));
            off += c.size;
        };
        c = c.offset(1);
    };
    
    if stk != 0 {
        f.emit(.sub, .Kl, TMP(Arm64Reg.SP), TMP(Arm64Reg.SP), rstk);
    };

    c := ca.ptr;
    for(i0, i1) { i | 
        if c.class.has(.Cptr) {
            f.emit(.blit1, .Kw, QbeNull, INT(c.t.size), QbeNull);
            f.emit(.blit0, .Kw, QbeNull, i.arg&[1], i.arg&[0]);
        };
        c = c.offset(1);
    }
}

fn selpar(f: *Qbe.Fn, i0: *Qbe.Ins, i1: *Qbe.Ins) Params = {
    tmp := Array(Qbe.Ref, 16).zeroed();
    ca := temp().alloc_zeroed(Class, ptr_diff(i0, i1));
    f.reset_scratch();

    cty := argsclass(i0, i1, ca.as_ptr(), f.globals);
    discard := Array(i32, 2).ptr_from_int(0);
    f.reg = arm64_argregs(CALL(cty.zext()), discard);

    il := Insl.ptr_from_int(0);
    t := tmp&.as_ptr();
    c := ca.as_ptr();
    for(i0, i1) { i |
        if i.op() == .parc && c.class == .CNone {
            sttmps(t, c.cls&.items(), c.nreg.zext(), i.to, f);
            stkblob(i.to, c, f, il&);
            t = t.offset(c.nreg.zext());
        };
        c = c.offset(1);
    };
    while => !il.is_null() {
        f.emit(il.i);
        il = il.link;
    };
    
    if f.retty >= 0 {
        cr := Class.zeroed(); 
        type := f.globals.types[].index(f.retty.zext());
        typclass(cr&, type, gpreg, fpreg, f.globals);
        if cr.class.has(.Cptr) {
            f.retr = f.newtmp("abi", .Kl);  // :SetRetr
            r8 := Arm64Reg.R8.raw();
            f.emit(.copy, .Kl, f.retr, TMP(r8), QbeNull);
            f.reg = f.reg.bit_or(BIT(r8));
        };
    };

    t := tmp&.as_ptr();
    off: i32 = 0;
    c := ca.as_ptr();
    for(i0, i1) { i | 
        if i.op() == .parc && !c.class.has(.Cptr) {
            if c.class.has(.Cstk) {
                off = align(off, c.align);
                f.tmp[i.to.val()].slot = -(off+2);
                off += c.size;
            } else {
                range(0, c.nreg.zext()) { n |
                    r := TMP(c.reg&[n]);
                    f.emit(.copy, c.cls&[n], t[], r, QbeNull);
                    t = t.offset(1);
                };
            };
        } else {
            if c.class.has(.Cstk) {
                // convert to a load from the stack slot
                off = align(off, c.align);
                o := if(is_parbh(i.op()), => rebase(i.op(), .loadsb, .parsb), => Qbe.O.load);
                f.emit(o, c.cls&[0], i.to, SLOT(-(off+2)), QbeNull);
                off += c.size;
            } else {
                f.emit(.copy, c.cls&[0], i.to, TMP(c.reg&[0].raw()), QbeNull);
            }
        };
        c = c.offset(1);
    };
    return(
        stk = align(off, 8),
        ngp = cty.shift_right_logical(5).bit_and(15),
        nfp = cty.shift_right_logical(9).bit_and(15),
    );
}

fn apple_selvaarg(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = 
    @emit_instructions(f, i.to, i.arg&[0], i.cls(), """
    @start
        %stk =l load %1
        %0 =2 load %stk
        %stk8 =l add %stk, 8
        storel %stk8, %1
    """);

// TODO: untested
// this is very similar to the sysv one but of course not exactly the same. 
fn arm64_selvaarg(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    isint     := is_int(i.cls());
    if_24_28  := f.getcon(if(isint, => 24, => 28));
    if_8_16   := f.getcon(if(isint, => 8, => 16));
    
    @emit_instructions((f, b), i.arg&[0], if_24_28, if_8_16, i.to, i.cls(), """
    @b
        %r0a =l add %0, %1
        %nr =l loadsw %r0a
        %r1a =w csltw %nr, 0
        jnz %r1a, @breg, @bstk
    @breg
        %r0b =l add %0, %2
        %r1b =l loadl %r0b
        %lreg =l add %r1b, %nr
        %r0c =w add %nr, %2
        %r1c =l add %0, %1
        storew %r0c, %r1c
        jmp @b0
    @bstk
        %lstk =l loadl %0
        %r1d =l add %lstk, 8
        storel %r1d, %r0d
        jmp @b0
    @b0
        %loc =l phi @breg %lreg, @bstk %lstk
        %4 =4 load %loc
    """);
}

fn apple_selvastart(f: *Qbe.Fn, p: Params, ap: Qbe.Ref) void = 
    @emit_instructions(f, f.getcon(p.stk.zext()), ap, """
    @start
        %stk =l addr S-1
        %arg =l add %stk, %0
        storel %arg, %1
    """);

// TODO: untested
fn arm64_selvastart(f: *Qbe.Fn, p: Params, ap: Qbe.Ref) void = 
    @emit_instructions(f, f.getcon((p.nfp.zext()-8)*16), f.getcon((p.ngp.zext()-8)*8), f.getcon(p.stk.zext() + 192), ap, """
    @start
        %ap28 =l add %3, 28
        storew %0, %ap28
        %ap24 =l add %3, 24
        storew %1, %ap24
        %ap16 =l add %3, 16
        %rsave =l addr S-1
        %save192 =l add %rsave, 192
        storel %save192, %ap16
        %ap8 =l add %3, 8
        %save64 =l add %rsave, 64
        storel %save64, %ap8
        %r0 =l add %rsave, %2
        storel %r0, %ap
    """);

fn arm64_abi(f: *Qbe.Fn) void = {
    T := f.globals.target;
    for_blocks f { b | 
        b.visit = 0;
    };
    
    /* lower parameters */
    i := f.find_past_last_param();
    m := f.globals;
    @if(use_threads) pthread_mutex_lock(m.types_mutex&).unwrap();
    p := selpar(f, f.start.ins.first, i);
    @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
    f.realloc_for_params(i);

    /* lower calls, returns, and vararg instructions */
    ::ptr_utils(Insl);
    il := ptr_from_int(Insl, 0);
    b := f.start;
    dowhile {
        continue :: local_return;
        b = b.link;
        if b.is_null() {
            b = f.start; /* do it last */
        };
        if(b.visit != 0, => continue(!b.identical(f.start))); // TODO: is this right? continue in a do-while is clunky
        f.reset_scratch();
        @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
        selret(b, f);
        @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
        for_insts_rev b { i |
            @match(i[].op()) {
                fn call() => {
                    @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
                    i0 := b.find_first_arg(i[]);
                    selcall(f, i0, i[], il&);
                    i[] = i0;
                    @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
                }
                fn vastart() => if T.apple {
                    apple_selvastart(f, p, i.arg&[0]);
                } else {
                    arm64_selvastart(f, p, i.arg&[0]);
                };
                fn vaarg() => if T.apple {
                    apple_selvaarg(f, b, i[]);
                } else {
                    arm64_selvaarg(f, b, i[]);
                };
                fn arg()  => panic("unreachable");
                fn argc() => panic("unreachable");
                fn syscall() => {
                    panic("TODO: implement syscall convention on arm64");
                }
                @default => {
                    f.emit(i[][]);
                };
            };
        };
        if b.identical(f.start) {
            while => !il.is_null() { // :Explain it's becuase emit is backwards 
                f.emit(il.i);
                il = il.link;
            };
        };
        f.copy_instructions_from_scratch(b);
        !b.identical(f.start)
    };

    if f.globals.debug["A".char()] {
        out := f.globals.debug_out;
        write(out, "\n> After ABI lowering:\n");
        printfn(f, out);
    };
}

::enum_basic(ArgClass);
fn has(a: ArgClass, b: ArgClass) bool #inline = 
    (@as(i64) a.raw().zext()).bit_and(b.raw().zext()) != 0;

fn with(a: *ArgClass, b: ArgClass) void #inline = {
    a[] = @as(ArgClass) @as(u8) (@as(i64) a[].raw().zext()).bit_or(b.raw().zext()).trunc();
}

fn TMP(r: Arm64Reg) Qbe.Ref #redirect(i64, Qbe.Ref);
