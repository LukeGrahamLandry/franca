// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! - Assign stack slots to allocs in the start block.
//! - Most instructions must have both operands in registers. 
//! - Cmp can encode some immediates. But only on the right so you might have to swap and invert. 
//! - add/ldr/str can encode some immediates.
//! - Fuse chained memory offsets with stack slots. 
//! - Combine cmp+jnz into jmp.cc.

fn arm64_isel(f: *Qbe.Fn) void = {
    f.assign_alloc_slots();
    // In this pass, if `t.slot != -1` then `TMP(t) == addr SLOT(t.slot)`
    
    mem := list(*Qbe.Ins, temp());
    for_blocks f { b |
        f.globals.curi[] = f.scratch_start(); // TODO: remove this once we always reset it at the end of a pass. 
        
        for_jump_targets b { sb |
            for_phi sb { p |
                n := index_in_phi(b, p);
                f.fixarg(p.arg&.index(n), p.cls, true, b.id);
            };
        };
        f.seljmp(b);
        
        for_insts_rev b { i | 
            f.sel(i[][], b.id, mem&);
        };

        f.fuse_addressing(mem.items(), b);
        mem&.clear();
        
        f.copy_instructions_from_scratch(b);
    };
    if f.globals.debug["I".char()] {
        write(f.globals.debug_out, "\n> After instruction selection:\n");
        printfn(f, f.globals.debug_out);
    };
}

//
// This will compress a sequence like `add x1, x29, #24; add x1, x1, #8; ldr x1, [x1]` into `ldr x1, [x29, #32]`
// It only handles chains that occur within a single becuase we have tmp.def pointers into scratch space. 
// Also mul+add to madd. 
//
// TODO:
// - fold into .CAddr offset (tho maybe thats a fold thing), and maybe that conflicts with the idea of deduplicating CAddr to reduce relocations required. 
// - fold addr+add even if we dont see the ldr/str
//
fn fuse_addressing(f: *Qbe.Fn, mem: []*Qbe.Ins, b: *Qbe.Blk) void = {
    if(mem.len == 0, => return());
    when_debug(f, .InstSelect) { out |
        @fmt_write(f.globals.debug_out, "%: \n", b.name());
    };
    for mem& { i | 
        continue :: local_return;
        iarg := i.arg&;
        if i.op() == .add {
            // note: not tracking use counts
            def := f.get_temporary(iarg[1])[].def;
            iarg[1] = f.new_mem(def.arg&[0], def.arg&[1], 0, 255);
            @debug_assert(def.op() == .mul && is_int(i.cls()), "not int mul");
            continue();
        };
        
        store := is_store(i.op());
        addr := f.get_temporary(iarg[store.int()]);
        when_debug(f, .InstSelect) { out |
            if store {
                @fmt_write(f.globals.debug_out, "   store ");
            } else {
                @fmt_write(f.globals.debug_out, "   ");
                printref(i.to, f, f.globals.debug_out);
                @fmt_write(f.globals.debug_out, " = load ");
            };
            
            @fmt_write(f.globals.debug_out, "[%]", addr.name());
        };
        @debug_assert(store || iarg[1] == QbeNull, "load already has second arg");
        off := 0;
        size: i64 = if(store, => size_of_store(i).zext(), => size_of_load(i).zext());
        new_addr := fuse_addr(f, iarg[store.int()], off&, b.id, size, addr.nuse == 1);
        if new_addr != iarg[store.int()] && off.mod(size) == 0 {
            if new_addr == QbeNull {
                iarg[store.int()] = SLOT(off / 4);
            } else {
                if store {
                    if off == 0 {
                        iarg[1] = new_addr;
                    } else {
                        iarg[1] = f.new_mem(new_addr, QbeNull, off, 1);
                    };
                } else {
                    iarg[0] = new_addr;
                    iarg[1] = INT(off);
                };
            };
        };
        
        when_debug(f, .InstSelect) { out |
            @fmt_write(f.globals.debug_out, "\n");
        };
    };
    when_debug(f, .InstSelect) { out |
        @fmt_write(f.globals.debug_out, "\n");
    };
}

fn fuse_addr(f: *Qbe.Fn, a: Qbe.Ref, total_off: *i64, bid: i32, size: i64, kill: bool) Qbe.Ref  = {
    max_off := size * 4096;
    addr := f.get_temporary(a);
    in_this_block := !addr.def.is_null() && addr.defining_block == bid;
    if(!in_this_block, => return(a));
    def_a := addr.def.arg&;
    //@debug_assert(addr.nuse > 0);
    // TODO: this should work but there's somewhere im not keeping track of count properly, i don't super care tho,
    //       rega gets rid of dead code anyway, doing it here just makes it easier to see whats going on in debug output. -- Nov 3
    //       tho seljmp looks at use counts so its a bit sketchy that we're fucking with them. 
    kill_addr :: fn() => {
        if kill {
            //addr.nuse -= 1;
            //if addr.nuse == 0 {
            //    when_debug(f, .InstSelect) { out |
            //        @fmt_write(f.globals.debug_out, "(kill %) ", addr.name());
            //    };
            //    addr.def.set_nop();
            //    addr.def = Qbe.Ins.ptr_from_int(0);
            //};
        };
    };
    
    @match(addr.def.op()) {
        fn add() => {
            if(rtype(def_a[0]) != .RTmp, => return(a));
            if f.get_int(def_a[1]) { off | 
                off := total_off[] + off;
                if(off < 0 || off >= max_off || off.mod(size) != 0, => return(a));
                base := f.get_temporary(def_a[0]);
                when_debug(f, .InstSelect) { out |
                    @fmt_write(f.globals.debug_out, "[% + %]", base.name(), off);
                };
                prev := total_off[];
                total_off[] = off;
                final := fuse_addr(f, def_a[0], total_off, bid, size, addr.nuse == 1);
                if final != a {
                    kill_addr();
                } else {
                    total_off[] = prev;
                    when_debug(f, .InstSelect) { out |
                        @fmt_write(f.globals.debug_out, "NO!");
                    };
                };
                return(final);
            };
        }
        // TODO: stop representing RSlot as divided by 4 so you don't have to skip doing this for access of a single byte 3/4 of the time. -- Nov 3
        fn addr() => {
            if(f.dynalloc, => return(a));
            @debug_assert(rtype(def_a[0]) == .RSlot, "can only addr stack slot");
            slot := rsval(def_a[0]).intcast();
            off := total_off[] + slot * 4;
            if(slot < 0 || off < 0 || off >= max_off || off.mod(size) != 0 || off.mod(4) != 0, => return(a));
            when_debug(f, .InstSelect) { out |
                @fmt_write(f.globals.debug_out, "[S% + %][S%]", slot, total_off[], off / 4);
            };
            total_off[] = off;
            kill_addr();
            return(QbeNull);
        }
        @default => ();
    };
    a
}

fn sel(f: *Qbe.Fn, i: Qbe.Ins, blk: i32, mem: *List(*Qbe.Ins)) void = {
    if is_alloc(i&.op()) {
        // note: i0 is what we're about to emit not what we just emitted
        i0 := f.globals.curi[].offset(-1); // :LookAtLastInst
        salloc(i.to, i.arg&[0], f);
        f.fixarg(i0.arg&.index(0), .Kl, false, blk);
        return();
    };
    ck := Qbe.Cls.zeroed();
    cc: i32 = 0;
    if iscmp(i&.op(), ck&, cc&) {
        i0 := f.emit(.flagieq, i&.cls(), i.to, QbeNull, QbeNull);
        if f.selcmp(i.arg, ck, blk) {
            x := i0.op().raw() + cmpop(cc);
            i0.set_op(@as(Qbe.O) x);
        } else {
            x := i0.op().raw() + cc;
            i0.set_op(@as(Qbe.O) x);
        };
        return();
    };
    // TODO: what does the callable check do? i don't have a test that needs it. 
    if i&.op() == .asm || (i&.op() == .call && f.callable(i.arg&[0])) {  // .asm has const u32 arg that we don't want to fix
        f.emit(i);
        return();
    };
    o := i&.op();
    if o != .nop {
        new_ins := f.emit(i);
        iarg := new_ins.arg&;
        
        // 610KB, amazing!
        if is_load(o) && rtype(iarg[0]) == .RTmp {
            mem.push(new_ins);
        };
        if is_store(o) && rtype(iarg[1]) == .RTmp {
            mem.push(new_ins);
        };
        if rtype(i.to) == .RTmp {
            t := f.get_temporary(i.to);
            t.def = new_ins;
        };
        k0 := argcls(i&, 0);
        k1 := argcls(i&, 1);
        // this isn't really worth it
        if o == .add && is_int(k0) {
            if f.get_int(iarg[0]) { off |
                iarg.items().swap(0, 1);
                @debug_assert(k0 == k1);
            };
        };
        f.fixarg(iarg[0]&, k0, false, blk);
        // TODO: do this for sub, etc as well. this alone makes the compiler go from 2485100 -> 2089352 bytes of code. thats insane.
        //       this looks branchy but it generates so much less code that its faster even before accounting for it generating better code for itself. 
        if o == .add && is_int(k1) {
            // could convert to sub `if off < 0 && off > -4096` but that never happens in self-compile and then would have to add a case to fuse_addr -- Nov 7

            // ~358KB, amazing!
            if f.is_u12(iarg[1]) && iarg[0] != TMP(Arm64Reg.SP) {
                return();
            };
            // ~10KB, meh
            arg1 := iarg[1];
            if rtype(arg1) == .RTmp {
                arg1 := f.get_temporary(arg1);
                if arg1.nuse == 1 && !arg1.def.is_null() && arg1.def.op() == .mul && arg1.defining_block == blk {
                    mem.push(new_ins);
                };
            };  
        };
        // ~1KB, not really worth it
        if o == .sub && is_int(k1) {
            if f.is_u12(iarg[1]) && iarg[0] != TMP(Arm64Reg.SP) {
                return();
            };
        };
        if o == .shl || o == .shr || o == .sar {
            if f.get_int(iarg[1]) { _ |
                return();
            };
        };
        f.fixarg(iarg[1]&, k1, false, blk);
    };
}

fn is_u12(f: *Qbe.Fn, r: Qbe.Ref) bool = {
    if(rtype(r) != .RCon, => return(false));
    c := f.get_constant(r);
    if(c.type != .CBits, => return(false));
    x := c.bits.i;
    x.bit_and(1.shift_left(12) - 1) == x
}

fn get_int(f: *Qbe.Fn, r: Qbe.Ref) ?i64 = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type != .CBits, => return(.None));
    (Some = c.bits.i)
}

// This is where we combine cmp+jnz.
fn seljmp(f: *Qbe.Fn, b: *Qbe.Blk) void = {
    // TODO: could flip this exit and assert condition so its just one !=jnz in release mode. 
    if b.jmp.type != .jnz {
        msg :: "the frontend only makes jnz conditionals and selret in abi pass removes other types of ret.";
        @debug_assert(b.jmp.type == .ret0 || b.jmp.type == .jmp || b.jmp.type == .hlt, msg);
        return();
    };
    // If the jump condition was created in this basic block, is only used once, and was made by a comparison, 
    // we can pull the comparison forward into the jump instruction and just jump based on the cpu flags set by the cmp instruction.
    // Otherwise, we have to actually produce the value and compare it to 0. 
    
    jump_arg := b.jmp.arg;
    b.jmp.arg = QbeNull;
    if find_source_instruction(b, jump_arg) { i |
        uses := f.get_temporary(jump_arg)[].nuse;
        cc: i32 = 0;
        cmp_cls := Qbe.Cls.zeroed();
        if uses == 1 && iscmp(i.op(), cmp_cls&, cc&) {
            if f.selcmp(i.arg, cmp_cls, b.id) {
                cc = cmpop(cc);
            };
            xxx: i32 = Qbe.J.jfieq.raw() + cc;
            b.jmp.type = @as(Qbe.J) xxx;
            i.set_nop();
            return();
        } // else fallthrough
    };
    // TODO: init array from tuple literal
    f.selcmp(init(@slice(jump_arg, QbeConZero)), .Kw, b.id);
    b.jmp.type = .jfine;
}

// Try to compare to a constant using an immidiate. 
// Returns true if you need to invert the condition because we swapped the arguments. 
// note: we handle the terminator first so we only get here if it we didn't jump on the result of this comparison.
fn selcmp(f: *Qbe.Fn, arg: Array(Qbe.Ref, 2), k: Qbe.Cls, blk: i32) bool = {
    if !is_int(k) {
        // fcmp can't take an immdiate, so make sure both args are registers. (TODO: it can compare to 0 tho)
        iarg := f.emit(.afcmp, k, QbeNull, arg&[0], arg&[1])[].arg&;
        f.fixarg(iarg[0]&, k, false, blk);
        f.fixarg(iarg[1]&, k, false, blk);
        return(false);
    };
    
    // cmp can only have an immediate on the right. so we might have to swap them and invert the condition. 
    needed_swap := rtype(arg&[0]) == .RCon;
    if needed_swap {
        // TODO: not true for function pointers i guess which is fair because we don't technically know if the symbols will resolve to the same place. 
        //@debug_assert(rtype(arg&[1]) != .RCon, "cmp of constant should already have been folded. we do handle this case below tho so its fine if you want to remove that pass and this check.");
        arg&.items().swap(0, 1);
    };
    
    cant_fit_immediate := true;
    cmp := Qbe.O.acmp;
    r_right := arg&[1];
    if rtype(r_right) == .RCon {
        c := f.get_constant(r_right);
        n := 0; // TODO: just use multiple return values.
        // TODO: annoying that i don't have fallthrough for @match
        imm := imm(c, k, n&);
        cant_fit_immediate = @is(imm, .PosLow24, .NegLow24, .Iother);
        if (@is(imm, .NegLow12, .NegHigh12)) {
            // if we're comparing to a negative number, there's a different instruction for that. 
            cmp = .acmn;
            r_right = f.getcon(n);
        };
    };
    
    iarg := f.emit(cmp, k, QbeNull, arg&[0], r_right)[].arg&;
    f.fixarg(iarg[0]&, k, false, blk); // it could be the address of a stack slot or thread local, i guess?
    if cant_fit_immediate {
        f.fixarg(iarg[1]&, k, false, blk);
    };
    needed_swap
}

::enum_basic(Qbe.RegKind);
::enum_basic(Qbe.SymType);
// Force the value to be in a register (if its a constant, produce it in a different reg and then move).
fn fixarg(f: *Qbe.Fn, pr: *Qbe.Ref, k: Qbe.Cls, phi: bool, blk: i32) void = {
    r_original := pr[];
    @match(rtype(r_original)) {
        fn RCon() => {
            c := f.get_constant(r_original);
            // Accessing a thread local requires an incantation. 
            if f.globals.target.apple && c.type == .CAddr && c.sym.type == .SThr {
                r1 := f.newtmp("isel", .Kl);
                pr[] = r1;
                if c.bits.i != 0 {
                    r2 := f.newtmp("isel", .Kl);
                    cc: Qbe.Con = (type = .CBits, bits = (i = c.bits.i));
                    // this is a constant arg that wont go through isel
                    // qbe arm64/emit.c/emitf can write a literal second but doesn't check if it can take advantage of that other than here
                    // i don't want to deal with instruction encoding for that until i extend this to apply to all instructions that have an imm varient. 
                    r3_con := f.newcon(cc&);    
                    r3 := f.newtmp("isel", .Kl);
                    f.emit(.add, .Kl, r1, r2, r3);
                    f.emit(.copy, .Kl, r3, r3_con, QbeNull);
                    r1 = r2;
                };
                R0 :: Qbe.Rxxx + 1;
                f.emit(.copy, .Kl, r1, TMP(R0), QbeNull);
                r1 := f.newtmp("isel", .Kl);
                r2 := f.newtmp("isel", .Kl);
                f.emit(.call, .Kw, QbeNull, r1, CALL(33));
                f.emit(.copy, .Kl, TMP(R0), r2, QbeNull);
                f.emit(.load, .Kl, r1, r2, QbeNull);
                cc := c[];
                cc.bits.i = 0;
                r3 := f.newcon(cc&);
                f.emit(.copy, .Kl, r2, r3, QbeNull);
                return();
            };
            // phis are just mov which can have an immediate
            if(is_int(k) && phi, => return());
            
            r_output := f.newtmp("isel", k);
            // Most instructions can't have immediates. 
            // So we add a new instruction that produces the constant, so the actual operation has registers for both inputs. 
            // TODO: there are a few instructions that have a small range of immediates. like add_im/sub_im can take a u12 (optionally shifted).
            if is_int(k) {
                f.emit(.copy, k, r_output, r_original, QbeNull);
            } else {
                // They don't give you float immediates (even for mov). 
                // So we load the bits into an integer register and then move it to a float register. 
                // TODO: qbe loads float constants from memory (and so does clang), so perhaps thats much faster. 
                //       but I don't want to deal with the different backends yet (jit vs aot) so it feels easier to just put it in an int register and then move it to float.
                //       I tried this change in qbe and it passed thier tests so i think it is just a performance thing. 
                //       Perhaps I will change this back at some point. It feels weird to me to let instruction selection produce data constants tho. 
                // TODO: assert in bounds based on `KWIDE(k) ? 8 : 4`?
                r_int := f.newtmp("isel", .Kl);
                f.emit(.cast, k, r_output, r_int, QbeNull);
                f.emit(.copy, .Kl, r_int, r_original, QbeNull);
            };
            pr[] = r_output;
        }
        fn RTmp() => {
            slot_offset := f.get_temporary(r_original)[].slot;
            if slot_offset != -1 {
                // This tmp holds the address of a stack slot. Produce it by offseting the frame pointer. 
                r_slot := f.newtmp("isel", .Kl);
                def := f.emit(.addr, .Kl, r_slot, SLOT(slot_offset.zext()), QbeNull);
                t := f.get_temporary(r_slot);
                t.def = def;
                t.nuse = 1;
                t.ndef = 1;
                t.defining_block = blk;
                pr[] = r_slot;
            }; // otherwise its already in a register
        }
        @default => ();
    }; // early returns
}

fn callable(f: *Qbe.Fn, r: Qbe.Ref) bool = {
    if rtype(r) == .RTmp {
        return(true);
    };
    
    // TODO: should have a get_constant that returns an optional? 
    if rtype(r) == .RCon {
        c := f.get_constant(r);
        if c.type == .CAddr && c.bits.i == 0 {
            return(true);
        }
    };
    false
}

Imm :: @enum(i64) (
    Iother,
    PosLow12,
    PosHigh12,
    PosLow24,
    NegLow12,
    NegHigh12,
    NegLow24,
);
::if(Imm);  ::enum_basic(Imm);

fn sign_extend_low32(x: i64) i64 = {
    x: u32 = x.trunc();
    x: i32 = x.bitcast();
    x.intcast()
}

fn imm(c: *Qbe.Con, k: Qbe.Cls, pn: *i64) Imm = {
    if c.type != .CBits {
        return(.Iother);
    };
    n := c.bits.i;
    if k == .Kw {
        // sign extend the low 32 bits. 
        // qbe did `n = (int32_t)n;` which clang says is sxtw. 
        n = sign_extend_low32(n);
    };
    positive := true;
    if n < 0 {
        positive = false;
        n = -n;  // this handles MIN_i64 wrong but it doesn't matter because it doesn't match any of these cases anyway
    };
    pn[] = n;
    if n.bit_and(0x000fff) == n {
        return(if(positive, => .PosLow12, => .NegLow12));
    };
    if n.bit_and(0xfff000) == n {
        return(if(positive, => .PosHigh12, => .NegHigh12));
    };
    if n.bit_and(0xffffff) == n {
        return(if(positive, => .PosLow24, => .NegLow24));
    };
    .Iother
}

//
// https://dinfuehr.github.io/blog/encoding-of-immediate-values-on-aarch64/
// and/orr/eor take bit mask immediates which can encode a small subset of values. 
// - The value must be a pattern of e=2/4/8/16/32/64 bits repeated for the whole register (ie. if e=4 it will be repeated 16 times). 
// - The repeated thing must have all its one bits together or all its zero bits together. 
//   ie. e=8 cannot have 10011101, but it can have 00111000 or 11000111.
//   (equivalently: all ones must be together but you're allowed to rotate and wrap around). 
// - The value cannot be all zeros or all ones. 
//
//   immr = how much to rotate
// N:imms = (variable width fixed bit pattern encoding e):(number of consecutive ones at the end of the pre-rotation mask - 1)
//
// trailing zeros are filled in with 1s count. 
// 2  = 0b0111110
// 4  = 0b0111100
// 8  = 0b0111000
// 16 = 0b0110000
// 32 = 0b0100000
// 64 = 0b1000000
//

fn repeat_pattern(x: i64, pattern_size: i64) i64 = {
    core := x.bit_and(1.shift_left(pattern_size) - 1);
    x := 0;
    range(0, 64 / pattern_size) { _ |
        x = x.bit_or(core);
        core = core.shift_left(pattern_size);
    };
}

// note: not tested yet, only called by emit
fn arm64_logimm(x: i64, k: Qbe.Cls) bool = {
    if (k == .Kw) {
        x = x.bit_and(0xffffffff).bit_or(x.shift_left(32));
    };
    if x.bit_and(1) != 0 {
        x = x.bit_not();
    };
    if(x == 0, => return(true));
    if(x == 0xaaaaaaaaaaaaaaaa, => return(true));
    
    n := x.bit_and(0xf);
    check :: fn() => return(n.bit_and(n + n.bit_and(-n)) == 0);
    if(0x1111111111111111 * n == x, => check());
    n = x.bit_and(0xff);
    if(0x0101010101010101 * n == x, => check());
    n = x.bit_and(0xffff);
    if(0x0001000100010001 * n == x, => check());
    n = x.bit_and(0xffffffff);
    if(0x0000000100000001 * n == x, => check());
    n = x;
    check()
}

fn trunc(x: i32) i16 = { // TODO
    ptr_cast_unchecked(i32, i16, x&)[]
}

// this only makes sense once we're in ssa form
fn find_source_instruction(b: *Qbe.Blk, search: Qbe.Ref) ?*Qbe.Ins #inline = {
    // backwards because used when looking for cmp for the terminating jmp so its probably near the end. 
    for_insts_rev b { i | 
        if i.to == search {
            return(Some = i[])
        }
    };
    .None
}
