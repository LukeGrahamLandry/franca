// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! - Assign stack slots to allocs in the start block.
//! - Most instructions must have both operands in registers. 
//! - Cmp can encode some immediates. But only on the right so you might have to swap and invert. 
//! - add/ldr/str can encode some immediates. 
//!   as can shifts and or/xor/and. 
//! - Fuse chained memory offsets with stack slots. 
//! - Combine cmp+jnz into jmp.cc.

// TODO: reuse flag setting instructions like amd64/isel

fn arm64_isel(f: *Qbe.Fn) void = {
    native_isel(f, fn(r, k, i, f) => fixarg(f, r, k, !i.is_null()), seljmp, sel_inst);
}

sel_inst :: fn(i: **Qbe.Ins, b: *Qbe.Blk, f: *Qbe.Fn) void = {
    if i[].op() == .sel1 {
        selsel_arm64(f, b, i[]);
    } else {
        if i[].op() == .cas1 {
            k := i[].cls();
            cas0, cas1 := find_cas(i[], b);
            // TODO: should really make the frontend ensure this but i don't want to deal with it right now
            //@debug_assert_eq(ptr_diff(cas0, cas1), 1, "non-adjacent cas");
            cas1 := f.emit(cas1[]);
            f.fixarg(cas0.arg&.index(0), k);
            f.fixarg(cas1.arg&.index(0), k);
            f.fixarg(cas1.arg&.index(1), k);
            cas1.arg&[0] = f.new_mem(cas1.arg&[0], cas1.arg&[1], 0, 0);
            cas1.arg&[1] = cas0.arg&[0];
            cas0.set_nop();
        } else {
            f.sel(i[][], b.id);
        };
    };
};

fn get_slot(f: *Qbe.Fn, r: Qbe.Ref) ?i64 = {
    @match(rtype(r)) {
        fn RSlot() => return(Some = rsval(r).intcast());
        fn RTmp() => {
            slot := f.get_temporary(r)[].slot;
            if slot != -1 {
                return(Some = slot.intcast());
            };
        }
        @default => ();
    };
    .None
}

//
// This will compress a sequence like `add x1, x29, #24; add x1, x1, #8; ldr x1, [x1]` into `ldr x1, [x29, #32]`
// (doesn't need to recurse because of the prepass of collapse_addr)

fn fix_memarg(f: *Qbe.Fn, i: *Qbe.Ins) *Qbe.Ref = {
    iarg := i.arg&;
    store := is_store(i.op());
    @debug_assert(store || is_load(i.op()), "bad mem op");
    @debug_assert(store || iarg[1] == QbeNull, "load already has second arg");
    size: i64 = if(store, => size_of_store(i).zext(), => size_of_load(i).zext());
    r := iarg[store.int()]&;
    new_addr, off := fuse_addr(f, r[], size);
    
    max_off := size * 4096;
    if(off < 0 || off >= max_off || off.mod(size) != 0, => return(r));
    
    chuse(new_addr, 1, f);
    chuse(r[], -1, f);
    
    @debug_assert(off.mod(size) == 0);
    if new_addr != r[] {
        if new_addr == QbeNull {
            r[] = SLOT(off);
        } else {
            if store {
                if off == 0 {
                    iarg[1] = new_addr;
                } else {
                    m := f.new_mem(new_addr, QbeNull, off, 1);
                    iarg[1] = m;
                    r = f.get_memory(m)[].base&;
                };
            } else {
                iarg[0] = new_addr;
                iarg[1] = INT(off);
            };
        };
    };
    r
}

fn fuse_addr(f: *Qbe.Fn, a: Qbe.Ref, size: i64) Ty(Qbe.Ref, i64) = {
    max_off := size * 4096;
    if f.get_slot(a) { slot |
        return(QbeNull, slot);
    };
    addr := f.get_temporary(a);
    // :ReferencingOldBlock
    if(addr.def.is_null() || addr.def.to != a, => return(a, 0));
    def_a := addr.def.arg&;
    @debug_assert_gt(addr.nuse, 0, "nuse");
    @match(addr.def.op()) {
        fn add() => if f.get_int(def_a[1]) { off | 
            @debug_assert(def_a[0] != QbeNull, "add missing arg");
            return(def_a[0], off);
        };
        fn addr() => {
            slot := f.get_slot(def_a[0]).expect("can only addr stack slot");
            return(QbeNull, slot);
        }
        @default => ();
    };
    (a, 0)
}

fn sel(f: *Qbe.Fn, i: Qbe.Ins, blk: i32) void = {
    if(try_kill_inst(f, i&), => return());
    
    if is_alloc(i&.op()) {
        // note: i0 is what we're about to emit not what we just emitted
        i0 := f.globals.curi.offset(-1); // :LookAtLastInst
        is_dyn := salloc(i.to, i.arg&[0], f);
        f.fixarg(i0.arg&.index(0), .Kl);  // .salloc
        if is_dyn {
            // TODO: do the bitfield immediate thing here. it just doesn't matter because i barely use alloca. 
            f.fixarg(i0.offset(-1)[].arg&.index(1), .Kl);  // .and
        };
        return();
    };
    ck := Qbe.Cls.zeroed();
    cc: i32 = 0;
    if iscmp(i&.op(), ck&, cc&) {
        i0 := f.emit(.flagieq, i&.cls(), i.to, QbeNull, QbeNull);
        if f.selcmp(i.arg, ck, blk) {
            x := i0.op().raw() + cmp_swap(cc);
            i0.set_op(@as(Qbe.O) x);
        } else {
            x := i0.op().raw() + cc;
            i0.set_op(@as(Qbe.O) x);
        };
        return();
    };
    if (i&.op() == .call && f.callable(i.arg&[0])) {
        f.emit(i);
        return();
    };
    o := i&.op();
    @match(o) {
        // .asm has const u32 arg that we don't want to fix
        fn asm() => f.emit(i);
        fn dbgloc() => f.emit(i);
        fn nop() => ();
        fn ones() => { 
            // you can only popcnt on a simd/float register
            f_in := f.newtmp("isel", .Kd);
            f_out := f.newtmp("isel", .Kd);
            ki := i&.cls();
            kf: Qbe.Cls = @if(ki.is_wide(), .Kd, .Ks);
            f.emit(.cast, ki, i.to, f_out, QbeNull);
            f.emit(.ones, kf, f_out, f_in, QbeNull);
            icast := f.emit(.cast, kf, f_in, i.arg&[0], QbeNull);
            f.fixarg(icast.arg&.index(0), ki);
        };
        fn rotl() => {
            new_ins := f.emit(i);
            iarg := new_ins.arg&;
            k0 := argcls(i&, 0);
            new_ins.set_op(.rotr);
            width := 1.shift_left(5 + k0.is_wide().int());
            if f.get_int(iarg[1]) { imm |
                iarg[1] = f.getcon(width - bit_and(imm, width - 1));
                return();
            };
            r0 := f.newtmp("isel", k0);
            r1 := f.newtmp("isel", k0);
            ii := f.emit(.sub, k0, r0, f.getcon(width), r1);
            f.fixarg(ii.arg&.index(0), k0);
            ii := f.emit(.and, k0, r1, iarg[1], f.getcon(width - 1));
            f.fixarg(ii.arg&.index(1), k0);
            f.fixarg(ii.arg&.index(0), k0);
            iarg[1] = r0;
        };
        @default => {
            new_ins := f.emit(i);
            iarg := new_ins.arg&;
            
            if rtype(i.to) == .RTmp {
                t := f.get_temporary(i.to);
                t.def = new_ins;
            };
            if is_load(o) && rtype(iarg[0]) == .RTmp {
                r := fix_memarg(f, new_ins);
                if rtype(r[]) != .RSlot {
                    f.fixarg(r, .Kl);
                };
                return();
            };
            k0 := argcls(i&, 0);
            k1 := argcls(i&, 1);
            
            commutes := (@is(o, .and, .or, .xor, .add));
            if commutes && rtype(iarg[0]) == .RCon {
                i.arg&.items().swap(0, 1);
            };
            
            if o == .ctz {  // 0b101101011000000000110 is ctz... but it doesn't exist? I guess i don't have FEAT_CSSC?
                new_ins.set_op(.clz);
                r := f.newtmp("isel", k0);
                i := f.emit(.bitswap, k0, r, iarg[0], QbeNull);  // new emit must be before fixarg (because emit is backwards)
                f.fixarg(i.arg&.index(0), k0);
                iarg[0] = r;
                return();
            };

            copy_from_con := i&.op() == .copy && rtype(iarg[0]) == .RCon && !isreg(i.to);
            if !copy_from_con {
                f.fixarg(iarg[0]&, k0);
            };
            if is_store(o) && rtype(iarg[1]) == .RTmp {
                r := fix_memarg(f, new_ins);
                if rtype(r[]) != .RSlot {
                    f.fixarg(r, .Kl);
                };
                return();
            };
            // TODO: do this for sub, etc as wel. this alone makes the compiler go from 2485100 -> 2089352 bytes of code. thats insane.
            //       this looks branchy but it generates so much less code that its faster even before accounting for it generating better code for itself. 
            if o == .add && is_int(k1) {
                // ~358KB, amazing! (that was before other addr folding i think, now its less impressive )
                if f.get_int(iarg[1]) { imm |
                    if imm.bit_and(1.shift_left(12) - 1) == imm {
                        if imm == 0 {
                            iarg[1] = QbeNull;
                            new_ins.set_op(.copy);
                        };
                        return();
                    };
                    if imm < 0 && imm > -4096 {  // 3kb, meh
                        new_ins.set_op(.sub);
                        iarg[1] = f.getcon(-imm);
                        return();
                    }
                };
                // ~10KB, meh
                arg1 := iarg[1];
                if rtype(arg1) == .RTmp {
                    a1 := arg1;
                    arg1 := f.get_temporary(arg1);
                    def := arg1.def;
                    // :ReferencingOldBlock
                    if arg1.nuse == 1 && !def.is_null() && def.op() == .mul && def.to == a1 {
                        f.fixarg(def.arg&.index(0), .Kl);
                        f.fixarg(def.arg&.index(1), .Kl);
                        iarg[1] = f.new_mem(def.arg&[0], def.arg&[1], 0, 255);
                        def.set_nop();
                        return();
                    };
                };  
            };
            // ~1KB, not really worth it
            if o == .sub && is_int(k1) {
                if f.get_int(iarg[1]) { imm |
                    if imm.bit_and(1.shift_left(12) - 1) == imm {
                        if imm == 0 {
                            iarg[1] = QbeNull;
                            new_ins.set_op(.copy);
                        };
                        return();
                    };
                };
            };
            
            if (@is(o, .shl, .shr, .sar, .rotr)) {
                warn_if_unlikely_argument(f, i&, iarg[1]);
                // Since these are done modulo data size, we can fit any constant in the immediate. 
                if f.get_int(iarg[1]) { _ |
                    return();
                };
            };
            if (@is(o, .and, .or, .xor)) {
                if f.get_int(iarg[1]) { value |
                    enc :: import("@/backend/arm64/bits.fr").encode_bitmask;
                    if enc(value, k1.is_wide()) { inst |
                        iarg[1] = INT(inst);
                        return();
                    };
                };
            };
            
            f.fixarg(iarg[1]&, k1);
        };
    };
}

// This is where we combine cmp+jnz.
fn seljmp(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    @debug_assert_ne(b.jmp.type, .switch);
    
    if b.jmp.type != .jnz {
        msg :: "the frontend only makes jnz conditionals and selret in abi pass removes other types of ret.";
        @debug_assert(@is(b.jmp.type, .ret0, .jmp, .hlt), msg);
        return();
    };
    // If the jump condition was created in this basic block, is only used once, and was made by a comparison, 
    // we can pull the comparison forward into the jump instruction and just jump based on the cpu flags set by the cmp instruction.
    // Otherwise, we have to actually produce the value and compare it to 0. 
    
    jump_arg := b.jmp.arg;
    b.jmp.arg = QbeNull;
    if find_source_instruction(b, jump_arg) { i |
        uses := f.get_temporary(jump_arg)[].nuse;
        cc: i32 = 0;
        cmp_cls := Qbe.Cls.zeroed();
        if uses == 1 && iscmp(i.op(), cmp_cls&, cc&) {
            if f.selcmp(i.arg, cmp_cls, b.id) {
                cc = cmp_swap(cc);
            };
            xxx: i32 = Qbe.J.jfieq.raw() + cc;
            b.jmp.type = @as(Qbe.J) xxx;
            i.set_nop();
            return();
        } // else fallthrough
    };
    f.selcmp(@array(jump_arg, QbeConZero), .Kw, b.id); // :jnz_is_Kw
    b.jmp.type = .jfine;
}

// TODO: this is so similar to seljmp. but im afraid trying to factor them together would just make it more confusing. 
fn selsel_arm64(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    isel0 := i.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    r := isel0.arg&[0];
    if rtype(r) != .RTmp {
        // This should never happen because of constant folding but let's play it safe. 
        r = f.newtmp("isel", .Kw);
    };
    cr0, cr1 := (r, QbeConZero);
    c: i32 = Qbe.Cmp.Cine.raw();
    needed_swap := false;
    cmp_cls := Qbe.Cls.Kw; // :jnz_is_Kw // t.cls;
    if find_source_instruction(b, r) { fi |
        t := f.get_temporary(r)[];
        if t.nuse == 1 && iscmp(fi.op(), cmp_cls&, c&) {
            needed_swap = rtype(fi.arg&[0]) == .RCon && is_int(cmp_cls); // :arm64_cmp_will_swap
            if needed_swap {
                // selcmp will swap the args so don't do that to crN here, 
                // but we need to update the condition so we use the right one in selKCC.
                c = cmp_swap(c);
            };
            cr0 = fi.arg&[0];
            cr1 = fi.arg&[1];
            fi.set_nop();
        };
    };
    
    isel1 := i;
    @debug_assert_eq(isel1.op(), .sel1);
    o := @as(Qbe.O) @as(i32) Qbe.O.selieq.raw() + c;
    isel1.set_op(o);
    sel(f, isel1[], b.id);
    
    swapped := f.selcmp(@array(cr0, cr1), cmp_cls, b.id);
    @debug_assert_eq(needed_swap, swapped, "confused about cmp swapping");
    if r != isel0.arg&[0] {
        f.emit(.copy, .Kw, r, isel0.arg&[0], QbeNull);
    };
    isel0.set_nop();
}

// Try to compare to a constant using an immidiate. 
// Returns true if you need to invert the condition because we swapped the arguments. 
// note: we handle the terminator first so we only get here if it we didn't jump on the result of this comparison.
fn selcmp(f: *Qbe.Fn, arg: Array(Qbe.Ref, 2), k: Qbe.Cls, blk: i32) bool = {
    if !is_int(k) {
        // fcmp can't take an immdiate, so make sure both args are registers. (TODO: it can compare to 0 tho)
        iarg := f.emit(.afcmp, k, QbeNull, arg&[0], arg&[1])[].arg&;
        f.fixarg(iarg[0]&, k);
        f.fixarg(iarg[1]&, k);
        return(false);
    };
    
    // cmp can only have an immediate on the right. so we might have to swap them and invert the condition. 
    needed_swap := rtype(arg&[0]) == .RCon;  // :arm64_cmp_will_swap
    if needed_swap {
        arg&.items().swap(0, 1);
    };
    
    cant_fit_immediate := true;
    cmp := Qbe.O.acmp;
    r_right := arg&[1];
    if rtype(r_right) == .RCon {
        c := f.get_constant(r_right);
        n := 0;
        // TODO: annoying that i don't have fallthrough for @match
        imm := imm(c, k, n&);
        ::enum(@type imm);
        cant_fit_immediate = @is(imm, .PosLow24, .NegLow24, .Iother);
        if (@is(imm, .NegLow12, .NegHigh12)) {
            // if we're comparing to a negative number, there's a different instruction for that. 
            cmp = .acmn;
            r_right = f.getcon(n);
        };
    };
    
    iarg := f.emit(cmp, k, QbeNull, arg&[0], r_right)[].arg&;
    f.fixarg(iarg[0]&, k); // it could be the address of a stack slot or thread local, i guess?
    if cant_fit_immediate {
        f.fixarg(iarg[1]&, k);
    };
    needed_swap
}

fn fixarg();
fn fixarg(f: *Qbe.Fn, pr: *Qbe.Ref, k: Qbe.Cls) void = 
    fixarg(f, pr, k, false);

// Force the value to be in a register (if its a constant, produce it in a different reg and then move).
fn fixarg(f: *Qbe.Fn, pr: *Qbe.Ref, k: Qbe.Cls, phi: bool) void = {
    ::enum_basic(Qbe.RegKind);

    r_original := pr[];
    @match(rtype(r_original)) {
        fn RCon() => {
            c := f.get_constant(r_original);
           
            // phis are just mov which can have an immediate
            if(is_int(k) && phi, => return());
            
            r_output := f.newtmp("isel", k);
            // Most instructions can't have immediates. 
            // So we add a new instruction that produces the constant, so the actual operation has registers for both inputs. 
            if is_int(k) {
                f.emit(.copy, k, r_output, r_original, QbeNull);
            } else {
                // They don't give you float immediates (even for mov). 
                // So we load the bits into an integer register and then move it to a float register. 
                // TODO: qbe loads float constants from memory (and so does clang), so perhaps thats much faster. 
                //       but I don't want to deal with the different backends yet (jit vs aot) so it feels easier to just put it in an int register and then move it to float.
                //       I tried this change in qbe and it passed thier tests so i think it is just a performance thing. 
                //       Perhaps I will change this back at some point. It feels weird to me to let instruction selection produce data constants tho. 
                ki := @if(k == .Ks, Qbe.Cls.Kw, .Kl);
                r_int := f.newtmp("isel", ki);
                f.emit(.cast, k, r_output, r_int, QbeNull);
                f.emit(.copy, ki, r_int, r_original, QbeNull);
            };
            pr[] = r_output;
        }
        fn RTmp() => {
            slot_offset := f.get_temporary(r_original)[].slot;
            if slot_offset != -1 {
                // This tmp holds the address of a stack slot. Produce it by offseting the frame pointer. 
                // note: newtmp, not assigning to the old alloca tmp. 
                //       keeps ssa and spill() relies on t.slot being unset on all used tmps. 
                r_slot := f.newtmp("isel", .Kl);  // :IselRSlotGetsNewTmp
                def := f.emit(.addr, .Kl, r_slot, SLOT(slot_offset.zext()), QbeNull);
                t := f.get_temporary(r_slot);
                t.def = def;
                t.nuse = 1;
                t.ndef = 1;
                pr[] = r_slot;
            }; // otherwise its already in a register
        }
        @default => ();
    }; // early returns
}

fn callable(f: *Qbe.Fn, r: Qbe.Ref) bool = {
    // TODO: should have a get_constant that returns an optional? 
    if rtype(r) == .RCon {
        c := f.get_constant(r);
        return c.type() == .CAddr && c.bits() == 0;
    };
    rtype(r) == .RTmp
}

Imm :: @enum(i64) (
    Iother,
    PosLow12,
    PosHigh12,
    PosLow24,
    NegLow12,
    NegHigh12,
    NegLow24,
);

fn sign_extend_low32(x: i64) i64 = {
    x: u32 = x.trunc();
    x: i32 = x.bitcast();
    x.intcast()
}

fn imm(c: *Qbe.Con, k: Qbe.Cls, pn: *i64) Imm = {
    ::if(Imm);  ::enum_basic(Imm);
    if c.type() != .CBits {
        return(.Iother);
    };
    n := c.bits();
    if k == .Kw {
        n = sign_extend_low32(n);
    };
    positive := true;
    if n < 0 {
        positive = false;
        n = -n;  // this handles MIN_i64 wrong but it doesn't matter because it doesn't match any of these cases anyway
    };
    pn[] = n;
    if n.bit_and(0x000fff) == n {
        return(if(positive, => .PosLow12, => .NegLow12));
    };
    if n.bit_and(0xfff000) == n {
        return(if(positive, => .PosHigh12, => .NegHigh12));
    };
    if n.bit_and(0xffffff) == n {
        return(if(positive, => .PosLow24, => .NegLow24));
    };
    .Iother
}

// this only makes sense once we're in ssa form
fn find_source_instruction(b: *Qbe.Blk, search: Qbe.Ref) ?*Qbe.Ins #inline = {
    // backwards because used when looking for cmp for the terminating jmp so its probably near the end. 
    for_insts_rev b { i | 
        if i.to == search {
            return(Some = i[])
        }
    };
    .None
}

#use("@/backend/arm64/target.fr");
#use("@/backend/lib.fr");
#use("@/backend/abi.fr");
