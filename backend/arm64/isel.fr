// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! - Assign stack slots to allocs in the start block.
//! - Most instructions must have both operands in registers. 
//! - Cmp can encode some immediates. But only on the right so you might have to swap and invert. 
//! - add/ldr/str can encode some immediates.
//! - Fuse chained memory offsets with stack slots. 
//! - Combine cmp+jnz into jmp.cc.

// TODO: reuse flag setting instructions like amd64/isel

fn arm64_isel(f: *Qbe.Fn) void = {
    f.assign_alloc_slots();
    // In this pass, if `t.slot != -1` then `TMP(t) == addr SLOT(t.slot)`
    for_blocks f { b |
        for_insts_forward b { i |
            collapse_op_arm64(f, i, b.id);
        };
    };
    
    for_blocks f { b |
        f.globals.curi[] = f.scratch_start(); // TODO: remove this once we always reset it at the end of a pass. 
        
        for_jump_targets b { sb |
            for_phi sb { p |
                n := index_in_phi(b, p);
                f.fixarg(p.arg&.index(n), p.cls, true, b.id);
            };
        };
        f.seljmp(b);
        
        for_insts_rev b { i | 
            if i[].op() == .sel1 {
                selsel_arm64(f, b, i[]);
            } else {
                f.sel(i[][], b.id);
            };
        };

        f.copy_instructions_from_scratch(b);
    };
    if f.globals.debug["I".char()] {
        write(f.globals.debug_out, "\n> After instruction selection:\n");
        printfn(f, f.globals.debug_out);
    };
}

fn collapse_op_arm64(f: *Qbe.Fn, i: *Qbe.Ins, bid: i32) void = {
    @if(is_int(i.cls()))
    @match(i.op()) {
        fn add() => {
            // you could swap if the left is constant, but i found it happens 62 out of 174835 times you get here -- Jan 18, 2025. 
            if f.get_int(i.arg&[1]) { off | 
                if f.is_symbol(i.arg&[0]) {
                    // If this symbol is only used once in the block it makes sense to fold the offset into the adrp+add,
                    // But it seems the more common case is you use the same symbol multiple times and read different offests, 
                    // So it's nicer to create the constant in a register once and then fold all the small offsets into the memory instructions. 
                    // (opt/slots deduplicates addr(RCon)) + fewer relocations should make linker stuff faster (?).
                    // -- Jan 19, 2025
                    
                    // c := f.get_constant(i.arg&[0])[];
                    // c.bits.i += off;
                    // i[] = make_ins(.copy, .Kl, i.to, f.newcon(c&), QbeNull);
                    return();
                };
                slot := f.get_slot(i.arg&[0]) || { 
                    r := i.arg&[0];
                    if(rtype(r) != .RTmp || isreg(r), => return()); 
                    t := f.get_temporary(r);
                    if(t.def.is_null(), => return());
                    if t.defining_block != bid {
                        // we might not have got there yet.
                        collapse_op_arm64(f, t.def, t.defining_block);
                    };
                    @match(t.def.op()) {
                        fn addr() => f.get_slot(t.def.arg&[0]).expect("this happens before other isel so only RSlot can be addr");
                        fn copy() => f.get_slot(t.def.arg&[0]) || return();
                        fn add()  => {
                            if f.get_int(t.def.arg&[1]) { off2 | 
                                if off + off2 < 4096 {
                                    chuse(t.def.arg&[0], 1, f);
                                    chuse(r, -1, f);
                                    i.arg&[0] = t.def.arg&[0];
                                    i.arg&[1] = f.getcon(off + off2);
                                    return(collapse_op_arm64(f, i, bid));
                                };
                            };
                            return()
                        }
                        @default => return();
                    }
                };
                // negative slots are abi push stuff
                if slot >= 0 {
                    new := slot + off;
                    if new >= 0 {
                        chuse(i.arg&[0], -1, f);
                        i[] = make_ins(.addr, .Kl, i.to, SLOT(new), QbeNull);
                    };
                };
            };
        }
        fn ceqw() => fix_ceqw_not(f, i);  // TODO: this is probably a dumb place to do this. 
        @default => ();
    };
}

fn get_slot(f: *Qbe.Fn, r: Qbe.Ref) ?i64 = {
    @match(rtype(r)) {
        fn RSlot() => return(Some = rsval(r).intcast());
        fn RTmp() => {
            slot := f.get_temporary(r)[].slot;
            if slot != -1 {
                return(Some = slot.intcast());
            };
        }
        @default => ();
    };
    .None
}

//
// This will compress a sequence like `add x1, x29, #24; add x1, x1, #8; ldr x1, [x1]` into `ldr x1, [x29, #32]`
// (doesn't need to recurse because of the prepass of collapse_op_arm64)

fn fix_memarg(f: *Qbe.Fn, i: *Qbe.Ins, bid: i32) *Qbe.Ref = {
    iarg := i.arg&;
    store := is_store(i.op());
    @debug_assert(store || is_load(i.op()), "bad mem op");
    @debug_assert(store || iarg[1] == QbeNull, "load already has second arg");
    size: i64 = if(store, => size_of_store(i).zext(), => size_of_load(i).zext());
    r := iarg[store.int()]&;
    new_addr, off := fuse_addr(f, r[], bid, size);
    
    max_off := size * 4096;
    if(off < 0 || off >= max_off || off.mod(size) != 0, => return(r));
    if(new_addr == QbeNull && off.mod(4) != 0, => return(r));
    
    chuse(new_addr, 1, f);
    chuse(r[], -1, f);
    
    @debug_assert(off.mod(size) == 0);
    if new_addr != r[] {
        if new_addr == QbeNull {
            r[] = SLOT(off);
        } else {
            if store {
                if off == 0 {
                    iarg[1] = new_addr;
                } else {
                    m := f.new_mem(new_addr, QbeNull, off, 1);
                    iarg[1] = m;
                    r = f.get_memory(m)[].base&;
                };
            } else {
                iarg[0] = new_addr;
                iarg[1] = INT(off);
            };
        };
    };
    r
}

fn fuse_addr(f: *Qbe.Fn, a: Qbe.Ref, bid: i32, size: i64) Ty(Qbe.Ref, i64) = {
    max_off := size * 4096;
    if f.get_slot(a) { slot |
        return(QbeNull, slot);
    };
    addr := f.get_temporary(a);
    if(addr.def.is_null() || addr.defining_block != bid, => return(a, 0));
    def_a := addr.def.arg&;
    @debug_assert(addr.nuse > 0);
    @match(addr.def.op()) {
        fn add() => if f.get_int(def_a[1]) { off | 
            @debug_assert(def_a[0] != QbeNull, "add missing arg");
            return(def_a[0], off);
        };
        // TODO: stop representing RSlot as divided by 4 so you don't have to skip doing this for access of a single byte 3/4 of the time. -- Nov 3
        fn addr() => {
            slot := f.get_slot(def_a[0]).expect("can only addr stack slot");
            return(QbeNull, slot);
        }
        @default => ();
    };
    (a, 0)
}

fn sel(f: *Qbe.Fn, i: Qbe.Ins, blk: i32) void = {
    if(try_kill_inst(f, i&), => return());
    
    if is_alloc(i&.op()) {
        // note: i0 is what we're about to emit not what we just emitted
        i0 := f.globals.curi[].offset(-1); // :LookAtLastInst
        is_dyn := salloc(i.to, i.arg&[0], f);
        f.fixarg(i0.arg&.index(0), .Kl, false, blk);  // .salloc
        if is_dyn {
            f.fixarg(i0.offset(-1)[].arg&.index(1), .Kl, false, blk);  // .and
        };
        return();
    };
    ck := Qbe.Cls.zeroed();
    cc: i32 = 0;
    if iscmp(i&.op(), ck&, cc&) {
        i0 := f.emit(.flagieq, i&.cls(), i.to, QbeNull, QbeNull);
        if f.selcmp(i.arg, ck, blk) {
            x := i0.op().raw() + cmp_swap(cc);
            i0.set_op(@as(Qbe.O) x);
        } else {
            x := i0.op().raw() + cc;
            i0.set_op(@as(Qbe.O) x);
        };
        return();
    };
    // TODO: what does the callable check do? i don't have a test that needs it. 
    if (i&.op() == .call && f.callable(i.arg&[0])) {  // .asm has const u32 arg that we don't want to fix
        f.emit(i);
        return();
    };
    o := i&.op();
    
    @match(o) {
        fn asm() => f.emit(i);
        fn nop() => ();
        fn ones() => { 
            // you can only popcnt on a simd/float register
            f_in := f.newtmp("isel", .Kd);
            f_out := f.newtmp("isel", .Kd);
            ki := i&.cls();
            kf: Qbe.Cls = @if(ki.is_wide(), .Kd, .Ks);
            f.emit(.cast, ki, i.to, f_out, QbeNull);
            f.emit(.ones, kf, f_out, f_in, QbeNull);
            icast := f.emit(.cast, kf, f_in, i.arg&[0], QbeNull);
            f.fixarg(icast.arg&.index(0), ki, false, blk);
        };
        fn rotl() => {
            new_ins := f.emit(i);
            iarg := new_ins.arg&;
            k0 := argcls(i&, 0);
            new_ins.set_op(.rotr);
            width := 1.shift_left(5 + k0.is_wide().int());
            if f.get_int(iarg[1]) { imm |
                iarg[1] = f.getcon(width - bit_and(imm, width - 1));
                return();
            };
            r0 := f.newtmp("isel", k0);
            r1 := f.newtmp("isel", k0);
            ii := f.emit(.sub, k0, r0, f.getcon(width), r1);
            f.fixarg(ii.arg&.index(0), k0, false, blk);
            ii := f.emit(.and, k0, r1, iarg[1], f.getcon(width - 1));
            f.fixarg(ii.arg&.index(1), k0, false, blk);
            f.fixarg(ii.arg&.index(0), k0, false, blk);
            iarg[1] = r0;
        };
        fn trace_prev() => {
            @assert_eq(rtype(i.arg&[0]), .RTmp);
            f.emit(.load, .Kl, i.to, i.arg&[0], QbeNull);
        }
        fn trace_return() => {
            @assert_eq(rtype(i.arg&[0]), .RTmp);
            f.emit(.load, .Kl, i.to, i.arg&[0], INT(8));
        }
        @default => {
            new_ins := f.emit(i);
            iarg := new_ins.arg&;
            
            if rtype(i.to) == .RTmp {
                t := f.get_temporary(i.to);
                t.def = new_ins;
            };
            if is_load(o) && rtype(iarg[0]) == .RTmp {
                r := fix_memarg(f, new_ins, blk);
                if rtype(r[]) != .RSlot {
                    f.fixarg(r, .Kl, false, blk);
                };
                return();
            };
            k0 := argcls(i&, 0);
            k1 := argcls(i&, 1);
            copy_from_con := i&.op() == .copy && rtype(iarg[0]) == .RCon && !isreg(i.to);
            if !copy_from_con {
                f.fixarg(iarg[0]&, k0, false, blk);
            };
            if is_store(o) && rtype(iarg[1]) == .RTmp {
                r := fix_memarg(f, new_ins, blk);
                if rtype(r[]) != .RSlot {
                    f.fixarg(r, .Kl, false, blk);
                };
                return();
            };
            // TODO: do this for sub, etc as wel. this alone makes the compiler go from 2485100 -> 2089352 bytes of code. thats insane.
            //       this looks branchy but it generates so much less code that its faster even before accounting for it generating better code for itself. 
            if o == .add && is_int(k1) {
                // could convert to sub `if off < 0 && off > -4096` but that never happens in self-compile and then would have to add a case to fuse_addr -- Nov 7
    
                // ~358KB, amazing! (that was before other addr folding i think, now its less impressive )
                if f.get_int(iarg[1]) { imm |
                    if imm.bit_and(1.shift_left(12) - 1) == imm && iarg[0] != TMP(Arm64Reg.SP) {
                        if imm == 0 {
                            iarg[1] = QbeNull;
                            new_ins.set_op(.copy);
                        };
                        return();
                    };
                };
                // ~10KB, meh
                arg1 := iarg[1];
                if rtype(arg1) == .RTmp {
                    arg1 := f.get_temporary(arg1);
                    def := arg1.def;
                    if arg1.nuse == 1 && !def.is_null() && def.op() == .mul && arg1.defining_block == blk {
                        f.fixarg(def.arg&.index(0), .Kl, false, blk);
                        f.fixarg(def.arg&.index(1), .Kl, false, blk);
                        iarg[1] = f.new_mem(def.arg&[0], def.arg&[1], 0, 255);
                        def.set_nop();
                        return();
                    };
                };  
            };
            // ~1KB, not really worth it
            if o == .sub && is_int(k1) {
                if f.get_int(iarg[1]) { imm |
                    if imm.bit_and(1.shift_left(12) - 1) == imm && iarg[0] != TMP(Arm64Reg.SP) {
                        if imm == 0 {
                            iarg[1] = QbeNull;
                            new_ins.set_op(.copy);
                        };
                        return();
                    };
                };
            };
            
            if (@is(o, .shl, .shr, .sar, .rotr)) {
                // Since these are done modulo data size, we can fit any constant in the immediate. 
                if f.get_int(iarg[1]) { _ |
                    return();
                };
            };
            if o == .ctz {  // 0b101101011000000000110 is ctz... but it doesn't exist? I guess i don't have FEAT_CSSC?
                new_ins.set_op(.clz);
                r := f.newtmp("isel", k0);
                f.emit(.bitswap, k0, r, iarg[0], QbeNull);
                iarg[0] = r;
            };
            
            f.fixarg(iarg[1]&, k1, false, blk);
            
            if new_ins.op() == .cas0 { 
                while => new_ins.offset(1).op() != .cas1 {
                    j := new_ins.offset(1);
                    t := new_ins[]; new_ins[] = j[]; j[] = t;
                    new_ins = j;
                };
            }
        };
    };
}

// This is where we combine cmp+jnz.
fn seljmp(f: *Qbe.Fn, b: *Qbe.Blk) void = {
    @debug_assert_ne(b.jmp.type, .switch);
    
    // TODO: could flip this exit and assert condition so its just one !=jnz in release mode. 
    if b.jmp.type != .jnz {
        msg :: "the frontend only makes jnz conditionals and selret in abi pass removes other types of ret.";
        @debug_assert(b.jmp.type == .ret0 || b.jmp.type == .jmp || b.jmp.type == .hlt, msg);
        return();
    };
    // If the jump condition was created in this basic block, is only used once, and was made by a comparison, 
    // we can pull the comparison forward into the jump instruction and just jump based on the cpu flags set by the cmp instruction.
    // Otherwise, we have to actually produce the value and compare it to 0. 
    
    jump_arg := b.jmp.arg;
    b.jmp.arg = QbeNull;
    if find_source_instruction(b, jump_arg) { i |
        uses := f.get_temporary(jump_arg)[].nuse;
        cc: i32 = 0;
        cmp_cls := Qbe.Cls.zeroed();
        if uses == 1 && iscmp(i.op(), cmp_cls&, cc&) {
            if f.selcmp(i.arg, cmp_cls, b.id) {
                cc = cmp_swap(cc);
            };
            xxx: i32 = Qbe.J.jfieq.raw() + cc;
            b.jmp.type = @as(Qbe.J) xxx;
            i.set_nop();
            return();
        } // else fallthrough
    };
    // TODO: init array from tuple literal
    f.selcmp(init(@slice(jump_arg, QbeConZero)), .Kw, b.id); // :jnz_is_Kw
    b.jmp.type = .jfine;
}

// TODO: this is so similar to seljmp. but im afraid trying to factor them together would just make it more confusing. 
fn selsel_arm64(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    isel0 := i.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    r := isel0.arg&[0];
    if rtype(r) != .RTmp {
        // This should never happen because of constant folding but let's play it safe. 
        r = f.newtmp("isel", .Kw);
    };
    cr0, cr1 := (r, QbeConZero);
    c: i32 = Qbe.Cmp.Cine.raw();
    needed_swap := false;
    cmp_cls := Qbe.Cls.Kw; // :jnz_is_Kw // t.cls;
    if find_source_instruction(b, r) { fi |
        t := f.get_temporary(r)[];
        if t.nuse == 1 && iscmp(fi.op(), cmp_cls&, c&) {
            needed_swap = rtype(fi.arg&[0]) == .RCon && is_int(cmp_cls); // :arm64_cmp_will_swap
            if needed_swap {
                // selcmp will swap the args so don't do that to crN here, 
                // but we need to update the condition so we use the right one in selKCC.
                c = cmp_swap(c);
            };
            cr0 = fi.arg&[0];
            cr1 = fi.arg&[1];
            fi.set_nop();
        };
    };
    
    isel1 := i;
    @debug_assert_eq(isel1.op(), .sel1);
    o := @as(Qbe.O) @as(i32) Qbe.O.selieq.raw() + c;
    isel1.set_op(o);
    sel(f, isel1[], b.id);
    
    // TODO: init array from tuple literal
    swapped := f.selcmp(init(@slice(cr0, cr1)), cmp_cls, b.id);
    @debug_assert_eq(needed_swap, swapped, "confused about cmp swapping");
    if r != isel0.arg&[0] {
        f.emit(.copy, .Kw, r, isel0.arg&[0], QbeNull);
    };
    isel0.set_nop();
}

// Try to compare to a constant using an immidiate. 
// Returns true if you need to invert the condition because we swapped the arguments. 
// note: we handle the terminator first so we only get here if it we didn't jump on the result of this comparison.
fn selcmp(f: *Qbe.Fn, arg: Array(Qbe.Ref, 2), k: Qbe.Cls, blk: i32) bool = {
    if !is_int(k) {
        // fcmp can't take an immdiate, so make sure both args are registers. (TODO: it can compare to 0 tho)
        iarg := f.emit(.afcmp, k, QbeNull, arg&[0], arg&[1])[].arg&;
        f.fixarg(iarg[0]&, k, false, blk);
        f.fixarg(iarg[1]&, k, false, blk);
        return(false);
    };
    
    // cmp can only have an immediate on the right. so we might have to swap them and invert the condition. 
    needed_swap := rtype(arg&[0]) == .RCon;  // :arm64_cmp_will_swap
    if needed_swap {
        // TODO: not true for function pointers i guess which is fair because we don't technically know if the symbols will resolve to the same place. 
        //@debug_assert(rtype(arg&[1]) != .RCon, "cmp of constant should already have been folded. we do handle this case below tho so its fine if you want to remove that pass and this check.");
        arg&.items().swap(0, 1);
    };
    
    cant_fit_immediate := true;
    cmp := Qbe.O.acmp;
    r_right := arg&[1];
    if rtype(r_right) == .RCon {
        c := f.get_constant(r_right);
        n := 0; // TODO: just use multiple return values.
        // TODO: annoying that i don't have fallthrough for @match
        imm := imm(c, k, n&);
        cant_fit_immediate = @is(imm, .PosLow24, .NegLow24, .Iother);
        if (@is(imm, .NegLow12, .NegHigh12)) {
            // if we're comparing to a negative number, there's a different instruction for that. 
            cmp = .acmn;
            r_right = f.getcon(n);
        };
    };
    
    iarg := f.emit(cmp, k, QbeNull, arg&[0], r_right)[].arg&;
    f.fixarg(iarg[0]&, k, false, blk); // it could be the address of a stack slot or thread local, i guess?
    if cant_fit_immediate {
        f.fixarg(iarg[1]&, k, false, blk);
    };
    needed_swap
}

::enum_basic(Qbe.RegKind);
// Force the value to be in a register (if its a constant, produce it in a different reg and then move).
fn fixarg(f: *Qbe.Fn, pr: *Qbe.Ref, k: Qbe.Cls, phi: bool, blk: i32) void = {
    r_original := pr[];
    @match(rtype(r_original)) {
        fn RCon() => {
            c := f.get_constant(r_original);
           
            // phis are just mov which can have an immediate
            if(is_int(k) && phi, => return());
            
            r_output := f.newtmp("isel", k);
            // Most instructions can't have immediates. 
            // So we add a new instruction that produces the constant, so the actual operation has registers for both inputs. 
            // TODO: there are a few instructions that have a small range of immediates. like add_im/sub_im can take a u12 (optionally shifted).
            if is_int(k) {
                f.emit(.copy, k, r_output, r_original, QbeNull);
            } else {
                // They don't give you float immediates (even for mov). 
                // So we load the bits into an integer register and then move it to a float register. 
                // TODO: qbe loads float constants from memory (and so does clang), so perhaps thats much faster. 
                //       but I don't want to deal with the different backends yet (jit vs aot) so it feels easier to just put it in an int register and then move it to float.
                //       I tried this change in qbe and it passed thier tests so i think it is just a performance thing. 
                //       Perhaps I will change this back at some point. It feels weird to me to let instruction selection produce data constants tho. 
                // TODO: assert in bounds based on `KWIDE(k) ? 8 : 4`?
                r_int := f.newtmp("isel", .Kl);
                f.emit(.cast, k, r_output, r_int, QbeNull);
                f.emit(.copy, .Kl, r_int, r_original, QbeNull);
            };
            pr[] = r_output;
        }
        fn RTmp() => {
            slot_offset := f.get_temporary(r_original)[].slot;
            if slot_offset != -1 {
                // This tmp holds the address of a stack slot. Produce it by offseting the frame pointer. 
                r_slot := f.newtmp("isel", .Kl);
                def := f.emit(.addr, .Kl, r_slot, SLOT(slot_offset.zext()), QbeNull);
                t := f.get_temporary(r_slot);
                t.def = def;
                t.nuse = 1;
                t.ndef = 1;
                t.defining_block = blk;
                pr[] = r_slot;
            }; // otherwise its already in a register
        }
        @default => ();
    }; // early returns
}

fn callable(f: *Qbe.Fn, r: Qbe.Ref) bool = {
    if rtype(r) == .RTmp {
        return(true);
    };
    
    // TODO: should have a get_constant that returns an optional? 
    if rtype(r) == .RCon {
        c := f.get_constant(r);
        if c.type == .CAddr && c.bits.i == 0 {
            return(true);
        }
    };
    false
}

Imm :: @enum(i64) (
    Iother,
    PosLow12,
    PosHigh12,
    PosLow24,
    NegLow12,
    NegHigh12,
    NegLow24,
);
::if(Imm);  ::enum_basic(Imm);

fn sign_extend_low32(x: i64) i64 = {
    x: u32 = x.trunc();
    x: i32 = x.bitcast();
    x.intcast()
}

fn imm(c: *Qbe.Con, k: Qbe.Cls, pn: *i64) Imm = {
    if c.type != .CBits {
        return(.Iother);
    };
    n := c.bits.i;
    if k == .Kw {
        // sign extend the low 32 bits. 
        // qbe did `n = (int32_t)n;` which clang says is sxtw. 
        n = sign_extend_low32(n);
    };
    positive := true;
    if n < 0 {
        positive = false;
        n = -n;  // this handles MIN_i64 wrong but it doesn't matter because it doesn't match any of these cases anyway
    };
    pn[] = n;
    if n.bit_and(0x000fff) == n {
        return(if(positive, => .PosLow12, => .NegLow12));
    };
    if n.bit_and(0xfff000) == n {
        return(if(positive, => .PosHigh12, => .NegHigh12));
    };
    if n.bit_and(0xffffff) == n {
        return(if(positive, => .PosLow24, => .NegLow24));
    };
    .Iother
}

// this only makes sense once we're in ssa form
fn find_source_instruction(b: *Qbe.Blk, search: Qbe.Ref) ?*Qbe.Ins #inline = {
    // backwards because used when looking for cmp for the terminating jmp so its probably near the end. 
    for_insts_rev b { i | 
        if i.to == search {
            return(Some = i[])
        }
    };
    .None
}
