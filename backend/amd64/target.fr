#use("@/backend/amd64/sysv.fr");

X86cc :: Amd.X86cc;

fn fill_target_amd(t: *Qbe.Target, apple: bool) void = {
    Emit :: import("@/backend/amd64/emit.fr");

    t.finish_passes = finish_qbe_passes_native;
    t.isel = import("@/backend/amd64/isel.fr").amd64_isel;
    t.abi1 = amd64_sysv_abi;
    t.argregs = amd64_sysv_argregs;
    t.retregs = amd64_sysv_retregs;
    t.fixup = Emit.fixup_amd64;
    t.emit_fn = Emit.emit_func_amd64;
    
    ::enum(Amd64Reg);
    ::List(Amd64Reg);
    
    ::List(X86cc);
    
    t.apple = apple;
    t.gpr0 = zext Amd64Reg.RAX.raw();
    t.ngpr = 16; 
    t.fpr0 = zext Amd64Reg.XMM0.raw();
    t.nfpr = zext Amd64Reg.XMM14.raw() - Amd64Reg.XMM0.raw() + 1 /* reserve XMM15 */;
    t.rglob = BIT(Amd64Reg.RBP).bit_or(BIT(Amd64Reg.RSP));
    t.caller_saved = amd64_sysv_rsave.interpret_as_bytes().bytes_to_bit_mask();
    t.nrsave&[0] = len(@slice(Amd64Reg.RAX, .RCX, .RDX, .RSI, .RDI, .R8, .R9, .R10, .R11)).trunc();
    t.nrsave&[1] = t.nfpr;
    t.memargs = amd64_memargs;
    nsave := intcast(t.nrsave&[0] + t.nrsave&[1]);
    @assert_eq(amd64_sysv_rsave.len(), nsave, "nrsave");
    @debug_assert_eq(t.caller_saved.count_ones(), trunc nsave);
}

// 
// Note: explicit memory access like load/store/cas HAS NO MEM ARGS HERE. 
// The number returned here is WHICH (not how many) arguments are allowed to be accessed 
// indirectly from thier spill slot in memory if there's a lot of register pressure. 
// 0 means none. 1 means only the first. 2 means either (but not both in the same instruction). 
//
fn amd64_memargs(o: Qbe.O) i32 = {
    @debug_assert(!o.between(.ceqw, .cuod), "C**K is frontend only");
    if o.between(.acmp, .flagfuo) || o.between(.ceqw, .blit1) || o.between(.div, .urem) || o.between(.sign, .salloc) || o.is_sel_flag() || (@is(o, .cas0, .cas1)) {
        return(0);
    };
    if o.between(.add, .sub) || o.between(.mul, .xor) {
        return(2);
    };
    if o == .neg || o == .swap || o.between(.sar, .shl) || o.between(.xidiv, .xtest) || (@is(o, .rotl, .rotr, .clz, .ctz)) {
        return(1);
    };
    if (@is(o, .asm, .syscall, .byteswap, .trace_start)) {  
        return(0);
    };
    @panic("unknown op for amd64_memargs")
}

// Note: these numbers are NOT the ones you encode in the instructions. ints are of by 1, floats are off by 1+16
Amd64Reg :: @enum(u8) (
    RXX, // reserve zero for QbeNull
    
    // ints (ordered by encoding, NOT by usage in calling convention). 
    RAX, RCX, RDX, RBX, RSP, RBP, RSI, RDI,
    R8, R9, R10, R11, R12, R13, R14, R15,
    
    /* sse */
    XMM0, XMM1, XMM2,  XMM3,  XMM4,  XMM5,  XMM6,  XMM7,
    XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15,
);

amd_condition_codes :: @const_slice( // :CmpOrder
    // Cieq Cine Cisge Cisgt Cisle Cislt Ciuge Ciugt Ciule Ciult
    X86cc.e, .ne,  .ge,  .g,  .le,  .l,  .ae,  .a,  .be,  .b,
    // Cfeq Cfge Cfgt Cfle Cflt Cfne Cfo  Cfuo
         .e, .ae, .a, .be, .b,  .ne, .np, .p
);

fn TMP(r: Amd64Reg) Qbe.Ref = TMP(@as(i64) r.raw().zext());
fn BIT(r: Amd64Reg) u64 = BIT(@as(i64) r.raw().zext());
fn CALL(r: RCallAmd) Qbe.Ref = CALL(@as(i64) r.repr.zext());

#use("@/backend/lib.fr");
