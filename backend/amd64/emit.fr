//! Unlike Qbe, we don't rely on a seperate assembler. We just put the bytes in memory and jump there. 
//! This means we also have to handle updating forward references to functions that haven't been compiled yet. 

#use("@/backend/lib.fr");
#use(Amd);

AmdState :: @struct(
    f: *Qbe.Fn,
    m: *QbeModule,
    // For leaf functions with statically known stack frame size, 
    // we don't need to bother saving RBP and can just index slots from RSP instead. 
    fp    := Amd64Reg.RBP,
    fsz   := 0,
    nclob := 0,
    a: List(u8),
    start_of_function: i64,
    patches: List(Patch),
);

fn emit_func_amd64(f: *Qbe.Fn) []Ty(Fixup, Qbe.Sym) = {
    m := f.globals;
    @debug_assert_eq(m.goal.arch, .x86_64, "how did we get here?");
    code := m.segments&[.Code]&;
    prev := code.next;
    e: AmdState = (
        f = f,
        m = m,
        a = @if(m.goal.blink_smc,
            m.scratch_for_blink_smc,
            (maybe_uninit = (ptr = code.next, len = code.cap() - code.len()), len = 0, gpa = panicking_allocator),
        ),
        start_of_function = code.len(),
        patches = list(f.nblk.zext(), temp()),
    );
    
    emit_the_code(e&);
    @debug_assert(identical(prev, code.next), "can't have multiple threads emitting code");

    // 
    // blink_smc; July 4, 2025.
    // blink (a linux emulator) handles W+X memory by mapping it as R+X and catching the 
    // fault every time you write to it (which is slow since i want to jit code byte by byte). 
    // so instead of weaving the writes to that memory in with other work, emit code into scratch 
    // space and copy a whole function at once. blink's readme claims that a loop with jmpCC 
    // instead of normal jmp instructions will only eat one fault... that appears to have regressed.  
    // (with some changes to jnz handling below i can produce a copy_bytes() that doesn't use jmp, 
    // and it still faults for each i64 copied. easier to repro by trying blink's smc2_test.c and 
    // observing that there are 300+ faults instead of the advertised 20). anyway, the rep+movsb 
    // instruction implements memcpy() and blink handles that with ~1 fault per call, so 
    // that's what I use here. it's only necessary for JitOnly+linux+x86_64 (and you can 
    // even detect running under blink with the CPUID instruction) but my compiler's so 
    // slow that this copy is negligable so behaving consistantly on all platforms seems nicer for now. 
    //
    if m.goal.blink_smc {
        if query_current_arch() == .x86_64 {
            repmovs :: AsmFunction(fn(dest: *u8, src: *u8, count: i64) void = (), @const_slice(
                brk(0),  // unreachable
            ), (fn(out) = @asm_x64( // (rdi, rsi, rdx)
                encode_bin(PrimaryOp.MovReg, X86Reg.rcx, X86Reg.rdx),
                0xF3, 0b01001000, 0xA4, // REP MOVS
                PrimaryOp.Ret,
            ) out));
            repmovs(code.next, e.a.maybe_uninit.ptr, e.a.len);
        } else {
            code.next.slice(e.a.len).copy_from(e.a.items());
        }
    };
    code.next = code.next.offset(e.a.len());
    
    empty()
}

#use("@/lib/sys/process.fr");
SysV :: import("@/backend/amd64/sysv.fr");

fn fixup_amd64(self: *QbeModule, symbol: *SymbolInfo, fixup: *Fixup, new_got_reloc: *?Fixup) void = {
    address := symbol.jit_addr;
    local  := symbol.kind == .Local;
    @match(fixup.type&) {
        fn DataAbsolute(it) => {
            rawptr.ptr_from_raw(fixup.patch_at)[] = address.offset(it.increment);
        }
        fn RipDisp32(it) => {
            distance := ptr_diff(fixup.patch_at, address); // :size_of_displacement
            if !local {
                if !it.call {
                    when_debug(self, .Patch, fn(out) => @fmt(out, "# patch addr % refs %=%\n", fixup.patch_at, address, symbol.name));

                };
                
                if !it.call {                    
                    // In .addr we assumed we could LEA to create the address but since it's an import, 
                    // it needs to be converted into a load from the GOT.
                    // this is super hacky. i feel like we could guess wrong since we're not decoding from the beginning. 
                    opcode := u8.ptr_from_raw(fixup.patch_at).offset(-2); // [?rex:1, opcode:1, modrm:1, patch_at:offset:4]
                    @assert_eq(opcode[], 0x8D, "only LEA can be converted to GOT load (for $%)", symbol.name);
                    opcode[] = 0x8B; // load
                    // TODO: need to make sure modrm is .Indirect00, .rbp
                    
                    @assert_eq(it.increment, 0, "cannot fold offset into GOT load");
                    if self.goal.type == .Relocatable {
                       it.got_load = true;
                       return(); // that's not my department
                    };
                    
                    new_got_reloc[] = ensure_got_slot(self, symbol, address).or_else(=> new_got_reloc[]);
                    got_addr := self.segment_address(.ConstantData, symbol.got_lookup_offset);
                    distance = ptr_diff(fixup.patch_at, got_addr);
                } else {
                    if(self.goal.type == .Relocatable, => return());  // that's not my department
                    
                    when_debug(self, .Patch, fn(out) => @fmt(out, "# patch call % calls %\n", fixup.patch_at, symbol.name));
                    if symbol.offset == -1 {
                        size_of_jmp_encoding :: 6;
                        trampoline := reserve_stub(self, symbol, Array(u8, size_of_jmp_encoding));
                        new_got_reloc[] = ensure_got_slot(self, symbol, address).or_else(=> new_got_reloc[]);
                        distance := self.distance_between(.Code, symbol.offset + size_of_jmp_encoding, .ConstantData, symbol.got_lookup_offset);
                        trampoline := fixed_list(trampoline.items());
                        if self.goal.exe_debug_symbol_table {  // :ShowSymbolNamesForStubs
                            self.local_needs_reloc&.push(symbol.id);
                        };
                        
                        encode_indirect_call(trampoline&, distance.trunc(), false);
                    };
                    stub_trampoline := self.segment_address(.Code, symbol.offset);
                    distance = ptr_diff(fixup.patch_at, stub_trampoline); // :size_of_displacement
                };
            } else {
                if !it.call {
                    when_debug(self, .Patch) { out |
                        @fmt(out, "# patch addr % refs '%' = %[%]\n", fixup.patch_at, symbol.name, symbol.segment, symbol.offset);
                    };
                };
            };// early returns
            
            // :IncOffByFour
            // I computed the distance from the start of the displacement slot 
            // but the cpu wants it from the end of the instruction. 
            size_of_displacement :: 4;
            distance -= size_of_displacement;
            
            @debug_assert(fits_in_i32(distance), "can't reference that far");
            u32.ptr_from_raw(fixup.patch_at)[] = trunc(distance + it.increment);
        }
        @default => panic("amd64 unexpected fixup type");
    }
}

// glibc expects the stack to be 16 byte aligned, before the call instruction. 
// so then call pushes return address and it's unaligned, and then saving rbp realigns it. 
// and we need the vararg slots for saving floats to be 16 aligned

fn framesz(e: *AmdState) void = {
    /* specific to NAlign == 3 */
    o := 0;
    if !e.f.leaf {
        // checking if clobber save area is aligned to 16 bytes
        o = e.f.reg.bit_and(SysV'amd64_sysv_rclob_mask).count_ones().zext().mod(2);
    };
    
    f := e.f.slot / 4;
    f = bit_and(f + 3, -4);
    if f > 0 && e.fp == .RSP && e.f.salign == 4 {
        f += 2;
    };
    e.fsz = 4*f.zext() + 8*o + (176)*e.f.vararg.int();
}

fn next_inst_offset(self: *AmdState) i64 = self.a.len;

fn emit_the_code(e: *AmdState) void = {
    // It seems you need this to allow being called through a function pointer, that's such a pain in the ass bro... 
    // It stops people from doing evil ROP stuff or whatever. 
    // The magic words to google are "Control-Flow Enforcement Technology"
    // TODO: This is extra incentive to keep track of which things are allowed to be called indirectly 
    //       (which would also be nice for removing functions if all calls are inlined). 
    e.a&.push_u32(endbr64);
    
    f := e.f;
    need_frame := !f.leaf || f.vararg || f.dynalloc;
    if need_frame {
        e.fp = .RBP;
        // TODO: this doesn't need modrm(w=1), it's always 64 bit
        e.a&.encode_op_reg(PrimaryOp.PushBase, X86Reg.rbp);
        e.a&.encode_bin(PrimaryOp.MovReg, X86Reg.rbp, X86Reg.rsp);
    } else {
        e.fp = .RSP;
    };
    framesz(e);
    if e.fsz != 0 {
        @debug_assert_ult(e.fsz, MAX_i32);
        // TODO: using sub with a 1 byte immediate would save 3 bytes every function with a small frame
        e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -e.fsz);
    };
    if f.vararg {
        o := -176;
        for SysV'amd64_sysv_rsave.slice(0, 6) { r | 
            e.a&.encode_non_sp_offset(true, PrimaryOp.MovReg, r.int_id(), .rbp, o);
            o += 8;
        };
        range(0, 8) { n |
            // mov [rbp + o] xmmn
            e.a&.push(0x0F);
            e.a&.push(0x29);
            reg := @as(X86Reg) n;
            e.a&.push(pack_modrm(ModrmMode.Indirect00, X86Reg.rsp, reg)); // mod=0, rm=sp means use sib
            e.a&.push(pack_sib(SibScale.One, X86Reg.rbp, X86Reg.rbp));
            e.a&.push_u32(o.trunc());
            o += 16;
        };
    };
    to_save := f.reg.bit_and(SysV'amd64_sysv_rclob_mask);
    for_bits to_save { r |
        // sysv has no callee saved floats
        r := @as(Amd64Reg) @as(u8) r.trunc();
        e.a&.encode_op_reg(PrimaryOp.PushBase, r.int_id());
        e.nclob += 1;
    };

    local_labels := temp().alloc_zeroed(i64, e.f.nblk.zext());
    for_blocks f { b | 
        local_labels[b.id.zext()] = e.next_inst_offset();
        for_insts_forward b { i |
            emitins(i[], e);
        };
        @match(b.jmp.type) { 
            fn hlt() => e.a&.push(@as(u8) PrimaryOp.Int3);
            fn ret0() => {
                if f.dynalloc {
                    // recreate the statically known part of the frame. 
                    e.a&.encode_bin(PrimaryOp.MovReg, X86Reg.rsp, X86Reg.rbp);
                    e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -(e.fsz + e.nclob * 8));
                };
                for_bits_rev to_save { r | 
                    r := @as(Amd64Reg) @as(u8) r.trunc();
                    e.a&.encode_op_reg(PrimaryOp.PopBase, r.int_id());
                };
                ::enum_basic(Amd64Reg);
                @match(e.fp) {
                    fn RBP() => e.a&.push(@as(u8) PrimaryOp.Leave);
                    fn RSP() => if e.fsz != 0 {
                        e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, e.fsz);
                    };
                    @default => unreachable();
                };
                e.a&.push(@as(u8) PrimaryOp.Ret);
            }
            fn switch() => unreachable();
            fn jmp() => if !b.s1.identical(b.link) {
                e.jmp(b.s1, -1);
            };
            @default => {
                c := b.jmp.type.raw() - Qbe.J.jfieq.raw();
                @debug_assert(0 <= c && c < Qbe.NCmp, "unhandled jump.");
                if b.link.identical(b.s2) {
                    s := b.s1;
                    b.s1 = b.s2;
                    b.s2 = s;
                } else {
                    c = cmpneg(c);
                };
                
                e.jmp(b.s2, c);
                if !b.s1.identical(b.link) {
                    e.jmp(b.s1, -1);
                };
            };
        };
    };
    
    each e.patches { p |
        e.apply_patch(local_labels, p);
    };
}

// cond: -1 == Always
fn jmp(e: *AmdState, s: *Qbe.Blk, cond: i32) void #inline = {
    e.patches&.push(offset_from_start = e.a.len, cond = cond, target_bid = s.id);
    e.push_int3(if(cond == -1, => 5, => 6));  // conditional branch is a 2 byte opcode. 
}

fn apply_patch(e: *AmdState, labels: []i64, patch: *Patch) void = {
    dest := labels[patch.target_bid.zext()];
    @debug_assert(dest != 0, "missing target block %", patch.target_bid);
    write_jump_patch(e, dest, patch.offset_from_start, patch.cond);
}

fn write_jump_patch(e: *AmdState, dest: i64, src: i64, cond: i32) void = {
    // TODO: this is a dump nounification. 
    //patch_at := e.m.segments&[.Code].mmapped.ptr.offset(e.start_of_function + src);
    patch_at := e.a.maybe_uninit.ptr.offset(src);
    patch_size := @if(cond == -1, 5, 6);
    patch_bytes: List(u8) = fixed_list(ptr = patch_at, len = patch_size);
    // The offset is from **the end** of the jump instruction
    offset := dest - src - patch_size; 
    @debug_assert(offset.abs() < 1.shift_left(31), "what a big function you have");
    if cond >= 0 {
        cc := amd_condition_codes[cond.zext()];
        patch_bytes&.encode_jmp(cc, offset);
    } else {
        encode_imm32_only(patch_bytes&, PrimaryOp.JmpImm32, offset); 
    };
}

// note: you can have negative offsets!
fn slot(r: Qbe.Ref, e: *AmdState) i64 = {
    s: i64 = rsval(r).intcast();
    fslot := e.f.slot.intcast();
    // I kinda can't do this now that the folding is better since maybe you just wanted to compute a garbage address (the align.ssa test does this). 
    //@debug_assert_le(s, fslot, "slot OOB");
    /* specific to NAlign == 3 */
    if s < 0 {
        if(e.fp == .RSP, => return(-s - 8 + e.fsz + e.nclob*8));
        return(-s);
    };
    if(e.fp == .RSP, => return(s + e.nclob*8));
    if(e.f.vararg, => return(-176 -(fslot - s)));
    -(fslot - s)
}

fn emitcopy(dest: Qbe.Ref, src: Qbe.Ref, k: Qbe.Cls, e: *AmdState) void = {
    i := make_ins(.copy, k, dest, src, QbeNull);
    emitins(i, e);
}

amd64_xmm_scratch :: TMP(@as(i64) Amd64Reg.XMM0.raw().zext() + 15);

fn convert_to_2address(e: *AmdState, i: *Qbe.Ins) void = {
    if(i.to == QbeNull, => return());
    if i.arg&[1] == QbeNull {
        i.arg&[1] = i.to;
        i.arg&.items().swap(0, 1);
        return();
    };
    o := i.op();
    commutative := @is(o, .add, .and, .xor, .or, .mul, .min, .max);
    if commutative && i.arg&[1] == i.to {
        i.arg&.items().swap(0, 1);
    };
    if i.arg&[0] != i.to {
        @debug_assert(i.arg&[1] != i.to, "cannot convert to 2-address");
        emitcopy(i.to, i.arg&[0], i.cls(), e);
        i.arg&[0] = i.to;
    };
}

fn emitins(i: Qbe.Ins, e: *AmdState) void = {
    t0 := rtype(i.arg&[0]);
    t1 := rtype(i.arg&[1]);
    
    fix_memarg_slot(e, i.arg&.index(0));
    fix_memarg_slot(e, i.arg&.index(1));
    if i&.op() != .copy {
        fix_memarg_slot(e, i.to&);  // TODO: this will break when we try to check for equality
    };
    
    @match(i&.op()) {
        /* just do nothing for nops, they are inserted
         * by some passes */
        fn nop() => ();
        fn mul() => {
            /* here, we try to use the 3-addresss form
            * of multiplication when possible */
            if rtype(i.arg&[1]) == .RCon {
                i.arg&.items().swap(0, 1);
            };
            if is_int(i&.cls())
            && rtype(i.arg&[0]) == .RCon
            && rtype(i.arg&[1]) == .RTmp {
                c := e.f.get_constant(i.arg&[0]);
                r0, r1 := (i.to.int_reg_d(), i.arg&[1].int_reg_d());
                e.a&.push(pack_rex_si(i&.cls() == .Kl, r0, r1, DONT_CARE_REG));  // TODO: elide
                e.a&.push(0x69);
                e.a&.push(pack_modrm(ModrmMode.Direct, r1, r0));
                e.push_imm(c.bits(), 4);
                return();
            };
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        fn sub() => {
            /* we have to use the negation trick to handle
            * some 3-address subtractions */
            // because it's not commutative 
            if i.to == i.arg&[1] && i.arg&[0] != i.to {
                // TODO: make a .ssa test that gets here
                ineg := make_ins(.neg, i&.cls(), i.to, i.to, QbeNull);
                emitins(ineg, e);
                i&.set_op(.add);
                emitins(i, e);
                return();
            };
            
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        fn sign() => {
            if i&.cls() == .Kl {    
                e.a&.push(0b01001000);  // w=true
            };
            e.a&.push(@as(u8) PrimaryOp.SignExtendAxToDxAx); // CQO
        }
        fn neg() => {
            if is_int(i&.cls()) {
                if i.to != i.arg&[0] {
                    ins := make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull);
                    emitins(ins, e);
                    i.arg&[0] = i.to;
                };
                assemble(e, i&);
            } else {
                // You can't just lower these in isel because we insert new ones to handle 3-address subtract. 
                // TODO: this is so garbage. im doing extra work just to do a worse job. should just do what qbe does. 
                
                @debug_assert_ne(amd64_xmm_scratch, i.arg&[0]);
                
                // xmm15 = 0 
                xmm15 :: int_reg_d(amd64_xmm_scratch);  // TODO: compiler hangs if you typo `xmm15 :: int_reg_d(xmm15);`
                e.a&.push(0x66);
                e.a&.push(pack_rex_si(true, xmm15, xmm15, DONT_CARE_REG));
                e.a&.push_all(@slice(0x0F, 0xEF)); // PXOR
                e.a&.push(pack_modrm(.Direct, xmm15, xmm15));
                
                // xmm15 = 0 - a0
                ins := make_ins(.sub, i&.cls(), amd64_xmm_scratch, amd64_xmm_scratch, i.arg&[0]);
                emitins(ins, e);
                
                // to = xmm15
                ins := make_ins(.copy, i&.cls(), i.to, amd64_xmm_scratch, QbeNull);
                emitins(ins, e);
            };
        }
        fn div() => {
            // TODO: .ssa test that gets here
            /* use xmm15 to adjust the instruction when the
            * conversion to 2-address in emitf() would fail */
            if i.to == i.arg&[1] {
                @debug_assert(!is_int(i&.cls()), "isel converts int div to xdiv");
                ins := make_ins(.copy, i&.cls(), amd64_xmm_scratch, i.to, QbeNull);
                emitins(ins, e);
                ins := make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull);
                emitins(ins, e);
                i.arg&[0] = i.to;
                i.arg&[1] = amd64_xmm_scratch;
            };
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        // TODO: kinda garbage that this is a weird super instruction that's copy+load+store+addr
        fn copy() => {
            /* copies are used for many things; see a note
            * to understand how to load big constants:
            * https://c9x.me/notes/2015-09-19.html */
            @debug_assert(rtype(i.to) != .RMem);
            if(i.to == QbeNull || i.arg&[0] == QbeNull, => return());
            if(i.to == i.arg&[0], => return());
            
            is_long_const := i&.cls() == .Kl && t0 == .RCon && e.f.get_constant(i.arg&[0]).type() == .CBits;
            if is_long_const {
                value := e.f.con[i.arg&[0].val()]&.bits();
                if rtype(i.to) == .RSlot && (value < MIN_i32 || value > MAX_i32) {
                    slot_hi := SLOT(i.to.rsval() + 1*4);
                    hi := value.shift_right_logical(32);
                    lo := value.bit_and(1.shift_left(32) - 1);
                    ins := make_ins(.storew, .Kw, QbeNull, e.f.getcon(lo), i.to);
                    emitins(ins, e);
                    ins := make_ins(.storew, .Kw, QbeNull, e.f.getcon(hi), slot_hi);
                    emitins(ins, e);
                    return();
                };
            };
            if t0 == .RCon {
                c := e.f.get_constant(i.arg&[0]);
                if isreg(i.to) {
                    if c.type() == .CAddr {
                        ins := make_ins(.addr, i&.cls(), i.to, i.arg&[0], QbeNull);
                        emitins(ins, e);
                        return();
                    };
                    if c.type() == .CBits && is_int(i&.cls()) {
                        value := c.bits();
                        
                        // TODO: doesn't work because it sets flags but isel doesn't know!
                        // This is a silly size optimisation: 2 byte encoding instead of 6,
                        // but it happens 5-10k times... so that's quite a bit of savings. 
                        // always Kw because high bits are zeroed anyway so you don't need REX byte if it's one of the low registers. 
                        //if value == 0 {
                        //    ins := make_ins(.xor, .Kw, i.to, i.to, i.to);
                        //    emitins(ins, e);
                        //    return();
                        //};
                        
                        if value >= 0 {  // free zero extend
                            i&.set_cls(.Kw);
                        };
                        
                        if value < MIN_i32 || value > MAX_i32 {
                            encode_imm64(e.a&, i.to.int_reg_d(), value.bitcast());
                            return();
                        }; // else fallthrough to assemble
                    }
                };
                @debug_assert_ne(c.type(), .CAddr, "copy dest must be reg");
            };
            if rtype(i.to) == .RSlot && (t0 == .RSlot || t0 == .RMem) {
                // rega does this shit when there's a lot of register pressure i guess?
                // TODO: lower it better so it's not a sepecial case here. 
                k := @if(is_wide(i&.cls()), Qbe.Cls.Kd, .Ks);
                str := @if(is_wide(i&.cls()), Qbe.O.stored, .stores);
                ins := make_ins(.load, k, amd64_xmm_scratch, i.arg&[0], QbeNull);
                fix_memarg_slot(e, ins.arg&.index(0)); // TODO: do i need this?
                emitins(ins, e);
                ins := make_ins(str, .Kw, QbeNull, amd64_xmm_scratch, i.to);
                fix_memarg_slot(e, ins.arg&.index(1)); // TODO: do i need this?
                emitins(ins, e);
                return();
            };
            
            if t0 == .RTmp {
                @debug_assert_eq(i.arg&[0].val() <= 16, i&.cls().is_int(), "copy cls mismatch");
            };
            
            fix_memarg_slot(e, i.to&);
            
            i.arg&[1] = i.to;
            i.arg&.swap(0, 1);
            assemble(e, i&);
        }
        fn addr() => {
            // if we want to got_indirection_instead_of_patches, pessimistically use a got load 
            // if it's not known to be local to this module (since we're not allowed to patch). 
            // Otherwise, only use got if we already know it's an import (and save a patch that way). 
            // TODO: i don't understand why the later case only happens on linux but 
            //       what you have to do to patch to a load later is super sketchy 
            //       anyway so the less of that the better.       -- Feb 23, 2025. 
            r := i.arg&[0];
            if rtype(r) == .RCon {
                c := e.f.get_constant(r);
                @debug_assert(c.type() == .CAddr);
                done := false;
                use_symbol(e.m, c.sym) { symbol | 
                    done = symbol.kind != .Local && (e.m.goal.got_indirection_instead_of_patches || symbol.kind == .DynamicPatched);
                    if done {
                        if ensure_got_slot(e.m, symbol, symbol.jit_addr) { it |
                            push_fixup(e.m, symbol, it);
                        };
                        
                        // TODO: very copy paste 
                        size_of_load_encoding :: 7;
                        current_offset := e.start_of_function + e.a.len();
                        distance := e.m.distance_between(.Code, current_offset + size_of_load_encoding, .ConstantData, symbol.got_lookup_offset);
                        
                        dest := int_reg_d(i.to);
                        e.a&.push(pack_rex_si(true, dest, DONT_CARE_REG, DONT_CARE_REG)); 
                        e.a&.push(0x8B);  // load
                        e.a&.push(pack_modrm(.Indirect00, .rbp, dest));   // disp is RIP relative
                        e.a&.push_u32(distance.trunc());
                        done = true;
                        // TODO: i feel like i need to add a RipDisp32(got_load=true) here if .Relocatable? 
                    };
                }; 
                if done {
                    return();
                }
            };
            
            // note: LEA interprets RMem differently than other instructions
            i.arg&[1] = i.to;
            i.arg&.swap(0, 1);
            assemble(e, i&);
        }
        fn call() => {
            @match(rtype(i.arg&[0])) {
                fn RCon() => {
                    c := e.f.get_constant(i.arg&[0]);
                    off := c.bits();
                    @debug_assert(c.type() == .CAddr, "cannot call a constant");
                    @debug_assert_eq(off, 0, "call to offset from symbol $% seems like a mistake?", e.m.str(c.sym));
                    call_symbol(e, c.sym, i.arg&[1] != Qbe.RCALL_TAIL_SENTINAL);
                }
                fn RTmp() => encode_call_reg(e.a&, ModrmMode.Direct, int_reg_d(i.arg&[0]));  
                // TODO: we could allow memory argument? 
                @default => panic("invalid call argument");
            }
        }
        fn salloc() => {
            /* there is no good reason why this is here
            * maybe we should split Osalloc in 2 different
            * instructions depending on the result
            */
            // why not just do math on RSP directly like arm abi does? 
            // one should be changed so they're the same? 
            
            @debug_assert(e.fp == .RBP, "can't salloc when no frame");
            i.arg&[1] = TMP(Amd64Reg.RSP);
            i.arg&.items().swap(0, 1);
            i&.set_op(.sub);
            assemble(e, i&);
            
            if i.to != QbeNull {
                emitcopy(i.to, TMP(Amd64Reg.RSP), .Kl, e);
            };
        }
        fn swap() => {
            if is_int(i&.cls()) {
                assemble(e, i&);
                return();
            };
            /* for floats, there is no swap instruction
            * so we use xmm15 as a temporary
            */
            emitcopy(amd64_xmm_scratch, i.arg&[0], i&.cls(), e);
            emitcopy(i.arg&[0], i.arg&[1], i&.cls(), e);
            emitcopy(i.arg&[1], amd64_xmm_scratch, i&.cls(), e);
        }
        fn storeb() => {
            @debug_assert(rtype(i.arg&[1]) != .RTmp, "storeb to tmp");
            // this is painful because everywhere else the range of a byte immediate is signed but here its unsigned. 
            if e.f.get_int(i.arg&[0]) { c |
                c = c.bit_and(1.shift_left(8) - 1);
                i.arg&[0] = TMP(reg_for_extension(0));
                b := encode(e, i&, false, 1);
                @debug_assert(b.force_rex);
                push_instruction(e, b, @slice(0xC6));
                e.a&.push(@as(u8) c.trunc());
                return();
            };
        
            assemble(e, i&); 
        }
        fn storeh() => {
            if e.f.get_int(i.arg&[0]) { c |
                c = c.bit_and(1.shift_left(16) - 1);
                i.arg&[0] = TMP(reg_for_extension(0));
                b := encode(e, i&, false, 2);
                b.prefix = operand_16bit_prefix;
                push_instruction(e, b, @slice(0xC7));
                e.a&.reserve_type(u16)[] = c.trunc();
                return();
            };
        
            assemble(e, i&);
        }
        fn dbgloc() => e.m.add_debug_info(i&, e.a.len);
        // TODO: allow non-multiple-of-4 blocks of asm. for now you just have to pad with nops because im being consistant with the arm one. 
        fn asm() => {  
            imm := e.f.get_int(i.arg&[0]).expect("op 'asm' arg is constant int");
            e.push_imm(imm, 4);
        }
        fn byteswap() => {
            k := i&.cls();
            if i.to != i.arg&[0] {
                emitcopy(i.to, i.arg&[0], k, e);
            };
            r := i.to.int_reg_d();
            e.a&.push(pack_rex_si(k.is_wide(), DONT_CARE_REG, r, DONT_CARE_REG));
            e.a&.push(TWO_BYTE_OP_PREFIX);
            e.a&.push(0xC8 + trunc(@as(i64) bit_and(@as(i64) r, @as(i64) 0b0111)));
        }
        @default => {
            o := i&.op();
            if is_flag(i&.op()) {  // Reading a bit from the flags register
                cc := @as(i32) i&.op().raw() - Qbe.O.flagieq.raw();
                cc := amd_condition_codes[cc.zext()];
                @debug_assert(rtype(i.to) == .RTmp, "we don't allow mem arg for setcc"); // because then the zero extending is even more painful. 
                r0 := int_reg_d(i.to);
                
                e.a&.push(pack_rex_si(false, r0, r0, DONT_CARE_REG));  // Don't elide. See EncodedArgs.force_rex
                e.a&.encode_2cc(TwoByteOp.SetCC, cc);
                e.a&.push(pack_modrm(ModrmMode.Direct, r0, DONT_CARE_REG));
                
                // setcc doesn't zero the upper bits, so we zero extend as an extra instruction. 
                // we could get complicated and keep track of known bits but I suspect that's well into deminishing returns territory. 
                e.a&.push(pack_rex_si(false, r0, r0, DONT_CARE_REG));  // Don't elide. See EncodedArgs.force_rex
                e.a&.push(TWO_BYTE_OP_PREFIX);
                e.a&.push(@as(u8) TwoByteOp.MovZX);
                e.a&.push(pack_modrm(ModrmMode.Direct, r0, r0));
                return();
            };
            
            if is_sel_flag(o) {  // Choosing between two registers based on the flags 
                // there's no 3-reg select, so we emulate it with a conditional move. 
                
                cc := @as(i32) i&.op().raw() - Qbe.O.selieq.raw();
                ::if(Ty(i32, Qbe.Ref));
                cond, r := if i.to == i.arg&[1] {
                    (cc, i.arg&[0])
                } else {
                    if i.to != i.arg&[0] {
                        emitins(make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull), e);
                    };
                    (cmpneg(cc), i.arg&[1])
                };
                
                dest, src := (int_reg_d(i.to), int_reg_d(r));
                cc := amd_condition_codes[cond.zext()];
                rex := pack_rex_si(i&.cls().is_wide(), dest, src, DONT_CARE_REG);
                if(rex != 0b01000000, => e.a&.push(rex));
                e.a&.encode_2cc(TwoByteOp.CMovCC, cc);
                e.a&.push(pack_modrm(ModrmMode.Direct, src, dest));
                return();
            };
            
            if is_load(o) {
                // TODO: this is wasteful allocation just to smuggle one bit of information on which modrm mode to use. 
                @debug_assert(rtype(i.arg&[0]) != .RTmp, "load from tmp");
                
                if o != .load {
                    // convert to extXX and let the memory argument express the load. 
                    o = rebase(o, .extsb, .loadsb);
                    if o == .extsw && i&.cls() == .Kw {
                        o = .copy;
                    };
                    i&.set_op(o);
                };
                i.arg&[1] = i.to;
                i.arg&.swap(0, 1);
            } else {
                convert_to_2address(e, i&);
            };
            // because we choose arg size based on output size, without this is does a 64 bit mov which doesn't zero the top bytes.
            if o == .extuw {
                i&.set_cls(.Kw);
            };
            
            assemble(e, i&);
        };
    }; // early returns
}

fn call_symbol(e: *AmdState, sym: Qbe.Sym, set_link: bool) void #once = {
    use_symbol(e.m, sym) { symbol |
        kind := symbol.kind;
        
        off := 0;
        current_offset := e.start_of_function + e.a.len();
        @match(kind) {
            fn Local() => {
                encoded_size :: 5;  //  address is relatitive to the start of the next instruction
                off += distance_between(e.m, .Code, current_offset + encoded_size, symbol.segment, symbol.offset);
                when_debug(e.f, .Patch) { out |
                    @fmt(out, "# call offset % for '%' %[%]\n", off, symbol.name, symbol.segment, symbol.offset);
                };
                encode_direct_call(e.a&, off.trunc(), set_link);
                // TODO: on arm we do a patch if goal==Relocatable, should be same here? or remove there? 
            }
            @default => {
                call_through_got := e.m.goal.got_indirection_instead_of_patches || (symbol.kind == .DynamicPatched && e.m.goal.type != .Relocatable);
                @if(!call_through_got) when_debug(e.f, .Patch) { out |
                    @fmt(out, "# pending call to %\n", symbol.name);
                };
                if call_through_got {
                    // TODO: this is very copy-paste from where we do stuff for __stubs. 
                    
                    if ensure_got_slot(e.m, symbol, symbol.jit_addr) { it |
                        push_fixup(e.m, symbol, it);
                    };
                    encoded_size :: 6;
                    off += e.m.distance_between(.Code, current_offset + encoded_size, .ConstantData, symbol.got_lookup_offset);
                    encode_indirect_call(e.a&, off.trunc(), set_link);
                } else {
                    // Otherwise this will get patched to a local or __stub call later. 
                    patch_at := e.next_no_write().offset(1);
                    fix: Fixup = (
                        patch_at = patch_at, 
                        type = (RipDisp32 = (increment = 0, call = true)),
                    );
                    push_fixup(e.m, symbol, fix);
                    encode_direct_call(e.a&, 0, set_link);
                };
            };
        };
    };
}

// different from e.a.last() when blink_smc
fn next_no_write(e: *AmdState) rawptr = 
    u8.raw_from_ptr(e.m.segments&[.Code].mmapped.ptr.offset(e.start_of_function + e.a.len));

fn push_imm(e: *AmdState, value: i64, byte_count: i64) void = {
    bytes: []u8 = (ptr = ptr_cast_unchecked(i64, u8, value&), len = byte_count); // little endian
    e.a&.push_all(bytes);
}

fn push_instruction(e: *AmdState, b: EncodedArgs, ops: []u8) void = {
    if b.prefix != 0 {
        e.a&.push(b.prefix);
    };
    
    // If we're not using 64 bit operand size and don't need a register extension, don't emit a wasted byte. 
    // However, when using the low byte of RAX/RCX/RDX/RBX, missing rex gives you bits 8-16 instead 
    // which maybe is cool you're on an 8086 or whatever but is never what we want so here we are. 
    if b.rex != 0b01000000 || b.force_rex {
        e.a&.push(b.rex);
    };
    e.a&.push_all(ops);
    e.a&.push(b.modrm);
    if b.sib { sib | 
        e.a&.push(sib);
    };
    ::enum(@type b.disp_size);
    if b.disp_size != .D0 {
        if b.patch { p | 
            off, sym := p;
            use_symbol(e.m, sym) { symbol |
                if symbol.kind == .Local {
                    // +4 for the size of the encoded displacement. address is relatitive to the start of the next instruction
                    b.disp += distance_between(e.m, .Code, e.start_of_function + e.a.len + 4, symbol.segment, symbol.offset);
                    @debug_assert(b.disp.abs() < 1.shift_left(b.disp_size.bits()));
                    when_debug(e.f, .Patch) { out |
                        @fmt(out, "# direct offset % for '%' %[%]\n", b.disp, symbol.name, symbol.segment, symbol.offset);
                    };
                } else {
                    @debug_assert(symbol.kind != .DynamicPatched, "TODO: somehow have to convert this to a got load? $% % %", symbol.name, symbol.jit_addr, ops[0]);
                    // TODO: should probably respect this but the main point was for arm which is less forgiving about cache coherency
                    //@debug_assert(!e.m.goal.got_indirection_instead_of_patches, "TODO: somehow have to convert this to a got load");
                    when_debug(e.f, .Patch) { out |
                        @fmt(out, "# pending ref to '%'\n", symbol.name);
                    };
                };
                patch_at := e.next_no_write();
                if symbol.kind != .Local || e.m.goal.type == .Relocatable {
                    push_fixup(e.m, symbol, (patch_at = patch_at, type = (RipDisp32 = (increment = off, call = false))));
                };
            };
        };
        e.push_imm(b.disp, b.disp_size.bits() / 8);
    } else {
        @debug_assert(b.patch.is_none());
    };
    // after: immediate
}

// this invalidates f.mem pointers!
fn fix_memarg_slot(e: *AmdState, r: *Qbe.Ref) void = {
    if rtype(r[]) == .RSlot {
        r[] = e.f.new_mem(r[], QbeNull, 0, 0);
    };
    if(rtype(r[]) != .RMem, => return());
    m := e.f.get_memory(r[]);
    if rtype(m.base) == .RSlot {
        @debug_assert_eq(m.offset&.type(), .CBits); 
        m.offset&.set_bits(m.offset&.bits() + slot(m.base, e));
        m.base = TMP(e.fp);
    };
}

// for reserving space that needs to be patched later. 
fn push_int3(e: *AmdState, count: i64) void = {
    range(0, count) { _ |
        e.a&.push(@as(u8) PrimaryOp.Int3);
    };
}

EncodedArgs :: @struct(
    modrm: u8,
    sib: ?u8,
    rex: u8,
    force_rex: bool,
    disp: i64 = 0,
    disp_size := Displacement.D0,
    patch: ?Ty(i64, Qbe.Sym) = .None,
    prefix: u8 = 0,
);

fn encode(e: *AmdState, i: *Qbe.Ins, w: bool, immediate_size: i64) EncodedArgs = {
    force_rex := @is(i.op(), .extub, .extsb, .loadub, .loadsb, .storeb);
    r0, r1 := (i.arg&[0], i.arg&[1]);
    t0, t1 := (rtype(r0), rtype(r1));
    @debug_assert(r0 != QbeNull && r1 != QbeNull, "we only look at the arg slots becuase x64 is 2-address");
    if t0 == t1 && t0 == .RTmp {  // both in registers, thats easy!
        rr0, rr1 := (r0.int_reg_d(), r1.int_reg_d());
        return(modrm = pack_modrm(ModrmMode.Direct, rr1, rr0), rex = pack_rex_si(w, rr0, rr1, DONT_CARE_REG), sib = .None, force_rex = force_rex);
    };
    
    if t1 == .RMem {
        @debug_assert(t0 != .RMem, "cannot encode two memory arguments");
        m := e.f.get_memory(r1)[]; // copy!
        
        if m.base == QbeNull && m.index == QbeNull {
            @debug_assert(t0 != .RCon, "TODO: con + disp?");
            return encode_con(m.offset&, r0, force_rex, immediate_size, w);
        };
        @debug_assert(m.offset&.type() != .CAddr, "TODO: con + reg?");
        @debug_assert(!(m.base == QbeNull && m.index != QbeNull && m.scale == 1), "isel should fix this so we can encode better");
        
        valid_offset := fits_in_i32(m.offset&.bits());
        if !valid_offset {
            printfn(e.f, e.f.globals.debug_out);
        };
        @debug_assert(valid_offset, "cannot encode offset %", m.offset&.bits()); 
        small := m.offset&.bits() <= 127 && m.offset&.bits() >= -128;
        no_disp := m.offset&.bits() == 0;
        disp_size: Displacement = @if(no_disp, .D0, @if(small, .D8, .D32)); // more - than +!
        
        special_base := m.base == TMP(Amd64Reg.RBP) || m.base == TMP(Amd64Reg.R13);
        if special_base && disp_size == .D0 {
            // Indirect00 with base=RBP decodes as RIP+disp32 so encode it as [r/m + disp8=0] instead.  
            disp_size = .D8;
            no_disp = false;
        };
        
        @debug_assert(m.index != TMP(Amd64Reg.RSP), "cannot encode index");
        if m.index == QbeNull {
            m.index = TMP(Amd64Reg.RSP); // sentinal for just base+disp
        };
        
        disp := m.offset&.bits();
        mode: ModrmMode = @match(disp_size) {
            fn D0()  => .Indirect00;
            fn D8()  => .Indirect01;
            fn D32() => .Indirect10;
        };
        
        if m.base == QbeNull {
            m.base = TMP(Amd64Reg.RBP); // sentinal for just (index*s)+disp32
            mode = .Indirect00;
            disp_size = .D32;
            no_disp = false;
        };
        
        ri, rb, rr0 := (m.index.int_reg_d(), m.base.int_reg_d(), r0.int_reg_d());
        
        // if we're not encoding an index register, don't need to spend a byte on SIB, just put base in R/M. 
        // but some combinations are unencodable that way: R/M = rsp/r12 is how you ask to use SIB.
        // So if you actaully want those registers, you have to use them in both R/M and SIB.base. 
        dont_need_sib := m.index == TMP(Amd64Reg.RSP) && m.base != TMP(Amd64Reg.RSP) && m.base != TMP(Amd64Reg.R12);
        if dont_need_sib {
            return(disp_size = disp_size, disp = disp, modrm = pack_modrm(mode, rb, rr0), rex = pack_rex_si(w, rr0, rb, DONT_CARE_REG), sib = .None, force_rex = force_rex);
        };
       
        return(disp_size = disp_size, disp = disp, modrm = pack_modrm(mode, .rsp, rr0), rex = pack_rex_si(w, rr0, rb, ri), sib = (Some = pack_sib(m.scale.to_scale(), ri, rb)), force_rex = force_rex);
    };

    if t1 == .RCon {
        c := e.f.get_constant(i.arg&[1]);
        return encode_con(c, r0, force_rex, immediate_size, w);
    };
    
    @panic("TODO: we can't encode that yet")
}

encode_con :: fn(c: *Qbe.Con, r0: Qbe.Ref, force_rex: bool, immediate_size: i64, w: bool) EncodedArgs = {
    rr0 := r0.int_reg_d();
    
    if c.type() == .CAddr {
        // The distance is relative to the end of the instruction, 
        // so if we have another immediate after the displacement, we have to account for its size. 
        // becuase the linker doesn't give a shit about what instructions we're using, 
        // its just gonna plonk in the distance from the patch to the symbol. 
        // (it's tempting to do the -4 for the displacement here but that requires removing :IncOffByFour everywhere else, 
        // and mach-o gives it to you for free so we'd have to add 4 there which feels even more confusing)
        off := c.bits() - immediate_size;
        return(
            patch = (Some = (off, c.sym)), 
            disp_size = .D32,
            disp = off,
            modrm = pack_modrm(.Indirect00, .rbp, rr0),  // disp is RIP relative
            rex = pack_rex_si(w, rr0, DONT_CARE_REG, DONT_CARE_REG), 
            sib = .None,
            force_rex = force_rex,
        );
    };
    
    // If it's a situation where both args are constant, the other one gets added by push_imm later, 
    // and r0 will still be an RTmp because it's used as an opcode extension (`ext` field in AmdEncodingEntry). 
    if c.type() == .CBits {
        disp := c.bits();
        return(
            disp_size = .D32, 
            disp = disp, 
            modrm = pack_modrm(.Indirect00, .rsp, rr0),  // absolute address in disp
            rex = pack_rex_si(w, rr0, .rbp, .rsp),             // ^
            sib = (Some = pack_sib(1.to_scale(), .rsp, .rbp)), // ^
            force_rex = force_rex,
        );
    };
    
    @panic("TODO: we can't encode that yet")
};

fn to_scale(s: i32) SibScale = @switch(s) {
    @case(0) => .One; // hopefully you're using a mode that ignores this!
    @case(1) => .One;
    @case(2) => .Two;
    @case(4) => .Four;
    @case(8) => .Eight;
    @default => @panic("Invalid sib scale %", s);
}

fn bits(d: Displacement) i64 = @match(d) {
    fn D0() => 0;
    fn D8() => 8;
    fn D32() => 32;
};

fn int_reg_d(r: Qbe.Ref) X86Reg = {
    @debug_assert_eq(rtype(r), .RTmp, "int_reg_d");
    int_id(@as(Amd64Reg) @as(u8) r.val().trunc())
}

// TODO: convert my instruction encoding to use Amd64Reg instead of X64Reg. 

fn int_id(r: Amd64Reg) X86Reg #inline = {
    @debug_assert(r != .RXX);
    i: i64 = r.raw().zext() - 1; // -1 for RXX
    int_count :: 16;
    if i >= int_count {
        i -= int_count;
    };
    @as(X86Reg) i
}

fn reg_for_extension(v: u8) Amd64Reg = 
    @as(Amd64Reg) @as(u8) v + 1;

AmdArgType :: @enum(u8) (M, R, imm32, imm8, none, F);    
    
fn arg_t(e: *AmdState, r: Qbe.Ref) AmdArgType = @match(rtype(r)) {
    fn RTmp() => @if(r.val() < Amd64Reg.XMM0.raw().zext(), .R, .F);
    fn RMem() => .M;
    fn RCon() => {
        c := e.f.get_constant(r);
        @match(c.type()) {
            fn CBits() => @if(c.bits() <= 127 && c.bits() >= -128, .imm8, .imm32);
            fn CAddr() => .M; // rel
        }
    }
    @default => .none;
};

fn assemble(e: *AmdState, i: *Qbe.Ins) void = {
    // aaaa that test only exists for registers in the other direction is such a hack! fix this properly please! -- Nov 14 :FUCKED
    if i.op() == .xcmp || (i.op() == .xtest && rtype(i.arg&[0]) == .RCon) {  // // HACK. TODO: at least move this to the outer match in emitins
        i.arg&.items().swap(0, 1);
    };
    a0_old, a1_old := (i.arg&[0], i.arg&[1]);
    a0, a1 := (arg_t(e, i.arg&[0]), arg_t(e, i.arg&[1]));
    
    w := i.cls() == .Kl || i.cls() == .Kd || i.op() == .storel || i.op() == .stored;
    ext_index := int((i.arg&[1] == QbeNull || (rtype(i.arg&[1]) == .RCon && e.f.get_constant(i.arg&[1]).type() == .CBits)));  
    
    table, offsets := @run amd64_encoding_table();
    idx: i64 = offsets[i.op().raw().zext()].zext();
    entry := table.index(idx);
    dowhile {
        matches :: fn(w: AmdArgType, h: AmdArgType) bool =>
            w == h || (w == .imm32 && h == .imm8) || (w == .M && h == .R);
        @debug_assert(entry.qbe_op == i.op().raw().intcast().trunc(), "didn't find encoding for % % % in % %", i.op(), a0, a1, e.f.name(), e.f);
        bad := !matches(entry.a0, a0) || !matches(entry.a1, a1);
        if bad {
            idx += 1;
            entry = table.index(idx);
        };
        bad
    };
    
    // :UglyFloatCast this happens AFTER looking up the instruction because we have to tell the directions apart 
    if i.op() == .cast && is_int(i.cls()) {
        i.arg&.items().swap(0, 1);
    };
    
    opcode: []u8 = (ptr = entry.opcode&.as_ptr(), len = entry.opcode_len.zext());
    
    if entry.ext != 0 {  // encoded as TMP number so ext=0 is RXX not RAX
        // hack because it thinks the con is redundant but rcx is fixed
        shift := @is(i.op(), .shl, .sar, .shr, .rotl, .rotr);
        if shift && rtype(i.arg&[ext_index]) == .RTmp && i.arg&[int(ext_index == 0)] == TMP(Amd64Reg.RCX) {
            ext_index = int(ext_index == 0);
        };
        // one argument is an immediate or fixed register and its slot in modrm holds an opcode extension instead. 
        i.arg&[ext_index] = TMP(@as(i64) entry.ext.zext()); 
        i.arg&.items().swap(0, 1); // HACK
    };
    
    if rtype(i.arg&[0]) == .RMem || (rtype(i.arg&[1]) != .RMem && rtype(i.arg&[0]) == .RCon) {
        @debug_assert(rtype(i.arg&[0]) == .RMem || e.f.get_constant(i.arg&[0]).type() == .CAddr);
        // which direction it goes doesn't matter because the memory always has to be in SIB
        i.arg&.items().swap(0, 1); // HACK
    };
    immediate_size := @if_else {
        @if(entry.a0 == .imm8 || entry.a1 == .imm8) => 1;
        @if(entry.a0 == .imm32 || entry.a1 == .imm32) => 4;
        @else => 0;
    };
    // TODO: this is confusing! for the float<->int casts, you have to know the width of both sides and one is stored in the opcode in my ir. 
    //       and in the asm, the prefix means Ks vs Kd and REX.W means Kw vs Kl 
    prefix := entry.prefix;
    if prefix == 0xF2 && !w && !(@is(i.op(), .exts, .truncd, .dtosi)) {
        prefix = 0xF3; // this seems like the easiest way to do this to me
    };
    if i.op() == .xcmp && i.cls() == .Ks {
        prefix = 0;
    };
    if (@is(i.op(), .swtof, .sltof)) {
        w = i.op() == .sltof;
    };
    b := encode(e, i, w, immediate_size);
    b.prefix = prefix;
    push_instruction(e, b, opcode);
    if entry.a0 == .imm8 {
        push_imm(e, e.f.get_constant(a0_old).bits(), 1);
    };
    if entry.a1 == .imm8 {
        push_imm(e, e.f.get_constant(a1_old).bits(), 1);
    };
    if entry.a0 == .imm32 {
        imm := e.f.get_constant(a0_old).bits();
        push_imm(e, imm, 4);
    };
    if entry.a1 == .imm32 {
        imm := e.f.get_constant(a1_old).bits();
        push_imm(e, imm, 4);
    };
}

fn amd64_encoding_data(e: FatExpr) FatExpr #macro = {
    ::enum(AmdArgType); 
    ArgType :: AmdArgType;
    ops := e&.items();
    assert_eq(size_of(AmdEncodingEntry), size_of(i64));  // doesn't actually matter, just don't bloat it frivolously. 
    // TODO: truncate the table since the last many are all not needed
    offset_by_op := ast_alloc().alloc_zeroed(u16, Qbe.O.enum_count());
    lookup_table := AmdEncodingEntry.list(ops.len, ast_alloc());
    each ops { full_op | 
        loc := full_op.loc;
        full_op := full_op.items();
        qbe_o := const_eval(Qbe.O)(full_op[0]);
        @ct_assert(offset_by_op[qbe_o.raw().zext()] == 0, loc, "repeated op in table");
        offset_by_op[qbe_o.raw().zext()] = lookup_table.len.trunc();
        
        each full_op.slice(1, full_op.len) { components | 
            entry := AmdEncodingEntry.zeroed();
            entry.qbe_op = qbe_o.raw().intcast().trunc(); // overflow is doesn't matter, we just use the change as safety check
            components := components.items();
            entry.a0 = const_eval(ArgType)(components[0]);
            entry.a1 = const_eval(ArgType)(components[1]);
            
            fn compile_bytes(e: *FatExpr) []u8 = {
                o := u8.list(ast_alloc());
                each e.items() { b | 
                    o&.push(const_eval(u8)(b[]));
                };
                o.items()
            }
            
            e := components.index(2);
            o := compile_bytes(e);
            @switch(o.len) {
                @case(1) => {
                    entry.opcode&[0] = o[0];
                };
                @case(2) => {
                    entry.opcode&[0] = o[0];
                    entry.opcode&[1] = o[1];
                };
                @default => compile_error("bad opcode length", e.loc);
            };
            entry.opcode_len = o.len.trunc();
            entry.ext = 0;
            if components.len > 3 {
                e := compile_ast(components[3]);
                if e.ty == void {
                    @ct_assert(components.len > 4, e.loc, "explicit () extension implies prefix follows");
                    prefix := compile_bytes(components.index(4));
                    @ct_assert(prefix.len == 1, e.loc, "expected one prefix byte");
                    entry.prefix = prefix[0];
                } else {
                    e := const_eval(u8)(e);
                    entry.ext = TMP(reg_for_extension(e)).val().trunc();
                    @debug_assert(entry.ext != 0);
                };
            };
            lookup_table&.push(entry);
        };
    };
    
    lookup_table&.push(zeroed(@type lookup_table[0]));  // sentinal for safety check
    @literal (lookup_table.items(), offset_by_op)
}

AmdEncodingEntry :: @struct(
    opcode: Array(u8, 2),
    ext: u8,
    qbe_op: u8,
    a0: AmdArgType,
    a1: AmdArgType,
    prefix: u8,  // 0 if none
    opcode_len: u8,
);

// We assume prefix REX.W changes 32 to 64.
// .(OP, .(arg1, arg2, (opcode), ext?))
// Shift arg2 is always CL (ensured by isel), and it uses its extension slot. 
// You can always do (.R, .R, ...) by just using ModrmMode.Direct00 
//     note: that means the order of rules (.R, .M) and (.M, .R) matters (evan if the instruction commutes because one arg is also output)
// imm args are sign extended to the operation size. 
// Magic numbers transcribed from https://www.felixcloutier.com/x86
// Always put the imm8 before imm32 since imm32 will match smaller numbers too for store
// There are a bunch of hacks in assemble() for specific instructions to make the data here work. 
fn amd64_encoding_table() Ty([]AmdEncodingEntry, []u16) = @amd64_encoding_data (
    (.add, 
        (.M, .imm8, (0x83), 0),
        (.M, .imm32, (0x81), 0),
        (.R, .M, (0x03)),
        (.M, .R, (0x01)),
        (.F, .F, (0x0F, 0x58), (), (0xF2)), // addsd
    ),
    (.xcmp, 
        (.M, .imm8, (0x83), 7),
        (.M, .imm32, (0x81), 7),
        (.R, .M, (0x3B)),
        (.M, .R, (0x39)),
        (.F, .F, (0x0F, 0x2E), (), (0x66)), // ucomisd
    ),
    (.shr, 
        (.M, .imm8, (0xC1), 5),
        (.M, .R, (0xD3), 5),
    ),
    (.sar, 
        (.M, .imm8, (0xC1), 7),
        (.M, .R, (0xD3), 7),
    ),
    (.shl, 
        (.M, .imm8, (0xC1), 4),
        (.M, .R, (0xD3), 4),
    ),
    (.and, 
        (.M, .imm8, (0x83), 4),
        (.M, .imm32, (0x81), 4),
        (.R, .M, (0x23)),
        (.M, .R, (0x21)),
    ),
    (.or, 
        (.M, .imm8, (0x83), 1),
        (.M, .imm32, (0x81), 1),
        (.R, .M, (0x0B)),
        (.M, .R, (0x09)),
    ),
    (.storew,
        // note: the imm8 version would only store 1 byte! there's no `Move imm8 to r/m32.`
        (.imm32, .M, (0xC7), 0),
        (.R, .M, (0x89)),
    ),
    (.storel,
        (.imm32, .M, (0xC7), 0),
        (.R, .M, (0x89)),
    ),
    (.storeb, 
        (.R, .M, (0x88)),
    ),
    (.storeh, 
        (.R, .M, (0x89), (), (operand_16bit_prefix)),
    ),
    // TODO: whats the difference between movsd and movq? 
    (.stored,
        (.F, .M, (0x0F, 0x11), (), (0xF2)), // movsd
    ),
    (.stores,
        (.F, .M, (0x0F, 0x11), (), (0xF3)), // movss
    ),
    (.xidiv,
        (.M, .none, (0xF7), 7),
    ),
    (.xdiv,
        (.M, .none, (0xF7), 6),
    ),
    (.xor, 
        (.M, .imm8, (0x83), 6),
        (.M, .imm32, (0x81), 6),
        (.R, .M, (0x33)),
        (.M, .R, (0x33)),
    ),
    (.sub, 
        (.M, .imm8, (0x83), 5),
        (.M, .imm32, (0x81), 5),
        (.R, .M, (0x2B)),
        (.M, .R, (0x29)),
        (.F, .F, (0x0F, 0x5C), (), (0xF2)), // subsd
    ),
    (.xtest, 
        (.M, .imm32, (0xF7), 0),
        (.R, .M, (0x85)),
    ),
    (.load, 
        (.R, .M, (0x8B)), 
        (.F, .M, (0x0F, 0x10), (), (0xF2)), // movsd
    ),
    (.extsw, 
        (.R, .M, (0x63)), // MOVSXD
    ),
    (.extsb, 
        (.R, .M, (0x0F, 0xBE)), // MOVSX
    ),
    (.extub, 
        (.R, .M, (0x0F, 0xB6)), // MOVZX
    ),
    (.extsh, 
        (.R, .M, (0x0F, 0xBF)), // MOVSX
    ),
    (.extuh, 
        (.R, .M, (0x0F, 0xB7)), // MOVZX
    ),
    (.extuw,
        (.R, .M, (0x8B)), // mov of 32 bits zero extends
    ),
    (.addr, 
        (.R, .M, (0x8D)), // lea
    ),
    (.neg, 
        (.M, .none, (0xF7), 3),
    ),
    (.mul, 
        (.R, .M, (0x0F, 0xAF)),
        (.F, .F, (0x0F, 0x59), (), (0xF2)), // mulsd
    ),
    (.div, 
        (.F, .F, (0x0F, 0x5E), (), (0xF2)), // divsd
    ),
    (.copy, 
        (.M, .imm32, (0xC7), 0), // "Move imm32 sign extended to 64-bits to r/m64."
        (.R, .M, (0x8B)),
        (.M, .R, (0x89)), // TODO: is this right? 
        (.F, .F, (0x0F, 0x10), (), (0xF2)), // movsd
        (.F, .M, (0x0F, 0x10), (), (0xF2)),
        (.M, .F, (0x0F, 0x11), (), (0xF2)),
    ),
    (.swap, // TODO: confirm RMem order if we ever generate it for those (or RSlot)
        (.R, .M, (0x87)), // xchg
    ),
    // :UglyFloatCast we need to tell the directions apart but the instructions always take the float arg on the same side so one has to swap after. 
    (.cast, 
        (.F, .M, (0x0F, 0x6E), (), (0x66)), // movd:movq f<-i
        (.M, .F, (0x0F, 0x7E), (), (0x66)), // movd:movq i<-f
    ),
    (.exts,
        (.F, .F, (0x0F, 0x5A), (), (0xF3)), // cvtss2sd  
    ),
    (.dtosi,
        (.R, .F, (0x0F, 0x2C), (), (0xF2)), // cvttsd2si  
    ),
    (.swtof,
        (.F, .R, (0x0F, 0x2A), (), (0xF2)), // cvtsi2sd  
    ),
    (.sltof,
        (.F, .R, (0x0F, 0x2A), (), (0xF2)), // cvtsi2sd  
    ),
    (.stosi,
        (.R, .F, (0x0F, 0x2C), (), (0xF3)), // cvttss2si  
    ),
    (.truncd,
        (.F, .F, (0x0F, 0x5A), (), (0xF2)), // cvtsd2ss  
    ),
    (.rotr, 
        (.M, .imm8, (0xC1), 1),
        (.M, .R, (0xD3), 1),
    ),
    (.rotl, 
        (.M, .imm8, (0xC1), 0),
        (.M, .R, (0xD3), 0),
    ),
    (.cas1,
        (.M, .R, (0x0F, 0xB1), (), (0xF0)), // lock CMPXCHG
    ),
    (.ones,
        (.M, .R, (0x0F, 0xB8), (), (0xF3)), // POPCNT
    ),
    // :EmitClzAsBsr isel does a -(32 or 64) to let us support old cpus
    (.clz,
        (.M, .R, (0x0F, 0xBD)), // BSR not LZCNT(0xF3)
    ),
    (.ctz,
        (.M, .R, (0x0F, 0xBC)), // BSF not TZCNT(0xF3)
    ),
    (.max, 
        (.F, .F, (0x0F, 0x5F), (), (0xF2)),
    ),
    (.min, 
        (.F, .F, (0x0F, 0x5D), (), (0xF2)),
    ),
    (.sqrt, 
        (.F, .F, (0x0F, 0x51), (), (0xF2)),
    ),
);

amd_condition_codes :: @const_slice( // :CmpOrder
    // Cieq Cine Cisge Cisgt Cisle Cislt Ciuge Ciugt Ciule Ciult
Amd.X86cc.e, .ne,  .ge,  .g,  .le,  .l,  .ae,  .a,  .be,  .b,
    // Cfeq Cfge Cfgt Cfle Cflt Cfne Cfo  Cfuo
         .e, .ae, .a, .be, .b,  .ne, .np, .p
);

#use("@/backend/amd64/target.fr");
