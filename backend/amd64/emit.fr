//! Unlike Qbe, we don't rely on a seperate assembler. We just put the bytes in memory and jump there. 
//! This means we also have to handle updating forward references to functions that haven't been compiled yet. 

AmdState :: @struct(
    f: *Qbe.Fn,
    m: *QbeModule,
    // For leaf functions with statically known stack frame size, 
    // we don't need to bother saving RBP and can just index slots from RSP instead. 
    fp    := Amd64Reg.RBP,
    fsz   := 0,
    nclob := 0,
    a: List(u8),
    start_of_function: i64,
    patches: List(Patch),
);

fn emit_func_amd64(f: *Qbe.Fn) void = {
    m := f.globals;
    @debug_assert_eq(m.goal.arch, .x86_64, "how did we get here?");
    code := m.segments&[.Code]&;
    prev := code.next;
    e: AmdState = (
        f = f,
        m = m,
        a = (maybe_uninit = (ptr = code.next, len = code.cap() - code.len()), len = 0, gpa = panicking_allocator),
        start_of_function = code.len(),
        patches = list(f.nblk.zext(), temp()),
    );
    
    emit_the_code(e&);
    @debug_assert(identical(prev, code.next), "can't have multiple threads emitting code");
    code.next = code.next.offset(e.a.len());
}
SysV :: import("@/backend/amd64/sysv.fr");

fn fixup_amd64(self: *QbeModule, symbol: *SymbolInfo, fixup: *Fixup, new_got_reloc: *?Fixup) void = {
    address := symbol.jit_addr;
    local  := symbol.kind == .Local;
    @match(fixup.type) {
        fn DataAbsolute(it) => {
            reloc := ptr_cast_unchecked(u8, rawptr, fixup.patch_at);
            reloc[] = address.offset(it.increment);
        }
        fn RipDisp32(it) => {
            distance := ptr_diff(u8.raw_from_ptr(fixup.patch_at), address); // :size_of_displacement
            if !local {
                if (@is(self.goal.type, .Relocatable)) {
                    return(); // that's not my department
                };
                
                if !it.call {
                    if self.debug["T".char()] {
                        @fmt_write(self.debug_out, "# patch addr % refs %=%\n", u8.int_from_ptr(fixup.patch_at), address, symbol.name);
                    };
                };
                
                if !it.call {
                    // In .addr we assumed we could LEA to create the address but since it's an import, 
                    // it needs to be converted into a load from the GOT.
                    // this is super hacky. i feel like we could guess wrong since we're not decoding from the beginning. 
                    opcode := fixup.patch_at.offset(-2); // [?rex:1, opcode:1, modrm:1, patch_at:offset:4]
                    @assert(opcode[] == 0x8D || opcode[] == 0x8B, "only LEA can be converted to GOT load");
                    opcode[] = 0x8B; // load
                    // TODO: need to make sure modrm is .Indirect00, .rbp
                    new_got_reloc[] = ensure_got_slot(self, symbol, address).or_else(=> new_got_reloc[]);
                    got_addr := self.segment_address(.ConstantData, symbol.got_lookup_offset);
                    distance = ptr_diff(fixup.patch_at, got_addr);
                } else {
                    if self.debug["T".char()] {
                        @fmt_write(self.debug_out, "# patch call % calls %\n", u8.int_from_ptr(fixup.patch_at), symbol.name);
                    };
                    if symbol.offset == -1 {
                        // create a new trampoline in the __stubs section. 
                        code := self.segments&.index(.Code);
                        lock(self.icache_mutex&);
                        symbol.offset = self.stubs&.len() + COMMANDS_SIZE_GUESS;
                        size_of_jmp_encoding :: 6;
                        trampoline := self.stubs&.reserve(Array(u8, size_of_jmp_encoding));
                        unlock(self.icache_mutex&);
                        new_got_reloc[] = ensure_got_slot(self, symbol, address).or_else(=> new_got_reloc[]);
                        distance := self.distance_between(.Code, symbol.offset + size_of_jmp_encoding, .ConstantData, symbol.got_lookup_offset);
                        trampoline := fixed_list(trampoline.items());
                        
                        // jmp near [rip + distance]
                        dyn_jmp_extension := @as(X86Reg) 0b0100; // not call!
                        trampoline&.push(0xff);
                        trampoline&.push(pack_modrm(.Indirect00, .rbp, dyn_jmp_extension));   // disp is RIP relative
                        trampoline&.push_u32(distance.trunc());
                    };
                    stub_trampoline := self.segment_address(.Code, symbol.offset);
                    distance = ptr_diff(fixup.patch_at, stub_trampoline); // :size_of_displacement
                };
                fence();
            } else {
                if !it.call {
                    if self.debug["T".char()] {
                        @fmt_write(self.debug_out, "# patch addr % refs '%' = %[%]\n", u8.int_from_ptr(fixup.patch_at), symbol.name, symbol.segment, symbol.offset);
                    };
                };
            };// early returns
            
            size_of_displacement :: 4;
            distance -= size_of_displacement;
            @assert_lt(distance.abs(), MAX_i32, "can't reference that far");
            ptr_cast_unchecked(u8, u32, fixup.patch_at)[] = distance.trunc() + it.increment;
        }
        @default => panic("amd64 unexpected fixup type");
    }
}

// glibc expects the stack to be 16 byte aligned, before the call instruction. 
// so then call pushes return address and it's unaligned, and then saving rbp realigns it. 
// and we need the vararg slots for saving floats to be 16 aligned

fn framesz(e: *AmdState) void = {
    /* specific to NAlign == 3 */
    // I think this is just doing e.nclob.mod(16) != 0 but befire we counted eclob. 
    o := 0;
    if !e.f.leaf {
        for SysV'amd64_sysv_rclob { r |
            xx := e.f.reg.shift_right_logical(r.raw().zext());
            o = o.bit_xor(xx.bitcast());
        };
        o = o.bit_and(1);
    };
    
    f := e.f.slot / 4;
    f = bit_and(f + 3, -4);
    if f > 0 && e.fp == .RSP && e.f.salign == 4 {
        f += 2;
    };
    e.fsz = 4*f.zext() + 8*o + (176)*e.f.vararg.int();
}

fn next_inst_offset(self: *AmdState) i64 = self.a.len;

fn emit_the_code(e: *AmdState) void = {
    // It seems you need this to allow being called through a function pointer, that's such a pain in the ass bro... 
    // It stops people from doing evil ROP stuff or whatever. 
    // The magic words to google are "Control-Flow Enforcement Technology"
    // TODO: This is extra incentive to keep track of which things are allowed to be called indirectly 
    //       (which would also be nice for removing functions if all calls are inlined). 
    e.a&.push_u32(0xfa1e0ff3); // endbr64
    
    f := e.f;
    need_frame := !f.leaf || f.vararg || f.dynalloc;
    if need_frame {
        e.fp = .RBP;
        e.a&.encode_op_reg(PrimaryOp.PushBase, X86Reg.rbp);
        e.a&.encode_bin(PrimaryOp.MovReg, X86Reg.rbp, X86Reg.rsp);
    } else {
        e.fp = .RSP;
    };
    framesz(e);
    if e.fsz != 0 {
        @debug_assert_lt(e.fsz, MAX_i32);
        e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -e.fsz);
    };
    if f.vararg {
        o := -176;
        for SysV'amd64_sysv_rsave.slice(0, 6) { r | 
            e.a&.encode_non_sp_offset(true, PrimaryOp.MovReg, r.int_id(), .rbp, o);
            o += 8;
        };
        range(0, 8) { n |
            // mov [rbp + o] xmmn
            e.a&.push(0x0F);
            e.a&.push(0x29);
            reg := @as(X86Reg) n;
            e.a&.push(pack_modrm(ModrmMode.Indirect00, X86Reg.rsp, reg)); // mod=0, rm=sp means use sib
            e.a&.push(pack_sib(SibScale.One, X86Reg.rbp, X86Reg.rbp));
            e.a&.push_u32(o.trunc());
            o += 16;
        };
    };
    for SysV'amd64_sysv_rclob { r | 
        if f.reg.bit_and(BIT(r)) != 0 {
            e.a&.encode_op_reg(PrimaryOp.PushBase, r.int_id());
            e.nclob += 1;
        };
    };

    local_labels := temp().alloc_zeroed(i64, e.f.nblk.zext());
    for_blocks f { b | 
        local_labels[b.id.zext()] = e.next_inst_offset();
        for_insts_forward b { i |
            emitins(i[], e);
        };
        @match(b.jmp.type) { 
            fn hlt() => e.a&.push(@as(u8) PrimaryOp.Int3);
            fn ret0() => {
                if f.dynalloc {
                    // recreate the statically known part of the frame. 
                    e.a&.encode_bin(PrimaryOp.MovReg, X86Reg.rsp, X86Reg.rbp);
                    e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -(e.fsz + e.nclob * 8));
                };
                for_rev SysV'amd64_sysv_rclob { r | 
                    if f.reg.bit_and(BIT(r)) != 0 {
                        e.a&.encode_op_reg(PrimaryOp.PopBase, r.int_id());
                    };
                };
                ::enum_basic(Amd64Reg);
                @match(e.fp) {
                    fn RBP() => e.a&.push(@as(u8) PrimaryOp.Leave);
                    fn RSP() => if e.fsz != 0 {
                        e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, e.fsz);
                    };
                    @default => unreachable();
                };
                e.a&.push(@as(u8) PrimaryOp.Ret);
            }
            fn switch() => unreachable();
            fn jmp() => if !b.s1.identical(b.link) {
                e.jmp(b.s1, -1);
            };
            @default => {
                c := b.jmp.type.raw() - Qbe.J.jfieq.raw();
                @assert(0 <= c && c < Qbe.NCmp, "unhandled jump.");
                if b.link.identical(b.s2) {
                    s := b.s1;
                    b.s1 = b.s2;
                    b.s2 = s;
                } else {
                    c = cmpneg(c);
                };
                e.jmp(b.s2, c);
                if !b.s1.identical(b.link) {
                    e.jmp(b.s1, -1);
                };
            };
        };
    };
    
    each e.patches { p |
        e.apply_patch(local_labels, p);
    };
}

// cond: -1 == Always
fn jmp(e: *AmdState, s: *Qbe.Blk, cond: i32) void #inline = {
    e.patches&.push(offset_from_start = e.a.len, cond = cond, target_bid = s.id);
    e.push_int3(if(cond == -1, => 5, => 6));  // conditional branch is a 2 byte opcode. 
}

fn apply_patch(e: *AmdState, labels: []i64, patch: *Patch) void = {
    dest := labels[patch.target_bid.zext()];
    @debug_assert(dest != 0, "missing target block %", patch.target_bid);
    write_jump_patch(e, dest, patch.offset_from_start, patch.cond);
}

fn write_jump_patch(e: *AmdState, dest: i64, src: i64, cond: i32) void = {
    // TODO: this is a dump nounification. 
    patch_at := e.m.segments&[.Code].mmapped.ptr.offset(e.start_of_function + src);
    patch_size := @if(cond == -1, 5, 6);
    patch_bytes: List(u8) = (maybe_uninit = (ptr = patch_at, len = patch_size), len = 0, gpa = panicking_allocator);
    // The offset is from **the end** of the jump instruction
    offset := dest - src - patch_size; 
    @debug_assert(offset.abs() < 1.shift_left(31), "what a big function you have");
    if cond >= 0 {
        cc := amd_condition_codes[cond.zext()];
        patch_bytes&.encode_jmp(cc, offset);
    } else {
        encode_imm32_only(patch_bytes&, PrimaryOp.JmpImm32, offset); 
    };
}

// note: you can have negative offsets!
fn slot(r: Qbe.Ref, e: *AmdState) i64 = {
    s: i64 = rsval(r).intcast();
    fslot := e.f.slot.intcast();
    // I kinda can't do this now that the folding is better since maybe you just wanted to compute a garbage address (the align.ssa test does this). 
    //@debug_assert_le(s, fslot, "slot OOB");
    /* specific to NAlign == 3 */
    if s < 0 {
        if(e.fp == .RSP, => return(-s - 8 + e.fsz + e.nclob*8));
        return(-s);
    };
    if(e.fp == .RSP, => return(s + e.nclob*8));
    if(e.f.vararg, => return(-176 -(fslot - s)));
    -(fslot - s)
}

fn emitcopy(dest: Qbe.Ref, src: Qbe.Ref, k: Qbe.Cls, e: *AmdState) void = {
    i := make_ins(.copy, k, dest, src, QbeNull);
    emitins(i, e);
}

amd64_xmm_scratch :: TMP(Amd64Reg.XMM0.raw() + 15);

fn convert_to_2address(e: *AmdState, i: *Qbe.Ins) void = {
    if(i.to == QbeNull, => return());
    if i.arg&[1] == QbeNull {
        i.arg&[1] = i.to;
        i.arg&.items().swap(0, 1);
        return();
    };
    o := i.op();
    commutative := o == .add || o == .and || o == .xor || o == .or || o == .mul;
    if commutative && i.arg&[1] == i.to {
        i.arg&.items().swap(0, 1);
    };
    if i.arg&[0] != i.to {
        @debug_assert(i.arg&[1] != i.to, "cannot convert to 2-address");
        emitcopy(i.to, i.arg&[0], i.cls(), e);
        i.arg&[0] = i.to;
    };
}

fn emitins(i: Qbe.Ins, e: *AmdState) void = {
    t0 := rtype(i.arg&[0]);
    t1 := rtype(i.arg&[1]);
    
    fix_memarg_slot(e, i.arg&.index(0));
    fix_memarg_slot(e, i.arg&.index(1));
    if i&.op() != .copy {
        fix_memarg_slot(e, i.to&);  // TODO: this will break when we try to check for equality
    };
    
    @match(i&.op()) {
        /* just do nothing for nops, they are inserted
         * by some passes */
        fn nop() => ();
        fn mul() => {
            /* here, we try to use the 3-addresss form
            * of multiplication when possible */
            if rtype(i.arg&[1]) == .RCon {
                i.arg&.items().swap(0, 1);
            };
            if is_int(i&.cls())
            && rtype(i.arg&[0]) == .RCon
            && rtype(i.arg&[1]) == .RTmp {
                c := e.f.get_constant(i.arg&[0]);
                r0, r1 := (i.to.int_reg_d(), i.arg&[1].int_reg_d());
                e.a&.push(pack_rex_si(i&.cls() == .Kl, r0, r1, DONT_CARE_REG));  // TODO: elide
                e.a&.push(0x69);
                e.a&.push(pack_modrm(ModrmMode.Direct, r1, r0));
                e.push_imm(c.bits.i, 4);
                return();
            };
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        fn sub() => {
            /* we have to use the negation trick to handle
            * some 3-address subtractions */
            // because it's not commutative 
            if i.to == i.arg&[1] && i.arg&[0] != i.to {
                // TODO: make a .ssa test that gets here
                ineg := make_ins(.neg, i&.cls(), i.to, i.to, QbeNull);
                emitins(ineg, e);
                i&.set_op(.add);
                emitins(i, e);
                return();
            };
            
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        fn sign() => {
            if i&.cls() == .Kl {    
                e.a&.push(0b01001000);  // w=true
            };
            e.a&.push(@as(u8) PrimaryOp.SignExtendAxToDxAx); // CQO
        }
        fn neg() => {
            if is_int(i&.cls()) {
                if i.to != i.arg&[0] {
                    ins := make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull);
                    emitins(ins, e);
                    i.arg&[0] = i.to;
                };
                assemble(e, i&);
            } else {
                // You can't just lower these in isel because we insert new ones to handle 3-address subtract. 
                // TODO: this is so garbage. im doing extra work just to do a worse job. should just do what qbe does. 
                
                @debug_assert_ne(amd64_xmm_scratch, i.arg&[0]);
                
                // xmm15 = 0 
                xmm15 :: int_reg_d(amd64_xmm_scratch);  // TODO: compiler hangs if you typo `xmm15 :: int_reg_d(xmm15);`
                e.a&.push(0x66);
                e.a&.push(pack_rex_si(true, xmm15, xmm15, DONT_CARE_REG));
                e.a&.push_all(@slice(0x0F, 0xEF)); // PXOR
                e.a&.push(pack_modrm(.Direct, xmm15, xmm15));
                
                // xmm15 = 0 - a0
                ins := make_ins(.sub, i&.cls(), amd64_xmm_scratch, amd64_xmm_scratch, i.arg&[0]);
                emitins(ins, e);
                
                // to = xmm15
                ins := make_ins(.copy, i&.cls(), i.to, amd64_xmm_scratch, QbeNull);
                emitins(ins, e);
            };
        }
        fn div() => {
            // TODO: .ssa test that gets here
            /* use xmm15 to adjust the instruction when the
            * conversion to 2-address in emitf() would fail */
            if i.to == i.arg&[1] {
                @debug_assert(!is_int(i&.cls()), "isel converts int div to xdiv");
                ins := make_ins(.copy, i&.cls(), amd64_xmm_scratch, i.to, QbeNull);
                emitins(ins, e);
                ins := make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull);
                emitins(ins, e);
                i.arg&[0] = i.to;
                i.arg&[1] = amd64_xmm_scratch;
            };
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        // TODO: kinda garbage that this is a weird super instruction that's copy+load+store+addr
        fn copy() => {
            /* copies are used for many things; see a note
            * to understand how to load big constants:
            * https://c9x.me/notes/2015-09-19.html */
            @debug_assert(rtype(i.to) != .RMem);
            if(i.to == QbeNull || i.arg&[0] == QbeNull, => return());
            if(i.to == i.arg&[0], => return());
            
            is_long_const := i&.cls() == .Kl && t0 == .RCon && e.f.get_constant(i.arg&[0])[].type == .CBits;
            if is_long_const {
                value := e.f.con[i.arg&[0].val()].bits.i;
                if rtype(i.to) == .RSlot && (value < MIN_i32 || value > MAX_i32) {
                    slot_hi := SLOT(i.to.rsval() + 1*4);
                    hi := value.shift_right_logical(32);
                    lo := value.bit_and(1.shift_left(32) - 1);
                    ins := make_ins(.storew, .Kw, QbeNull, e.f.getcon(lo), i.to);
                    emitins(ins, e);
                    ins := make_ins(.storew, .Kw, QbeNull, e.f.getcon(hi), slot_hi);
                    emitins(ins, e);
                    return();
                };
            };
            if t0 == .RCon {
                c := e.f.get_constant(i.arg&[0]);
                if isreg(i.to) {
                    if c.type == .CAddr {
                        ins := make_ins(.addr, i&.cls(), i.to, i.arg&[0], QbeNull);
                        emitins(ins, e);
                        return();
                    };
                    if c.type == .CBits && is_int(i&.cls()) {
                        value := c.bits.i;
                        
                        // TODO: doesn't work because it sets flags but isel doesn't know!
                        // This is a silly size optimisation: 2 byte encoding instead of 6,
                        // but it happens 5-10k times... so that's quite a bit of savings. 
                        // always Kw because high bits are zeroed anyway so you don't need REX byte if it's one of the low registers. 
                        //if value == 0 {
                        //    ins := make_ins(.xor, .Kw, i.to, i.to, i.to);
                        //    emitins(ins, e);
                        //    return();
                        //};
                        
                        if value >= 0 {  // free zero extend
                            i&.set_cls(.Kw);
                        };
                        
                        if value < MIN_i32 || value > MAX_i32 {
                            encode_imm64(e.a&, i.to.int_reg_d(), value.bitcast());
                            return();
                        }; // else fallthrough to assemble
                    }
                };
            };
            if rtype(i.to) == .RSlot && (t0 == .RSlot || t0 == .RMem) {
                // rega does this shit when there's a lot of register pressure i guess?
                // TODO: lower it better so it's not a sepecial case here. 
                k := @if(is_wide(i&.cls()), Qbe.Cls.Kd, .Ks);
                str := @if(is_wide(i&.cls()), Qbe.O.stored, .stores);
                ins := make_ins(.load, k, amd64_xmm_scratch, i.arg&[0], QbeNull);
                fix_memarg_slot(e, ins.arg&.index(0)); // TODO: do i need this?
                emitins(ins, e);
                ins := make_ins(str, .Kw, QbeNull, amd64_xmm_scratch, i.to);
                fix_memarg_slot(e, ins.arg&.index(1)); // TODO: do i need this?
                emitins(ins, e);
                return();
            };
            
            if t0 == .RTmp {
                @debug_assert_eq(i.arg&[0].val() <= 16, i&.cls().is_int(), "copy cls mismatch");
            };
            
            fix_memarg_slot(e, i.to&);
            
            i.arg&[1] = i.to;
            i.arg&.swap(0, 1);
            assemble(e, i&);
        }
        fn addr() => {
            // if we want to got_indirection_instead_of_patches, pessimistically use a got load 
            // if it's not known to be local to this module (since we're not allowed to patch). 
            // Otherwise, only use got if we already know it's an import (and save a patch that way). 
            // TODO: i don't understand why the later case only happens on linux but 
            //       what you have to do to patch to a load later is super sketchy 
            //       anyway so the less of that the better.       -- Feb 23, 2025. 
            r := i.arg&[0];
            if rtype(r) == .RCon {
                c := e.f.get_constant(r);
                @debug_assert(c.type == .CAddr);
                done := false;
                use_symbol(e.m, c.sym.id) { symbol | 
                    done = symbol.kind != .Local && (e.m.got_indirection_instead_of_patches || symbol.kind == .DynamicPatched);
                    if done {
                        if ensure_got_slot(e.m, symbol, symbol.jit_addr) { it |
                            push_fixup(e.m, symbol, it);
                        };
                        
                        // TODO: very copy paste 
                        size_of_load_encoding :: 7;
                        current_offset := e.start_of_function + e.a.len();
                        distance := e.m.distance_between(.Code, current_offset + size_of_load_encoding, .ConstantData, symbol.got_lookup_offset);
                        
                        dest := int_reg_d(i.to);
                        e.a&.push(pack_rex_si(true, dest, DONT_CARE_REG, DONT_CARE_REG)); 
                        e.a&.push(0x8B);  // load
                        e.a&.push(pack_modrm(.Indirect00, .rbp, dest));   // disp is RIP relative
                        e.a&.push_u32(distance.trunc());
                        done = true;
                    };
                }; 
                if done {
                    return();
                }
            };
            
            // note: LEA interprets RMem differently than other instructions
            i.arg&[1] = i.to;
            i.arg&.swap(0, 1);
            assemble(e, i&);
        }
        fn call() => {
            @match(rtype(i.arg&[0])) {
                fn RCon() => {
                    c := e.f.get_constant(i.arg&[0]);
                    off := c.bits.i;
                    @debug_assert(c.type == .CAddr, "cannot call a constant");
                    use_symbol(e.m, c.sym.id) { symbol |
                        @debug_assert_eq(off, 0, "call to offset from symbol $% seems like a mistake?", symbol.name);
                        current_offset := e.start_of_function + e.a.len();
                        @match(symbol.kind) {
                            fn Local() => {
                                encoded_size :: 5;  //  address is relatitive to the start of the next instruction
                                off += distance_between(e.m, .Code, current_offset + encoded_size, symbol.segment, symbol.offset);
                                if e.m.debug["T".char()] {
                                    write(e.m.debug_out, items(@format("# call offset % for '%' %[%]\n", off, symbol.name, symbol.segment, symbol.offset) temp()));
                                };
                                e.a&.push(@as(u8) PrimaryOp.CallImm32); 
                                e.a&.push_u32(signed_truncate(off, 32));
                            }
                            @default => {
                                call_through_got := e.m.got_indirection_instead_of_patches || (symbol.kind == .DynamicPatched && e.m.goal.type != .Relocatable);
                                if !call_through_got && e.m.debug["T".char()] {
                                    write(e.m.debug_out, items(@format("# pending call to %\n", symbol.name) temp()));
                                };
                                if call_through_got {
                                    // TODO: this is very copy-paste from where we do stuff for __stubs. 
                                    
                                    if ensure_got_slot(e.m, symbol, symbol.jit_addr) { it |
                                        push_fixup(e.m, symbol, it);
                                    };
                                    encoded_size :: 6;
                                    off += e.m.distance_between(.Code, current_offset + encoded_size, .ConstantData, symbol.got_lookup_offset);
                                    
                                    // call near [rip + distance]
                                    dyn_call_extension := @as(X86Reg) 0b0010; 
                                    e.a&.push(0xff);
                                    e.a&.push(pack_modrm(.Indirect00, .rbp, dyn_call_extension));   // disp is RIP relative
                                    e.a&.push_u32(off.trunc());
                                } else {
                                    // Otherwise this will get patched to a local or __stub call later. 
                                    patch_at := e.a.maybe_uninit.ptr.offset(e.a.len + 1);
                                    fix: Fixup = (patch_at = patch_at, type = (RipDisp32 = (increment = c.bits.i.trunc(), call = true)));
                                    push_fixup(e.m, symbol, fix);
                                    e.a&.push(@as(u8) PrimaryOp.CallImm32); 
                                    e.a&.push_u32(0);
                                };
                            };
                        };
                    };
                }
                fn RTmp() => encode_call_reg(e.a&, ModrmMode.Direct, int_reg_d(i.arg&[0]));  
                // TODO: we could allow memory argument? 
                @default => panic("invalid call argument");
            }
        }
        fn salloc() => {
            /* there is no good reason why this is here
            * maybe we should split Osalloc in 2 different
            * instructions depending on the result
            */
            // why not just do math on RSP directly like arm abi does? 
            // one should be changed so they're the same? 
            
            @debug_assert(e.fp == .RBP, "can't salloc when no frame");
            i.arg&[1] = TMP(Amd64Reg.RSP);
            i.arg&.items().swap(0, 1);
            i&.set_op(.sub);
            assemble(e, i&);
            
            if i.to != QbeNull {
                emitcopy(i.to, TMP(Amd64Reg.RSP), .Kl, e);
            };
        }
        fn swap() => {
            if is_int(i&.cls()) {
                assemble(e, i&);
                return();
            };
            /* for floats, there is no swap instruction
            * so we use xmm15 as a temporary
            */
            emitcopy(amd64_xmm_scratch, i.arg&[0], i&.cls(), e);
            emitcopy(i.arg&[0], i.arg&[1], i&.cls(), e);
            emitcopy(i.arg&[1], amd64_xmm_scratch, i&.cls(), e);
        }
        fn storeb() => {
            @debug_assert(rtype(i.arg&[1]) != .RTmp, "storeb to tmp");
            // this is painful because everywhere else the range of a byte immediate is signed but here its unsigned. 
            if e.f.get_int(i.arg&[0]) { c |
                c = c.bit_and(1.shift_left(8) - 1);
                i.arg&[0] = TMP(reg_for_extension(0));
                b := encode(e, i&, false, 1);
                @debug_assert(b.force_rex);
                push_instruction(e, b, @slice(0xC6));
                e.a&.push(@as(u8) c.trunc());
                return();
            };
        
            assemble(e, i&); 
        }
        fn storeh() => {
            // TODO: has RTmp already been converted to RMem for all stores?
            
            if e.f.get_int(i.arg&[0]) { c |
                c = c.bit_and(1.shift_left(16) - 1);
                i.arg&[0] = TMP(reg_for_extension(0));
                b := encode(e, i&, false, 2);
                b.prefix = operand_16bit_prefix;
                push_instruction(e, b, @slice(0xC7));
                e.a&.reserve_type(u16)[] = c.trunc();
                return();
            };
        
            assemble(e, i&);
        }
        fn dbgloc() => e.m.add_debug_info(i&, e.a.len);
        // TODO: allow non-multiple-of-4 blocks of asm. for now you just have to pad with nops because im being consistant with the arm one. 
        fn asm() => {  
            imm := e.f.get_int(i.arg&[0]).expect("op 'asm' arg is constant int");
            e.push_imm(imm, 4);
        }
        fn syscall() => {
            e.a&.push(0x0F);
            e.a&.push(0x05);
        }
        fn byteswap() => {
            k := i&.cls();
            if i.to != i.arg&[0] {
                emitcopy(i.to, i.arg&[0], k, e);
            };
            r := i.to.int_reg_d();
            e.a&.push(pack_rex_si(k.is_wide(), DONT_CARE_REG, r, DONT_CARE_REG));
            e.a&.push(TWO_BYTE_OP_PREFIX);
            e.a&.push(0xC8 + trunc(@as(i64) bit_and(@as(i64) r, @as(i64) 0b0111)));
        }
        fn trace_start() => emitcopy(i.to, TMP(Amd64Reg.RBP), i&.cls(), e);
        @default => {
            o := i&.op();
            if is_flag(i&.op()) {  // Reading a bit from the flags register
                cc := @as(i32) i&.op().raw() - Qbe.O.flagieq.raw();
                cc := amd_condition_codes[cc.zext()];
                @debug_assert(rtype(i.to) == .RTmp, "we don't allow mem arg for setcc"); // because then the zero extending is even more painful. 
                r0 := int_reg_d(i.to);
                
                e.a&.push(pack_rex_si(false, r0, r0, DONT_CARE_REG));  // Don't elide. See EncodedArgs.force_rex
                e.a&.encode_2cc(TwoByteOp.SetCC, cc);
                e.a&.push(pack_modrm(ModrmMode.Direct, r0, DONT_CARE_REG));
                
                // setcc doesn't zero the upper bits, so we zero extend as an extra instruction. 
                // we could get complicated and keep track of known bits but I suspect that's well into deminishing returns territory. 
                e.a&.push(pack_rex_si(false, r0, r0, DONT_CARE_REG));  // Don't elide. See EncodedArgs.force_rex
                e.a&.push(TWO_BYTE_OP_PREFIX);
                e.a&.push(@as(u8) TwoByteOp.MovZX);
                e.a&.push(pack_modrm(ModrmMode.Direct, r0, r0));
                return();
            };
            
            if is_sel_flag(o) {  // Choosing between two registers based on the flags 
                // there's no 3-reg select, so we emulate it with a conditional move. 
                
                cc := @as(i32) i&.op().raw() - Qbe.O.selieq.raw();
                ::if(Ty(i32, Qbe.Ref));
                cond, r := if i.to == i.arg&[1] {
                    (cc, i.arg&[0])
                } else {
                    if i.to != i.arg&[0] {
                        emitins(make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull), e);
                    };
                    (cmpneg(cc), i.arg&[1])
                };
                
                dest, src := (int_reg_d(i.to), int_reg_d(r));
                cc := amd_condition_codes[cond.zext()];
                rex := pack_rex_si(i&.cls().is_wide(), dest, src, DONT_CARE_REG);
                if(rex != 0b01000000, => e.a&.push(rex));
                e.a&.encode_2cc(TwoByteOp.CMovCC, cc);
                e.a&.push(pack_modrm(ModrmMode.Direct, src, dest));
                return();
            };
            
            if is_load(o) {
                // TODO: this is wasteful allocation just to smuggle one bit of information on which modrm mode to use. 
                @debug_assert(rtype(i.arg&[0]) != .RTmp, "load from tmp");
                
                if o != .load {
                    // convert to extXX and let the memory argument express the load. 
                    o = rebase(o, .extsb, .loadsb);
                    if o == .extsw && i&.cls() == .Kw {
                        o = .copy;
                    };
                    i&.set_op(o);
                };
                i.arg&[1] = i.to;
                i.arg&.swap(0, 1);
            } else {
                convert_to_2address(e, i&);
            };
            // because we choose arg size based on output size, without this is does a 64 bit mov which doesn't zero the top bytes.
            if o == .extuw {
                i&.set_cls(.Kw);
            };
            
            assemble(e, i&);
        };
    }; // early returns
}

fn push_imm(e: *AmdState, value: i64, byte_count: i64) void = {
    bytes: []u8 = (ptr = ptr_cast_unchecked(i64, u8, value&), len = byte_count); // little endian
    e.a&.push_all(bytes);
}

fn push_instruction(e: *AmdState, b: EncodedArgs, ops: []u8) void = {
    if b.prefix != 0 {
        e.a&.push(b.prefix);
    };
    
    // If we're not using 64 bit operand size and don't need a register extension, don't emit a wasted byte. 
    // However, when using the low byte of RAX/RCX/RDX/RBX, missing rex gives you bits 8-16 instead 
    // which maybe is cool you're on an 8086 or whatever but is never what we want so here we are. 
    if b.rex != 0b01000000 || b.force_rex {
        e.a&.push(b.rex);
    };
    e.a&.push_all(ops);
    e.a&.push(b.modrm);
    if b.sib { sib | 
        e.a&.push(sib);
    };
    if b.disp_size != .D0 {
        if b.patch { p | 
            off, sym := p;
            use_symbol(e.m, sym) { symbol |
                if symbol.kind == .Local {
                    // +4 for the size of the encoded displacement. address is relatitive to the start of the next instruction
                    b.disp += distance_between(e.m, .Code, e.start_of_function + e.a.len + 4, symbol.segment, symbol.offset);
                    @debug_assert(b.disp.abs() < 1.shift_left(b.disp_size.bits()));
                    if e.m.debug["T".char()] {
                        write(e.m.debug_out, items(@format("# direct offset % for '%' %[%]\n", b.disp, symbol.name, symbol.segment, symbol.offset) temp()));
                    };
                } else {
                    @debug_assert(symbol.kind != .DynamicPatched, "TODO: somehow have to convert this to a got load? $% % %", symbol.name, symbol.jit_addr, ops[0]);
                    @debug_assert(!e.m.got_indirection_instead_of_patches, "TODO: somehow have to convert this to a got load");
                    if e.m.debug["T".char()] {
                        write(e.m.debug_out, items(@format("# pending ref to '%'\n", symbol.name) temp()));
                    };
                };
                patch_at := e.a.maybe_uninit.ptr.offset(e.a.len);
                if symbol.kind != .Local || e.m.goal.type == .Relocatable {
                    push_fixup(e.m, symbol, (patch_at = patch_at, type = (RipDisp32 = (increment = off.trunc(), call = false))));
                };
            };
        };
        e.push_imm(b.disp, b.disp_size.bits() / 8);
    } else {
        @debug_assert(b.patch.is_none());
    };
    // after: immediate
}

// this invalidates f.mem pointers!
fn fix_memarg_slot(e: *AmdState, r: *Qbe.Ref) void = {
    if rtype(r[]) == .RSlot {
        r[] = e.f.new_mem(r[], QbeNull, 0, 0);
    };
    if(rtype(r[]) != .RMem, => return());
    m := e.f.get_memory(r[]);
    if rtype(m.base) == .RSlot {
        @debug_assert(@is(m.offset.type, .CBits, .CUndef), "bits"); 
        if m.offset.type == .CUndef {
            m.offset.bits.i = 0;
        };
        m.offset.bits.i = m.offset.bits.i + slot(m.base, e);
        m.offset.type = .CBits;
        m.base = TMP(e.fp);
    };
}

// for reserving space that needs to be patched later. 
fn push_int3(e: *AmdState, count: i64) void = {
    range(0, count) { _ |
        e.a&.push(@as(u8) PrimaryOp.Int3);
    };
}

EncodedArgs :: @struct(
    modrm: u8,
    sib: ?u8,
    rex: u8,
    force_rex: bool,
    disp: i64 = 0,
    disp_size := Displacement.D0,
    patch: ?Ty(i64, u32) = .None,
    prefix: u8 = 0,
);

fn encode(e: *AmdState, i: *Qbe.Ins, w: bool, immediate_size: i64) EncodedArgs = {
    force_rex := @is(i.op(), .extub, .extsb, .loadub, .loadsb, .storeb);
    r0, r1 := (i.arg&[0], i.arg&[1]);
    t0, t1 := (rtype(r0), rtype(r1));
    @debug_assert(r0 != QbeNull && r1 != QbeNull, "we only look at the arg slots becuase x64 is 2-address");
    if t0 == t1 && t0 == .RTmp {  // both in registers, thats easy!
        rr0, rr1 := (r0.int_reg_d(), r1.int_reg_d());
        return(modrm = pack_modrm(ModrmMode.Direct, rr1, rr0), rex = pack_rex_si(w, rr0, rr1, DONT_CARE_REG), sib = .None, force_rex = force_rex);
    };
    
    if t1 == .RMem {
        @debug_assert(t0 != .RMem, "cannot encode two memory arguments");
        m := e.f.get_memory(r1)[]; // copy!
        
        if m.offset.type == .CAddr {
            @debug_assert(m.base == QbeNull && m.index == QbeNull, "cannot encode const SIB");
            c := m.offset&;
            @debug_assert(t0 != .RCon, "TODO: con + disp?");
            // :copy_paste_con_addr
            rr0 := r0.int_reg_d();
            off := c.bits.i - immediate_size;
            return(
                patch = (Some = (off, c.sym.id)), 
                disp_size = .D32,
                disp = off,
                modrm = pack_modrm(.Indirect00, .rbp, rr0),  // disp is RIP relative
                rex = pack_rex_si(w, rr0, DONT_CARE_REG, DONT_CARE_REG), 
                sib = .None, 
                force_rex = force_rex,
            );
        };
        
        valid_offset := fits_in_i32(m.offset.bits.i);
        if !valid_offset {
            printfn(e.f, e.f.globals.debug_out);
        };
        @debug_assert(valid_offset, "cannot encode offset %", m.offset.bits.i); 
        small := m.offset.bits.i <= 127 && m.offset.bits.i >= -128;
        no_disp := m.offset.bits.i == 0 && m.index != TMP(Amd64Reg.RBP) && m.index != TMP(Amd64Reg.R13);
        disp_size: Displacement = @if(no_disp, .D0, @if(small, .D8, .D32)); // more - than +!
        
        special_base := m.base == TMP(Amd64Reg.RBP) || m.base == TMP(Amd64Reg.R13);
        if special_base && disp_size == .D0 {
            disp_size = .D8;
            no_disp = false;
        };
        
        @debug_assert(m.index != TMP(Amd64Reg.RSP), "cannot encode index");
        if m.index == QbeNull {
            m.index = TMP(Amd64Reg.RSP); // sentinal for just base+disp
        };
        
        disp := m.offset.bits.i;
        mode: ModrmMode = @match(disp_size) {
            fn D0()  => .Indirect00;
            fn D8()  => .Indirect01;
            fn D32() => .Indirect10;
        };
        
        if m.base == QbeNull {
            @debug_assert(m.index != TMP(Amd64Reg.RSP), "no base and no index makes x64 a dull boy $%", e.f.name());
            m.base = TMP(Amd64Reg.RBP); // sentinal for just (index*s)+disp32
            mode = .Indirect00;
            disp_size = .D32;
            no_disp = false;
        };
        if m.offset.type == .CUndef {
            m.offset.bits.i = 0;
        };
        
        ri, rb, rr0 := (m.index.int_reg_d(), m.base.int_reg_d(), r0.int_reg_d());
        dont_need_sib := no_disp && m.index == TMP(Amd64Reg.RSP) && m.base != TMP(Amd64Reg.RSP) && m.base != TMP(Amd64Reg.R12);
        if dont_need_sib {
            // this is a lot of hassle to save one byte
            return(modrm = pack_modrm(mode, rb, rr0), rex = pack_rex_si(w, rr0, rb, DONT_CARE_REG), sib = .None, force_rex = force_rex);
        };
       
        return(disp_size = disp_size, disp = disp, modrm = pack_modrm(mode, .rsp, rr0), rex = pack_rex_si(w, rr0, rb, ri), sib = (Some = pack_sib(m.scale.to_scale(), ri, rb)), force_rex = force_rex);
    };
    
    if t1 == .RCon {
        c := e.f.get_constant(i.arg&[1]);
        if c.type == .CAddr {
            // :copy_paste_con_addr
            rr0 := r0.int_reg_d();
            // The distance is relative to the end of the instruction, 
            // so if we have another immediate after the displacement, we have to account for its size. 
            // becuase the linker doesn't give a shit about what instructions we're using, 
            // its just gonna plonk in the distance from the patch to the symbol. 
            // TODO: maybe it would be better if we also did the -4 for the displacement here but that requires removing it everywhere else. 
            off := c.bits.i - immediate_size;
            return(
                patch = (Some = (off, c.sym.id)), 
                disp_size = .D32,
                disp = off,
                modrm = pack_modrm(.Indirect00, .rbp, rr0),  // disp is RIP relative
                rex = pack_rex_si(w, rr0, DONT_CARE_REG, DONT_CARE_REG), 
                sib = .None,
                force_rex = force_rex,
            );
        };
        
        // TODO: this might be backwards because you might be ina situation where both args are constant. 
        //       we're adding a source of confusion for zero benifit because i don't want to deal with changing the isel right now. 
        if c.type == .CBits && t0 == .RTmp {
            rr0 := r0.int_reg_d();
            disp := c.bits.i;
            return(
                disp_size = .D32, 
                disp = disp, 
                modrm = pack_modrm(.Indirect00, .rsp, rr0),  // absolute address in disp
                rex = pack_rex_si(w, rr0, .rbp, .rsp),             // ^
                sib = (Some = pack_sib(1.to_scale(), .rsp, .rbp)), // ^
                force_rex = force_rex,
            );
        };
    };
    
    @panic("TODO: we can't encode that yet")
}

fn to_scale(s: i32) SibScale = @switch(s) {
    @case(0) => .One; // hopefully you're using a mode that ignores this!
    @case(1) => .One;
    @case(2) => .Two;
    @case(4) => .Four;
    @case(8) => .Eight;
    @default => @panic("Invalid sib scale %", s);
}

fn bits(d: Displacement) i64 = @match(d) {
    fn D0() => 0;
    fn D8() => 8;
    fn D32() => 32;
};

fn int_reg_d(r: Qbe.Ref) X86Reg = {
    @debug_assert_eq(rtype(r), .RTmp, "int_reg_d");
    int_id(@as(Amd64Reg) @as(i32) r.val().trunc())
}

// TODO: convert my instruction encoding to use Amd64Reg instead of X64Reg. 

fn int_id(r: Amd64Reg) X86Reg #inline = {
    @debug_assert(r != .RXX);
    i: i64 = r.raw().zext() - 1; // -1 for RXX
    int_count :: 16;
    if i >= int_count {
        i -= int_count;
    };
    @as(X86Reg) i
}

fn reg_for_extension(v: u8) Amd64Reg = 
    @as(Amd64Reg) @as(i32) v.zext() + 1;

AmdArgType :: @enum(u8) (M, R, imm32, imm8, none, F);    
    
fn arg_t(e: *AmdState, r: Qbe.Ref) AmdArgType = @match(rtype(r)) {
    fn RTmp() => @if(r.val() < Amd64Reg.XMM0.raw().zext(), .R, .F);
    fn RMem() => .M;
    fn RCon() => {
        c := e.f.get_constant(r);
        @match(c.type) {
            fn CBits() => @if(c.bits.i <= 127 && c.bits.i >= -128, .imm8, .imm32);
            fn CAddr() => .M; // rel
            @default => unreachable();
        }
    }
    @default => .none;
};

fn assemble(e: *AmdState, i: *Qbe.Ins) void = {
    // aaaa that test only exists for registers in the other direction is such a hack! fix this properly please! -- Nov 14 :FUCKED
    if i.op() == .xcmp || (i.op() == .xtest && rtype(i.arg&[0]) == .RCon) {  // // HACK. TODO: at least move this to the outer match in emitins
        i.arg&.items().swap(0, 1);
    };
    a0_old, a1_old := (i.arg&[0], i.arg&[1]);
    a0, a1 := (arg_t(e, i.arg&[0]), arg_t(e, i.arg&[1]));
    
    w := i.cls() == .Kl || i.cls() == .Kd || i.op() == .storel || i.op() == .stored;
    ext_index := int((i.arg&[1] == QbeNull || (rtype(i.arg&[1]) == .RCon && e.f.get_constant(i.arg&[1])[].type == .CBits)));  
    
    xxx :: amd64_encoding_table();
    table, offsets := xxx;
    idx: i64 = offsets[i.op().raw().zext()].zext();
    entry := table.index(idx);
    dowhile {
        matches :: fn(w: AmdArgType, h: AmdArgType) bool =>
            w == h || (w == .imm32 && h == .imm8) || (w == .M && h == .R);
        @debug_assert(entry.qbe_op == i.op().raw().intcast().trunc(), "didn't find encoding for % % % in %", i.op(), a0, a1, e.f.name());
        bad := !matches(entry.a0, a0) || !matches(entry.a1, a1);
        if bad {
            idx += 1;
            entry = table.index(idx);
        };
        bad
    };
    
    // :UglyFloatCast this happens AFTER looking up the instruction because we have to tell the directions apart 
    if i.op() == .cast && is_int(i.cls()) {
        i.arg&.items().swap(0, 1);
    };
    
    opcode: []u8 = (ptr = entry.opcode&.as_ptr(), len = entry.opcode_len.zext());
    
    if entry.ext != 0 {  // encoded as TMP number so ext=0 is RXX not RAX
        // hack because it thinks the con is redundant but rcx is fixed
        shift := @is(i.op(), .shl, .sar, .shr, .rotl, .rotr);
        if shift && rtype(i.arg&[ext_index]) == .RTmp && i.arg&[int(ext_index == 0)] == TMP(Amd64Reg.RCX) {
            ext_index = int(ext_index == 0);
            //i.arg&.items().swap(0, 1); // HACK
        };
        // one argument is an immediate or fixed register and its slot in modrm holds an opcode extension instead. 
        i.arg&[ext_index] = TMP(@as(i64) entry.ext.zext()); 
        i.arg&.items().swap(0, 1); // HACK
    };
    
    if rtype(i.arg&[0]) == .RMem || (rtype(i.arg&[1]) != .RMem && rtype(i.arg&[0]) == .RCon) {
        @debug_assert(rtype(i.arg&[0]) == .RMem || e.f.get_constant(i.arg&[0])[].type == .CAddr);
        // which direction it goes doesn't matter because the memory always has to be in SIB
        i.arg&.items().swap(0, 1); // HACK
    };
    immediate_size := @if_else {
        @if(entry.a0 == .imm8 || entry.a1 == .imm8) => 1;
        @if(entry.a0 == .imm32 || entry.a1 == .imm32) => 4;
        @else => 0;
    };
    // TODO: this is confusing! for the float<->int casts, you have to know the width of both sides and one is stored in the opcode in my ir. 
    //       and in the asm, the prefix means Ks vs Kd and REX.W means Kw vs Kl 
    prefix := entry.prefix;
    if prefix == 0xF2 && !w && !(@is(i.op(), .exts, .truncd, .dtosi)) {
        prefix = 0xF3; // this seems like the easiest way to do this to me
    };
    if (@is(i.op(), .swtof, .sltof)) {
        w = i.op() == .sltof;
    };
    b := encode(e, i, w, immediate_size);
    b.prefix = prefix;
    push_instruction(e, b, opcode);
    if entry.a0 == .imm8 {
        push_imm(e, e.f.get_constant(a0_old)[].bits.i, 1);
    };
    if entry.a1 == .imm8 {
        push_imm(e, e.f.get_constant(a1_old)[].bits.i, 1);
    };
    if entry.a0 == .imm32 {
        imm := e.f.get_constant(a0_old)[].bits.i;
        push_imm(e, imm, 4);
    };
    if entry.a1 == .imm32 {
        imm := e.f.get_constant(a1_old)[].bits.i;
        push_imm(e, imm, 4);
    };
}

fn amd64_encoding_data(e: FatExpr) FatExpr #macro = {
    ::enum(AmdArgType); 
    ArgType :: AmdArgType;
    ops := e&.items();
    // TODO: truncate the table since the last many are all not needed
    offset_by_op := ast_alloc().alloc_zeroed(u16, Qbe.O.enum_count());
    lookup_table := AmdEncodingEntry.list(ops.len, ast_alloc());
    each ops { full_op | 
        loc := full_op.loc;
        full_op := full_op.items();
        qbe_o := const_eval(Qbe.O)(full_op[0]);
        @ct_assert(offset_by_op[qbe_o.raw().zext()] == 0, loc, "repeated op in table");
        offset_by_op[qbe_o.raw().zext()] = lookup_table.len.trunc();
        
        each full_op.slice(1, full_op.len) { components | 
            entry := AmdEncodingEntry.zeroed();
            entry.qbe_op = qbe_o.raw().intcast().trunc(); // overflow is doesn't matter, we just use the change as safety check
            components := components.items();
            entry.a0 = const_eval(ArgType)(components[0]);
            entry.a1 = const_eval(ArgType)(components[1]);
            
            fn compile_bytes(e: *FatExpr) []u8 = {
                o := u8.list(ast_alloc());
                each e.items() { b | 
                    o&.push(const_eval(u8)(b[]));
                };
                o.items()
            }
            
            e := components.index(2);
            o := compile_bytes(e);
            @switch(o.len) {
                @case(1) => {
                    entry.opcode&[0] = o[0];
                };
                @case(2) => {
                    entry.opcode&[0] = o[0];
                    entry.opcode&[1] = o[1];
                };
                @default => compile_error("bad opcode length", e.loc);
            };
            entry.opcode_len = o.len.trunc();
            entry.ext = 0;
            if components.len > 3 {
                e := compile_ast(components[3]);
                if e.ty == void {
                    @ct_assert(components.len > 4, e.loc, "explicit () extension implies prefix follows");
                    prefix := compile_bytes(components.index(4));
                    @ct_assert(prefix.len == 1, e.loc, "expected one prefix byte");
                    entry.prefix = prefix[0];
                } else {
                    e := const_eval(u8)(e);
                    entry.ext = TMP(reg_for_extension(e)).val().trunc();
                    @debug_assert(entry.ext != 0);
                };
            };
            lookup_table&.push(entry);
        };
    };
    
    lookup_table&.push(zeroed(@type lookup_table[0]));  // sentinal for safety check
    @literal (lookup_table.items(), offset_by_op)
}

AmdEncodingEntry :: @struct(
    opcode: Array(u8, 2),
    ext: u8,
    qbe_op: u8,
    a0: AmdArgType,
    a1: AmdArgType,
    prefix: u8,  // 0 if none
    opcode_len: u8,
);

// We assume prefix REX.W changes 32 to 64.
// .(OP, .(arg1, arg2, (opcode), ext?))
// Shift arg2 is always CL (ensured by isel), and it uses its extension slot. 
// You can always do (.R, .R, ...) by just using ModrmMode.Direct00 
//     note: that means the order of rules (.R, .M) and (.M, .R) matters (evan if the instruction commutes because one arg is also output)
// imm args are sign extended to the operation size. 
// Magic numbers transcribed from https://www.felixcloutier.com/x86
// Always put the imm8 before imm32 since imm32 will match smaller numbers too for store
// There are a bunch of hacks in assemble() for specific instructions to make the data here work. 
fn amd64_encoding_table() Ty([]AmdEncodingEntry, []u16) = @amd64_encoding_data (
    (.add, 
        (.M, .imm8, (0x83), 0),
        (.M, .imm32, (0x81), 0),
        (.R, .M, (0x03)),
        (.M, .R, (0x01)),
        (.F, .F, (0x0F, 0x58), (), (0xF2)), // addsd
    ),
    (.xcmp, 
        (.M, .imm8, (0x83), 7),
        (.M, .imm32, (0x81), 7),
        (.R, .M, (0x3B)),
        (.M, .R, (0x39)),
        // TODO: f32?
        (.F, .F, (0x0F, 0x2E), (), (0x66)), // ucomisd
    ),
    (.shr, 
        (.M, .imm8, (0xC1), 5),
        (.M, .R, (0xD3), 5),
    ),
    (.sar, 
        (.M, .imm8, (0xC1), 7),
        (.M, .R, (0xD3), 7),
    ),
    (.shl, 
        (.M, .imm8, (0xC1), 4),
        (.M, .R, (0xD3), 4),
    ),
    (.and, 
        (.M, .imm8, (0x83), 4),
        (.M, .imm32, (0x81), 4),
        (.R, .M, (0x23)),
        (.M, .R, (0x21)),
    ),
    (.or, 
        (.M, .imm8, (0x83), 1),
        (.M, .imm32, (0x81), 1),
        (.R, .M, (0x0B)),
        (.M, .R, (0x09)),
    ),
    (.storew,
        // note: the imm8 version would only store 1 byte! there's no `Move imm8 to r/m32.`
        (.imm32, .M, (0xC7), 0),
        (.R, .M, (0x89)),
    ),
    (.storel,
        (.imm32, .M, (0xC7), 0),
        (.R, .M, (0x89)),
        (.imm32, .imm32, (0xC7), 0),  // TODO: this is stupid. the only time this would ever happen is when writting a test that stores to a constant and catches a segsev
    ),
    (.storeb, 
        (.R, .M, (0x88)),
    ),
    (.storeh, 
        (.R, .M, (0x89), (), (operand_16bit_prefix)),
    ),
    // TODO: whats the difference between movsd and movq? 
    (.stored,
        (.F, .M, (0x0F, 0x11), (), (0xF2)), // movsd
    ),
    (.stores,
        (.F, .M, (0x0F, 0x11), (), (0xF3)), // movss
    ),
    (.xidiv,
        (.M, .none, (0xF7), 7),
    ),
    (.xdiv,
        (.M, .none, (0xF7), 6),
    ),
    (.xor, 
        (.M, .imm8, (0x83), 6),
        (.M, .imm32, (0x81), 6),
        (.R, .M, (0x33)),
        (.M, .R, (0x33)),
    ),
    (.sub, 
        (.M, .imm8, (0x83), 5),
        (.M, .imm32, (0x81), 5),
        (.R, .M, (0x2B)),
        (.M, .R, (0x29)),
        (.F, .F, (0x0F, 0x5C), (), (0xF2)), // subsd
    ),
    (.xtest, 
        (.M, .imm32, (0xF7), 0),
        (.R, .M, (0x85)),
    ),
    (.load, 
        (.R, .M, (0x8B)), 
        (.F, .M, (0x0F, 0x10), (), (0xF2)), // movsd
    ),
    (.extsw, 
        (.R, .M, (0x63)), // MOVSXD
    ),
    (.extsb, 
        (.R, .M, (0x0F, 0xBE)), // MOVSX
    ),
    (.extub, 
        (.R, .M, (0x0F, 0xB6)), // MOVZX
    ),
    (.extsh, 
        (.R, .M, (0x0F, 0xBF)), // MOVSX
    ),
    (.extuh, 
        (.R, .M, (0x0F, 0xB7)), // MOVZX
    ),
    (.extuw,
        (.R, .M, (0x8B)), // mov of 32 bits zero extends
    ),
    (.addr, 
        (.R, .M, (0x8D)), // lea
    ),
    (.neg, 
        (.M, .none, (0xF7), 3),
    ),
    (.mul, 
        (.R, .M, (0x0F, 0xAF)),
        (.F, .F, (0x0F, 0x59), (), (0xF2)), // mulsd
    ),
    (.div, 
        (.F, .F, (0x0F, 0x5E), (), (0xF2)), // divsd
    ),
    (.copy, 
        (.M, .imm32, (0xC7), 0), // "Move imm32 sign extended to 64-bits to r/m64."
        (.R, .M, (0x8B)),
        (.M, .R, (0x89)), // TODO: is this right? 
        (.F, .F, (0x0F, 0x10), (), (0xF2)), // movsd
        (.F, .M, (0x0F, 0x10), (), (0xF2)),
        (.M, .F, (0x0F, 0x11), (), (0xF2)),
    ),
    (.swap, // TODO: confirm RMem order if we ever generate it for those (or RSlot)
        (.R, .M, (0x87)), // xchg
    ),
    // :UglyFloatCast we need to tell the directions apart but the instructions always take the float arg on the same side so one has to swap after. 
    (.cast, 
        (.F, .M, (0x0F, 0x6E), (), (0x66)), // movd:movq f<-i
        (.M, .F, (0x0F, 0x7E), (), (0x66)), // movd:movq i<-f
    ),
    (.exts,
        (.F, .F, (0x0F, 0x5A), (), (0xF3)), // cvtss2sd  
    ),
    (.dtosi,
        (.R, .F, (0x0F, 0x2C), (), (0xF2)), // cvttsd2si  
    ),
    (.swtof,
        (.F, .R, (0x0F, 0x2A), (), (0xF2)), // cvtsi2sd  
    ),
    (.sltof,
        (.F, .R, (0x0F, 0x2A), (), (0xF2)), // cvtsi2sd  
    ),
    (.stosi,
        (.R, .F, (0x0F, 0x2C), (), (0xF3)), // cvttss2si  
    ),
    (.truncd,
        (.F, .F, (0x0F, 0x5A), (), (0xF2)), // cvtsd2ss  
    ),
    (.rotr, 
        (.M, .imm8, (0xC1), 1),
        (.M, .R, (0xD3), 1),
    ),
    (.rotl, 
        (.M, .imm8, (0xC1), 0),
        (.M, .R, (0xD3), 0),
    ),
    (.cas1,
        (.M, .R, (0x0F, 0xB1), (), (0xF0)), // lock CMPXCHG
    ),
    (.ones,
        (.M, .R, (0x0F, 0xB8), (), (0xF3)), // POPCNT
    ),
    // TODO: do i care about old cpus where these instructions are executed as BSR/BSF which doesn't do the right thing for zero?
    (.clz,
        (.M, .R, (0x0F, 0xBD), (), (0xF3)), // LZCNT
    ),
    (.ctz,
        (.M, .R, (0x0F, 0xBC), (), (0xF3)), // TZCNT
    ),
);

#use("@/backend/amd64/target.fr");
