//! Unlike Qbe, we don't rely on a seperate assembler. We just put the bytes in memory and jump there. 
//! This means we also have to handle updating forward references to functions that haven't been compiled yet. 

AmdState :: @struct(
    f: *Qbe.Fn,
    m: *QbeModule,
    // For leaf functions with statically known stack frame size, 
    // we don't need to bother saving RBP and can just index slots from RSP instead. 
    fp    := Amd64Reg.RBP,
    fsz   := 0,
    nclob := 0,
    a: List(u8),
    start_of_function: i64,
    last_saved_offset := 0,
);

fn emit_func_amd64(m: *QbeModule, f: *Qbe.Fn, file: *FILE) void = {
    @debug_assert_eq(m.goal.arch, .x86_64, "how did we get here?");
    code := m.segments&[.Code]&;
    start := code.next;
    name_id := f.lnk.id;
    maybe_add_export(m, name_id, f.lnk.export);
    m.do_jit_fixups(name_id, .Code, code.len());
    
    e: AmdState = (
        f = f,
        m = m,
        a = (maybe_uninit = (ptr = code.next, len = code.cap() - code.len()), len = 0, gpa = panicking_allocator),
        start_of_function = code.len(),
    );
    
    //printfn(f, f.globals.debug_out);
    emit_the_code(e&);
    code.next = code.next.offset(e.a.len() - e.last_saved_offset);
    m.last_function_end = code.next;
    use_symbol(m, name_id) { s | 
        s.size = e.a.len();
        @debug_assert(s.size != 0, "zero size function?");
    };
    maybe_llvm_dis(f, e.a.items(), "--arch=x86-64");
}

fn fixup_amd64(self: *QbeModule, symbol: *SymbolInfo, fixup: *Fixup, new_got_reloc: *?Fixup) void = {
    address := symbol.jit_addr;
    local  := symbol.kind == .Local;
    @match(fixup.type) {
        fn DataAbsolute() => {
            reloc := ptr_cast_unchecked(u8, rawptr, fixup.patch_at);
            reloc[] = address;
        }
        fn RipDisp32(it) => {
            distance := ptr_diff(u8.raw_from_ptr(fixup.patch_at), address); // :size_of_displacement
            if !local {
                if (@is(self.goal.type, .Relocatable)) {
                    return(); // that's not my department
                };
                
                if !it.call {
                    if self.debug["T".char()] {
                        @fmt_write(self.debug_out, "# patch addr % refs %=%\n", u8.int_from_ptr(fixup.patch_at), address, symbol.name);
                    };
                };
                @assert(it.call, "TODO: have to convert to load from got somehow. does that mean every forward reference needs nop padding?");
                
                if self.debug["T".char()] {
                    @fmt_write(self.debug_out, "# patch call % calls %\n", u8.int_from_ptr(fixup.patch_at), symbol.name);
                };
                if symbol.offset == -1 {
                    // create a new trampoline in the __stubs section. 
                    code := self.segments&.index(.Code);
                    symbol.offset = self.stubs&.len() + COMMANDS_SIZE_GUESS;
                    size_of_jmp_encoding :: 6;
                    trampoline := self.stubs&.reserve(Array(u8, size_of_jmp_encoding));
                    new_got_reloc[] = ensure_got_slot(self, symbol, address).or_else(=> new_got_reloc[]);
                    distance := self.distance_between(.Code, symbol.offset + size_of_jmp_encoding, .ConstantData, symbol.got_lookup_offset);
                    trampoline: List(u8) = (maybe_uninit = trampoline.items(), len = 0, gpa = panicking_allocator);
                    
                    // jmp near [rip + distance]
                    dyn_jmp_extension := @as(X86Reg) 0b0100; // not call!
                    trampoline&.push(0xff);
                    trampoline&.push(pack_modrm(.Indirect00, .rbp, dyn_jmp_extension));   // disp is RIP relative
                    trampoline&.push_u32(distance.trunc());
                };
                stub_trampoline := self.segment_address(.Code, symbol.offset);
                distance = ptr_diff(fixup.patch_at, stub_trampoline); // :size_of_displacement
            } else {
                if !it.call {
                    if self.debug["T".char()] {
                        @fmt_write(self.debug_out, "# patch addr % refs '%' = %[%]\n", u8.int_from_ptr(fixup.patch_at), symbol.name, symbol.segment, symbol.offset);
                    };
                };
            };// early returns
            
            size_of_displacement :: 4;
            distance -= size_of_displacement;
            @assert_lt(distance.abs(), MAX_i32, "can't reference that far");
            ptr_cast_unchecked(u8, u32, fixup.patch_at)[] = distance.trunc() + it.increment;
        }
        @default => panic("amd64 unexpected fixup type");
    }
}

fn framesz(e: *AmdState) void = {
    /* specific to NAlign == 3 */
    o := 0;
    if !e.f.leaf {
        for amd64_sysv_rclob { r |
            xx := e.f.reg.shift_right_logical(r.raw().zext());
            o = o.bit_xor(xx.bitcast());
        };
        o = o.bit_and(1);
    };
    f := e.f.slot;
    f = bit_and(f + 3, -4);
    if f > 0 && e.fp == .RSP && e.f.salign == 4 {
        f += 2;
    };
    e.fsz = 4*f.zext() + 8*o + 176*e.f.vararg.int();
}

fn next_inst_offset(self: *AmdState) i64 = self.a.len;

fn emit_the_code(e: *AmdState) void = {
    // It seems you need this to allow being called through a function pointer, that's such a pain in the ass bro... 
    // It stops people from doing evil ROP stuff or whatever. 
    // The magic words to google are "Control-Flow Enforcement Technology"
    // TODO: This is extra incentive to keep track of which things are allowed to be called indirectly 
    //       (which would also be nice for removing functions if all calls are inlined). 
    e.a&.push_u32(0xfa1e0ff3); // endbr64
    
    f := e.f;
    if !f.leaf || f.vararg || f.dynalloc {
        e.fp = .RBP;
        e.a&.encode_op_reg(PrimaryOp.PushBase, X86Reg.rbp);
        e.a&.encode_bin(PrimaryOp.MovReg, X86Reg.rbp, X86Reg.rsp);
    } else {
        e.fp = .RSP;
    };
    framesz(e);
    if e.fsz != 0 {
        @debug_assert(e.fsz < MAX_i32);
        e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, -e.fsz);
    };
    if f.vararg {
        o := -176;
        for amd64_sysv_rsave.slice(0, 6) { r | 
            e.a&.encode_non_sp_offset(true, PrimaryOp.MovReg, r.int_id(), .rbp, o);
            o += 8;
        };
        range(0, 8) { n |
            // mov [rbp + o] xmmn
            e.a&.push(0x0F);
            e.a&.push(0x29);
            reg := @as(X86Reg) n;
            e.a&.push(pack_modrm(ModrmMode.Indirect00, X86Reg.rsp, reg)); // mod=0, rm=sp means use sib
            e.a&.push(pack_sib(SibScale.One, X86Reg.rbp, X86Reg.rbp));
            e.a&.push_u32(o.trunc());
            o += 16;
        };
    };
    for amd64_sysv_rclob { r | 
        if f.reg.bit_and(BIT(r)) != 0 {
            e.a&.encode_op_reg(PrimaryOp.PushBase, r.int_id());
            e.nclob += 1;
        };
    };

    local_labels := temp().alloc_zeroed(i64, e.f.nblk.zext());
    patches: List(Patch) = list(temp());
    
    for_blocks f { b | 
        local_labels[b.id.zext()] = e.next_inst_offset();
        for_insts_forward b { i |
            emitins(i[], e);
        };
        @match(b.jmp.type) { 
            fn hlt() => e.a&.push(@as(u8) PrimaryOp.Int3);
            fn ret0() => {
                if f.dynalloc {
                    panic("TODO: dynalloc");
                    //fprintf(f,
                    //    "\tmovq %%rbp, %%rsp\n"
                    //    "\tsubq $%"PRIu64", %%rsp\n",
                    //    e.fsz + e.nclob * 8);
                };
                for_rev amd64_sysv_rclob { r | 
                    if f.reg.bit_and(BIT(r)) != 0 {
                        e.a&.encode_op_reg(PrimaryOp.PopBase, r.int_id());
                    };
                };
                ::enum_basic(Amd64Reg);
                @match(e.fp) {
                    fn RBP() => e.a&.push(@as(u8) PrimaryOp.Leave);
                    fn RSP() => if e.fsz != 0 {
                        e.a&.encode_imm(PrimaryOp.AddImm32, X86Reg.rsp, e.fsz);
                    };
                    @default => unreachable();
                };
                e.a&.push(@as(u8) PrimaryOp.Ret);
            }
            @default => {
                @debug_assert(!b.s1.is_null());
                if b.jmp.type != .jmp {
                    @debug_assert(!b.s2.is_null());
                    c := b.jmp.type.raw() - Qbe.J.jfieq.raw();
                    @assert(0 <= c && c < Qbe.NCmp, "unhandled jump");
                    if b.link.identical(b.s2) {
                        s := b.s1;
                        b.s1 = b.s2;
                        b.s2 = s;
                    } else {
                        c = cmpneg(c);
                    };
                    // :copypaste
                    id: i64 = b.s2.id.zext();
                    patch: Patch = (offset_from_start = e.a.len, cond = (Some = c.zext()), target_bid = id);
                    e.push_int3(6); // 2 byte opcode
                    if local_labels[id] == 0 {
                        patches&.push(patch);
                    } else {
                        // TODO: use smaller jumps when possible 
                        e.apply_patch(local_labels, patch&);
                    };
                    // fall through
                };
                if !b.s1.identical(b.link) {
                    id: i64 = b.s1.id.zext();
                    patch: Patch = (offset_from_start = e.a.len, cond = .None, target_bid = id);
                    e.push_int3(5);
                    if local_labels[id] == 0 {
                        patches&.push(patch);
                    } else {
                        // TODO: use smaller jumps when possible 
                        e.apply_patch(local_labels, patch&);
                    };
                };
            };
        };
    };
    
    each patches { p |
        e.apply_patch(local_labels, p);
    };
}

fn apply_patch(e: *AmdState, labels: []i64, patch: *Patch) void = {
    src := patch.offset_from_start;
    dest := labels[patch.target_bid];
    @debug_assert(dest != 0, "missing target block %", patch.target_bid);
    // TODO: this is a dump nounification. 
    patch_at := e.m.segments&[.Code].mmapped.ptr.offset(e.start_of_function + src);
    patch_size := @if(patch.cond.is_none(), 5, 6);
    patch_bytes: List(u8) = (maybe_uninit = (ptr = patch_at, len = patch_size), len = 0, gpa = panicking_allocator);
    // The offset is from **the end** of the jump instruction
    offset := dest - src - patch_size; 
    @debug_assert(offset.abs() < 1.shift_left(31), "what a big function you have");
    if patch.cond { cc | 
        cc := amd_condition_codes[cc];
        patch_bytes&.encode_jmp(cc, offset);
    } else {
        encode_imm32_only(patch_bytes&, PrimaryOp.JmpImm32, offset); 
    };
}

// note: you can have negative offsets!
fn slot(r: Qbe.Ref, e: *AmdState) i64 = {
    s: i64 = rsval(r).intcast();
    fslot := e.f.slot.intcast();
    @debug_assert_le(s, fslot);
    /* specific to NAlign == 3 */
    if s < 0 {
        if(e.fp == .RSP, => return(4*-s - 8 + e.fsz + e.nclob*8));
        return(4 * -s);
    };
    if(e.fp == .RSP, => return(4*s + e.nclob*8));
    if(e.f.vararg, => return(-176 + -4 * (fslot - s)));
    -4 * (fslot - s)
}

fn emitcopy(r1: Qbe.Ref, r2: Qbe.Ref, k: Qbe.Cls, e: *AmdState) void = {
    i := make_ins(.copy, k, r1, r2, QbeNull);
    emitins(i, e);
}

amd64_xmm_scratch :: TMP(Amd64Reg.XMM0.raw() + 15);

fn convert_to_2address(e: *AmdState, i: *Qbe.Ins) void = {
    if(i.to == QbeNull, => return());
    if i.arg&[1] == QbeNull {
        i.arg&[1] = i.to;
        i.arg&.items().swap(0, 1);
        return();
    };
    o := i.op();
    commutative := o == .add || o == .and || o == .xor || o == .or || o == .mul;
    if commutative && i.arg&[1] == i.to {
        i.arg&.items().swap(0, 1);
    };
    if i.arg&[0] != i.to {
        @debug_assert(i.arg&[1] != i.to, "cannot convert to 2-address");
        emitcopy(i.to, i.arg&[0], i.cls(), e);
        i.arg&[0] = i.to;
    };
}

fn emitins(i: Qbe.Ins, e: *AmdState) void = {
    t0 := rtype(i.arg&[0]);
    t1 := rtype(i.arg&[1]);
    
    fix_memarg_slot(e, i.arg&.index(0));
    fix_memarg_slot(e, i.arg&.index(1));
    if i&.op() != .copy {
        fix_memarg_slot(e, i.to&);  // TODO: this will break when we try to check for equality
    };
    
    @match(i&.op()) {
        /* just do nothing for nops, they are inserted
         * by some passes */
        fn nop() => ();
        fn mul() => {
            /* here, we try to use the 3-addresss form
            * of multiplication when possible */
            if rtype(i.arg&[1]) == .RCon {
                i.arg&.items().swap(0, 1);
            };
            if is_int(i&.cls())
            && rtype(i.arg&[0]) == .RCon
            && rtype(i.arg&[1]) == .RTmp {
                c := e.f.get_constant(i.arg&[0]);
                r0, r1 := (i.to.int_reg_d(), i.arg&[1].int_reg_d());
                e.a&.push(pack_rex_si(i&.cls() == .Kl, r0, r1, DONT_CARE_REG));  // TODO: elide
                e.a&.push(0x69);
                e.a&.push(pack_modrm(ModrmMode.Direct, r1, r0));
                e.push_imm(c.bits.i, 4);
                return();
            };
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        fn sub() => {
            /* we have to use the negation trick to handle
            * some 3-address subtractions */
            // because it's not commutative 
            if i.to == i.arg&[1] && i.arg&[0] != i.to {
                // TODO: make a .ssa test that gets here
                ineg := make_ins(.neg, i&.cls(), i.to, i.to, QbeNull);
                emitins(ineg, e);
                i&.set_op(.add);
                emitins(i, e);
                return();
            };
            
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        fn sign() => {
            if i&.cls() == .Kl {    
                e.a&.push(0b01001000);  // w=true
            };
            e.a&.push(@as(u8) PrimaryOp.SignExtendAxToDxAx); // CQO
        }
        fn neg() => {
            if is_int(i&.cls()) {
                if i.to != i.arg&[0] {
                    ins := make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull);
                    emitins(ins, e);
                    i.arg&[0] = i.to;
                };
                assemble(e, i&);
            } else {
                // You can't just lower these in isel because we insert new ones to handle 3-address subtract. 
                // TODO: this is so garbage. im doing extra work just to do a worse job. should just do what qbe does. 
                
                @debug_assert(amd64_xmm_scratch != i.arg&[0]);
                
                // xmm15 = 0 
                xmm15 :: int_reg_d(amd64_xmm_scratch);  // TODO: compiler hangs if you typo `xmm15 :: int_reg_d(xmm15);`
                e.a&.push(0x66);
                e.a&.push(pack_rex_si(true, xmm15, xmm15, DONT_CARE_REG));
                e.a&.push_all(@slice(0x0F, 0xEF)); // PXOR
                e.a&.push(pack_modrm(.Direct, xmm15, xmm15));
                
                // xmm15 = 0 - a0
                ins := make_ins(.sub, i&.cls(), amd64_xmm_scratch, amd64_xmm_scratch, i.arg&[0]);
                emitins(ins, e);
                
                // to = xmm15
                ins := make_ins(.copy, i&.cls(), i.to, amd64_xmm_scratch, QbeNull);
                emitins(ins, e);
            };
        }
        fn div() => {
            // TODO: .ssa test that gets here
            /* use xmm15 to adjust the instruction when the
            * conversion to 2-address in emitf() would fail */
            if i.to == i.arg&[1] {
                @debug_assert(!is_int(i&.cls()), "isel converts int div to xdiv");
                ins := make_ins(.copy, i&.cls(), amd64_xmm_scratch, i.to, QbeNull);
                emitins(ins, e);
                ins := make_ins(.copy, i&.cls(), i.to, i.arg&[0], QbeNull);
                emitins(ins, e);
                i.arg&[0] = i.to;
                i.arg&[1] = amd64_xmm_scratch;
            };
            convert_to_2address(e, i&);
            assemble(e, i&);
        }
        // TODO: kinda garbage that this is a weird super instruction that's copy+load+store+addr
        fn copy() => {
            /* copies are used for many things; see a note
            * to understand how to load big constants:
            * https://c9x.me/notes/2015-09-19.html */
            @debug_assert(rtype(i.to) != .RMem);
            if(i.to == QbeNull || i.arg&[0] == QbeNull, => return());
            if(i.to == i.arg&[0], => return());
            
            is_long_const := i&.cls() == .Kl && t0 == .RCon && e.f.get_constant(i.arg&[0])[].type == .CBits;
            if is_long_const {
                value := e.f.con[i.arg&[0].val()].bits.i;
                if rtype(i.to) == .RSlot && (value < MIN_i32 || value > MAX_i32) {
                    printfn(e.f, e.f.globals.debug_out);
                    panic("TODO: set slot");
                    //emitf("movl %0, %=", &i, e);
                    //emitf("movl %0>>32, 4+%=", &i, e);
                    return();
                };
            };
            if t0 == .RCon {
                c := e.f.get_constant(i.arg&[0]);
                if isreg(i.to) {
                    if c.type == .CAddr {
                        ins := make_ins(.addr, i&.cls(), i.to, i.arg&[0], QbeNull);
                        emitins(ins, e);
                        return();
                    };
                    if c.type == .CBits && is_int(i&.cls()) {
                        value := c.bits.i;
                        if value < MIN_i32 || value > MAX_i32 {
                            encode_imm64(e.a&, i.to.int_reg_d(), value.bitcast());
                            return();
                        }; // else fallthrough to assemble
                    }
                };
            };
            if rtype(i.to) == .RSlot && (t0 == .RSlot || t0 == .RMem) {
                // rega does this shit when there's a lot of register pressure i guess?
                // TODO: lower it better so it's not a sepecial case here. 
                k := @if(is_wide(i&.cls()), Qbe.Cls.Kd, .Ks);
                str := @if(is_wide(i&.cls()), Qbe.O.stored, .stores);
                ins := make_ins(.load, k, amd64_xmm_scratch, i.arg&[0], QbeNull);
                fix_memarg_slot(e, ins.arg&.index(0)); // TODO: do i need this?
                emitins(ins, e);
                ins := make_ins(str, .Kw, QbeNull, amd64_xmm_scratch, i.to);
                fix_memarg_slot(e, ins.arg&.index(1)); // TODO: do i need this?
                emitins(ins, e);
                return();
            };
            
            fix_memarg_slot(e, i.to&);
            
            i.arg&[1] = i.to;
            i.arg&.swap(0, 1);
            assemble(e, i&);
        }
        fn addr() => {
            if e.m.got_indirection_instead_of_patches {
                r := i.arg&[0];
                if rtype(r) == .RCon {
                    c := e.f.get_constant(r);
                    @debug_assert(c.type == .CAddr);
                    done := false;
                    use_symbol(e.m, c.sym.id) { symbol | 
                        done = symbol.kind != .Local;
                        if done {
                            if ensure_got_slot(e.m, symbol, symbol.jit_addr) { it |
                                symbol.fixups&.push(it, e.m.gpa);
                            };
                            
                            // TODO: very copy paste 
                            size_of_load_encoding :: 7;
                            current_offset := e.start_of_function + e.a.len();
                            distance := e.m.distance_between(.Code, current_offset + size_of_load_encoding, .ConstantData, symbol.got_lookup_offset);
                            
                            dest := int_reg_d(i.to);
                            e.a&.push(pack_rex_si(true, dest, DONT_CARE_REG, DONT_CARE_REG)); 
                            e.a&.push(0x8B);  // load
                            e.a&.push(pack_modrm(.Indirect00, .rbp, dest));   // disp is RIP relative
                            e.a&.push_u32(distance.trunc());
                            done = true;
                        };
                    }; 
                    if done {
                        return();
                    }
                };
            };
            
            // note: LEA interprets RMem differently than other instructions
            i.arg&[1] = i.to;
            i.arg&.swap(0, 1);
            assemble(e, i&);
        }
        fn call() => {
            @match(rtype(i.arg&[0])) {
                fn RCon() => {
                    c := e.f.get_constant(i.arg&[0]);
                    @debug_assert_eq(c.bits.i, 0, "call to offset from symbol seems like a mistake?");
                    off := c.bits.i;
                    use_symbol(e.m, c.sym.id) { symbol |
                        current_offset := e.start_of_function + e.a.len();
                        @match(symbol.kind) {
                            fn Local() => {
                                encoded_size :: 5;  //  address is relatitive to the start of the next instruction
                                off += distance_between(e.m, .Code, current_offset + encoded_size, symbol.segment, symbol.offset);
                                if e.m.debug["T".char()] {
                                    write(e.m.debug_out, items(@format("# call offset % for '%' %[%]\n", off, symbol.name, symbol.segment, symbol.offset) temp()));
                                };
                                e.a&.push(@as(u8) PrimaryOp.CallImm32); 
                                e.a&.push_u32(signed_truncate(off, 32));
                            }
                            @default => {
                                call_through_got := e.m.got_indirection_instead_of_patches || (symbol.kind == .DynamicPatched && e.m.goal.type != .Relocatable);
                                if !call_through_got && e.m.debug["T".char()] {
                                    write(e.m.debug_out, items(@format("# pending call to %\n", symbol.name) temp()));
                                };
                                if call_through_got {
                                    // TODO: this is very copy-paste from where we do stuff for __stubs. 
                                    
                                    if ensure_got_slot(e.m, symbol, symbol.jit_addr) { it |
                                        symbol.fixups&.push(it, e.m.gpa);
                                    };
                                    encoded_size :: 6;
                                    off += e.m.distance_between(.Code, current_offset + encoded_size, .ConstantData, symbol.got_lookup_offset);
                                    
                                    // call near [rip + distance]
                                    dyn_call_extension := @as(X86Reg) 0b0010; 
                                    e.a&.push(0xff);
                                    e.a&.push(pack_modrm(.Indirect00, .rbp, dyn_call_extension));   // disp is RIP relative
                                    e.a&.push_u32(off.trunc());
                                } else {
                                    // Otherwise this will get patched to a local or __stub call later. 
                                    patch_at := e.a.maybe_uninit.ptr.offset(e.a.len + 1);
                                    fix: Fixup = (patch_at = patch_at, type = (RipDisp32 = (increment = c.bits.i.trunc(), call = true)));
                                    symbol.fixups&.push(fix, e.m.gpa);
                                    e.a&.push(@as(u8) PrimaryOp.CallImm32); 
                                    e.a&.push_u32(0);
                                };
                            };
                        };
                    };
                }
                fn RTmp() => encode_call_reg(e.a&, ModrmMode.Direct, int_reg_d(i.arg&[0]));  
                // TODO: we could allow memory argument? 
                @default => panic("invalid call argument");
            }
        }
        fn salloc() => {
            /* there is no good reason why this is here
            * maybe we should split Osalloc in 2 different
            * instructions depending on the result
            */
            @debug_assert(e.fp == .RBP);
            
            i.arg&[1] = TMP(Amd64Reg.RSP);
            i.arg&.items().swap(0, 1);
            i&.set_op(.sub);
            assemble(e, i&);
            
            if i.to != QbeNull {
                emitcopy(i.to, TMP(Amd64Reg.RSP), .Kl, e);
            };
        }
        fn swap() => {
            if is_int(i&.cls()) {
                assemble(e, i&);
                return();
            };
            /* for floats, there is no swap instruction
            * so we use xmm15 as a temporary
            */
            emitcopy(amd64_xmm_scratch, i.arg&[0], i&.cls(), e);
            emitcopy(i.arg&[0], i.arg&[1], i&.cls(), e);
            emitcopy(i.arg&[1], amd64_xmm_scratch, i&.cls(), e);
        }
        fn storeb() => {
            @debug_assert(rtype(i.arg&[1]) != .RTmp, "storeb to tmp");
            // this is painful because everywhere else the range of a byte immediate is signed but here its unsigned. 
            if e.f.get_int(i.arg&[0]) { c |
                c = c.bit_and(1.shift_left(8) - 1);
                i.arg&[0] = TMP(reg_for_extension(0));
                b := encode(e, i&, false, 1);
                @debug_assert(b.force_rex);
                push_instruction(e, b, @slice(0xC6));
                e.a&.push(@as(u8) c.trunc());
                return();
            };
        
            assemble(e, i&); 
        }
        fn storeh() => {
            // TODO: has RTmp already been converted to RMem for all stores?
            
            if e.f.get_int(i.arg&[0]) { c |
                c = c.bit_and(1.shift_left(16) - 1);
                i.arg&[0] = TMP(reg_for_extension(0));
                b := encode(e, i&, false, 2);
                b.prefix = operand_16bit_prefix;
                push_instruction(e, b, @slice(0xC7));
                e.a&.reserve_type(u16)[] = c.trunc();
                return();
            };
        
            assemble(e, i&);
        }
        fn dbgloc() => (); // TODO: we don't do debug info yet
        // TODO: allow non-multiple-of-4 blocks of asm. for now you just have to pad with nops because im being consistant with the arm one. 
        fn asm() => {  
            imm := e.f.get_int(i.arg&[0]).expect("op 'asm' arg is constant int");
            e.push_imm(imm, 4);
        }
        @default => {
            o := i&.op();
            if is_flag(i&.op()) {  // Reading a bit from the flags register
                cc := @as(i32) i&.op().raw() - Qbe.O.flagieq.raw();
                cc := amd_condition_codes[cc.zext()];
                @debug_assert(rtype(i.to) == .RTmp, "we don't allow mem arg for setcc"); // because then the zero extending is even more painful. 
                r0 := int_reg_d(i.to);
                
                e.a&.push(pack_rex_si(false, r0, r0, DONT_CARE_REG));  // Don't elide. See EncodedArgs.force_rex
                e.a&.encode_2cc(TwoByteOp.SetCC, cc);
                e.a&.push(pack_modrm(ModrmMode.Direct, r0, DONT_CARE_REG));
                
                // setcc doesn't zero the upper bits, so we zero extend as an extra instruction. 
                // we could get complicated and keep track of known bits but I suspect that's well into deminishing returns territory. 
                e.a&.push(pack_rex_si(false, r0, r0, DONT_CARE_REG));  // Don't elide. See EncodedArgs.force_rex
                e.a&.push(TWO_BYTE_OP_PREFIX);
                e.a&.push(@as(u8) TwoByteOp.MovZX);
                e.a&.push(pack_modrm(ModrmMode.Direct, r0, r0));
                return();
            };
            
            if is_load(o) {
                // TODO: this is wasteful allocation just to smuggle one bit of information on which modrm mode to use. 
                @debug_assert(rtype(i.arg&[0]) != .RTmp, "load from tmp");
                
                if o != .load {
                    // convert to extXX and let the memory argument express the load. 
                    o = rebase(o, .extsb, .loadsb);
                    if o == .extsw && i&.cls() == .Kw {
                        o = .copy;
                    };
                    i&.set_op(o);
                };
                i.arg&[1] = i.to;
                i.arg&.swap(0, 1);
            } else {
                convert_to_2address(e, i&);
            };
            // because we choose arg size based on output size, without this is does a 64 bit mov which doesn't zero the top bytes.
            if o == .extuw {
                i&.set_cls(.Kw);
            };
            
            assemble(e, i&);
        };
    }; // early returns
}

fn push_imm(e: *AmdState, value: i64, byte_count: i64) void = {
    bytes: []u8 = (ptr = ptr_cast_unchecked(i64, u8, value&), len = byte_count); // little endian
    e.a&.push_all(bytes);
}

fn push_instruction(e: *AmdState, b: EncodedArgs, ops: []u8) void = {
    if b.prefix != 0 {
        e.a&.push(b.prefix);
    };
    
    // If we're not using 64 bit operand size and don't need a register extension, don't emit a wasted byte. 
    // However, when using the low byte of RAX/RCX/RDX/RBX, missing rex gives you bits 8-16 instead 
    // which maybe is cool you're on an 8086 or whatever but is never what we want so here we are. 
    if b.rex != 0b01000000 || b.force_rex {
        e.a&.push(b.rex);
    };
    e.a&.push_all(ops);
    e.a&.push(b.modrm);
    if b.sib { sib | 
        e.a&.push(sib);
    };
    if b.disp_size != .D0 {
        if b.patch { p | 
            off, sym := p;
            use_symbol(e.m, sym) { symbol |
                if symbol.kind == .Local {
                    // +4 for the size of the encoded displacement. address is relatitive to the start of the next instruction
                    b.disp += distance_between(e.m, .Code, e.start_of_function + e.a.len + 4, symbol.segment, symbol.offset);
                    @debug_assert(b.disp.abs() < 1.shift_left(b.disp_size.bits()));
                    if e.m.debug["T".char()] {
                        write(e.m.debug_out, items(@format("# direct offset % for '%' %[%]\n", b.disp, symbol.name, symbol.segment, symbol.offset) temp()));
                    };
                } else {
                    @debug_assert(symbol.kind != .DynamicPatched, "TODO: somehow have to convert this to a got load?");
                    @debug_assert(!e.m.got_indirection_instead_of_patches, "TODO: somehow have to convert this to a got load");
                    if e.m.debug["T".char()] {
                        write(e.m.debug_out, items(@format("# pending ref to '%'\n", symbol.name) temp()));
                    };
                };
                patch_at := e.a.maybe_uninit.ptr.offset(e.a.len);
                if symbol.kind != .Local || e.m.goal.type == .Relocatable {
                    symbol.fixups&.push((patch_at = patch_at, type = (RipDisp32 = (increment = off.trunc(), call = false))), e.m.gpa);
                };
            };
        };
        e.push_imm(b.disp, b.disp_size.bits() / 8);
    } else {
        @debug_assert(b.patch.is_none());
    };
    // after: immediate
}

// this invalidates f.mem pointers!
fn fix_memarg_slot(e: *AmdState, r: *Qbe.Ref) void = {
    if rtype(r[]) == .RSlot {
        r[] = e.f.new_mem(r[], QbeNull, 0, 0);
    };
    if(rtype(r[]) != .RMem, => return());
    m := e.f.get_memory(r[]);
    if rtype(m.base) == .RSlot {
        @debug_assert(@is(m.offset.type, .CBits, .CUndef), "bits"); 
        if m.offset.type == .CUndef {
            m.offset.bits.i = 0;
        };
        m.offset.bits.i = m.offset.bits.i + slot(m.base, e);
        m.offset.type = .CBits;
        m.base = TMP(e.fp);
    };
}

// for reserving space that needs to be patched later. 
fn push_int3(e: *AmdState, count: i64) void = {
    range(0, count) { _ |
        e.a&.push(@as(u8) PrimaryOp.Int3);
    };
}

EncodedArgs :: @struct(
    modrm: u8,
    sib: ?u8,
    rex: u8,
    force_rex: bool,
    disp: i64 = 0,
    disp_size := Displacement.D0,
    patch: ?Ty(i64, u32) = .None,
    prefix: u8 = 0,
);

fn encode(e: *AmdState, i: *Qbe.Ins, w: bool, immediate_size: i64) EncodedArgs = {
    force_rex := @is(i.op(), .extub, .extsb, .loadub, .loadsb, .storeb);
    r0, r1 := (i.arg&[0], i.arg&[1]);
    t0, t1 := (rtype(r0), rtype(r1));
    @debug_assert(r0 != QbeNull && r1 != QbeNull, "we only look at the arg slots becuase x64 is 2-address");
    if t0 == t1 && t0 == .RTmp {  // both in registers, thats easy!
        rr0, rr1 := (r0.int_reg_d(), r1.int_reg_d());
        return(modrm = pack_modrm(ModrmMode.Direct, rr1, rr0), rex = pack_rex_si(w, rr0, rr1, DONT_CARE_REG), sib = .None, force_rex = force_rex);
    };
    
    if t1 == .RMem {
        @debug_assert(t0 != .RMem, "cannot encode two memory arguments");
        m := e.f.get_memory(r1)[]; // copy!
        
        if m.offset.type == .CAddr {
            @debug_assert(m.base == QbeNull && m.index == QbeNull, "cannot encode const SIB");
            c := m.offset&;
            @debug_assert(t0 != .RCon, "TODO: con + disp?");
            // :copy_paste_con_addr
            rr0 := r0.int_reg_d();
            off := c.bits.i - immediate_size;
            return(
                patch = (Some = (off, c.sym.id)), 
                disp_size = .D32,
                disp = off,
                modrm = pack_modrm(.Indirect00, .rbp, rr0),  // disp is RIP relative
                rex = pack_rex_si(w, rr0, DONT_CARE_REG, DONT_CARE_REG), 
                sib = .None, 
                force_rex = force_rex,
            );
        };
        
        valid_offset := fits_in_i32(m.offset.bits.i);
        if !valid_offset {
            printfn(e.f, e.f.globals.debug_out);
        };
        @debug_assert(valid_offset, "cannot encode offset %", m.offset.bits.i); 
        small := m.offset.bits.i <= 127 && m.offset.bits.i >= -128;
        no_disp := m.offset.bits.i == 0 && m.index != TMP(Amd64Reg.RBP) && m.index != TMP(Amd64Reg.R13);
        disp_size: Displacement = @if(no_disp, .D0, @if(small, .D8, .D32)); // more - than +!
        
        special_base := m.base == TMP(Amd64Reg.RBP) || m.base == TMP(Amd64Reg.R13);
        if special_base && disp_size == .D0 {
            disp_size = .D8;
            no_disp = false;
        };
        
        @debug_assert(m.index != TMP(Amd64Reg.RSP), "cannot encode index");
        if m.index == QbeNull {
            m.index = TMP(Amd64Reg.RSP); // sentinal for just base+disp
        };
        
        disp := m.offset.bits.i;
        mode: ModrmMode = @match(disp_size) {
            fn D0()  => .Indirect00;
            fn D8()  => .Indirect01;
            fn D32() => .Indirect10;
        };
        
        if m.base == QbeNull {
            @debug_assert(m.index != TMP(Amd64Reg.RSP), "no base and no index makes x64 a dull boy");
            m.base = TMP(Amd64Reg.RBP); // sentinal for just (index*s)+disp32
            mode = .Indirect00;
            disp_size = .D32;
            no_disp = false;
        };
        if m.offset.type == .CUndef {
            m.offset.bits.i = 0;
        };
        
        ri, rb, rr0 := (m.index.int_reg_d(), m.base.int_reg_d(), r0.int_reg_d());
        dont_need_sib := no_disp && m.index == TMP(Amd64Reg.RSP) && m.base != TMP(Amd64Reg.RSP) && m.base != TMP(Amd64Reg.R12);
        if dont_need_sib {
            // this is a lot of hassle to save one byte
            return(modrm = pack_modrm(mode, rb, rr0), rex = pack_rex_si(w, rr0, rb, DONT_CARE_REG), sib = .None, force_rex = force_rex);
        };
       
        return(disp_size = disp_size, disp = disp, modrm = pack_modrm(mode, .rsp, rr0), rex = pack_rex_si(w, rr0, rb, ri), sib = (Some = pack_sib(m.scale.to_scale(), ri, rb)), force_rex = force_rex);
    };
    
    if t1 == .RCon {
        c := e.f.get_constant(i.arg&[1]);
        if c.type == .CAddr {
            // :copy_paste_con_addr
            rr0 := r0.int_reg_d();
            // The distance is relative to the end of the instruction, 
            // so if we have another immediate after the displacement, we have to account for its size. 
            // becuase the linker doesn't give a shit about what instructions we're using, 
            // its just gonna plonk in the distance from the patch to the symbol. 
            // TODO: maybe it would be better if we also did the -4 for the displacement here but that requires removing it everywhere else. 
            off := c.bits.i - immediate_size;
            return(
                patch = (Some = (off, c.sym.id)), 
                disp_size = .D32,
                disp = off,
                modrm = pack_modrm(.Indirect00, .rbp, rr0),  // disp is RIP relative
                rex = pack_rex_si(w, rr0, DONT_CARE_REG, DONT_CARE_REG), 
                sib = .None,
                force_rex = force_rex,
            );
        };
        
        // TODO: this might be backwards because you might be ina situation where both args are constant. 
        //       we're adding a source of confusion for zero benifit because i don't want to deal with changing the isel right now. 
        if c.type == .CBits && t0 == .RTmp {
            rr0 := r0.int_reg_d();
            disp := c.bits.i;
            return(
                disp_size = .D32, 
                disp = disp, 
                modrm = pack_modrm(.Indirect00, .rsp, rr0),  // absolute address in disp
                rex = pack_rex_si(w, rr0, .rbp, .rsp),             // ^
                sib = (Some = pack_sib(1.to_scale(), .rsp, .rbp)), // ^
                force_rex = force_rex,
            );
        };
    };
    
    @panic("TODO: we can't encode that yet")
}

fn to_scale(s: i32) SibScale = @switch(s) {
    @case(0) => .One; // hopefully you're using a mode that ignores this!
    @case(1) => .One;
    @case(2) => .Two;
    @case(4) => .Four;
    @case(8) => .Eight;
    @default => @panic("Invalid sib scale %", s);
}

fn bits(d: Displacement) i64 = @match(d) {
    fn D0() => 0;
    fn D8() => 8;
    fn D32() => 32;
};

fn int_reg_d(r: Qbe.Ref) X86Reg = 
    int_id(@as(Amd64Reg) @as(i32) r.val().trunc());

// TODO: convert my instruction encoding to use Amd64Reg instead of X64Reg. having both is mega dumb. 
//       but also qbe puts them in the wrong order so i have to commit to not sharing target struct 
//       if i want the numbers to match thier encoding so its less painful. ugh. -- Nov 8

// TODO: losing my mind!
fn int_id(r: Amd64Reg) X86Reg = {
    ::List(X86Reg);
    table :: @const_slice(X86Reg.zeroed(), 
        .rax, .rcx, .rdx, .rsi, .rdi, .r8, .r9, .r10, .r11, .rbx, .r12, .r13, .r14, .r15, .rbp, .rsp, // integers
        .rax, .rcx, .rdx, .rbx, .rsp, .rbp, .rsi, .rdi, .r8, .r9, .r10, .r11, .r12, .r13, .r14, .r15, // floats are in order
    );
    @debug_assert(r.raw().zext() < table.len(), "expected register");
    table[r.raw().zext()]
}

// i need an air tag on my mind. 
// so i don't lose it. 
fn reg_for_extension(v: u8) Amd64Reg = {
    table :: @const_slice(Amd64Reg.RAX, .RCX, .RDX, .RBX, .RSP, .RBP, .RSI, .RDI);
    table[v.zext()]
}

AmdArgType :: @enum(u8) (M, R, imm32, imm8, none, F);    ::enum(AmdArgType);
    
fn arg_t(e: *AmdState, r: Qbe.Ref) AmdArgType = @match(rtype(r)) {
    fn RTmp() => @if(r.val() < Amd64Reg.XMM0.raw().zext(), .R, .F);
    fn RMem() => .M;
    fn RCon() => {
        c := e.f.get_constant(r);
        @match(c.type) {
            fn CBits() => @if(c.bits.i <= 127 && c.bits.i >= -128, .imm8, .imm32);
            fn CAddr() => .M; // rel
            @default => unreachable();
        }
    }
    @default => .none;
};

fn assemble(e: *AmdState, i: *Qbe.Ins) void = {
    //printins(e.f, i, e.f.globals.debug_out);
    // aaaa that test only exists for registers in the other direction is such a hack! fix this properly please! -- Nov 14 :FUCKED
    if i.op() == .xcmp || (i.op() == .xtest && rtype(i.arg&[0]) == .RCon) {  // // HACK. TODO: at least move this to the outer match in emitins
        i.arg&.items().swap(0, 1);
    };
    a0_old, a1_old := (i.arg&[0], i.arg&[1]);
    a0, a1 := (arg_t(e, i.arg&[0]), arg_t(e, i.arg&[1]));
    
    w := i.cls() == .Kl || i.cls() == .Kd || i.op() == .storel || i.op() == .stored;
    ext_index := int((i.arg&[1] == QbeNull || (rtype(i.arg&[1]) == .RCon && e.f.get_constant(i.arg&[1])[].type == .CBits)));  
    
    xxx :: amd64_encoding_table();
    table, offsets := xxx;
    idx: i64 = offsets[i.op().raw().zext()].zext();
    entry := table.index(idx);
    dowhile {
        matches :: fn(w: AmdArgType, h: AmdArgType) bool =>
            w == h || (w == .imm32 && h == .imm8) || (w == .M && h == .R);
        @debug_assert(entry.qbe_op == i.op().raw().intcast().trunc(), "didn't find encoding for % % % in %", i.op().get_name(), a0, a1, e.f.name());
        bad := !matches(entry.a0, a0) || !matches(entry.a1, a1);
        if bad {
            idx += 1;
            entry = table.index(idx);
        };
        bad
    };
    
    // :UglyFloatCast this happens AFTER looking up the instruction because we have to tell the directions apart 
    if i.op() == .cast && is_int(i.cls()) {
        i.arg&.items().swap(0, 1);
    };
    
    opcode: []u8 = (ptr = entry.opcode&.as_ptr(), len = entry.opcode_len.zext());
    
    if entry.ext != 0 {  // encoded as TMP number so ext=0 is RXX not RAX
        // hack because it thinks the con is redundant but rcx is fixed
        shift := i.op() == .shl || i.op() == .sar || i.op() == .shr;
        if shift && rtype(i.arg&[ext_index]) == .RTmp && i.arg&[int(ext_index == 0)] == TMP(Amd64Reg.RCX) {
            ext_index = int(ext_index == 0);
            //i.arg&.items().swap(0, 1); // HACK
        };
        // one argument is an immediate or fixed register and its slot in modrm holds an opcode extension instead. 
        i.arg&[ext_index] = TMP(@as(i64) entry.ext.zext()); 
        i.arg&.items().swap(0, 1); // HACK
    };
    
    if rtype(i.arg&[0]) == .RMem || (rtype(i.arg&[1]) != .RMem && rtype(i.arg&[0]) == .RCon) {
        @debug_assert(rtype(i.arg&[0]) == .RMem || e.f.get_constant(i.arg&[0])[].type == .CAddr);
        // which direction it goes doesn't matter because the memory always has to be in SIB
        i.arg&.items().swap(0, 1); // HACK
    };
    immediate_size := @if_else {
        @if(entry.a0 == .imm8 || entry.a1 == .imm8) => 1;
        @if(entry.a0 == .imm32 || entry.a1 == .imm32) => 4;
        @else => 0;
    };
    b := encode(e, i, w, immediate_size);
    prefix := entry.prefix;
    if prefix == 0xF2 && !w && i.op() != .truncd {
        prefix = 0xF3; // this seems like the easiest way to do this to me
    };
    b.prefix = prefix;
    push_instruction(e, b, opcode);
    if entry.a0 == .imm8 {
        push_imm(e, e.f.get_constant(a0_old)[].bits.i, 1);
    };
    if entry.a1 == .imm8 {
        push_imm(e, e.f.get_constant(a1_old)[].bits.i, 1);
    };
    if entry.a0 == .imm32 {
        imm := e.f.get_constant(a0_old)[].bits.i;
        push_imm(e, imm, 4);
    };
    if entry.a1 == .imm32 {
        imm := e.f.get_constant(a1_old)[].bits.i;
        push_imm(e, imm, 4);
    };
}

fn amd64_encoding_data(e: FatExpr) FatExpr #macro = {
    ArgType :: AmdArgType;
    ops := e&.items();
    offset_by_op := ast_alloc().alloc_zeroed(u16, Qbe.O.enum_count());
    lookup_table := AmdEncodingEntry.list(ops.len, ast_alloc());
    each ops { full_op | 
        loc := full_op.loc;
        full_op := full_op.items();
        qbe_o := const_eval(Qbe.O)(full_op[0]);
        @ct_assert(offset_by_op[qbe_o.raw().zext()] == 0, loc, "repeated op in table");
        offset_by_op[qbe_o.raw().zext()] = lookup_table.len.trunc();
        
        each full_op.slice(1, full_op.len) { components | 
            entry := AmdEncodingEntry.zeroed();
            entry.qbe_op = qbe_o.raw().intcast().trunc(); // overflow is doesn't matter, we just use the change as safety check
            components := components.items();
            entry.a0 = const_eval(ArgType)(components[0]);
            entry.a1 = const_eval(ArgType)(components[1]);
            
            fn compile_bytes(e: *FatExpr) []u8 = {
                o := u8.list(ast_alloc());
                each e.items() { b | 
                    o&.push(const_eval(u8)(b[]));
                };
                o.items()
            }
            
            e := components.index(2);
            o := compile_bytes(e);
            @switch(o.len) {
                @case(1) => {
                    entry.opcode&[0] = o[0];
                };
                @case(2) => {
                    entry.opcode&[0] = o[0];
                    entry.opcode&[1] = o[1];
                };
                @default => compile_error("bad opcode length", e.loc);
            };
            entry.opcode_len = o.len.trunc();
            entry.ext = 0;
            if components.len > 3 {
                e := compile_ast(components[3]);
                if e.ty == void {
                    @ct_assert(components.len > 4, e.loc, "explicit () extension implies prefix follows");
                    prefix := compile_bytes(components.index(4));
                    @ct_assert(prefix.len == 1, e.loc, "expected one prefix byte");
                    entry.prefix = prefix[0];
                } else {
                    e := const_eval(u8)(e);
                    entry.ext = TMP(reg_for_extension(e)).val().trunc();
                    @debug_assert(entry.ext != 0);
                };
            };
            lookup_table&.push(entry);
        };
    };
    
    lookup_table&.push(zeroed(@type lookup_table[0]));  // sentinal for safety check
    @literal (lookup_table.items(), offset_by_op)
}

AmdEncodingEntry :: @struct(
    opcode: Array(u8, 2),
    ext: u8,
    qbe_op: u8,
    a0: AmdArgType,
    a1: AmdArgType,
    prefix: u8,  // 0 if none
    opcode_len: u8,
);

// We assume prefix REX.W changes 32 to 64.
// .(OP, .(arg1, arg2, (opcode), ext?))
// Shift arg2 is always CL (ensured by isel), and it uses its extension slot. 
// You can always do (.R, .R, ...) by just using ModrmMode.Direct00 
//     note: that means the order of rules (.R, .M) and (.M, .R) matters (evan if the instruction commutes because one arg is also output)
// imm args are sign extended to the operation size. 
// Magic numbers transcribed from https://www.felixcloutier.com/x86
// Always put the imm8 before imm32 since imm32 will match smaller numbers too for store
fn amd64_encoding_table() Ty([]AmdEncodingEntry, []u16) = @amd64_encoding_data (
    (.add, 
        (.M, .imm8, (0x83), 0),
        (.M, .imm32, (0x81), 0),
        (.R, .M, (0x03)),
        (.M, .R, (0x01)),
        (.F, .F, (0x0F, 0x58), (), (0xF2)), // addsd
    ),
    (.xcmp, 
        (.M, .imm8, (0x83), 7),
        (.M, .imm32, (0x81), 7),
        (.R, .M, (0x3B)),
        (.M, .R, (0x39)),
        // TODO: f32?
        (.F, .F, (0x0F, 0x2E), (), (0x66)), // ucomisd
    ),
    (.shr, 
        (.M, .imm8, (0xC1), 5),
        (.M, .R, (0xD3), 5),
    ),
    (.sar, 
        (.M, .imm8, (0xC1), 7),
        (.M, .R, (0xD3), 7),
    ),
    (.shl, 
        (.M, .imm8, (0xC1), 4),
        (.M, .R, (0xD3), 4),
    ),
    (.and, 
        (.M, .imm8, (0x83), 4),
        (.M, .imm32, (0x81), 4),
        (.R, .M, (0x23)),
        (.M, .R, (0x21)),
    ),
    (.or, 
        (.M, .imm8, (0x83), 1),
        (.M, .imm32, (0x81), 1),
        (.R, .M, (0x0B)),
        (.M, .R, (0x09)),
    ),
    (.storew,
        // note: the imm8 version would only store 1 byte! there's no `Move imm8 to r/m32.`
        (.imm32, .M, (0xC7), 0),
        (.R, .M, (0x89)),
    ),
    (.storel,
        (.imm32, .M, (0xC7), 0),
        (.R, .M, (0x89)),
        (.imm32, .imm32, (0xC7), 0),  // TODO: this is stupid. the only time this would ever happen is when writting a test that stores to a constant and catches a segsev
        // TODO: why does abi stuff insert these working with floats??
        (.F, .M, (0x0F, 0x11), (), (0xF2)), // movsd
    ),
    (.storeb, 
        (.R, .M, (0x88)),
    ),
    (.storeh, 
        (.R, .M, (0x89), (), (operand_16bit_prefix)),
    ),
    // TODO: whats the difference between movsd and movq? 
    (.stored,
        (.F, .M, (0x0F, 0x11), (), (0xF2)), // movsd
    ),
    (.stores,
        (.F, .M, (0x0F, 0x11), (), (0xF3)), // movss
    ),
    (.xidiv,
        (.M, .none, (0xF7), 7),
    ),
    (.xdiv,
        (.M, .none, (0xF7), 6),
    ),
    (.xor, 
        (.M, .imm8, (0x83), 6),
        (.M, .imm32, (0x81), 6),
        (.R, .M, (0x33)),
        (.M, .R, (0x33)),
    ),
    (.sub, 
        (.M, .imm8, (0x83), 5),
        (.M, .imm32, (0x81), 5),
        (.R, .M, (0x2B)),
        (.M, .R, (0x29)),
        (.F, .F, (0x0F, 0x5C), (), (0xF2)), // subsd
    ),
    (.xtest, 
        (.M, .imm32, (0xF7), 0),
        (.R, .M, (0x85)),
    ),
    (.load, 
        (.R, .M, (0x8B)), 
        (.F, .M, (0x0F, 0x10), (), (0xF2)), // movsd
    ),
    (.extsw, 
        (.R, .M, (0x63)), // MOVSXD
    ),
    (.extsb, 
        (.R, .M, (0x0F, 0xBE)), // MOVSX
    ),
    (.extub, 
        (.R, .M, (0x0F, 0xB6)), // MOVZX
    ),
    (.extuh, 
        (.R, .M, (0x0F, 0xB7)), // MOVZX
    ),
    (.extuw,
        (.R, .M, (0x8B)), // mov of 32 bits zero extends
    ),
    (.addr, 
        (.R, .M, (0x8D)), // lea
    ),
    (.neg, 
        (.M, .none, (0xF7), 3),
    ),
    (.mul, 
        (.R, .M, (0x0F, 0xAF)),
        (.F, .F, (0x0F, 0x59), (), (0xF2)), // mulsd
    ),
    (.div, 
        (.F, .F, (0x0F, 0x5E), (), (0xF2)), // divsd
    ),
    (.copy, 
        (.M, .imm32, (0xC7), 0), // "Move imm32 sign extended to 64-bits to r/m64."
        (.R, .M, (0x8B)),
        (.M, .R, (0x89)), // TODO: is this right? 
        (.F, .F, (0x0F, 0x10), (), (0xF2)), // movsd
        (.F, .M, (0x0F, 0x10), (), (0xF2)),
        (.M, .F, (0x0F, 0x11), (), (0xF2)),
    ),
    (.swap, // TODO: confirm RMem order if we ever generate it for those (or RSlot)
        (.R, .M, (0x87)), // xchg
    ),
    // :UglyFloatCast we need to tell the directions apart but the instructions always take the float arg on the same side so one has to swap after. 
    (.cast, 
        (.F, .M, (0x0F, 0x6E), (), (0x66)), // movd:movq f<-i
        (.M, .F, (0x0F, 0x7E), (), (0x66)), // movd:movq i<-f
    ),
    // TODO: i have low faith that i correctly transcribed the numbers
    (.exts,
        (.F, .F, (0x0F, 0x5A), (), (0xF3)), // cvtss2sd  
    ),
    (.dtosi,
        (.R, .F, (0x0F, 0x2C), (), (0xF2)), // cvttsd2si  
    ),
    (.swtof,
        (.F, .R, (0x0F, 0x2A), (), (0xF2)), // cvtsi2sd  
    ),
    (.sltof,
        (.F, .R, (0x0F, 0x2A), (), (0xF2)), // cvtsi2sd  
    ),
    (.stosi,
        (.R, .F, (0x0F, 0x2C), (), (0xF2)), // cvttss2si  
    ),
    (.truncd,
        (.F, .F, (0x0F, 0x5A), (), (0xF2)), // cvtsd2ss  
    ),
    // _wtos `cvtsi2ss	%rax, %xmm0` vs `cvtsi2ss	%eax, %xmm0`
    // ltos `shrq	%cl, %rsi` vs `rorq	%cl, %rbp`
    // fptrunc `cvtsi2sd	%eax, %xmm0` vs `cvtsi2sd	%rax, %xmm0` and `	cvttsd2si` vs `cvttss2si`
    // fpneg `	movq	%xmm0, %rax` vs `movd	%xmm0, %eax`
);
