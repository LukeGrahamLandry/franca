// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

/* For x86_64, do the following:
 *
 * - check that constants are used only in
 *   places allowed
 * - ensure immediates always fit in 32b
 * - expose machine register contraints
 *   on instructions like division.
 * - implement fast locals (the streak of
 *   constant allocX in the first basic block)
 * - recognize complex addressing modes
 *
 * Invariant: the use counts that are used
 *            in sel() must be sound.  This
 *            is not so trivial, maybe the
 *            dce should be moved out...
 */

/* instruction selection requires use counts */
fn amd64_isel(f: *Qbe.Fn) void = {
    f.assign_alloc_slots();
    // In this pass, if `t.slot != -1` then `TMP(t) == addr SLOT(t.slot)`
    
    @if(NEW_ADDR_FOLDING)
    for_blocks f { b | 
        for_insts_forward b { i |
            collapse_op_amd64(f, i, b.id);
        };
    }; 
    
    for_blocks f { b | 
        f.reset_scratch();
        for_jump_targets b { sb | 
            for_phi sb { p |
                a := index_in_phi(b, p);
                fixarg(p.arg.index(a), p.cls, Qbe.Ins.ptr_from_int(0), f);
            };
        };
        seljmp(b, f);
        for_insts_rev b { i |
            @match(i[].op()) {
                fn sel1() => selsel_amd64(f, b, i[]);
                fn cas1() => sel_cas_amd64(f, b, i[]);
                @default  => sel(i[][], f);
            };
        };
        f.copy_instructions_from_scratch(b);
    };

    when_debug(f, .InstSelect) { out | 
        write(out, "\n> After instruction selection:\n");
        printfn(f, out);
    };
}

fn noimm(r: Qbe.Ref, f: *Qbe.Fn, o: Qbe.O) bool = {
    if(rtype(r) != .RCon, => return(false));
    c := f.get_constant(r);
    @match(c.type()) {
        /* we only support the 'small'
         * code model of the ABI, this
         * means that we can always
         * address data with 32bits */
        fn CAddr() => false;
        // jitting on linux you can reasonably have <4GB constant address, but since call instruction 
        // is relative, you might not be able to reach all the way down there in 32 bits of immediate. 
        // i think 64-bit macOS never gives out the bottom 4GB of address space so it doesn't come up. 
        fn CBits() => c.bits() < MIN_i32 || c.bits() > MAX_i32 || o == .call;
    }
}

fn rslot(r: Qbe.Ref, f: *Qbe.Fn) i32 = {
    if(rtype(r) != .RTmp, => return(-1));
    f.get_temporary(r)[].slot
}

// unstable!
fn hascon(r: Qbe.Ref, pc: **Qbe.Con, f: *Qbe.Fn) bool = @match(rtype(r)) {
    fn RCon() => {
        pc[] = f.get_constant(r);
        true
    }
    fn RMem() => {
        pc[] = f.get_memory(r)[].offset&;
        true
    }
    @default => false;
};

fn fixarg(r: *Qbe.Ref, k: Qbe.Cls, i: *Qbe.Ins, f: *Qbe.Fn) void = {
    T := f.globals.target;
    
    c := Qbe.Con.ptr_from_int(0);
    r1, r0 := (r[], r[]);
    s := rslot(r0, f);
    o := if(i.is_null(), => Qbe.O.copy, => i.op());
    @if_else {
        @if(!is_int(k) && rtype(r0) == .RCon) => {
            // Floats cannot be used as immediates. 
            // TODO: Qbe loads floats from memory instead. 
            //       clang does too so presumably its the correct choice. 
            //       but this is less of a pain for now. 
            
            r1 = f.newtmp("isel", k);
            r2 := f.newtmp("isel", .Kl);
            f.emit(.cast, k, r1, r2, QbeNull);
            f.emit(.copy, .Kl, r2, r0, QbeNull);
        };
        // 'copy' is the only instruction that's allowed a 64 bit immediate. 
        @if(o != .copy && k == .Kl && noimm(r0, f, o)) => {
            /* load constants that do not fit in
            * a 32bit signed integer into a
            * long temporary */
            r1 = f.newtmp("isel", .Kl);
            f.emit(.copy, .Kl, r1, r0, QbeNull);
        };
        @if(s != -1) => {
            /* load fast locals' addresses into
            * temporaries right before the
            * instruction */
            r1 = f.newtmp("isel", .Kl);
            f.emit(.addr, .Kl, r1, SLOT(s), QbeNull);
        };
        @if(!(is_store(o) && r.identical(i.arg&.index(1)))
        && !is_load(o) && o != .cas0 && o != .call && f.is_symbol(r0)) => {
            r1 = f.newtmp("isel", .Kl);
            f.emit(.addr, .Kl, r1, r0, QbeNull);
        };
        // no immediate allowed for selCC
        @if(o.is_sel_flag() && rtype(r[]) == .RCon) => {
            r1 = f.newtmp("isel", i.cls());
            f.emit(.copy, i.cls(), r1, r[], QbeNull);
        };
        @if(o == .cas1 && rtype(r[]) == .RCon) => {
            r1 = f.newtmp("isel", i.cls());
            f.emit(.copy, i.cls(), r1, r[], QbeNull);
        };
        @else => ();
    };
    r[] = r1;
}

NEW_ADDR_FOLDING :: false;

fn collapse_op_amd64(f: *Qbe.Fn, i: *Qbe.Ins, bid: i32) void = {
    if(!i.cls().is_int(), => return());
    if(!@is(i.op(), .add, .mul), => return());
    r := i.to;
    mem: Qbe.Addr = (base = r, index = QbeNull, scale = 1, offset = (type = .CBits, bits = (i = 0)));
    f.merge_amd_addr(mem&);
    when_debug(f, .InstSelect) { out |
        write(out, "\n");
    };
    @debug_assert(mem.offset.type != .CAddr || (mem.index == QbeNull && mem.base == QbeNull), "generated illegal addressing mode");
    if(mem.base == r && mem.index == QbeNull && mem.offset.bits.i == 0, => return());

    chuse(r, 1, f);  // because merge_amd_addr -1
    m := f.new_mem(mem);
    chuse(m, 1, f);
    i[] = make_ins(.addr, i.cls(), r, m, QbeNull);
}

// TODO: this would be better if you did it in the other order and converted everything to lea as a way of memoizing for future uses of the same address. 
//       then it would be O(amount of math + amount of dereferences) instead of O(amount of math * amount of dereferences)
fn merge_amd_addr(f: *Qbe.Fn, mem: *Qbe.Addr) void = {
    n := 0;
    loop {
        continue :: local_return;
        n += 1;
        @assert(n < 1000, "probably infinite looping in merge_amd_addr of %", f.name());
        
        when_debug(f, .InstSelect) { out |
            printmem(mem, f, out);
        };
        if rtype(mem.base) == .RTmp && !isreg(mem.base) {
            goto_index :: local_return;
            t := f.get_temporary(mem.base);
            if t.slot != -1 {
                mem.base = SLOT(t.slot);
                continue();
            };
            if t.def.is_null() {
                goto_index();
            };
            a0 := t.def.arg&[0];
            a1 := t.def.arg&[1];
            if a0 == mem.base || a1 == mem.base || t.def.to != mem.base {
                goto_index();
            };
            @match(t.def.op()) {
                fn add() => {
                    swap := false;
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                        swap = true;
                    };
                    if f.is_symbol(a0) {
                        if mem.index == QbeNull && mem.offset&.type() != .CAddr {
                            if f.get_int(a1) { off |
                                @debug_assert_eq(mem.scale, 1);
                                new_offset := mem.offset;
                                x := addcon(new_offset&, f.get_constant(a0), 1);
                                y := addcon(new_offset&, f.get_constant(a1), 1);
                                @debug_assert(x && y, "failed to merge constants in $%", f.name());
                                if fits_in_i32(new_offset&.bits()) {
                                    mem.offset = new_offset;
                                    mem.base = QbeNull;
                                    return();
                                };
                            };
                            // will get fixed-up later.
                            // it could be that both are CAddr but lea-lea-deref is better than lea-lea-add-deref
                            // tho we should really make sure to reuse the lea of con
                            mem.index = a1;
                            mem.base = a0;
                            continue();
                        };
                        
                        f.fix_slot(mem.index&);
                        return();
                    };
                    if f.get_int(a0) { off |
                        if fits_in_i32(off + mem.offset&.bits()) {
                            chuse(a1, 1, f);
                            chuse(mem.base, -1, f);
                            mem.offset&.set_bits(mem.offset&.bits() + off);
                            mem.base = a1;
                            continue();
                        };
                    };
                    if mem.index == QbeNull && !f.is_symbol(a0) {
                        chuse(a0, 1, f);
                        chuse(a1, 1, f);
                        chuse(mem.base, -1, f);
                        @debug_assert(mem.scale == 1);
                        mem.index = a1;
                        mem.base = a0;
                        continue();
                    };
                }
                fn mul() => {
                    if mem.scale == 1 {
                        t := mem.base; mem.base = mem.index; mem.index = t;
                        goto_index();
                    };
                }
                fn addr() => {
                    m := f.get_memory(a0);
                    if mem.index == QbeNull {
                        if mem.offset&.bits() == 0 && mem.offset&.type() != .CAddr {
                            chuse(mem.base, -1, f);
                            mem[] = m[];
                            continue();
                        };
                    };
                    if m.index == QbeNull && mem.offset&.type() != .CAddr {
                        chuse(mem.base, -1, f);
                        mem.offset.sym = m.offset.sym;
                        mem.offset&.set_bits(mem.offset&.bits() + m.offset&.bits());
                        mem.base = m.base;
                        continue();
                    };
                }
                @default => ();
            };
        };
        
        if mem.index == QbeNull && rtype(mem.base) == .RCon && addcon(mem.offset&, f.get_constant(mem.base), 1) {
            mem.base = QbeNull;
            return();
        };
        s: i64 = mem.scale.intcast();
        if rtype(mem.index) == .RTmp && !isreg(mem.index) {
            goto_done :: local_return;
            index_t := f.get_temporary(mem.index);
            if index_t.slot != -1 {
                if mem.scale != 1 {
                    @eprintln("scaling a stack addresses: %", index_t.name());
                    goto_done();  // panic?
                };
                if rtype(mem.base) == .RTmp {
                    base_t := f.get_temporary(mem.base);
                    if base_t.slot != -1 {
                        @eprintln("adding two stack addresses: % %", index_t.name(), base_t.name());
                        goto_done();  // panic?
                    };
                };
                t := mem.base; mem.base = mem.index; mem.index = t;
                continue();
            };
            if index_t.def.is_null() {
                goto_done();
            };
            a0 := index_t.def.arg&[0];
            a1 := index_t.def.arg&[1];
            if a0 == mem.index || a1 == mem.index || index_t.def.to != mem.index {
                goto_done();
            };
            @match(index_t.def.op()) {
                fn add() => {
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                    };
                    if f.get_int(a0) { off |
                        value := mem.offset&.bits() + off * s;
                        if fits_in_i32(value) {
                            chuse(mem.index, -1, f);
                            chuse(a1, 1, f);
                            mem.offset&.set_bits(value);
                            mem.index = a1;
                            continue();
                        };
                    };
                }
                fn mul() => {
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                        if rtype(a0) == .RTmp {
                            goto_done();
                        };
                    };
                    if f.get_int(a0) { arg |
                        if s == 1 && (arg == 1 || arg == 2 || arg == 4 || arg == 8) {
                            chuse(a1, 1, f);
                            chuse(mem.index, -1, f);
                            mem.scale = arg.intcast();
                            mem.index = a1;
                            continue();
                        };
                    };
                }
                @default => ();
            };
        };
        
        f.fix_slot(mem.base&);
        if mem.base == QbeNull && mem.scale == 1 {
            mem.base = mem.index;
            mem.index = QbeNull;
        };
        return();
    }
}

fn fix_slot(f: *Qbe.Fn, r: *Qbe.Ref) void = {
    s := rslot(r[], f);
    if s != -1 {
        r[] = SLOT(s);
    };
}

fn seladdr(r: *Qbe.Ref, f: *Qbe.Fn) void = {
    if(!NEW_ADDR_FOLDING && rtype(r[]) == .RMem, => return());  // trace_* get here
    if rtype(r[]) == .RCon {  
        done := true;
        if f.get_int(r[]) { addr | 
            valid_offset := fits_in_i32(addr);
            done = valid_offset;
        };
        if done {
            return();
        };
    };
    mem: Qbe.Addr = (base = r[], index = QbeNull, scale = 1, offset = con(Qbe.no_symbol, 0));
    if NEW_ADDR_FOLDING {
        if mem.base.rtype() == .RTmp {
            def := f.get_temporary(mem.base)[].def;
            if !def.is_null() && def.op() == .addr && def.arg&[0].rtype() == .RMem {
                chuse(mem.base, -1, f);
                mem = f.get_memory(def.arg&[0])[];
                chuse(mem.base, 1, f);
                chuse(mem.index, 1, f);
            };
        };
        f.fix_amd_mem(mem&);
    } else {
        f.merge_amd_addr(mem&);
        f.fix_amd_mem(mem&);
        when_debug(f, .InstSelect) { out |
            write(out, "\n");
        };
    };
    r[] = f.new_mem(mem);
}

fn fix_amd_mem(f: *Qbe.Fn, mem: *Qbe.Addr) void = {
    @debug_assert(mem.offset&.type() != .CAddr || (mem.index == QbeNull && mem.base == QbeNull), "generated illegal addressing mode");
    
    for (@slice(mem.index&, mem.base&)) { a |
        if rtype(a[]) == .RCon {
            r0 := f.newtmp("isel", .Kl);
            f.emit(@if(f.is_symbol(a[]), .addr, .copy), .Kl, r0, a[], QbeNull);
            a[] = r0;
        };
        
        f.fix_slot(a); 
    };
    
    base_val := mem.base.rsval();
    if rtype(mem.base) == .RSlot && base_val >= 0 {
        new := base_val.intcast() + mem.offset&.bits();
        if new >= 0 {
            mem.base = SLOT(new);
            mem.offset&.set_bits(0);
        };
    };
    
    valid_offset := fits_in_i32(mem.offset&.bits());
    if !valid_offset {
        // This makes sense when jitting, you're allowed to just stick hard coded references in your code. 
        @assert(mem.index == QbeNull && mem.base == QbeNull && mem.offset&.type() == .CBits, "TODO: folded memory access to >4GB constant address");
        r1 := f.newtmp("isel", .Kl);
        f.emit(.copy, .Kl, r1, f.getcon(mem.offset&.bits()), QbeNull);
        mem.offset&.set_bits(0);
        mem.base = r1;
    };
}

fn new_mem(f: *Qbe.Fn, mem: Qbe.Addr) Qbe.Ref #inline = {
    push(f.mem&, f.nmem&, mem);
    MEM(f.nmem.zext() - 1)
}

fn fits_in_i32(i: i64) bool = 
    i >= MIN_i32 && i <= MAX_i32

fn cmpswap(arg: *Array(Qbe.Ref, 2), cmp: i32) bool = {
    cmp := @as(Qbe.Cmp) cmp;
    @match(cmp) {
        fn Cflt() => true;
        fn Cfle() => true;
        fn Cfgt() => false;
        fn Cfge() => false;
        @default => rtype(arg[0]) == .RCon;
    }
}

fn selcmp(arg: []Qbe.Ref, k: Qbe.Cls, need_swap: bool, f: *Qbe.Fn) void = {
    if need_swap {
        arg.swap(0, 1);
    };
    icmp := f.emit(.xcmp, k, QbeNull, arg[1], arg[0]);
    
    if rtype(arg[0]) == .RCon {
        icmp.arg&[1] = f.newtmp("isel", k);
        new := f.emit(.copy, k, icmp.arg&[1], arg[0], QbeNull);
        fixarg(new.arg&.index(0), k, new, f);
    };
    fixarg(icmp.arg&.index(0), k, icmp, f);
    fixarg(icmp.arg&.index(1), k, icmp, f);
}

// Adapted from a qbe patch: https://lists.sr.ht/~mpu/qbe/patches/55968
// TODO: this is so similar to seljmp. but im afraid trying to factor them together would just make it more confusing. 
fn selsel_amd64(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) *Qbe.Ins = {
    isel0 := i.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    r  := isel0.arg&[0];
    t  := f.get_temporary(r); 
    fi := flagi(b.ins.first, isel0);
    cr0, cr1 := (QbeNull, QbeNull);
    gencmp, gencpy, need_swap := (false, false, false);
    k := t.cls;
    c := Qbe.Cmp.Cine.raw();
    @if_else {
        @if(fi.is_null() || fi.to != r) => {
            gencmp = true;
            cr0 = r;
            cr1 = QbeConZero;
        };
        @if(iscmp(fi.op(), k&, c&) && c != Qbe.Cmp.Cfeq.raw() && c != Qbe.Cmp.Cfne.raw()) => {  /* see sel() */
            need_swap = cmpswap(fi.arg&, c);
            if need_swap {
                c = cmp_swap(c);
            };
            if t.nuse == 1 {
                gencmp = true;
                cr0 = fi.arg&[0];
                cr1 = fi.arg&[1];
                fi.set_nop();
            };
        };
        @if(fi.op() == .and && t.nuse == 1
            && (rtype(fi.arg&[0]) == .RTmp ||
                rtype(fi.arg&[1]) == .RTmp)) => {
            fi.set_op(.xtest);
            fi.to = QbeNull;
            if rtype(fi.arg&[1]) == .RCon {
                r = fi.arg&[1]; 
                fi.arg&.items().swap(0, 1);
            };
        };
        /* since flags are not tracked in liveness,
         * the result of the flag-setting instruction
         * has to be marked as live */
        @else => {
            gencpy = t.nuse == 1;
        };
    };
    
    isel1 := i;
    @debug_assert_eq(isel1.op(), .sel1);
    isel1.set_op(@as(Qbe.O) @as(i32) Qbe.O.selieq.raw() + c);
    sel(isel1[], f);
    
    @debug_assert(nand(gencpy, gencmp));
    @if(gencmp) selcmp(@slice(cr0, cr1), k, need_swap, f);
    @if(gencpy) {f.emit(.copy, .Kw, QbeNull, r, QbeNull);};
    isel0.set_nop();
    isel0
}

fn sel(i: Qbe.Ins, f: *Qbe.Fn) void = {
    if(try_kill_inst(f, i&), => return());  // TODO
    
    i0 := f.globals.curi[]; // :LookAtLastInst
    sel_inner(i&, f);
    i1 := f.globals.curi[];
    
    while => i0.in_memory_after(i1) {
        @debug_assert(rslot(i0.arg&[0], f) == -1, "expected no slot 0");
        @debug_assert(rslot(i0.arg&[1], f) == -1, "expected no slot 1");
        i0 = i0.offset(-1);
    };
}

// cas0 %p
// %res =k cas1 %old_in, %new
// 
// RAX as %old_in and %res is implied. 
// The two explicit arguments of cmpxchg are %p and %new. 
fn sel_cas_amd64(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    rax :: TMP(Amd64Reg.RAX);
    cas0, cas1 := find_cas(i, b);
    // TODO: should really make the frontend ensure this but i don't want to deal with it right now
    //@debug_assert_eq(ptr_diff(cas0, cas1), 1, "non-adjacent cas");
    k := cas1.cls();
    // can't try_kill_inst since a cas is a store.
    
    // TODO: it's confusing that here i don't put a fake copy to RAX anywhere. 
    //       i guess rega doesn't touch reg copies at all, not just arounds calls. 
    f.emit(.copy, k, cas1.to, rax, QbeNull);
    cas1.to = QbeNull;
    cas1 := f.emit(cas1[]);
    i2   := f.emit(.copy, k, rax, cas1.arg&[0], QbeNull);
    fixarg(cas1.arg&.index(1), k, cas1, f);
    fixarg(i2.arg&.index(0), k, i2, f);
    
    seladdr(cas0.arg&.index(0), f);
    f.fixargs(cas0);
    cas1.arg&[0] = cas0.arg&[0];
    cas0.set_nop(); 
}

fn sel_inner(i: *Qbe.Ins, f: *Qbe.Fn) void = {
    k := i.cls();
    @match(i.op()) {
        fn cas0() => panic("loose cas0");
        fn uwtof() => {
            r0 := f.newtmp("utof", .Kl);
            f.emit(.sltof, k, i.to, r0, QbeNull);
            ins := f.emit(.extuw, .Kl, r0, i.arg&[0], QbeNull);
            fixarg(ins.arg&.index(0), k, ins, f);
        }
        fn ultof() => {
            emitter :: fn(f, o, k, r, a0, a1) => 
                sel(make_ins(o, k, r, a0, a1), f);
            
            kc, sh := @if(k == .Ks, (Qbe.Cls.Kw, 23), (Qbe.Cls.Kl, 52));
            @emit_instructions((f = f, emit = emitter), (i.arg&[0], i.to, kc, f.getcon(sh), k), """
            @start
                %mask    =l and %0, 1
                %isbig   =l shr %0, 63
                %divided =l shr %0, %isbig
                %or      =l or %mask, %divided
                %float   =4 sltof %or
                %cast    =2 cast %float
                %addend  =2 shl %isbig, %3
                %sum     =2 add %cast, %addend
                %1       =4 cast %sum
            """);
        }
        fn stoui()  => f.sel_ftoi(i, .Ks, .stosi, 0xdf000000);
        fn dtoui()  => f.sel_ftoi(i, .Kd, .dtosi, 0xc3e0000000000000);
        fn nop()    => ();
        fn asm()    => { f.emit(i[]); } // no fixargs, you're allowed an RCon u32 with the high bit set
        fn addr() => {
            @debug_assert(NEW_ADDR_FOLDING, "shouldn't have addr ops yet.");
            i := f.emit(i[]);
            m := f.get_memory(i.arg&[0]);
            f.fix_amd_mem(m);
            if m.base == QbeNull && m.index == QbeNull {
                r := f.newcon(m.offset&);
                i.arg&[0] = r;
                i.set_op(.copy);
            };
        }
        fn trace_prev() => {
            sel(make_ins(.load, i.cls(), i.to, f.new_mem(i.arg&[0], QbeNull, 0, 1), QbeNull), f);
        }
        fn trace_return() => {
            sel(make_ins(.load, i.cls(), i.to, f.new_mem(i.arg&[0], QbeNull, 8, 1), QbeNull), f);
        }
        @default => {
            if is_alloc(i.op()) {
                salloc(i.to, i.arg&[0], f);
                return();
            };
            if is_ext(i.op()) {
                if i.op() == .extuw && rtype(i.arg&[0]) == .RCon {
                    i.set_op(.copy);
                };
                f.sel_emit(i);
                return();
            };
            if is_load(i.op()) {
                new_ins := f.emit(i[]);
                r := new_ins.arg&.index(0);
                seladdr(r, f);
                @debug_assert(rtype(r[]) != .RTmp, "seladdr failed");
                f.fixargs(new_ins);
                return();
            };
            if is_store(i.op()) {
                if rtype(i.arg&[0]) == .RCon {
                    // doesn't matter that it's a float. store the bytes with an instruction that can have an immediate. 
                    if(i.op() == .stored, => i.set_op(.storel));
                    if(i.op() == .stores, => i.set_op(.storew));
                };
                i1 := f.emit(i[]);
                seladdr(i1.arg&.index(1), f);
                f.fixargs(i1);
                return();
            };
            
            kc := Qbe.Cls.Kw;
            x: i32 = 0;
            if iscmp(i.op(), kc&, x&) {
                xx := @as(Qbe.Cmp) x;
                @match(xx) {
                    /* zf is set when operands are
                     * unordered, so we may have to
                     * check pf */
                    fn Cfeq() => emit_check_order_flag(f, i, .and, .flagfo);
                    fn Cfne() => emit_check_order_flag(f, i, .or, .flagfuo);
                    @default => ();
                };
                swap := cmpswap(i.arg&, x);
                if swap {
                    x = cmp_swap(x);
                };
                flag_op := @as(Qbe.O) @as(i32) Qbe.O.flagieq.raw() + x;
                f.emit(flag_op, k, i.to, QbeNull, QbeNull);
                selcmp(i.arg&.items(), kc, swap, f);
                return();
            };
            if (@is(i.op(), .div, .rem, .udiv, .urem)) {
                f.sel_div(i);
                return();
            };
            if (@is(i.op(), .sar, .shr, .shl, .rotr, .rotl)) {
                f.sel_shift(i);
                return();
            };
            f.sel_emit(i);
        };
    }
}

fn emit_check_order_flag(f: *Qbe.Fn, i: *Qbe.Ins, merge: Qbe.O, flag: Qbe.O) void = {
    r0 := f.newtmp("isel", .Kw);
    r1 := f.newtmp("isel", .Kw);
    f.emit(merge, .Kw, i.to, r0, r1);
    f.emit(flag, i.cls(), r1, QbeNull, QbeNull);
    i.to = r0;
}

fn sel_emit(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    i1 := f.emit(i[]);
    f.fixargs(i1); 
}

fn fixargs(f: *Qbe.Fn, i1: *Qbe.Ins) void = {
    k0, k1 := (argcls(i1, 0), argcls(i1, 1));
    fixarg(i1.arg&.index(0), k0, i1, f);
    fixarg(i1.arg&.index(1), k1, i1, f);
}

fn sel_div(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    k := i.cls();
    if(!is_int(k), => return(f.sel_emit(i)));
    rax, rdx := (TMP(Amd64Reg.RAX), TMP(Amd64Reg.RDX));
    r0, r1 := @if(@is(i.op(), .div, .udiv), (rax, rdx), (rdx, rax));
    f.emit(.copy, k, i.to, r0, QbeNull);
    f.emit(.copy, k, QbeNull, r1, QbeNull);

    /* immediates not allowed for
    * divisions in x86
    */
    r0 := if(rtype(i.arg&[1]) == .RCon, => f.newtmp("isel", k), => i.arg&[1]);
    
    t := f.get_temporary(r0);
    if t.slot != -1 { // TODO: why
        @panic("unlikely argument % in %", t.name(), i.op());
    };
    if (@is(i.op(), .div, .rem)) {
        f.emit(.xidiv, k, QbeNull, r0, QbeNull);
        f.emit(.sign, k, rdx, rax, QbeNull);
    } else { // unsigned
        f.emit(.xdiv, k, QbeNull, r0, QbeNull);
        f.emit(.copy, k, rdx, QbeConZero, QbeNull);
    };
    ins := f.emit(.copy, k, rax, i.arg&[0], QbeNull);
    fixarg(ins.arg&.index(0), k, ins, f);
    if rtype(i.arg&[1]) == .RCon {
        f.emit(.copy, k, r0, i.arg&[1], QbeNull);
    };
}

// Variable width shift/rotate always take the width in RCX. 
fn sel_shift(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    r0 := i.arg&[1];
    if(rtype(r0) == .RCon, => return(f.sel_emit(i))); // TODO: unless you try to shift by an address by mistake
    t := f.get_temporary(r0);
    if t.slot != -1 { // TODO: why
        @panic("unlikely argument % in %", t.name(), i.op());
    };
    rcx := TMP(Amd64Reg.RCX);
    i.arg&[1] = rcx;
    f.emit(.copy, .Kw, QbeNull, rcx, QbeNull);
    i1 := f.emit(i[]);
    f.emit(.copy, .Kw, rcx, r0, QbeNull);
    fixarg(i1.arg&.index(0), argcls(i, 0), i1, f);
}

fn flagi(i0: *Qbe.Ins, i: *Qbe.Ins) *Qbe.Ins = {
    null := Qbe.Ins.ptr_from_int(0);
    while => i.in_memory_after(i0) {
        i = i.offset(-1);
        zflag, lflag := amd64_flag_table(i.op());
        if(zflag, => return(i));
        if(!lflag, => return(null));
    };
    null
}

// returns (SetsZeroFlag, LeavesFlags)
fn amd64_flag_table(o: Qbe.O) Ty(bool, bool) = {
    b :: fn($o1: Qbe.O, $o2: Qbe.O) => o.between(o1, o2);
    if b(.ceqw, .cuod) || b(.add, .neg) || b(.and, .shl) || b(.blit0, .blit1) || b(.xcmp, .xtest) || (@is(o, .ctz, .clz)) {
        return(true, false)
    };
    if b(.storeb, .cast) || o.between(.flagieq, .flagfuo) || o == .copy || o == .dbgloc || b(.nop, .addr) || o == .sel0 || o == .byteswap {
        return(false, true)
    };
    if b(.acmp, .call) || b(.div, .mul) || b(.swap, .xdiv) || (@is(o, .cas1, .sel1, .rotr, .rotl, .ones)) {
        return(false, false)
    };
    
    @panic("TODO: unhandled op for amd64_flag_table() %", o)
}

fn sel_ftoi(f: *Qbe.Fn, i: *Qbe.Ins, kc: Qbe.Cls, o: Qbe.O, t4: u64) void = {
    tmp := @uninitialized Array(Qbe.Ref, 7);
    tmp := tmp&;
    tmp[4] = f.getcon(t4.bitcast());
    i.set_op(o);
    k := i.cls();
    if k == .Kw {
        r0 := f.newtmp("ftou", .Kl);
        f.emit(.copy, .Kw, i.to, r0, QbeNull);
        i.set_cls(.Kl);
        i.to = r0;
        f.sel_emit(i);
        return();
    };
    // TODO: want to use emit_instructions but need to allow passing an op as a runtime arg. 
    /* %try0 =l {s,d}tosi %fp
    * %mask =l sar %try0, 63
    *
    *    mask is all ones if the first
    *    try was oob, all zeroes o.w.
    *
    * %fps ={s,d} sub %fp, (1<<63)
    * %try1 =l {s,d}tosi %fps
    *
    * %tmp =l and %mask, %try1
    * %res =l or %tmp, %try0
    */
    r0 := f.newtmp("ftou", kc);
    range(0, 4) { j |
        tmp[j] = f.newtmp("ftou", .Kl);
    }; 
    f.emit(.or, .Kl, i.to, tmp[0], tmp[3]);
    f.emit(.and, .Kl, tmp[3], tmp[2], tmp[1]);
    f.emit(i.op(), .Kl, tmp[2], r0, QbeNull);
    i1 := f.emit(.add, kc, r0, tmp[4], i.arg&[0]);
    fixarg(i1.arg&.index(0), kc, i1, f);
    fixarg(i1.arg&.index(1), kc, i1, f);
    f.emit(.sar, .Kl, tmp[1], tmp[0], f.getcon(63));
    ins := f.emit(i.op(), .Kl, tmp[0], i.arg&[0], QbeNull);
    fixarg(ins.arg&.index(0), .Kl, ins, f);
}

fn seljmp(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    if(@is(b.jmp.type, .ret0, .jmp, .hlt), => return()); // these already have a trivial single instruction implementation
    @debug_assert_ne(b.jmp.type, .switch);
    
    @debug_assert(b.jmp.type == .jnz);
    r := b.jmp.arg;
    @debug_assert(rtype(r) == .RTmp, "we shouldn't be jumping on a constant");
    t := f.get_temporary(r);
    b.jmp.arg = QbeNull;
    if b.s1.identical(b.s2) {
        chuse(r, -1, f);
        b.jmp.type = .jmp;
        b.s2 = Qbe.Blk.ptr_from_int(0);
        return();
    };
    fi := flagi(b.ins.first, b.ins.index_unchecked(b.nins.zext()));
    k := Qbe.Cls.Kw; // :jnz_is_Kw
    c: i32 = 0;
    @if_else {
        @if(fi.is_null() || fi.to != r) => {
            // The last flag setting op wasn't the comparison that created our argument. 
            // So we have to actually emit the compare to zero. 
            selcmp(@slice(r, QbeConZero), .Kw, false, f); // :jnz_is_Kw
            b.jmp.type = .jfine;
        };
        @if(iscmp(fi.op(), k&, c&)
            && c != Qbe.Cmp.Cfeq.raw() /* see sel() */
            && c != Qbe.Cmp.Cfne.raw()) => {
            swap := cmpswap(fi.arg&, c);
            if swap {
                c = cmp_swap(c);
            };
            if t.nuse == 1 {
                selcmp(fi.arg&.items(), k, swap, f);
                fi.set_nop();
            };
            b.jmp.type = @as(Qbe.J) @as(i32) Qbe.J.jfieq.raw() + c;
        };
        @if(fi.op() == .and && t.nuse == 1
            && (rtype(fi.arg&[0]) == .RTmp ||
                rtype(fi.arg&[1]) == .RTmp)) => {
            fi.set_op(.xtest);
            fi.to = QbeNull;
            b.jmp.type = .jfine;
            if rtype(fi.arg&[1]) == .RCon {
                r := fi.arg&[1];
                fi.arg&[1] = fi.arg&[0];
                fi.arg&[0] = r;
            };
        }; 
        @else => {
            /* since flags are not tracked in liveness,
             * the result of the flag-setting instruction
             * has to be marked as live */
            if t.nuse == 1 {
                f.emit(.copy, .Kw, QbeNull, r, QbeNull);
            };
            b.jmp.type = .jfine;
        };
    }
}

// c0 += c1 * m;
// returns false if this cannot be done. 
fn addcon(c0: *Qbe.Con, c1: *Qbe.Con, m: i64) bool = {
    if(m != 1 && c1.type() == .CAddr, => return(false));  // can't scale a symbol
    if c1.type() == .CAddr {
        if(c0.type() == .CAddr, => return(false));  // can't have two symbols
        c0.sym = c1.sym;
    };
    c0.set_bits(c1.bits() * m);
    true
}

#use("@/backend/amd64/target.fr");
