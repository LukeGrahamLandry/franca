// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

/* For x86_64, do the following:
 *
 * - check that constants are used only in
 *   places allowed
 * - ensure immediates always fit in 32b
 * - expose machine register contraints
 *   on instructions like division.
 * - implement fast locals (the streak of
 *   constant allocX in the first basic block)
 * - recognize complex addressing modes
 *
 * Invariant: the use counts that are used
 *            in sel() must be sound.  This
 *            is not so trivial, maybe the
 *            dce should be moved out...
 */

/* instruction selection
 * requires use counts (as given by parsing)
 */
fn amd64_isel(f: *Qbe.Fn) void = {
    f.assign_alloc_slots();
    // In this pass, if `t.slot != -1` then `TMP(t) == addr SLOT(t.slot)`

    /* process basic blocks */
    for_blocks f { b | 
        f.reset_scratch();
        for_jump_targets b { sb | 
            for_phi sb { p |
                a := index_in_phi(b, p);
                fixarg(p.arg.index(a), p.cls, Qbe.Ins.ptr_from_int(0), f);
            };
        };
        seljmp(b, f);
        for_insts_rev b { i |
            sel(i[][], f);
        };
        f.copy_instructions_from_scratch(b);
    };

    when_debug(f, .InstSelect) { out | 
        write(out, "\n> After instruction selection:\n");
        printfn(f, out);
    };
}

fn noimm(r: Qbe.Ref, f: *Qbe.Fn) bool = {
    if(rtype(r) != .RCon, => return(false));
    c := f.get_constant(r);
    @match(c.type) {
        /* we only support the 'small'
         * code model of the ABI, this
         * means that we can always
         * address data with 32bits
         */
        fn CAddr() => false;
        fn CBits() => c.bits.i < MIN_i32 || c.bits.i > MAX_i32;
        @default => panic("invalid constant");
    }
}

fn rslot(r: Qbe.Ref, f: *Qbe.Fn) i32 = {
    if(rtype(r) != .RTmp, => return(-1));
    f.get_temporary(r)[].slot
}

// unstable!
fn hascon(r: Qbe.Ref, pc: **Qbe.Con, f: *Qbe.Fn) bool = @match(rtype(r)) {
    fn RCon() => {
        pc[] = f.get_constant(r);
        true
    }
    fn RMem() => {
        pc[] = f.get_memory(r)[].offset&;
        true
    }
    @default => false;
};

fn fixarg(r: *Qbe.Ref, k: Qbe.Cls, i: *Qbe.Ins, f: *Qbe.Fn) void = {
    T := f.globals.target;
    
    c := Qbe.Con.ptr_from_int(0);
    r1, r0 := (r[], r[]);
    s := rslot(r0, f);
    o := if(i.is_null(), => Qbe.O.copy, => i.op());
    @if_else {
        @if(!is_int(k) && rtype(r0) == .RCon) => {
            // Floats cannot be used as immediates. 
            // TODO: Qbe loads floats from memory instead. 
            //       clang does too so presumably its the correct choice. 
            //       but this is less of a pain for now. 
            
            r1 = f.newtmp("isel", k);
            r2 := f.newtmp("isel", .Kl);
            f.emit(.cast, k, r1, r2, QbeNull);
            f.emit(.copy, .Kl, r2, r0, QbeNull);
        };
        @if(o != .copy && k == .Kl && noimm(r0, f)) => {
            /* load constants that do not fit in
            * a 32bit signed integer into a
            * long temporary
            */
            r1 = f.newtmp("isel", .Kl);
            f.emit(.copy, .Kl, r1, r0, QbeNull);
        };
        @if(s != -1) => {
            /* load fast locals' addresses into
            * temporaries right before the
            * instruction
            */
            r1 = f.newtmp("isel", .Kl);
            f.emit(.addr, .Kl, r1, SLOT(s), QbeNull);
        };
        @if(!(is_store(o) && r.identical(i.arg&.index(1)))
        && !is_load(o) && o != .call && f.is_symbol(r0)) => {
            r1 = f.newtmp("isel", .Kl);
            f.emit(.addr, .Kl, r1, r0, QbeNull);
        };
        @else => ();
    };
    r[] = r1;
}

// TODO: this would be better if you did it in the other order and converted everything to lea as a way of memoizing for future uses of the same address. 
//       then it would be O(amount of math + amount of dereferences) instead of O(amount of math * amount of dereferences)
fn merge_amd_addr(f: *Qbe.Fn, mem: *Qbe.Addr) void = {
    n := 0;
    loop {
        continue :: local_return;
        n += 1;
        @assert(n < 1000, "probably infinite looping in merge_amd_addr of %", f.name());
        
        when_debug(f, .InstSelect) { out |
            printmem(mem, f, out);
        };
        if rtype(mem.base) == .RTmp && !isreg(mem.base) {
            goto_index :: local_return;
            t := f.get_temporary(mem.base);
            if t.slot != -1 {
                mem.base = SLOT(t.slot);
                continue();
            };
            if t.def.is_null() {
                goto_index();
            };
            a0 := t.def.arg&[0];
            a1 := t.def.arg&[1];
            if a0 == mem.base || a1 == mem.base || t.def.to != mem.base {
                goto_index();
            };
            @match(t.def.op()) {
                fn add() => {
                    swap := false;
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                        swap = true;
                    };
                    if f.is_symbol(a0) {
                        if mem.index == QbeNull && mem.offset.type != .CAddr {
                            if f.get_int(a1) { off |
                                @debug_assert_eq(mem.scale, 1);
                                new_offset := mem.offset;
                                x := addcon(new_offset&, f.get_constant(a0), 1);
                                y := addcon(new_offset&, f.get_constant(a1), 1);
                                @debug_assert(x && y, "failed to merge constants");
                                if fits_in_i32(new_offset.bits.i) {
                                    mem.offset = new_offset;
                                    mem.base = QbeNull;
                                    return();
                                };
                            };
                            // will get fixed-up later.
                            // it could be that both are CAddr but lea-lea-deref is better than lea-lea-add-deref
                            // tho we should really make sure to reuse the lea of con
                            mem.index = a1;
                            mem.base = a0;
                            continue();
                        };
                        
                        s := rslot(mem.index, f);
                        if s != -1 {
                            if mem.scale == 1 {
                                mem.index = SLOT(s);  // address of a stack slot
                            } else {
                                @eprintln("scaling stack slot");
                            };
                        };
                        return();
                    };
                    if f.get_int(a0) { off |
                        if fits_in_i32(off + mem.offset.bits.i) {
                            chuse(a1, 1, f);
                            chuse(mem.base, -1, f);
                            mem.offset.bits.i += off;
                            mem.base = a1;
                            continue();
                        };
                    };
                    if mem.index == QbeNull && !f.is_symbol(a0) {
                        chuse(a0, 1, f);
                        chuse(a1, 1, f);
                        chuse(mem.base, -1, f);
                        @debug_assert(mem.scale == 1);
                        mem.index = a1;
                        mem.base = a0;
                        continue();
                    };
                }
                fn mul() => {
                    if mem.scale == 1 {
                        t := mem.base; mem.base = mem.index; mem.index = t;
                        goto_index();
                    };
                }
                @default => ();
            };
        };
        
        if mem.index == QbeNull && rtype(mem.base) == .RCon && addcon(mem.offset&, f.get_constant(mem.base), 1) {
            mem.base = QbeNull;
            return();
        };
        s: i64 = mem.scale.intcast();
        if rtype(mem.index) == .RTmp && !isreg(mem.index) {
            goto_done :: local_return;
            index_t := f.get_temporary(mem.index);
            if index_t.slot != -1 {
                if mem.scale != 1 {
                    @eprintln("scaling a stack addresses: %", index_t.name());
                    goto_done();  // panic?
                };
                if rtype(mem.base) == .RTmp {
                    base_t := f.get_temporary(mem.base);
                    if base_t.slot != -1 {
                        @eprintln("adding two stack addresses: % %", index_t.name(), base_t.name());
                        goto_done();  // panic?
                    };
                };
                t := mem.base; mem.base = mem.index; mem.index = t;
                continue();
            };
            if index_t.def.is_null() {
                goto_done();
            };
            a0 := index_t.def.arg&[0];
            a1 := index_t.def.arg&[1];
            if a0 == mem.index || a1 == mem.index || index_t.def.to != mem.index {
                goto_done();
            };
            @match(index_t.def.op()) {
                fn add() => {
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                    };
                    if f.get_int(a0) { off |
                        if fits_in_i32(mem.offset.bits.i + off * s) {
                            chuse(mem.index, -1, f);
                            chuse(a1, 1, f);
                            mem.offset.bits.i += off * s;
                            mem.index = a1;
                            continue();
                        };
                    };
                }
                fn mul() => {
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                    };
                    if f.get_int(a0) { arg |
                        if s == 1 && (arg == 1 || arg == 2 || arg == 4 || arg == 8) {
                            chuse(a1, 1, f);
                            chuse(mem.index, -1, f);
                            mem.scale = arg.intcast();
                            mem.index = a1;
                            continue();
                        };
                    };
                }
                @default => ();
            };
        };
        
        s := rslot(mem.base, f);
        if s != -1 {
            mem.base = SLOT(s);
        };
        return();
    }
}

fn is_symbol(f: *Qbe.Fn, r: Qbe.Ref) bool = 
    rtype(r) == .RCon && f.get_constant(r)[].type == .CAddr;

fn seladdr(r: *Qbe.Ref, f: *Qbe.Fn) void = {
    if rtype(r[]) == .RCon {  
        done := true;
        if f.get_int(r[]) { addr | 
            valid_offset := fits_in_i32(addr);
            done = valid_offset;
        };
        if done {
            return();
        };
    };
    mem := Qbe.Addr.zeroed();
    mem.base = r[];
    mem.scale = 1;
    mem.index = QbeNull;
    mem.offset.type = .CBits;
    f.merge_amd_addr(mem&);
    @debug_assert(mem.offset.type != .CAddr || (mem.index == QbeNull && mem.base == QbeNull), "generated illegal addressing mode");
    
    for (@slice(mem.index&, mem.base&)) { a |
        if rtype(a[]) == .RCon {
            r0 := f.newtmp("isel", .Kl);
            f.emit(@if(f.is_symbol(a[]), .addr, .copy), .Kl, r0, a[], QbeNull);
            a[] = r0;
        };
        
        @debug_assert_eq(rslot(a[], f), -1, "unfixed stack slot after address matching");
    };
        
    when_debug(f, .InstSelect) { out |
        write(out, "\n");
    };
    ty := mem.offset.type&;
    valid_offset := fits_in_i32(mem.offset.bits.i);
    if !valid_offset {
        // This makes sense when jitting, you're allowed to just stick hard coded references in your code. 
        @assert(mem.index == QbeNull && mem.base == QbeNull && ty[] == .CBits, "TODO: folded memory access to >4GB constant address");
        r1 := f.newtmp("isel", .Kl);
        f.emit(.copy, .Kl, r1, f.getcon(mem.offset.bits.i), QbeNull);
        mem.offset.bits.i = 0;
        mem.base = r1;
    };
    if mem.offset.bits.i == 0 && ty[] == .CBits {
        ty[] = .CUndef;
    };
        
    m: i64 = f.nmem.zext();
    f.nmem += 1;
    f.mem&.grow(m + 1);
    f.mem[m] = mem;
    r[] = MEM(m);
}

fn fits_in_i32(i: i64) bool = 
    i >= MIN_i32 && i <= MAX_i32

fn cmpswap(arg: *Array(Qbe.Ref, 2), cmp: i32) bool = {
    cmp := @as(Qbe.Cmp) cmp;
    @match(cmp) {
        fn Cflt() => true;
        fn Cfle() => true;
        fn Cfgt() => false;
        fn Cfge() => false;
        @default => rtype(arg[0]) == .RCon;
    }
}

fn selcmp(arg: []Qbe.Ref, k: Qbe.Cls, need_swap: bool, f: *Qbe.Fn) void = {
    if need_swap {
        arg.swap(0, 1);
    };
    icmp := f.emit(.xcmp, k, QbeNull, arg[1], arg[0]);
    
    if rtype(arg[0]) == .RCon {
        icmp.arg&[1] = f.newtmp("isel", k);
        new := f.emit(.copy, k, icmp.arg&[1], arg[0], QbeNull);
        fixarg(new.arg&.index(0), k, new, f);
    };
    fixarg(icmp.arg&.index(0), k, icmp, f);
    fixarg(icmp.arg&.index(1), k, icmp, f);
}

fn sel(i: Qbe.Ins, f: *Qbe.Fn) void = {
    if(try_kill_inst(f, i&), => return());
    
    i0 := f.globals.curi[]; // :LookAtLastInst
    sel_inner(i&, f);
    i1 := f.globals.curi[];
    
    while => i0.in_memory_after(i1) {
        @debug_assert(rslot(i0.arg&[0], f) == -1);
        @debug_assert(rslot(i0.arg&[1], f) == -1);
        i0 = i0.offset(-1);
    };
}

fn sel_inner(i: *Qbe.Ins, f: *Qbe.Fn) void = {
    k := i.cls();
    @match(i.op()) {
        fn div()  => f.sel_div(i);
        fn rem()  => f.sel_div(i);
        fn udiv() => f.sel_div(i);
        fn urem() => f.sel_div(i);
        fn sar()  => f.sel_shift(i);
        fn shr()  => f.sel_shift(i);
        fn shl()  => f.sel_shift(i);
        fn uwtof() => {
            r0 := f.newtmp("utof", .Kl);
            f.emit(.sltof, k, i.to, r0, QbeNull);
            ins := f.emit(.extuw, .Kl, r0, i.arg&[0], QbeNull);
            fixarg(ins.arg&.index(0), k, ins, f);
        }
        fn ultof() => {
            /* %mask =l and %arg.0, 1
            * %isbig =l shr %arg.0, 63
            * %divided =l shr %arg.0, %isbig
            * %or =l or %mask, %divided
            * %float =d sltof %or
            * %cast =l cast %float
            * %addend =l shl %isbig, 52
            * %sum =l add %cast, %addend
            * %result =d cast %sum
            */
            
            tmp := @uninitialized Array(Qbe.Ref, 7);
            tmp := tmp&;
            r0 := f.newtmp("utof", k);
            kc, sh := @if(k == .Ks, (Qbe.Cls.Kw, 23), (Qbe.Cls.Kl, 52));
            range(0, 4) { j | 
                tmp[j] = f.newtmp("utof", .Kl);
            };
            range(4, 7) { j | 
                tmp[j] = f.newtmp("utof", kc);
            };
            f.emit(.cast, k, i.to, tmp[6], QbeNull);
            f.emit(.add, kc, tmp[6], tmp[4], tmp[5]);
            f.emit(.shl, kc, tmp[5], tmp[1], f.getcon(sh));
            f.emit(.cast, kc, tmp[4], r0, QbeNull);
            f.emit(.sltof, k, r0, tmp[3], QbeNull);
            f.emit(.or, .Kl, tmp[3], tmp[0], tmp[2]);
            ins := make_ins(.shr, .Kl, tmp[2], i.arg&[0], tmp[1]);
            sel(ins, f); // TODO: make sure this is right!      was ^ emit instead if make_ins and <-  *curi++ instead ins   
            ins := f.emit(.shr, .Kl, tmp[1], i.arg&[0], f.getcon(63));
            fixarg(ins.arg&.index(0), .Kl, ins, f);
            ins := f.emit(.and, .Kl, tmp[0], i.arg&[0], f.getcon(1));
            fixarg(ins.arg&.index(0), .Kl, ins, f);
        }
        fn stoui()  => f.sel_ftoi(i, .Ks, .stosi, 0xdf000000);
        fn dtoui()  => f.sel_ftoi(i, .Kd, .dtosi, 0xc3e0000000000000);
        fn nop()    => ();
        fn dbgloc() => f.sel_emit(i);
        fn call()   => f.sel_emit(i);
        fn syscall()   => f.sel_emit(i);
        fn salloc() => f.sel_emit(i);
        fn copy()   => f.sel_emit(i);
        fn add()    => f.sel_emit(i);
        fn sub()    => f.sel_emit(i);
        fn neg()    => f.sel_emit(i);
        fn mul()    => f.sel_emit(i);
        fn and()    => f.sel_emit(i);
        fn or()     => f.sel_emit(i);
        fn xor()    => f.sel_emit(i);
        fn xtest()  => f.sel_emit(i);
        fn stosi()  => f.sel_emit(i);
        fn dtosi()  => f.sel_emit(i);
        fn swtof()  => f.sel_emit(i);
        fn sltof()  => f.sel_emit(i);
        fn exts()   => f.sel_emit(i);
        fn truncd() => f.sel_emit(i);
        fn cast()   => f.sel_emit(i);
        fn asm()    => { f.emit(i[]); }
        @default => {
            if is_alloc(i.op()) {
                salloc(i.to, i.arg&[0], f);
                return();
            };
            if is_ext(i.op()) {
                if i.op() == .extuw && rtype(i.arg&[0]) == .RCon {
                    i.set_op(.copy);
                };
                f.sel_emit(i);
                return();
            };
            if is_load(i.op()) {
                i1 := f.emit(i[]);
                seladdr(i1.arg&.index(0), f);
                f.fixargs(i1);
                return();
            };
            if is_store(i.op()) {
                if rtype(i.arg&[0]) == .RCon {
                    // doesn't matter that it's a float. store the bytes with an instruction that can have an immediate. 
                    if(i.op() == .stored, => i.set_op(.storel));
                    if(i.op() == .stores, => i.set_op(.storew));
                };
                i1 := f.emit(i[]);
                seladdr(i1.arg&.index(1), f);
                f.fixargs(i1);
                return();
            };
            
            kc := Qbe.Cls.Kw;
            x: i32 = 0;
            if iscmp(i.op(), kc&, x&) {
                xx := @as(Qbe.Cmp) x;
                @match(xx) {
                    fn Cfeq() => {
                        /* zf is set when operands are
                        * unordered, so we may have to
                        * check pf
                        */
                        r0 := f.newtmp("isel", .Kw);
                        r1 := f.newtmp("isel", .Kw);
                        f.emit(.and, .Kw, i.to, r0, r1);
                        f.emit(.flagfo, k, r1, QbeNull, QbeNull);
                        i.to = r0;
                    }
                    fn Cfne() => {
                        r0 := f.newtmp("isel", .Kw);
                        r1 := f.newtmp("isel", .Kw);
                        f.emit(.or, .Kw, i.to, r0, r1);
                        f.emit(.flagfuo, k, r1, QbeNull, QbeNull);
                        i.to = r0;
                    }
                    @default => ();
                };
                swap := cmpswap(i.arg&, x);
                if swap {
                    x = cmpop(x);
                };
                flag_op := @as(Qbe.O) @as(i32) Qbe.O.flagieq.raw() + x;
                f.emit(flag_op, k, i.to, QbeNull, QbeNull);
                selcmp(i.arg&.items(), kc, swap, f);
                return();
            };
            @panic("unknown instruction %", i.op().get_name());
        };
    }
}

fn sel_emit(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    i1 := f.emit(i[]);
    f.fixargs(i1); 
}

fn fixargs(f: *Qbe.Fn, i1: *Qbe.Ins) void = {
    k0, k1 := (argcls(i1, 0), argcls(i1, 1));
    fixarg(i1.arg&.index(0), k0, i1, f);
    fixarg(i1.arg&.index(1), k1, i1, f);
}

fn sel_div(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    k := i.cls();
    if(!is_int(k), => return(f.sel_emit(i)));
    rax, rdx := (TMP(Amd64Reg.RAX), TMP(Amd64Reg.RDX));
    r0, r1 := @if(@is(i.op(), .div, .udiv), (rax, rdx), (rdx, rax));
    f.emit(.copy, k, i.to, r0, QbeNull);
    f.emit(.copy, k, QbeNull, r1, QbeNull);

    /* immediates not allowed for
    * divisions in x86
    */
    r0 := if(rtype(i.arg&[1]) == .RCon, => f.newtmp("isel", k), => i.arg&[1]);
    
    t := f.get_temporary(r0);
    if t.slot != -1 { // TODO: why
        @panic("unlikely argument % in %", t.name(), i.op().get_name());
    };
    if (@is(i.op(), .div, .rem)) {
        f.emit(.xidiv, k, QbeNull, r0, QbeNull);
        f.emit(.sign, k, rdx, rax, QbeNull);
    } else { // unsigned
        f.emit(.xdiv, k, QbeNull, r0, QbeNull);
        f.emit(.copy, k, rdx, QbeConZero, QbeNull);
    };
    ins := f.emit(.copy, k, rax, i.arg&[0], QbeNull);
    fixarg(ins.arg&.index(0), k, ins, f);
    if rtype(i.arg&[1]) == .RCon {
        f.emit(.copy, k, r0, i.arg&[1], QbeNull);
    };
}

fn sel_shift(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    r0 := i.arg&[1];
    if(rtype(r0) == .RCon, => return(f.sel_emit(i))); // TODO: unless you try to shift by an address by mistake
    t := f.get_temporary(r0);
    if t.slot != -1 { // TODO: why
        @panic("unlikely argument % in %", t.name(), i.op().get_name());
    };
    rcx := TMP(Amd64Reg.RCX);
    i.arg&[1] = rcx;
    f.emit(.copy, .Kw, QbeNull, rcx, QbeNull);
    i1 := f.emit(i[]);
    f.emit(.copy, .Kw, rcx, r0, QbeNull);
    fixarg(i1.arg&.index(0), argcls(i, 0), i1, f);
}

fn flagi(i0: *Qbe.Ins, i: *Qbe.Ins) *Qbe.Ins = {
    null := Qbe.Ins.ptr_from_int(0);
    while => i.in_memory_after(i0) {
        i = i.offset(-1);
        zflag, lflag := amd64_flag_table(i.op());
        if(zflag, => return(i));
        if(!lflag, => return(null));
    };
    null
}

// returns (SetsZeroFlag, LeavesFlags)
fn amd64_flag_table(o: Qbe.O) Ty(bool, bool) = {
    b :: fn($o1: Qbe.O, $o2: Qbe.O) => o.between(o1, o2);
    if b(.ceqw, .cuod) || b(.add, .neg) || b(.and, .shl) || b(.blit0, .blit1) || b(.xcmp, .xtest) {
        return(true, false)
    };
    if b(.storeb, .cast) || o.between(.flagieq, .flagfuo) || o == .copy || o == .dbgloc || b(.nop, .addr) {
        return(false, true)
    };
    if b(.acmp, .call) || b(.div, .mul) || b(.swap, .xdiv) {
        return(false, false)
    };
    
    panic("TODO: unhandled op for amd64_flag_table()")
}

fn sel_ftoi(f: *Qbe.Fn, i: *Qbe.Ins, kc: Qbe.Cls, o: Qbe.O, t4: u64) void = {
    tmp := @uninitialized Array(Qbe.Ref, 7);
    tmp := tmp&;
    tmp[4] = f.getcon(t4.bitcast());
    i.set_op(o);
    k := i.cls();
    if k == .Kw {
        r0 := f.newtmp("ftou", .Kl);
        f.emit(.copy, .Kw, i.to, r0, QbeNull);
        i.set_cls(.Kl);
        i.to = r0;
        f.sel_emit(i);
        return();
    };
    /* %try0 =l {s,d}tosi %fp
    * %mask =l sar %try0, 63
    *
    *    mask is all ones if the first
    *    try was oob, all zeroes o.w.
    *
    * %fps ={s,d} sub %fp, (1<<63)
    * %try1 =l {s,d}tosi %fps
    *
    * %tmp =l and %mask, %try1
    * %res =l or %tmp, %try0
    */
    r0 := f.newtmp("ftou", kc);
    range(0, 4) { j |
        tmp[j] = f.newtmp("ftou", .Kl);
    }; 
    f.emit(.or, .Kl, i.to, tmp[0], tmp[3]);
    f.emit(.and, .Kl, tmp[3], tmp[2], tmp[1]);
    f.emit(i.op(), .Kl, tmp[2], r0, QbeNull);
    i1 := f.emit(.add, kc, r0, tmp[4], i.arg&[0]);
    fixarg(i1.arg&.index(0), kc, i1, f);
    fixarg(i1.arg&.index(1), kc, i1, f);
    f.emit(.sar, .Kl, tmp[1], tmp[0], f.getcon(63));
    ins := f.emit(i.op(), .Kl, tmp[0], i.arg&[0], QbeNull);
    fixarg(ins.arg&.index(0), .Kl, ins, f);
}

fn seljmp(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    if(@is(b.jmp.type, .ret0, .jmp, .hlt), => return()); // these already have a trivial single instruction implementation
    if b.jmp.type == .switch {  // so not worth it. 
        arg := b.jmp.arg;
        idx := f.newtmp("isel", .Kl);
        scratch := f.newtmp("isel", .Kl);
        if rtype(b.jmp.arg) != .RTmp {
            arg = f.newtmp("isel", .Kl);
        };
        
        table_size := get_jump_targets(b).len() - 1;
        f.emit(.xcmp, .Kl, QbeNull, f.getcon(table_size), arg);
        
        f.emit(.switch0, .Kl, scratch, idx, scratch);  // TODO: (slightly less) fucking garbage but if you don't have `scratch` on both sides it gets confused. 
        
        m := f.new_mem(arg, arg, 0, 4);
        f.emit(.addr, .Kl, idx, m, QbeNull);  // shorter encoding than *5
        
        if b.jmp.arg != arg {
            f.emit(.copy, .Kl, arg, b.jmp.arg, QbeNull);
        };
        
        // :( 
        f.emit(.copy, .Kl, scratch, QbeConZero, QbeNull);  // TODO: fucking garbage but if you don't do this it gets confused. 
        
        b.jmp.arg = scratch;
        return();
    };
    
    @debug_assert(b.jmp.type == .jnz);
    r := b.jmp.arg;
    @debug_assert(rtype(r) == .RTmp, "we shouldn't be jumping on a constant");
    t := f.get_temporary(r);
    b.jmp.arg = QbeNull;
    if b.s1.identical(b.s2) {
        chuse(r, -1, f);
        b.jmp.type = .jmp;
        b.s2 = Qbe.Blk.ptr_from_int(0);
        return();
    };
    fi := flagi(b.ins.first, b.ins.index(b.nins.zext()));
    k := Qbe.Cls.Kw;
    c: i32 = 0;
    @if_else {
        @if(fi.is_null() || fi.to != r) => {
            // The last flag setting op wasn't the comparison that created our argument. 
            // So we have to actually emit the compare to zero. 
            selcmp(@slice(r, QbeConZero), .Kw, false, f);
            b.jmp.type = .jfine;
        };
        @if(iscmp(fi.op(), k&, c&)
            && c != Qbe.Cmp.Cfeq.raw() /* see sel() */
            && c != Qbe.Cmp.Cfne.raw()) => {
            swap := cmpswap(fi.arg&, c);
            if swap {
                c = cmpop(c);
            };
            if t.nuse == 1 {
                selcmp(fi.arg&.items(), k, swap, f);
                fi.set_nop();
            };
            b.jmp.type = @as(Qbe.J) @as(i32) Qbe.J.jfieq.raw() + c;
        };
        @if(fi.op() == .and && t.nuse == 1
            && (rtype(fi.arg&[0]) == .RTmp ||
                rtype(fi.arg&[1]) == .RTmp)) => {
            fi.set_op(.xtest);
            fi.to = QbeNull;
            b.jmp.type = .jfine;
            if rtype(fi.arg&[1]) == .RCon {
                r := fi.arg&[1];
                fi.arg&[1] = fi.arg&[0];
                fi.arg&[0] = r;
            };
        }; 
        @else => {
            /* since flags are not tracked in liveness,
            * the result of the flag-setting instruction
            * has to be marked as live
            */
            if t.nuse == 1 {
                f.emit(.copy, .Kw, QbeNull, r, QbeNull);
            };
            b.jmp.type = .jfine;
        };
    }
}

// c0 += c1 * m;
// returns false if this cannot be done. 
fn addcon(c0: *Qbe.Con, c1: *Qbe.Con, m: i64) bool = {
    if(m != 1 && c1.type == .CAddr, => return(false));  // can't scale a symbol
    if (c0.type == .CUndef) {
        c0[] = c1[];
        c0.bits.i *= m;
    } else {
        if c1.type == .CAddr {
            if(c0.type == .CAddr, => return(false));  // can't have two symbols
            c0.type = .CAddr;
            c0.sym = c1.sym;
        };
        c0.bits.i += c1.bits.i * m;
    };
    true
}
