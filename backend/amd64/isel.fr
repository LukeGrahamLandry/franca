// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

/* For x86_64, do the following:
 *
 * - check that constants are used only in
 *   places allowed
 * - ensure immediates always fit in 32b
 * - expose machine register contraints
 *   on instructions like division.
 * - implement fast locals (the streak of
 *   constant allocX in the first basic block)
 * - recognize complex addressing modes
 *
 * Invariant: the use counts that are used
 *            in sel() must be sound.  This
 *            is not so trivial, maybe the
 *            dce should be moved out...
 */

//
// I want to make constant_folding() an optional pass which requires some unfortunate extra logic here 
// to make sure not to let all operands to an instruction be immediates (since that can't be encoded). 
// Those cases are unreachable normally because the folding will have removed them before you get here, 
// so they're in @if(!ENABLE_CONST_FOLD). 
//

/* instruction selection requires use counts */
fn amd64_isel(f: *Qbe.Fn) void =
    native_isel(f, fixarg, sel_terminator, sel_inst);

sel_inst :: fn(i: **Qbe.Ins, b: *Qbe.Blk, f: *Qbe.Fn) void = {
    @match(i[].op()) {
        fn sel1() => selsel_amd64(f, b, i[]);
        fn cas1() => sel_cas_amd64(f, b, i[]);
        @default  => sel(i[], f);
    };
};

fn noimm(r: Qbe.Ref, f: *Qbe.Fn, o: Qbe.O) bool = {
    if(rtype(r) != .RCon, => return(false));
    c := f.get_constant(r);
    @match(c.type()) {
        fn CAddr() => o == .call && c.bits() != 0;
        // jitting on linux you can reasonably have <4GB constant address, but since call instruction 
        // is relative, you might not be able to reach all the way down there in 32 bits of immediate. 
        // i think 64-bit macOS never gives out the bottom 4GB of address space so it doesn't come up. 
        fn CBits() => c.bits() < MIN_i32 || c.bits() > MAX_i32 || o == .call;
    }
}

fn rslot(r: Qbe.Ref, f: *Qbe.Fn) i32 = {
    if(rtype(r) != .RTmp, => return(-1));
    f.get_temporary(r)[].slot
}

fn fixarg(r: *Qbe.Ref, k: Qbe.Cls, i: *Qbe.Ins, f: *Qbe.Fn) void = {
    T := f.globals.target&;
    
    c := Qbe.Con.ptr_from_int(0);
    r1, r0 := (r[], r[]);
    s := rslot(r0, f);
    o := if(i.is_null(), => Qbe.O.copy, => i.op());
    @if_else {
        @if(!is_int(k) && rtype(r0) == .RCon) => {
            // Floats cannot be used as immediates. 
            // TODO: Qbe loads floats from memory instead. 
            //       clang does too so presumably its the correct choice. 
            //       but this is less of a pain for now. 
            
            r1 = f.newtmp("isel", k);
            r2 := f.newtmp("isel", .Kl);
            f.emit(.cast, k, r1, r2, QbeNull);
            f.emit(.copy, .Kl, r2, r0, QbeNull);
        };
        // 'copy' is the only instruction that's allowed a 64 bit immediate. 
        @if(o != .copy && k == .Kl && noimm(r0, f, o)) => {
            /* load constants that do not fit in
            * a 32bit signed integer into a
            * long temporary */
            r1 = f.newtmp("isel", .Kl);
            f.emit(.copy, .Kl, r1, r0, QbeNull);
        };
        @if(s != -1) => {
            /* load fast locals' addresses into
            * temporaries right before the
            * instruction */
            r1 = f.newtmp("isel", .Kl);  // :IselRSlotGetsNewTmp
            f.emit(.addr, .Kl, r1, SLOT(s), QbeNull);
        };
        @if(!(is_store(o) && r.identical(i.arg&.index(1)))
        && !is_load(o) && o != .cas0 && o != .call && f.is_symbol(r0)) => {
            r1 = f.newtmp("isel", .Kl);
            f.lea(r1, r0);
        };
        // no immediate allowed for selCC
        @if(o.is_sel_flag() && rtype(r[]) == .RCon) => {
            r1 = f.newtmp("isel", i.cls());
            f.emit(.copy, i.cls(), r1, r[], QbeNull);
        };
        @if(o == .cas1 && rtype(r[]) == .RCon) => {
            r1 = f.newtmp("isel", i.cls());
            f.emit(.copy, i.cls(), r1, r[], QbeNull);
        };
        @else => ();
    };
    r[] = r1;
}

fn lea(f: *Qbe.Fn, dest: Qbe.Ref, con_in: Qbe.Ref) void = {
    const := f.get_constant(con_in);
    if const.type() == .CBits {
        f.emit(@if(f.is_symbol(con_in), .addr, .copy), .Kl, dest, con_in, QbeNull);
        return();
    };
    
    if could_be_import(f, const) {
        bits := const.bits();
        if bits != 0 {
            @assert(fits_in_i32(bits), "TODO: >4gb offset??");
            r := f.newtmp("isel", .Kl);
            f.emit(.add, .Kl, dest, r, f.getcon(bits));
            dest = r;
            con_in = f.symcon(const.sym);
        };
    };

    f.emit(@if(f.is_symbol(con_in), .addr, .copy), .Kl, dest, con_in, QbeNull);
}

fn merge_amd_addr(f: *Qbe.Fn, mem: *Qbe.Addr) void = {
    n := 0;
    loop {
        continue :: local_return;
        n += 1;
        @debug_assert(n < 10, "probably infinite looping in merge_amd_addr of %", f.name());
        
        when_debug(f, .InstSelect) { out |
            printmem(mem, f, out);
        };
        if rtype(mem.base) == .RTmp && !isreg(mem.base) {
            goto_index :: local_return;
            t := f.get_temporary(mem.base);
            if t.slot != -1 {
                mem.base = SLOT(t.slot);
                continue();
            };
            if t.def.is_null() {
                goto_index();
            };
            a0 := t.def.arg&[0];
            a1 := t.def.arg&[1];
            // :ReferencingOldBlock
            // t.def will be wrong if it's in an already processed block and new instructions caused it to shift, 
            // so check that it's still pointing to the instruction that made our value. 
            // if it did reallocate and we get the old pre-isel copy, i think it's still fine
            // because this just cares about things that don't have interesting isel handling>? 
            //         talking about this check -> |                   |
            if a0 == mem.base || a1 == mem.base || t.def.to != mem.base {
                goto_index();
            };
            @match(t.def.op()) {
                fn add() => {
                    swap := false;
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                        swap = true;
                    };
                    if f.is_symbol(a0) {
                        if mem.index == QbeNull && mem.offset&.type() != .CAddr {
                            if f.get_int(a1) { off |
                                @debug_assert_eq(mem.scale, 1);
                                new_offset := mem.offset;
                                x := addcon(new_offset&, f.get_constant(a0), 1);
                                y := addcon(new_offset&, f.get_constant(a1), 1);
                                @debug_assert(x && y, "failed to merge constants in $%", f.name());
                                if fits_in_i32(new_offset&.bits()) {
                                    mem.offset = new_offset;
                                    mem.base = QbeNull;
                                    return();
                                };
                            };
                            // will get fixed-up later.
                            // it could be that both are CAddr but lea-lea-deref is better than lea-lea-add-deref
                            // tho we should really make sure to reuse the lea of con
                            mem.index = a1;
                            mem.base = a0;
                            continue();
                        };
                        
                        f.fix_slot(mem.index&);
                        return();
                    };
                    if f.get_int(a0) { off |
                        if fits_in_i32(off + mem.offset&.bits()) {
                            chuse(a1, 1, f);
                            chuse(mem.base, -1, f);
                            mem.offset&.set_bits(mem.offset&.bits() + off);
                            mem.base = a1;
                            continue();
                        };
                    };
                    if mem.index == QbeNull && !f.is_symbol(a0) {
                        chuse(a0, 1, f);
                        chuse(a1, 1, f);
                        chuse(mem.base, -1, f);
                        @debug_assert(mem.scale == 1);
                        mem.index = a1;
                        mem.base = a0;
                        continue();
                    };
                }
                fn mul() => {
                    if mem.scale == 1 {
                        t := mem.base; mem.base = mem.index; mem.index = t;
                        goto_index();
                    };
                }
                @default => ();
            };
        };
        
        if mem.index == QbeNull && rtype(mem.base) == .RCon && addcon(mem.offset&, f.get_constant(mem.base), 1) {
            mem.base = QbeNull;
            return();
        };
        s: i64 = mem.scale.intcast();
        if rtype(mem.index) == .RTmp && !isreg(mem.index) {
            goto_done :: local_return;
            index_t := f.get_temporary(mem.index);
            if index_t.slot != -1 {
                if mem.scale != 1 {
                    @eprintln("scaling a stack addresses: %", index_t.name());
                    goto_done();  // panic?
                };
                if rtype(mem.base) == .RTmp {
                    base_t := f.get_temporary(mem.base);
                    if base_t.slot != -1 {
                        @eprintln("adding two stack addresses: % %", index_t.name(), base_t.name());
                        goto_done();  // panic?
                    };
                };
                t := mem.base; mem.base = mem.index; mem.index = t;
                continue();
            };
            if index_t.def.is_null() {
                goto_done();
            };
            a0 := index_t.def.arg&[0];
            a1 := index_t.def.arg&[1];
            // :ReferencingOldBlock
            if a0 == mem.index || a1 == mem.index || index_t.def.to != mem.index {
                goto_done();
            };
            @match(index_t.def.op()) {
                fn add() => {
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                    };
                    if f.get_int(a0) { off |
                        value := mem.offset&.bits() + off * s;
                        if fits_in_i32(value) {
                            chuse(mem.index, -1, f);
                            chuse(a1, 1, f);
                            mem.offset&.set_bits(value);
                            mem.index = a1;
                            continue();
                        };
                    };
                }
                fn mul() => {
                    if rtype(a0) == .RTmp {
                        t := a0; a0 = a1; a1 = t; 
                        if rtype(a0) == .RTmp {
                            goto_done();
                        };
                    };
                    if f.get_int(a0) { arg |
                        if s == 1 && (arg == 1 || arg == 2 || arg == 4 || arg == 8) {
                            chuse(a1, 1, f);
                            chuse(mem.index, -1, f);
                            mem.scale = arg.intcast();
                            mem.index = a1;
                            continue();
                        };
                    };
                }
                @default => ();
            };
        };
        
        f.fix_slot(mem.base&);
        if mem.base == QbeNull && mem.scale == 1 {
            mem.base = mem.index;
            mem.index = QbeNull;
        };
        return();
    }
}

fn fix_slot(f: *Qbe.Fn, r: *Qbe.Ref) void = {
    s := rslot(r[], f);
    if s != -1 {
        r[] = SLOT(s);
    };
}

fn seladdr(r: *Qbe.Ref, f: *Qbe.Fn) void = {
    @debug_assert_ne(rtype(r[]), .RMem);
    mem: Qbe.Addr = (base = r[], index = QbeNull, scale = 1, offset = @run con(Qbe.no_symbol_S, 0));
    f.merge_amd_addr(mem&);
    when_debug(f, .InstSelect) { out |
        printmem(mem&, f, out);
    };
    f.fix_amd_mem(mem&, false);
    when_debug(f, .InstSelect) { out |
        @fmt(out, "\n");
    };
    r[] = f.new_mem(mem);
}

fn fix_amd_mem(f: *Qbe.Fn, mem: *Qbe.Addr, is_lea: bool) void = {
    @debug_assert(mem.offset&.type() != .CAddr || (mem.index == QbeNull && mem.base == QbeNull), "generated illegal addressing mode");
    
    refs := @slice(mem.index&, mem.base&);
    for refs { a |
        if rtype(a[]) == .RCon {
            r0 := f.newtmp("isel", .Kl);
            f.lea(r0, a[]);
            a[] = r0;
        };
        
        f.fix_slot(a); 
    };
    
    base_val := mem.base.rsval();
    if rtype(mem.base) == .RSlot && base_val >= 0 {
        new := base_val.intcast() + mem.offset&.bits();
        if new >= 0 {
            mem.base = SLOT(new);
            mem.offset&.set_bits(0);
        };
    };
    
    valid_offset := fits_in_i32(mem.offset&.bits());
    if !valid_offset {
        // This makes sense when jitting, you're allowed to just stick hard coded references in your code. 
        @assert(mem.index == QbeNull && mem.base == QbeNull && mem.offset&.type() == .CBits, "TODO: folded memory access to >4GB constant address");
        r1 := f.newtmp("isel", .Kl);
        f.emit(.copy, .Kl, r1, f.getcon(mem.offset&.bits()), QbeNull);
        mem.offset&.set_bits(0);
        mem.base = r1;
    };

    // "only LEA can be converted to GOT load [...]"
    // if we don't know it's a local symbol, it might be an import that ends up being dynamic 
    // and needing to become a GOT load, which can't be folded into a load/store/cas. 
    // As a fun bonus, you can't fit an increment from the symbol in the same instruction so we have to split out the add. 
    // Doing this generates worse code (?) tho so pledge_local is a hack to let the frontend forward 
    // declare when it knows something is local data it's going to create later. 
    // TODO: test that exercises this other than just supporting both @import_symbol for @/graphics and non-cc .ssa tests. 
    if !is_lea && could_be_import(f, mem.offset&) {
        if mem.base == QbeNull && mem.index != QbeNull && mem.scale == 1 {
            mem.base = mem.index;
            mem.index = QbeNull 
        };
        base := mem.base;
        mem.base = QbeNull;
        
        @assert(mem.index == QbeNull || mem.scale != 1, "TODO: fuck the got load needs to be alone");
        r0 := f.newtmp("isel", .Kl);
        bits := mem.offset&.bits();
        mem.offset.bits = (0, 0);
        
        f.emit(.addr, .Kl, r0, f.new_mem(mem[]), QbeNull);
        mem[] = (base = r0, index = base, offset = con(Qbe.no_symbol_S, bits), scale = 1);
    };
}

// TODO: unify with fold.fr/strong_constant
//       but then make sure it still makes sense for how wasm uses this. (it doesn't care about strong vs weak)
fn could_be_import(f: *Qbe.Fn, constant: *Qbe.Con) bool = {
    if(constant.type() == .CBits, => return(false)); 
    could_be_import(f.globals, constant.sym)
}

fn could_be_import(m: *Qbe.Module, id: Qbe.Sym) bool = {
    result := false;
    use_symbol(m, id) { s |
        result = s.kind != .Local && !s.pledge_local;
    };
    result
}

fn need_cmpswap(arg: *Array(Qbe.Ref, 2), cmp: i32) bool = {
    cmp := @as(Qbe.Cmp) cmp;
    @match(cmp) {
        fn Cflt() => true;
        fn Cfle() => true;
        fn Cfgt() => false;
        fn Cfge() => false;
        @default => rtype(arg[0]) == .RCon;
    }
}

fn selcmp(arg: []Qbe.Ref, k: Qbe.Cls, need_swap: bool, f: *Qbe.Fn) void = {
    if need_swap {
        arg.swap(0, 1);
    };
    icmp := f.emit(.xcmp, k, QbeNull, arg[1], arg[0]);
    
    if rtype(arg[0]) == .RCon {
        icmp.arg&[1] = f.newtmp("isel", k);
        new := f.emit(.copy, k, icmp.arg&[1], arg[0], QbeNull);
        fixarg(new.arg&.index(0), k, new, f);
    };
    fixarg(icmp.arg&.index(0), k, icmp, f);
    fixarg(icmp.arg&.index(1), k, icmp, f);
}

// Adapted from a qbe patch: https://lists.sr.ht/~mpu/qbe/patches/55968
// TODO: this is so similar to seljnz. but im afraid trying to factor them together would just make it more confusing. 
fn selsel_amd64(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) *Qbe.Ins = {
    isel0 := i.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    r := isel0.arg&[0];
    old_r := r;
    @if(!ENABLE_CONST_FOLD) if rtype(r) != .RTmp {
        r = f.newtmp("", .Kw);  // :jnz_is_Kw
    };
    t := f.get_temporary(r); 
    nuse := t.nuse;
    fi := flagi(between(b.ins.ptr, isel0));
    cr0, cr1 := (QbeNull, QbeNull);
    gencmp, gencpy, need_swap := (false, false, false);
    k := t.cls;
    c := Qbe.Cmp.Cine.raw();
    @if_else {
        @if(fi.is_null() || fi.to != r) => {
            gencmp = true;
            cr0 = r;
            cr1 = QbeConZero;
        };
        @if(iscmp(fi.op(), k&, c&) && c != Qbe.Cmp.Cfeq.raw() && c != Qbe.Cmp.Cfne.raw()) => {  /* see sel() */
            need_swap = need_cmpswap(fi.arg&, c);
            if need_swap {
                c = cmp_swap(c);
            };
            if nuse == 1 {
                gencmp = true;
                cr0 = fi.arg&[0];
                cr1 = fi.arg&[1];
                fi.set_nop();
            };
        };
        @if(fi.op() == .and && nuse == 1
            && (rtype(fi.arg&[0]) == .RTmp ||
                rtype(fi.arg&[1]) == .RTmp)) => {
            fi.set_op(.xtest);
            fi.to = QbeNull;
            if rtype(fi.arg&[1]) == .RCon {
                r = fi.arg&[1]; 
                fi.arg&.items().swap(0, 1);
            };
        };
        /* since flags are not tracked in liveness,
         * the result of the flag-setting instruction
         * has to be marked as live */
        @else => {
            gencpy = nuse == 1;
        };
    };
    
    isel1 := i;
    @debug_assert_eq(isel1.op(), .sel1);
    isel1.set_op(@as(Qbe.O) @as(i32) Qbe.O.selieq.raw() + c);
    sel(isel1, f);
    
    @debug_assert(nand(gencpy, gencmp));
    @if(gencmp) selcmp(@slice(cr0, cr1), k, need_swap, f);
    @if(gencpy) {f.emit(.copy, .Kw, QbeNull, r, QbeNull);};
    
    @if(!ENABLE_CONST_FOLD) if r != old_r {
        f.emit(.copy, .Kw, r, old_r, QbeNull);  // :jnz_is_Kw
    };
    isel0.set_nop();
    isel0
}

fn sel();
fn sel(f: *Qbe.Fn, o: Qbe.O, k: Qbe.Cls, r: Qbe.Ref, a: Qbe.Ref, b: Qbe.Ref) void = {
    i := make_ins(o, k, r, a, b);
    sel(i&, f);
} 

fn sel(i: *Qbe.Ins, f: *Qbe.Fn) void = {
    if(try_kill_inst(f, i), => return());  // TODO
    
    i0 := f.globals.curi; // :LookAtLastInst
    sel_inner(i, f);
    i1 := f.globals.curi;
    
    while => i0.in_memory_after(i1) {
        i0 = i0.offset(-1);
        @debug_assert(rslot(i0.arg&[0], f) == -1, "expected no slot 0");
        @debug_assert(rslot(i0.arg&[1], f) == -1, "expected no slot 1");
    };
}

// cas0 %p
// %res =k cas1 %old_in, %new
// 
// RAX as %old_in and %res is implied. 
// The two explicit arguments of cmpxchg are %p and %new. 
fn sel_cas_amd64(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    rax :: TMP(Amd64Reg.RAX);
    cas0, cas1 := find_cas(i, b);
    // TODO: should really make the frontend ensure this but i don't want to deal with it right now
    //@debug_assert_eq(ptr_diff(cas0, cas1), 1, "non-adjacent cas");
    k := cas1.cls();
    // can't try_kill_inst since a cas is a store.
    
    // TODO: it's confusing that here i don't put a fake copy to RAX anywhere. 
    //       i guess rega doesn't touch reg copies at all, not just arounds calls. 
    f.emit(.copy, k, cas1.to, rax, QbeNull);
    cas1.to = QbeNull;
    cas1 := f.emit(cas1);
    i2   := f.emit(.copy, k, rax, cas1.arg&[0], QbeNull);
    fixarg(cas1.arg&.index(1), k, cas1, f);
    fixarg(i2.arg&.index(0), k, i2, f);
    
    seladdr(cas0.arg&.index(0), f);
    f.fixargs(cas0);
    cas1.arg&[0] = cas0.arg&[0];
    cas0.set_nop(); 
}

fn sel_inner(i: *Qbe.Ins, f: *Qbe.Fn) void = {
    k := i.cls();
    @match(i.op()) {
        fn cas0() => panic("loose cas0");
        fn uwtof() => {
            r0 := f.newtmp("utof", .Kl);
            f.emit(.sltof, k, i.to, r0, QbeNull);
            ins := f.emit(.extuw, .Kl, r0, i.arg&[0], QbeNull);
            fixarg(ins.arg&.index(0), k, ins, f);
        }
        fn ultof() => {
            // TODO: the overload set should coerce to FuncId. :CompilerBug
            emitter :: fn(f, o, k, r, a0, a1) => 
                f.sel(o, k, r, a0, a1);
            
            kc, sh := @if(k == .Ks, (Qbe.Cls.Kw, 23), (Qbe.Cls.Kl, 52));
            @emit_instructions((f = f, emit = emitter), (i.arg&[0], i.to, kc, sh, k), """
            @start
                %mask    =l and %0, 1
                %isbig   =l shr %0, 63
                %divided =l shr %0, %isbig
                %or      =l or %mask, %divided
                %float   =4 sltof %or
                %cast    =2 cast %float
                %addend  =2 shl %isbig, %3
                %sum     =2 add %cast, %addend
                %1       =4 cast %sum
            """);
        }
        fn stoui()  => f.sel_ftoi(i, .Ks, .stosi, 0xdf000000);
        fn dtoui()  => f.sel_ftoi(i, .Kd, .dtosi, 0xc3e0000000000000);
        fn nop()    => ();
        fn addr() => unreachable();
        // TODO: do we trust that it leaves the destination unmodifed when input is zero? then this could be much simpler. 
        fn clz() => {
            // TODO: maybe i should just declare i don't care about cpus that don't have LZCNT and then this can be one instruction
            // :EmitClzAsBsr
            // r = bsr a0            # sets ZF if a0=0
            // r1 = ZF ? 1_1111 : r  # if input was 0 just want the high bit so need something for the xor to clear
            // to = xor r1, 0_1111   # if nonzero, subtract from bit count converts index to number of zeros
            r := f.newtmp("isel", k);
            r1 := f.newtmp("isel", k);
            mask := f.getcon(@if(k == .Kw, 31, 63));
            alt := f.getcon(@if(k == .Kw, 63, 127));
            f.sel(.xor, k, i.to, r1, mask);
            f.sel(.selieq, k, r1, alt, r);
            i.to = r;
            i1 := f.emit(i);
            f.fixargs(i1); 
        }
        fn ctz() => {
            // this relies on emit NOT using tzcnt because it sets flags differently
            // r = bsr a0            # sets ZF if a0=0
            // to = ZF ? 1_000 : r   # if input was 0 just want the high bit for operand size
            r := f.newtmp("isel", k);
            alt := f.getcon(@if(k == .Kw, 32, 64));
            f.sel(.selieq, k, i.to, alt, r);
            i.to = r;
            i1 := f.emit(i);
            f.fixargs(i1); 
        }
        @default => {
            if is_alloc(i.op()) {
                salloc(i.to, i.arg&[0], f);
                return();
            };
            if is_ext(i.op()) {
                if i.op() == .extuw && rtype(i.arg&[0]) == .RCon {
                    i.set_op(.copy);
                };
                f.sel_emit(i);
                return();
            };
            if is_load(i.op()) {
                new_ins := f.emit(i);
                r := new_ins.arg&.index(0);
                seladdr(r, f);
                @debug_assert(rtype(r[]) != .RTmp, "seladdr failed");
                f.fixargs(new_ins);
                return();
            };
            if is_store(i.op()) {
                if rtype(i.arg&[0]) == .RCon {
                    // doesn't matter that it's a float. store the bytes with an instruction that can have an immediate. 
                    if(i.op() == .stored, => i.set_op(.storel));
                    if(i.op() == .stores, => i.set_op(.storew));
                };
                i1 := f.emit(i);
                seladdr(i1.arg&.index(1), f);
                f.fixargs(i1);
                return();
            };
            
            kc := Qbe.Cls.Kw;
            x: i32 = 0;
            if iscmp(i.op(), kc&, x&) {
                xx := @as(Qbe.Cmp) x;
                @match(xx) {
                    /* zf is set when operands are
                     * unordered, so we may have to
                     * check pf */
                    fn Cfeq() => emit_check_order_flag(f, i, .and, .flagfo);
                    fn Cfne() => emit_check_order_flag(f, i, .or, .flagfuo);
                    @default => ();
                };
                swap := need_cmpswap(i.arg&, x);
                if swap {
                    x = cmp_swap(x);
                };
                flag_op := @as(Qbe.O) @as(i32) Qbe.O.flagieq.raw() + x;
                f.emit(flag_op, k, i.to, QbeNull, QbeNull);
                selcmp(i.arg&.items(), kc, swap, f);
                return();
            };
            if (@is(i.op(), .div, .rem, .udiv, .urem)) {
                f.sel_div(i);
                return();
            };
            if (@is(i.op(), .sar, .shr, .shl, .rotr, .rotl)) {
                f.sel_shift(i);
                return();
            };
            f.sel_emit(i);
        };
    }
}

fn emit_check_order_flag(f: *Qbe.Fn, i: *Qbe.Ins, merge: Qbe.O, flag: Qbe.O) void = {
    r0 := f.newtmp("isel", .Kw);
    r1 := f.newtmp("isel", .Kw);
    f.emit(merge, .Kw, i.to, r0, r1);
    f.emit(flag, i.cls(), r1, QbeNull, QbeNull);
    i.to = r0;
}

fn sel_emit(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    i1 := f.emit(i);
    f.fixargs(i1); 
}

fn fixargs(f: *Qbe.Fn, i1: *Qbe.Ins) void = {
    k0, k1 := (argcls(i1, 0), argcls(i1, 1));
    a0, a1 := (i1.arg&.index(0), i1.arg&.index(1));
    
    r, r1 := (QbeNull, QbeNull);
    @if(!ENABLE_CONST_FOLD) if i1.op() != .call && rtype(a0[]) == .RCon && (rtype(a1[]) == .RCon || k1 == .Kx) {
        r = f.newtmp("isel", k0);
        r1 = a0[];
        a0[] = r;
    };
    
    fixarg(a0, k0, i1, f);
    fixarg(a1, k1, i1, f);
    
    @if(!ENABLE_CONST_FOLD) if r != QbeNull {
        i := f.emit(.copy, k0, r, r1, QbeNull);
        fixarg(i.arg&.index(0), k0, i, f);
    };
}

fn sel_div(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    k := i.cls();
    if(!is_int(k), => return(f.sel_emit(i)));
    rax, rdx := (TMP(Amd64Reg.RAX), TMP(Amd64Reg.RDX));
    r0, r1 := @if(@is(i.op(), .div, .udiv), (rax, rdx), (rdx, rax));
    f.emit(.copy, k, i.to, r0, QbeNull);
    f.emit(.copy, k, QbeNull, r1, QbeNull);

    /* immediates not allowed for
    * divisions in x86
    */
    r0 := if(rtype(i.arg&[1]) == .RCon, => f.newtmp("isel", k), => i.arg&[1]);
    warn_if_unlikely_argument(f, i, r0);
    if (@is(i.op(), .div, .rem)) {
        f.emit(.xidiv, k, QbeNull, r0, QbeNull);
        f.emit(.sign, k, rdx, rax, QbeNull);
    } else { // unsigned
        f.emit(.xdiv, k, QbeNull, r0, QbeNull);
        f.emit(.copy, k, rdx, QbeConZero, QbeNull);
    };
    ins := f.emit(.copy, k, rax, i.arg&[0], QbeNull);
    fixarg(ins.arg&.index(0), k, ins, f);
    if rtype(i.arg&[1]) == .RCon {
        f.emit(.copy, k, r0, i.arg&[1], QbeNull);
    };
}

// Variable width shift/rotate always take the width in RCX. 
fn sel_shift(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    r0 := i.arg&[1];
    if(rtype(r0) == .RCon, => return(f.sel_emit(i))); // TODO: unless you try to shift by an address by mistake
    warn_if_unlikely_argument(f, i, r0);
    rcx := TMP(Amd64Reg.RCX);
    i.arg&[1] = rcx;
    f.emit(.copy, .Kw, QbeNull, rcx, QbeNull);
    i1 := f.emit(i);
    f.emit(.copy, .Kw, rcx, r0, QbeNull);
    fixarg(i1.arg&.index(0), argcls(i, 0), i1, f);
}

fn flagi(i: []Qbe.Ins) *Qbe.Ins = {
    null := Qbe.Ins.ptr_from_int(0);
    each_rev i { i |
        zflag, lflag := ops_table_amd_flag(i.op());
        if(zflag, => return(i));
        if(!lflag, => return(null));
    };
    null
}

fn sel_ftoi(f: *Qbe.Fn, i: *Qbe.Ins, kc: Qbe.Cls, o: Qbe.O, t4: u64) void = {
    tmp := @uninitialized Array(Qbe.Ref, 7);
    tmp := tmp&;
    tmp[4] = f.getcon(t4.bitcast());
    i.set_op(o);
    k := i.cls();
    if k == .Kw {
        r0 := f.newtmp("ftou", .Kl);
        f.emit(.copy, .Kw, i.to, r0, QbeNull);
        i.set_cls(.Kl);
        i.to = r0;
        f.sel_emit(i);
        return();
    };
    // TODO: want to use emit_instructions but need to allow passing an op as a runtime arg. 
    /* %try0 =l {s,d}tosi %fp
    * %mask =l sar %try0, 63
    *
    *    mask is all ones if the first
    *    try was oob, all zeroes o.w.
    *
    * %fps ={s,d} sub %fp, (1<<63)
    * %try1 =l {s,d}tosi %fps
    *
    * %tmp =l and %mask, %try1
    * %res =l or %tmp, %try0
    */
    r0 := f.newtmp("ftou", kc);
    range(0, 4) { j |
        tmp[j] = f.newtmp("ftou", .Kl);
    }; 
    f.emit(.or, .Kl, i.to, tmp[0], tmp[3]);
    f.emit(.and, .Kl, tmp[3], tmp[2], tmp[1]);
    f.emit(i.op(), .Kl, tmp[2], r0, QbeNull);
    i1 := f.emit(.add, kc, r0, tmp[4], i.arg&[0]);
    fixarg(i1.arg&.index(0), kc, i1, f);
    fixarg(i1.arg&.index(1), kc, i1, f);
    f.emit(.sar, .Kl, tmp[1], tmp[0], f.getcon(63));
    ins := f.emit(i.op(), .Kl, tmp[0], i.arg&[0], QbeNull);
    fixarg(ins.arg&.index(0), .Kl, ins, f);
}

fn sel_terminator(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    if(@is(b.jmp.type, .ret0, .jmp, .hlt), => return()); // these already have a trivial single instruction implementation
    @debug_assert_ne(b.jmp.type, .switch);
    
    old_r, r := (b.jmp.arg, QbeNull);
    @if(!ENABLE_CONST_FOLD) if rtype(old_r) != .RTmp {
        r = f.newtmp("", .Kw);  // :jnz_is_Kw
        b.jmp.arg = r;
    };
    
    seljnz(b, f);
    
    @if(!ENABLE_CONST_FOLD) if r != QbeNull {
        f.emit(.copy, .Kw, r, old_r, QbeNull);  // :jnz_is_Kw
    };
}

// this avoids inserting a cmp to zero if the last flag setting op produced the value being jumped on. 
// but it doesn't try super hard: doesn't reorder instructions and won't remove a cmp, just doesn't insert an extra. 
fn seljnz(b: *Qbe.Blk, f: *Qbe.Fn) void = {
    @debug_assert(b.jmp.type == .jnz);
    r := b.jmp.arg;
    @debug_assert(rtype(r) == .RTmp, "we shouldn't be jumping on a constant");
    t := f.get_temporary(r);
    b.jmp.arg = QbeNull;
    if b.s1.identical(b.s2) {
        chuse(r, -1, f);
        b.jmp.type = .jmp;
        b.s2 = Qbe.Blk.ptr_from_int(0);
        return();
    };
    fi := flagi(b.ins.items());
    k := Qbe.Cls.Kw; // :jnz_is_Kw
    c: i32 = 0;
    @if_else {
        @if(fi.is_null() || fi.to != r) => {
            // The last flag setting op wasn't the comparison that created our argument. 
            // So we have to actually emit the compare to zero. 
            selcmp(@slice(r, QbeConZero), .Kw, false, f); // :jnz_is_Kw
            b.jmp.type = .jfine;
        };
        @if(iscmp(fi.op(), k&, c&)
            && c != Qbe.Cmp.Cfeq.raw() /* see sel() */
            && c != Qbe.Cmp.Cfne.raw()) => {
            swap := need_cmpswap(fi.arg&, c);
            if swap {
                c = cmp_swap(c);
            };
            if t.nuse == 1 {
                selcmp(fi.arg&.items(), k, swap, f);
                fi.set_nop();
            };
            b.jmp.type = @as(Qbe.J) @as(i32) Qbe.J.jfieq.raw() + c;
        };
        @if(fi.op() == .and && t.nuse == 1
            && (rtype(fi.arg&[0]) == .RTmp ||
                rtype(fi.arg&[1]) == .RTmp)) => {
            fi.set_op(.xtest);
            fi.to = QbeNull;
            b.jmp.type = .jfine;
            if rtype(fi.arg&[1]) == .RCon {
                r := fi.arg&[1];
                fi.arg&[1] = fi.arg&[0];
                fi.arg&[0] = r;
            };
        }; 
        @else => {
            /* since flags are not tracked in liveness,
             * the result of the flag-setting instruction
             * has to be marked as live */
            if t.nuse == 1 {
                f.emit(.copy, .Kw, QbeNull, r, QbeNull);
            };
            b.jmp.type = .jfine;
        };
    }
}

// c0 += c1 * m;
// returns false if this cannot be done. 
fn addcon(c0: *Qbe.Con, c1: *Qbe.Con, m: i64) bool = {
    if(m != 1 && c1.type() == .CAddr, => return(false));  // can't scale a symbol
    if c1.type() == .CAddr {
        if(c0.type() == .CAddr, => return(false));  // can't have two symbols
        c0.sym = c1.sym;
    };
    c0.set_bits(c0.bits() + c1.bits() * m);
    true
}

#use("@/backend/amd64/target.fr");
#use("@/backend/lib.fr");
#use("@/backend/abi.fr");
