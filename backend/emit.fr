SegmentCursor :: @struct(
    mmapped: []u8,
    next: *u8,
);

SegmentType :: @enum(i64) (
    Code,
    ConstantData,
    MutableData,
    ZeroInitData,
);

SymbolBucket :: @struct(
    n: u32,
    data: QList(SymbolInfo),
);

// TODO: make this less chunky
SymbolInfo :: @struct(
    name: Str,
    inline: *Qbe.Fn, // nullable
    fixups: RawList(Fixup),
    // or -1 if not referenced yet. valid: DynamicPatched
    // this gets repurposed when fixups_locked. 
    // when targetting wasm this is the index of the function (need to offset by import count if imported)
    got_lookup_offset: i64,
    segment: SegmentType, // valid: Local
    //  local: offset in the segment
    // import: offset in stubs to trampoline (if function)
    offset: i64,
    jit_addr: rawptr, // valid when kind == .DynamicPatched and goal == .JitOnly
    shim_addr: rawptr, // TODO: it would be great if i didn't need this
    kind: SymbolKind,
    local_but_marked_for_relocation: bool,
    referenced: bool,  // took address or called before knowing it was inlinable so you must actually emit the function.
    pledge_local: bool,
    strong: bool,
    size: u32,  // only set for functions currently. 
    library: u32,  // for imports. index into m.libraries. 0 is reserved for locals
    wasm_type_index: i32,
    // this is a bit dumb because it's redundant with `name` but it means you can get
    // it from just a *SymbolInfo without calling intern() if you want to add it to a 
    // list from inside a use_symbol so the bucket is already locked. 
    id: Qbe.Sym,
    export: bool,
);

SymbolKind :: @enum(u8) (
    // We haven't compiled this yet so we've emitted references as lookups into a table in the data section. 
    // 
    // It might also be a dynamic import. 
    // - For aot, you may want to go through the fixups again and translate them to use a GOT section. 
    //   That way you don't make the loader waste time patching two tables. 
    //
    Pending,
    // We've already compiled this and we know its close to the code and addressable by a static offset. 
    Local,
    // This is an import and we've already patched it in the code. 
    DynamicPatched,
);

FixupType :: @tagged(
    // arm64
    //  local: adrp,add to the address directly
    // import: ??
    InReg: @struct(r: i64, increment: i64, got_load := false),
    // arm64
    //  local: b(l) to the address
    // import: b(l) to a trampoline in __stubs which adrp,ldr,br from __got
    //         but currently its adrp,add,br to the address directly
    Call: @struct(set_link: bool = true),
    // all 
    //  local: rebase.
    //         It's a local symbol but since we want its absolute address in memory, we need the loader to add our base address.  
    // import: bind.
    //         We want the absolute address of an import to be in memory. ie. __got uses this. 
    DataAbsolute: @struct(increment: i64),
    
    // amd64
    RipDisp32: @struct(
        increment: i64,
        call: bool, 
        got_load := false /*only used when .Relocatable*/
    ), // TODO: add safety checks for size of Qbe.Con.bits.i
    
    // wasm
    WasmIndex,
    WasmAddr: @struct(increment: u32, wide: bool),
);

Fixup :: @struct(
    patch_at: rawptr,
    type: FixupType,
);

fn reserve(s: *SegmentCursor, $T: Type) *T #generic = {
    space_left := s.mmapped.len - s.len(); 
    @assert_ge(space_left, T.size_of(), "not enough space in segment for reserve");
    ptr := s.next;
    @debug_assert(u8.int_from_ptr(ptr).mod(T.align_of()) == 0, "unaligned reserve");
    s.next = s.next.offset(T.size_of());
    ptr_cast_unchecked(u8, T, ptr)
}

fn len(s: *SegmentCursor) i64 = 
    ptr_diff(s.mmapped.ptr, s.next);

fn cap(s: *SegmentCursor) i64 = 
    s.mmapped.len;

// TODO: this is stupid
// only call this on the codegen thread!
fn set_library(m: *QbeModule, sym_id: Qbe.Sym, name: Str, weak: bool) void = {
    i := m.find_library(name) ||    
        m.new_library(name);
    
    use_symbol(m, sym_id) { s | 
        @debug_assert(s.library == 0 || s.library == i, "set_library(%) conflict % %", s.name, i, s.library);
        s.library = i;
        s.strong = s.strong || !weak;
        weak = !s.strong;
    };
    
    @if(enable_incremental()) if m.save { save |
        id := save.map_sym(m, sym_id);
        save.sym[id].segment = .Import;
        save.sym[id].imp.lib = (off = i - 1);
        save.sym[id].imp.weak = weak;
    };
}

fn set_library(m: *QbeModule, sym_id: Qbe.Sym, name: Str) void = set_library(m, sym_id, name, false);

fn new_library(m: *QbeModule, name: Str) u32 = {
    i: u32 = trunc(m.libraries.len);
    // :leak
    m.libraries&.push(name.shallow_copy(general_allocator()), general_allocator());
    
    @if(enable_incremental()) if m.save { save |
        save.lib&.push(name = save.push(name));
    };
    
    i
}

// :SLOW
fn find_library(m: *QbeModule, name: Str) ?u32 = {
    enumerate m.libraries { i, check |
        if name == check[] {
            return(Some = i.trunc());
        }
    };
    .None
}

// TODO: what about import stubs? 
// but also you only need to do this if you don't want WX memory so like meh. 
fn make_exec(self: *QbeModule) void = {
    ::enum(Arch); ::enum(@type self.goal.type);
    @debug_assert_eq(self.goal.arch, query_current_arch());
    @debug_assert_eq(self.goal.type, .JitOnly);
    if self.goal.expecting_wx {
        return();  // already executable so this is a nop
    };
    
    // TODO: this doesn't work with the new way i want to do stubs. shit. -- Oct 24
    segment := SegmentType.Code;
    prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
    segment := self.segments&.index(segment);
    end := segment.next&;
    len := self.start_of_writable_code.ptr_diff(end[]);
    if(len == 0, => return());
    PAGE_SIZE := page_size();
    segment.align_to(PAGE_SIZE);
    
    page_start := u8.int_from_ptr(self.start_of_writable_code) / PAGE_SIZE * PAGE_SIZE;
    
    // it seems to want `addr` to be page aligned
    res := Syscall'mprotect(rawptr_from_int(page_start), u8.int_from_ptr(end[]) - page_start, prot); 
    Posix :: import("@/lib/sys/posix.fr");
    @assert_eq(res.value, 0, "mprotect failed %", @if(::is_linking_libc(), Posix'errno()[], "??"));
    clear_instruction_cache(u8.raw_from_ptr(self.start_of_writable_code), u8.raw_from_ptr(end[]));  // IMPORTANT
    
    self.start_of_writable_code = end[];
}

fn get_addr(self: *QbeModule, id: Qbe.Sym) ?Ty(rawptr, i64) = {
    assert(!self.fixups_locked, "you can't get a jit pointer after emitting for aot becuase it stomps data with relocation info.");
    result: ?Ty(rawptr, i64) = .None;
    use_symbol(self, id) { symbol | 
        result = get_addr(self, symbol);
        if result.is_none() {
            msg :: "Tried to get_addr for uncompiled '%' because it was suspended for inlining. it should be marked as exported.";
            ::ptr_utils(Qbe.Fn);
            @debug_assert(symbol.inline.is_null(), msg, symbol.name);
        };
    };
    result
}

fn get_addr(self: *QbeModule, name: Str) ?rawptr = {
    if get_addr(self, self.intern(name)) { a |
        return(Some = a._0);
    };
    .None
}

fn get_addr(self: *QbeModule, symbol: *SymbolInfo) ?Ty(rawptr, i64) = {
    ::enum(SymbolKind);
    if(symbol.kind != .Local, => return(.None));
    segment := self.segments&[symbol.segment];
    addr    := segment.mmapped.ptr.offset(symbol.offset);
    (Some = (u8.raw_from_ptr(addr), symbol.size.zext()))
}

// TODO: add_asm functions don't go through this currently but they should
fn maybe_add_export(self: *QbeModule, id: Qbe.Sym, export: bool) void = if export {
    if (self.goal.arch == .wasm32 || @is(self.goal.type, .Relocatable, .Dynamic)) {
        self.exports&.push(id);
    };
    
    // this makes sure it will be emitted even if all uses are inlined. 
    use_symbol(self, id) { s | 
        mark_referenced(self, id, s);
        s.export = true;
    };
};

// TODO: can still use bss if there's only pointers and all data is zeros.
// TODO: be able to specify mutable or constant

Dat2 :: @struct {
    Reloc :: @struct(off: u32, id: Qbe.Sym, addend: i64 = 0);
    Template :: @tagged(Bytes: []u8, Zeroes: i64);
    template: Template;
    relocations: []Reloc; 
    align: u32 = 1;
    seg := SegmentType.MutableData;
    id: Qbe.Sym;
    export: bool = false;
};

// Reserve a chunk of memory in the module's data segment, fill it with the right bytes, and record the necessary relocations. 
// The memory in `dat` is not needed after this call, caller can free it. 
fn emit_data(self: *QbeModule, dat: Dat2) void = {
    assert(!self.fixups_locked, "you cannot add more data after emitting for aot");
    ::tagged(@type dat.template);
   
    //@print("% ", self.goal.type);
    //print_data(self, dat&, self.debug_out);
    //self.flush_debug();
    
    @if(enable_incremental())
    if self.save { save |
        each dat.relocations { r |
            use_symbol(self, r.id) { symbol | 
                mark_referenced(self, r.id, symbol);
            }
        };
        
        save.push(self, dat&);
        return();
    };
    
    if dat.template&.is(.Zeroes) && dat.relocations.len == 0 {
        // TODO: i want to remvoe this limitation so you can use this function
        //       to materialize things made by import_c or meta/parse. 
        //@debug_assert_ne(self.goal.type, .JitOnly, "TODO: the franca compiler doens't do this because the data's already in the comptime address space\nat the very least you need to mmap something for ZeroInitData before you can turn this on.");
        
        if self.goal.type == .JitOnly {
            @debug_assert_ne(dat.seg, .ZeroInitData, "no allocation for jitted bss");
        } else {
            dat.seg = .ZeroInitData;
        }
    };
    segment := self.segments&.index(dat.seg);

    // TODO: kinda dumb to pad short strings          :MiscompileCStr  -- May 11, 2025
    //       it seems like you only require 8 if there are relocations but it doesn't work
    //       suspicious that it breaks the tests that make the same string literal as a Str and a CStr
    //       yeah: `Expected (Hello Worldfail/usr/lib/libc.dylib == Hello World)`  
    // segment.align_to((@if(dat.relocations.len > 0, 8, 1)).max(dat.align.zext()));
    segment.align_to(8);
    
    next := segment.next&;
    start_off := segment.len();
    dest := @match(dat.template) {
        fn Zeroes(size) => next[].slice(size);  // it will already be zeroed since it's from the page allocator
        fn Bytes(bytes) => {
            dest := next[].slice(bytes.len);
            dest.copy_from(bytes);
            dest
        }
    };    
    next[] = next[].offset(dest.len);
    if self.goal.arch != .wasm32 {
        maybe_add_export(self, dat.id, dat.export);
        if self.goal.exe_debug_symbol_table {
            self.local_needs_reloc&.push(dat.id);
        };
    };
    self.do_jit_fixups(dat.id, dat.seg, start_off);
    
    each dat.relocations { r |
        @assert_eq(r.off.mod(8), 0, "sorry can't do unaligned constants yet");
        ptr := ptr_cast_unchecked(u8, i64, dest.index(r.off.zext()));
        use_symbol(self, r.id) { symbol | 
            mark_referenced(self, r.id, symbol);
            filled := false;
            if self.goal.type == .JitOnly {
                if !filled && !symbol.jit_addr.is_null() {
                    ptr[] = int_from_rawptr(symbol.jit_addr) + r.addend;
                    filled = true;
                };
                if !filled && !symbol.shim_addr.is_null() {
                    ptr[] = int_from_rawptr(symbol.shim_addr) + r.addend;
                    filled = true;
                }
            } else {
                ptr[] = 0;
            };
            // Since we need the absolute address in memory, even local symbols need a relocation. 
            push_fixup(self, symbol, (patch_at = i64.raw_from_ptr(ptr), type = (DataAbsolute = (increment = r.addend))));
            if self.goal.arch != .wasm32 && self.goal.type == .Relocatable && !symbol.local_but_marked_for_relocation {
                lock(self.intern_mutex&);
                self.local_needs_reloc&.push(r.id);
                unlock(self.intern_mutex&);
                symbol.local_but_marked_for_relocation = true;
            };
        };
    };
}

fn with_write_protect(m: *QbeModule, $body: @Fn() void) void = {
    xxx := m.need_write_protect;
    @if(xxx) apple_thread_jit_write_protect(false);
    body();
    @if(xxx) apple_thread_jit_write_protect(true);
}

MultiArchAsm :: EnumMap(Arch, []u8);

// :ThreadSafety we can't have both threads fucking with self.segments
fn add_code_bytes(m: *QbeModule, name_id: Qbe.Sym, code: *MultiArchAsm) void = {
    @if(enable_incremental())
    if m.save { save |
        save.push_asm(m, code, name_id);
        return();
    };
    
    bytes := code[m.goal.arch];
    
    use_symbol(m, name_id) { s |
        start_debug_info(m, s);
    };
    code := m.segments&[.Code]&;
    code.align_to(4);  // todo: only for arm
    start_offset := ptr_diff(code.mmapped.ptr, code.next);
    dest := slice(code.next, bytes.len);
    with_write_protect m {
        dest.copy_from(bytes);
    };
    code.next = code.next.offset(bytes.len);
    
    m.do_jit_fixups(name_id, .Code, start_offset);
    finish_function(m, dest, name_id);
}

fn finish_function(m: *QbeModule, dest: []u8, name_id: Qbe.Sym) void #inline = {
    // :SLOW
    lock(m.icache_mutex&);
    m.last_function_end = dest.ptr.offset(dest.len);
    use_symbol(m, name_id) { s |
        s.size = dest.len.trunc();
        end_debug_info(m, s);
    };
    
    if m.goal.exe_debug_symbol_table {
        m.local_needs_reloc&.push(name_id);
    };
    
    ::enum(@type m.goal.type);
    if m.goal.got_indirection_instead_of_patches && m.goal.type == .JitOnly && m.goal.arch == .aarch64 {
        clear_instruction_cache(u8.raw_from_ptr(dest.ptr), dest.len);
    };
    unlock(m.icache_mutex&);
}

// not a repro problem because the memory is already zeroed by page allocator
fn align_to(s: *SegmentCursor, align: i64) void = {
    extra := s.len().mod(align);
    if extra != 0 {
        s.next = s.next.offset(align - extra); 
    };
}

fn put_jit_addr(self: *QbeModule, name: Qbe.Sym, addr: rawptr) void = {
    use_symbol(self, name) { symbol |
        @assert(symbol.kind == .Pending, "(put_jit_addr) redeclared symbol %: %", name.id, symbol.name);
        symbol.kind = .DynamicPatched;
        self.do_fixups(addr, symbol);
    };
}

fn for_symbols(m: *QbeModule, $body: @Fn(id: Qbe.Sym, s: *SymbolInfo) void) void = {
    enumerate m.symbols& { h, bucket | 
        n := m.lock_bucket(bucket);
        syms := bucket.data.slice(0, n.zext());
        enumerate syms { i, symbol | 
            id := h + i.shift_left(IBits);
            body((id = trunc id), symbol);
        };
        bucket.n = n; // unlock
    };
}

// TODO: when doing aot we just have to ask for our segments to have the same spacing as they did now and then relative addressing will just work out. 
//       but for data constants with pointers we'll need to remember what relocations to ask the loader to do at runtime. 
//       would be great if my language had nice relative pointers and then you wouldn't have to deal with that as much
//
fn do_jit_fixups(self: *QbeModule, id: Qbe.Sym, segment: SegmentType, final_offset: i64) void = {
    inbounds := segment == .ZeroInitData || (final_offset >= 0 && final_offset < self.segments&[segment].mmapped.len);
    @assert(inbounds, "thats not in the segment");
    use_symbol(self, id) { symbol | 
        @debug_assert(int_from_ptr(@type symbol[], symbol) != 0, "fixups for a null symbol! (forgot to set Fn.lnk.id?)"); // only catches it if you haven't had something hash to 0 yet. 
        @assert(symbol.kind != .Local, "(do_jit_fixups) redeclared symbol %: % %[%]", id.id, symbol.name, segment, final_offset);
        symbol.segment = segment;
        symbol.offset = final_offset;
        symbol.kind = .Local;
        dest := self.segment_address(symbol.segment, symbol.offset);
        fixups := symbol.fixups&;
        self.do_fixups(dest, symbol);
        if self.goal.arch != .wasm32 {
            if self.goal.type == .JitOnly || self.goal.type == .Exe || self.goal.type == .Dynamic { //  || symbol.segment == .Code // :canireallynothavecodeberelative
                // Since it's a local symbol, we don't care about the relocations anymore. The linker never needs to know. 
                // TODO: actually thats also true for imports becuase calls go through __got. 
                // TODO: for jitonly you do need to keep __got for weak. 
                ::tagged(@type fixups[0].type);
                unordered_retain(fixups, fn(it) => it.type&.is(.DataAbsolute));
                //if(fixups.len == 0, => fixups.drop(self.gpa));
            } else {
                // you still care because the static linker wants to move things around. :track_got_reloc
                if !symbol.local_but_marked_for_relocation {
                    lock(self.intern_mutex&);
                    self.local_needs_reloc&.push(id);
                    unlock(self.intern_mutex&);
                    symbol.local_but_marked_for_relocation = true;
                };
            }; 
        };
    };
}

fn do_fixups(self: *QbeModule, address: rawptr, symbol: *SymbolInfo) void = {
    ::tagged(FixupType);
    with_write_protect self { 
        do_fixups_impl(self, address, symbol);
    };
}

fn do_fixups_impl(self: *QbeModule, address: rawptr, symbol: *SymbolInfo) void = {
    assert(!self.fixups_locked, "you cannot add more fixups after emitting for aot");
    ::enum(@type symbol.kind);
    @debug_assert_ne(symbol.kind, .Pending, "cannot do fixups of pending symbol");
    symbol.jit_addr = address;  
    if self.goal.type == .JitOnly {
        // TODO: should probably still have the frontend make a shim for weak symbols or it sucks to debug if you make a mistake. 
        @debug_assert(!symbol.strong || !address.is_null(), "do_fixups jit null symbol %", symbol.name);
    };
    
    if self.goal.arch == .wasm32 && symbol.segment == .Code && symbol.got_lookup_offset == -1 {
        // Need to wait until the end of compilation to know its index. 
        return();
    };
    
    fixups := symbol.fixups.items();
    if self.debug["T".char()] {
        @fmt_write(self.debug_out, "# % fixups for %\n", fixups.len, symbol.name);
    };
    new_got_reloc: ?Fixup = .None;
    fix := self.target.fixup;
    each fixups { fixup | 
        fix(self, symbol, fixup, new_got_reloc&);
    };
    if new_got_reloc { it | // avoided reallocating while iterating. rustc would be proud.
        push_fixup(self, symbol, it);
    };
}

fn ensure_got_slot(self: *QbeModule, symbol: *SymbolInfo, jit_address: rawptr) ?Fixup = {
    @debug_assert(!self.fixups_locked);
    new_got_reloc: ?Fixup = .None;
    if symbol.got_lookup_offset == -1 {
        // need to reserve a new __got entry.  :GotAtConstantDataBase 
        lock(self.icache_mutex&);
        symbol.got_lookup_offset = self.got&.len();
        got := self.got&.reserve(rawptr);  // :TodoErrorMessage
        unlock(self.icache_mutex&);
        if self.debug["T".char()] {
            @fmt_write(self.debug_out, "# reserve __got[%] = %\n", symbol.got_lookup_offset / 8, symbol.name);
        };
        new_got_reloc = (Some = (patch_at = rawptr.raw_from_ptr(got), type = (DataAbsolute = (increment = 0))));
        if !(self.goal.got_indirection_instead_of_patches && jit_address.is_null()) { // TODO: remove this condition
            got[] = jit_address; // because we don't loop again to handle `new_got_reloc`
        }
    };
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    got := rawptr.ptr_from_raw(got);
    if self.goal.got_indirection_instead_of_patches {
        if jit_address.is_null() {
            //@println("ensure_got_slot null %", symbol.name);
            got[] = @as(rawptr) (fn() Never = panic("ICE: tried to call got null jit_address"));
        } else {
            got[] = jit_address;
        };
    } else {
        @debug_assert_eq(got[], jit_address, "ensure_got_slot with different address");
    };
    new_got_reloc
}

fn emit_fn(f: *Qbe.Fn) void = {
    @if(enable_incremental())
    if f.globals.save { save | 
        save.push(f, true);
        return();
    };
    
    m := f.globals;
    code := m.segments&[.Code]&;
    start_offset := code.len();
    name_id := f.lnk.id;
    
    // We know we're emitting the function right here so we can do old fixups now. 
    maybe_add_export(m, name_id, f.lnk.export);
    m.do_jit_fixups(name_id, .Code, start_offset);
    
    use_symbol(m, name_id) { s |
        start_debug_info(m, s);
    };
    {m.target.emit_fn}(f);
    size := code.len() - start_offset;
    @assert_lt(code.len(), code.mmapped.len, "too much code");
    
    // :FuncSizeLimit 
    // b.cond gives you 19 bits, but encoded/4, but signed so that's 20 bits.  
    // you can remove this limit on the function but then have to add the check when emitting each branch. 
    @assert_lt(size, 1.shift_left(20), "we don't allow a function larger than one arm64 instruction can jump");
    
    code_bytes := code.mmapped.subslice(start_offset, size);
    finish_function(m, code_bytes, name_id);
    
    if m.debug["D".char()] {
        @fmt(m.debug_out, "# disassembly of '%' (% bytes)\n", m.str(f.lnk.id), code_bytes.len);
        dis := m.goal.dis;
        dis(m.debug_out, code_bytes, m.goal.arch);
    };
}

fn llvm_mc_dis(out: *List(u8), code: []u8, arch: Arch) void = {
    #use("@/lib/sys/subprocess.fr");
    #use("@/lib/sys/fs.fr");
    arch_name := @match(arch) {
        fn x86_64()  => "--arch=x86-64";
        fn aarch64() => "--arch=aarch64";
        fn wasm32()  => "--arch=wasm32";
        fn rv64()    => "--arch=riscv64";
    };
    hex: List(u8) = list(code.len * 5, temp());
    for code { byte |
        hex&.push_prefixed_hex_byte(byte);
        hex&.push_all(" ");
    };
    file := open_temp_file();
    file.fd&.write(hex.items());
    DIS :: "llvm-mc";
    // varient 1 means intel syntax. ignored when doing arm. 
    args := @slice(arch_name, "--disassemble", "-output-asm-variant=1", "--show-encoding", file&.s_name());
    _ok, o, e := exec_and_catch(DIS, args, temp());
    file.remove();
    out.push_all(o.items());
    out.push_all(e.items());
}

fn push_fixup(m: *QbeModule, symbol: *SymbolInfo, fix: Fixup) void #inline = {
    lock(m.intern_mutex&);
    a := m.forever&.borrow();
    symbol.fixups&.push(fix, a);
    unlock(m.intern_mutex&);
}

//////////////////////////////////
// Shared by native AOT targets //
//////////////////////////////////

fn main_entry_point_vaddr(m: *QbeModule) i64 = {
    @debug_assert_ne(m.goal.arch, .wasm32, "only makes sense for native targets");
    @debug_assert_ne(m.goal.type, .Dynamic, "shared libraries don't have an entry point");
    @debug_assert(m.fixups_locked, "this is not threadsafe");
    symbol := m.get_symbol_info(m.intern("main"));  // :HardcodeMain
    assert(symbol.kind == .Local, "no exported main function?");
    @debug_assert_ge(symbol.offset, m.goal.commands_size_guess + m.goal.size_of_stubs);
    symbol.offset
}

fn patch_pending_symbols(m: *QbeModule) void = {
    @debug_assert_ne(m.goal.arch, .wasm32, "only makes sense for native targets");
    // fill_from_libc is wrong because you might be cross compiling and disagree about what things exist. 
    for_symbols m { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending && symbol.fixups.len != 0 {
            m.imports&.push(id); 
            symbol.kind = .DynamicPatched;
            m.do_fixups(rawptr_from_int(0), symbol);
        };
    };
}

fn debug_log_byte_size(m: *QbeModule) void = {
    // TODO: remove
    code_count := m.segments&[.Code]&.len() - m.goal.commands_size_guess - m.goal.size_of_stubs;
    data_count := m.segments&[.MutableData]&.len();
    @if(show_backend_stats())
    @eprintln(">>> % bytes of code, % bytes of data.", code_count, data_count); 
}

fn live_segment_part(m: *QbeModule, s: SegmentType) []u8 = {
    s := m.segments&[s]&;
    s.mmapped.slice(0, ptr_diff(s.mmapped.ptr, s.next))
}

fn seal_debug_info(m: *QbeModule, debug_source_code: []u8, include: bool, files_bytes: []u8) void = {
    base_p    := m.intern("__franca_base_address");
    
    if m.goal.type == .Exe {
        // TODO: figure out how to make this work for .Relocatable
        //       it is needed for relocations without libc on linux (in franca_runtime_init). 
        m.do_jit_fixups(base_p, .Code, 0);
    };
    
    ::[]u64;
    if @is(m.goal.type, .Cached, .CachedEarly, .Relocatable) {
        b := FRHOSTED_MAGIC;
        m.emit_data(id = base_p, template = (Bytes = b&.slice(1).interpret_as_bytes()), align = 8, relocations = empty());
    };
    
    AotDebugInfo :: import("@/lib/crash_report.fr").AotDebugInfo;
    
    id := m.intern("__franca_aot_debug_info");
    if !include || @is(m.goal.type, .Relocatable, .Cached, .CachedEarly) {
        m.emit_data(id = id, template = (Zeroes = size_of(AotDebugInfo)), align = 8, relocations = empty());
        return();
    };
    // TODO: figure out how to make this work for .Relocatable
    
    code := m.segments&.index(.Code);
    data: AotDebugInfo = (
        code_segment = (ptr = zeroed(*u8), len = ptr_diff(code.mmapped.ptr, code.next) - m.text_padding()),
        headers = m.debug_info.headers.items(),
        payload = m.debug_info.data.items(),
        source = debug_source_code,
        files = files_bytes,
        nullable_cached_codemap = zeroed rawptr,
    );
    // TODO: don't force having a name for these. 
    names := @const_slice("__franca_code_segment", "__franca_debug_headers", "__franca_debug_data", "__franca_debug_source", "__franca_debug_files");

    // `code_segment` doesn't point to new data, it's already in the exe
    start_id := m.intern("__franca_code_segment");
    use_symbol(m, start_id) { s |
        s.segment = .Code;
        s.offset = m.text_padding();
        s.kind = .Local;
        s.jit_addr = u8.raw_from_ptr(code.mmapped.ptr.offset(m.text_padding()));
        mark_referenced(m, start_id, s);
    };
    
    i := 0;
    r := temp().alloc(Dat2.Reloc, 5);
    inline_for get_fields(AotDebugInfo) { $f | 
        @if(::(size_of(f[].ty) == 16)) {
            // a relocation for the data pointer of this field
            r[i] = (id = m.intern(names[i]), off = f[].byte_offset.trunc());
            if i > 0 { // skip code_segment since that's already in the executable 
                // a seperate symbol for the raw bytes this field references
                m.emit_data(id = r[i].id, template = (Bytes = AotDebugInfo.get_field_ptr(data&, f)[]), align = 8, relocations = empty());
            };
            i += 1;
        };
    };
    ::[]AotDebugInfo;
    m.emit_data(id = id, template = (Bytes = data&.slice(1).interpret_as_bytes()), align = 8, relocations = r);
}

#use("@/compiler/codemap.fr");
fn encode_debug_files(files: []File) Ty([]u8, []u8) = {
    source := u8.list(temp());
    headers := u8.list(temp());
    leb128_unsigned(headers&, files.len);
    for files { file | 
        source&.push_all(file.content);
        leb128_unsigned(headers&, file.content.len);
        leb128_unsigned(headers&, file.name.len);
        push_all(headers&, file.name);
    };
    (source.items(), headers.items())
}

fn start_debug_info(m: *QbeModule, s: *SymbolInfo) void = {
    d := m.debug_info&;
    @if(!d.enabled) return();
    leb128_unsigned(d.headers&, d.data.len);
    leb128_unsigned(d.data&, s.name.len);
    push_all(d.data&, s.name);
    d.last_off_func = 0;
    d.last_off_source = 0;
}

fn end_debug_info(m: *QbeModule, s: *SymbolInfo) void = {
    d := m.debug_info&;
    if m.goal.exe_debug_symbol_table {
        leb128_unsigned(d.macho_function_starts&, s.size.zext());
    };
    @if(!d.enabled) return();
    leb128_unsigned(d.headers&, s.size.zext());
    // TODO: length prefix but that's annoying
    leb128_unsigned(d.data&, 0);
    leb128_unsigned(d.data&, 0);
}

fn add_debug_info(m: *QbeModule, i: *Qbe.Ins, off_in_func: i64) void = {
    d := m.debug_info&;
    @if(!d.enabled) return();
    hi, lo := (rsval(i.arg&[0]).intcast(), rsval(i.arg&[1]).intcast());
    offset_in_source := hi.shift_left(16).bit_or(lo);
    leb128_unsigned(d.data&, off_in_func - d.last_off_func);
    d.last_off_func = off_in_func;
    leb128_signed(d.data&, offset_in_source - d.last_off_source);
    d.last_off_source = offset_in_source;
}

// TODO: collect_aot_fixups compiles if you typo to *Symbol, :FUCKED
FixP :: @struct(symbol: *SymbolInfo, fix: *Fixup);
fn collect_aot_fixups(m: *QbeModule) Ty([]FixP, []*SymbolInfo) = {
    // :SLOW keep a list because most will probably be local with no relocations. 
    fixups := list(FixP, temp());
    symbols := list(*SymbolInfo, temp());
    
    symbol_count := 0;
    for_symbols m { id, s | 
        first := true;
        each s.fixups& { f | 
            ::tagged(@type f.type);
            if f.type&.is(.DataAbsolute) { 
                fixups&.push(symbol = s, fix = f);
                if first {
                    first = false;
                    symbols&.push(s);
                }
            };
        };
    };
    @debug_assert_ge(symbols.len, m.imports.len, "more imports than symbols!"); 
    (fixups.items(), symbols.items())
}

Patch :: @struct(offset_from_start: i64, cond: i32, target_bid: i32);

fn store_op(k: Qbe.Cls) Qbe.O = {
    k := @as(i32) k;
    @debug_assert_lt(k, 4);
    @as(Qbe.O) @as(i32) Qbe.O.storew.raw() + k  // :StoreOrder
}

fn segment_address(self: *QbeModule, segment: SegmentType, offset_in_segment: i64) rawptr = {
    data := self.segments&[segment]&;
    u8.raw_from_ptr(data.mmapped.ptr.offset(offset_in_segment))
}

// This will get more complicated when i can have multiple of the same segment. 
fn distance_between(m: *QbeModule, start_segment: SegmentType, start_offset: i64, end_segment: SegmentType, end_offset: i64) i64 = {
    @debug_assert(end_offset >= 0 && start_offset >= 0);
    ptr_diff(m.segment_address(start_segment, start_offset), m.segment_address(end_segment, end_offset))
}

fn current_offset(m: *QbeModule, segment: SegmentType) i64 = {
    ptr_diff(m.segments&[segment].mmapped.ptr, m.segments&[segment].next)
}

fn align_to(m: *QbeModule, segment: SegmentType, align: i64) void = {
    m.segments&.index(segment).align_to(align);
}
