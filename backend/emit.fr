SegmentCursor :: @struct(
    mmapped: []u8,
    next: *u8,
);

SegmentType :: @enum(i64) (
    Code,
    ConstantData,
    MutableData,
    ZeroInitData,
);

SymbolBucket :: @struct(
    n: u32,
    data: QList(SymbolInfo),
);

// TODO: make this less chunky
SymbolInfo :: @struct(
    name: Str,
    inline: *Qbe.Fn, // nullable
    fixups: RawList(Fixup),
    // or -1 if not referenced yet. valid: DynamicPatched
    // this gets repurposed when fixups_locked. 
    // when targetting wasm this is the index of the function (need to offset by import count if imported)
    got_lookup_offset: i64,
    segment: SegmentType, // valid: Local
    //  local: offset in the segment
    // import: offset in stubs to trampoline (if function)
    offset: i64,
    kind: SymbolKind,
    local_but_marked_for_relocation: bool,
    is_thread_local: bool,  
    referenced: bool,  // took address or called before knowing it was inlinable so you must actually emit the function.
    jit_addr: rawptr, // valid when kind == .DynamicPatched and goal == .JitOnly
    size: i64,  // only set for functions currently. 
    shim_addr: rawptr, // TODO: it would be great if i didn't need this
    wasm_type_index: i32,
    //wasm_space: Wasm.ImportType, // TODO: chunky
);

::enum_basic(SymbolKind);

// TODO: deal with matching on an @enum(u8) on llvm
SymbolKind :: @enum(i64) (
    // We haven't compiled this yet so we've emitted references as lookups into a table in the data section. 
    // 
    // It might also be a dynamic import. 
    // - For aot, you may want to go through the fixups again and translate them to use a GOT section. 
    //   That way you don't make the loader waste time patching two tables. 
    //
    Pending,
    // We've already compiled this and we know its close to the code and addressable by a static offset. 
    Local,
    // This is an import and we've already patched it in the code. 
    DynamicPatched,
);

FixupType :: @tagged(
    //  local: adrp,add to the address directly
    // import: ??
    InReg: @struct(r: i64, increment: u32, got_load := false),
    //  local: bl to the address
    // import: bl to a trampoline in __stubs which adrp,ldr,br from __got
    //         but currently its adrp,add,br to the address directly
    Call,
    //  local: rebase.
    //         It's a local symbol but since we want its absolute address in memory, we need the loader to add our base address.  
    // import: bind.
    //         We want the absolute address of an import to be in memory. ie. __got uses this. 
    DataAbsolute: @struct(increment: i64),
    
    // amd64
    RipDisp32: @struct(increment: u32, call: bool), // TODO: add safety checks for size of Qbe.Con.bits.i
    
    // wasm
    WasmIndex,
    WasmAddr: @struct(increment: u32, wide: bool),
);

Fixup :: @struct(
    patch_at: *u8,
    type: FixupType,
);

fn reserve(s: *SegmentCursor, $T: Type) *T #generic = {
    space_left := s.mmapped.len - s.len(); 
    @assert_ge(space_left, T.size_of(), "not enough space in segment for reserve");
    ptr := s.next;
    @debug_assert(u8.int_from_ptr(ptr).mod(T.align_of()) == 0, "unaligned reserve");
    s.next = s.next.offset(T.size_of());
    ptr_cast_unchecked(u8, T, ptr)
}

fn len(s: *SegmentCursor) i64 = 
    ptr_diff(s.mmapped.ptr, s.next);

fn cap(s: *SegmentCursor) i64 = 
    s.mmapped.len;

// TODO: calling this multiple times doesn't play nice with import stubs. 
// but also you only need to do this if you don't want WX memory so like meh. 
fn make_exec(self: *QbeModule) void = {
    // TODO: this doesn't work with the new way i want to do stubs. shit. -- Oct 24
    segment := SegmentType.Code;
    prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
    segment := self.segments&.index(segment);
    end := segment.next&;
    len := self.start_of_writable_code.ptr_diff(end[]);
    if(len == 0, => return());
        
    PAGE_SIZE :: 16384; // TODO: ask the os for this. we're wasting hella memory on linux. 
    page_start := u8.int_from_ptr(end[]) / PAGE_SIZE * PAGE_SIZE;
    end[] = u8.ptr_from_int(page_start + PAGE_SIZE);
    self.start_of_writable_code = end[];
    
    // it seems to want `addr` to be page aligned
    res := mprotect(rawptr_from_int(page_start), u8.int_from_ptr(end[]) - page_start, prot); 
    @assert_eq(res.value, 0, "mprotect failed %", get_errno());
    clear_instruction_cache(u8.raw_from_ptr(self.start_of_writable_code), u8.raw_from_ptr(end[]));  // IMPORTANT
}

fn get_addr(self: *QbeModule, id: u32) ?Ty(rawptr, i64) = {
    assert(!self.fixups_locked, "you can't get a jit pointer after emitting for aot becuase it stomps data with relocation info.");
    result: ?Ty(rawptr, i64) = .None;
    use_symbol(self, id) { symbol | 
        result = get_addr(self, symbol);
    };
    result
}

fn get_addr(self: *QbeModule, name: Str) ?rawptr = {
    if get_addr(self, self.intern(name)) { a |
        return(Some = a._0);
    };
    .None
}

fn get_addr(self: *QbeModule, symbol: *SymbolInfo) ?Ty(rawptr, i64) = {
    if(symbol.kind != .Local, => return(.None));
    segment := self.segments&[symbol.segment];
    addr    := segment.mmapped.ptr.offset(symbol.offset);
    (Some = (u8.raw_from_ptr(addr), symbol.size))
}

fn maybe_add_export(self: *QbeModule, id: u32, export: bool) void = {
    if export && (self.goal.arch == .wasm32 || @is(self.goal.type, .Relocatable, .Dynamic)) {
        self.exports&.push(id);
    };
}

// TODO: can still use bss if there's only pointers and all data is zeros.
// TODO: instead of making frontend call this specifically, have the other 
//       one decide. so do_jit_fixups on DEnd instead and switch?
fn new_zeroed_symbol(self: *QbeModule, dat: *Qbe.Dat) void = {
    // the advantage of using bss is that you don't have to fill your executable with zeros
    // if you have large static arrays. when jitting, we need to use memory anyway so just 
    // treat it like normal data. 
    if self.goal.type == .JitOnly {
        dat.type = .DStart;
        self.new_emit_data(dat);
        dat.type = .DZ;
        self.new_emit_data(dat);
        dat.type = .DEnd;
        self.new_emit_data(dat);
    } else {
        seg := SegmentType.ZeroInitData;
        segment := self.segments&.index(seg);
        next := segment.next&;
        segment.align_to(8);
        maybe_add_export(self, dat.lnk.id, dat.lnk.export);
        off := segment.mmapped.ptr.ptr_diff(next[]);
        self.do_jit_fixups(dat.lnk.id, seg, off);
        next[] = next[].offset(dat.u.num);
    };
}

// TODO: be able to specify mutable or constant
fn new_emit_data(self: *QbeModule, dat: *Qbe.Dat) void = {
    ::enum(@type dat.type);
    
    if self.debug["P".char()] {
        print_dat(self, dat, self.debug_out);
    };
    emit := self.target.data;
    emit(self, dat);
}

fn emit_data_to_segment(self: *QbeModule, dat: *Qbe.Dat) void = 
    emit_data_to_segment(self, dat, .MutableData);

fn emit_data_to_segment(self: *QbeModule, dat: *Qbe.Dat, seg: SegmentType) void = {
    assert(!self.fixups_locked, "you cannot add more data after emitting for aot");
    data_segment := self.segments&.index(seg);
    segment := data_segment;
    next := segment.next&;
    ::enum(@type dat.type);
    @match(dat.type) {
        fn DZeroInit() => self.new_zeroed_symbol(dat);
        fn DStart() => {
            segment.align_to(8);
            id := dat.lnk.id;
            maybe_add_export(self, id, dat.lnk.export);
            off := segment.mmapped.ptr.ptr_diff(next[]);
            self.do_jit_fixups(id, seg, off);
        }
        fn DEnd() => {
            @assert_lt(ptr_diff(segment.mmapped.ptr, next[]), segment.mmapped.len, "too much data");  
        };
        fn DZ() => {
            next[].slice(dat.u.num).set_zeroed();
            next[] = next[].offset(dat.u.num);
        }
        fn DB() => {
            @debug_assert(!dat.is_ref, "can't have a byte sized pointer");
            len := if dat.is_str {
                s := dat.u.str;
                if dat.has_quotes_and_escapes {
                    // When building ir manually you'll probably just give us the raw bytes.
                    // But this makes it easy to run Qbe's text based tests. Really the parser should handle this. 
                    q :: "\"".ascii();
                    @debug_assert(s[0] == q && s[s.len - 1] == q, "strings are quoted");
                    s = s.slice(1, s.len - 1); // quotes
                    space_left := segment.mmapped.len - ptr_diff(segment.mmapped.ptr, next[]);
                    @assert(space_left > 0, "data segment full");
                    dest: List(u8) = (maybe_uninit = (ptr = next[], len = space_left), len = 0, gpa = panicking_allocator);
                    expand_string_escapes(s, dest&);
                    dest.len
                } else {
                    (@as([]u8) (ptr = next[], len = s.len)).copy_from(s);
                    s.len
                }
            } else {
                next[][] = dat.u.num.trunc();
                1
            };
            next[] = next[].offset(len);
        }
        fn DL() => {
            @assert_eq(u8.int_from_ptr(next[]).mod(8), 0, "sorry can't do unaligned constants yet");
            ptr := ptr_cast_unchecked(u8, i64, next[]);
            if dat.is_ref {
                id := dat.u.ref.id;
                use_symbol(self, id) { symbol | 
                    mark_referenced(self, id, symbol);
                    if self.get_addr(symbol) { value |
                        ptr[] = int_from_rawptr(value._0) + dat.u.ref.off;
                    };
                    // Since we need the absolute address in memory, even local symbols need a relocation. 
                    push_fixup(self, symbol, (patch_at = next[], type = (DataAbsolute = (increment = dat.u.ref.off))));
                    if self.goal.arch != .wasm32 && self.goal.type == .Relocatable && !symbol.local_but_marked_for_relocation {
                        @if(use_threads) pthread_mutex_lock(self.intern_mutex&).unwrap();
                        self.local_needs_reloc&.push(id);
                        @if(use_threads) pthread_mutex_unlock(self.intern_mutex&).unwrap();
                        symbol.local_but_marked_for_relocation = true;
                    };
                };
            } else {
                ptr[] = dat.u.num;
            };
            next[] = next[].offset(8);
        }
        fn DW() =>  {
            @debug_assert(!dat.is_ref, "can't have a w sized pointer");
            @assert_eq(u8.int_from_ptr(next[]).mod(4), 0, "sorry can't do unaligned constants yet");
            ptr_cast_unchecked(u8, u32, next[])[] = dat.u.num.trunc();
            next[] = next[].offset(4);
        }
        fn DH() =>  {
            @debug_assert(!dat.is_ref, "can't have a h sized pointer");
            @assert_eq(u8.int_from_ptr(next[]).mod(2), 0, "sorry can't do unaligned constants yet");
            ptr_cast_unchecked(u8, u16, next[])[] = dat.u.num.trunc();
            next[] = next[].offset(2);
        }
    } // returns
}

// :ThreadSafety we can't have both threads fucking with self.segments
fn add_code_bytes(m: *QbeModule, name_id: u32, bytes: []u8) void = {
    use_symbol(m, name_id) { s |
        start_debug_info(m, s);
    };
    code := m.segments&[.Code]&;
    code.align_to(4);  // todo: only for arm
    start_offset := ptr_diff(code.mmapped.ptr, code.next);
    dest := slice(code.next, bytes.len);
    dest.copy_from(bytes);
    code.next = code.next.offset(bytes.len);
    
    m.do_jit_fixups(name_id, .Code, start_offset);
    finish_function(m, dest, name_id);
}

// this sure does something. please run it so many times before trying to remove.
fn fence() void #asm #aarch64 = (
    @bits(0b11010101000000110011, 0b1111, 0b10111111),  // DMB SY
    ret()
);

// does this do anything? idk man. 
fn fence() void #asm #x86_bytes = (fn(out) = @asm_x64(
     0x0F, 0xAE, 0xF0, // MFENCE
    // 0x0F, 0x01, 0xE8, // SERIALIZE rosetta no like?
    
    //encode_bin(PrimaryOp.MovReg, X86Reg.r11, X86Reg.rbx),
    //encode_imm(X86Reg.rax, 0), 0x0F, 0xA2, // CPUID
    //encode_bin(PrimaryOp.MovReg, X86Reg.rbx, X86Reg.r11), // restore
    
    PrimaryOp.Ret
) out);

fn finish_function(m: *QbeModule, dest: []u8, name_id: u32) void #inline = {
    // :SLOW
    @if(use_threads) pthread_mutex_lock(m.icache_mutex&).unwrap();
    m.last_function_end = dest.ptr.offset(dest.len);
    use_symbol(m, name_id) { s |
        s.size = dest.len;
        end_debug_info(m, s);
    };
    if m.got_indirection_instead_of_patches && m.goal.type == .JitOnly && m.goal.arch == .aarch64 {
        clear_instruction_cache(u8.raw_from_ptr(dest.ptr), dest.len);
    };
    fence();
    @if(use_threads) pthread_mutex_unlock(m.icache_mutex&).unwrap();
}

fn align_to(s: *SegmentCursor, align: i64) void = {
    extra := s.len().mod(align);
    if extra != 0 {
        s.next = s.next.offset(align - extra); 
    };
}

fn put_jit_addr(self: *QbeModule, name: u32, addr: rawptr) void = {
    use_symbol(self, name) { symbol |
        @assert(symbol.kind == .Pending, "(put_jit_addr) redeclared symbol %: %", name, symbol.name);
        symbol.kind = .DynamicPatched;
        symbol.jit_addr = addr;
        self.do_fixups(u8.ptr_from_raw(addr), symbol);
    };
}

// :SLOW insanely inefficient way of doing this. especially when someone tries to add multiple libs. 
// TODO: store a list of the pending ones so don't have to iterate them all. 
// This is not what you want if you care about cross compiling!
fn fill_pending_dynamic_imports(self: *QbeModule, lib: DlHandle) void = {
    buf := u8.list(temp());
    for_symbols self { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending {
            self.imports&.push(id); 
            buf&.clear();
            buf&.push_all(symbol.name);
            buf&.push(0);
            s: CStr = (ptr = buf.maybe_uninit.ptr);
            found := lib.dlsym(s);
            if !found.is_null() {
                symbol.kind = .DynamicPatched;
                self.do_fixups(u8.ptr_from_raw(found), symbol);
            };
        };
    };
}

fn for_symbols(m: *QbeModule, $body: @Fn(id: u32, s: *SymbolInfo) void) void = {
    enumerate m.symbols& { h, bucket | 
        n := m.lock_bucket(bucket);
        syms := bucket.data.slice(0, n.zext());
        enumerate syms { i, symbol | 
            id := h + i.shift_left(IBits);
            body(trunc(id), symbol);
        };
        bucket.n = n; // unlock
    };
}

// This is not what you want if you care about cross compiling!
fn fill_from_libc(self: *QbeModule) void = {
    for find_os_libc_dylib() { libc_path | 
        libc_path := libc_path.maybe_borrow_cstr(temp());
        libc := dlopen(libc_path, .Lazy);
        if !libc.lib.is_null() { 
            self.fill_pending_dynamic_imports(libc);
        };
    };
}

// TODO: when doing aot we just have to ask for our segments to have the same spacing as they did now and then relative addressing will just work out. 
//       but for data constants with pointers we'll need to remember what relocations to ask the loader to do at runtime. 
//       would be great if my language had nice relative pointers and then you wouldn't have to deal with that as much
//
fn do_jit_fixups(self: *QbeModule, id: u32, segment: SegmentType, final_offset: i64) void = {
    @if(segment != .ZeroInitData) @assert(final_offset >= 0 && final_offset < self.segments&[segment].mmapped.len, "thats not in the segment");
    use_symbol(self, id) { symbol | 
        @debug_assert(int_from_ptr(@type symbol[], symbol) != 0, "fixups for a null symbol! (forgot to set Fn.lnk.id?)"); // only catches it if you haven't had something hash to 0 yet. 
        @assert(symbol.kind != .Local, "(do_jit_fixups) redeclared symbol %: %", id, symbol.name);
        symbol.segment = segment;
        symbol.offset = final_offset;
        symbol.kind = .Local;
        dest := self.segment_address(symbol.segment, symbol.offset);
        fixups := symbol.fixups&;
        if self.goal.type == .JitOnly && self.got_indirection_instead_of_patches && !symbol.shim_addr.is_null() {
            // :InsaneCaches
            //@assert(fixups.len == 1); // not true when shim + bounce. 
            //fixups.drop(self.gpa); 
            fixups[] = zeroed(@type fixups[]);
            // don't return here. do_fixups sets jit_addr
        };
        self.do_fixups(dest, symbol);
        if self.goal.arch != .wasm32 {
            if self.goal.type == .JitOnly || self.goal.type == .Exe || self.goal.type == .Dynamic { //  || symbol.segment == .Code // :canireallynothavecodeberelative
                // Since it's a local symbol, we don't care about the relocations anymore. The linker never needs to know. 
                // TODO: actually thats also true for imports becuase calls go through __got. 
                // TODO: for jitonly you do need to keep __got for weak. 
                unordered_retain(fixups, fn(it) => it.type&.is(.DataAbsolute));
                //if(fixups.len == 0, => fixups.drop(self.gpa));
            } else {
                // you still care because the static linker wants to move things around. :track_got_reloc
                if !symbol.local_but_marked_for_relocation {
                    @if(use_threads) pthread_mutex_lock(self.intern_mutex&).unwrap();
                    self.local_needs_reloc&.push(id);
                    @if(use_threads) pthread_mutex_unlock(self.intern_mutex&).unwrap();
                    symbol.local_but_marked_for_relocation = true;
                };
            }; 
        };
    };
}
::tagged(FixupType);

fn do_fixups(self: *QbeModule, address: *u8, symbol: *SymbolInfo) void = {
    assert(!self.fixups_locked, "you cannot add more fixups after emitting for aot");
    symbol.jit_addr = u8.raw_from_ptr(address);  
    if self.goal.type == .JitOnly {
        @debug_assert(!address.is_null(), "do_fixups jit null symbol %", symbol.name);
    };
    
    if self.goal.arch == .wasm32 && symbol.segment == .Code && symbol.got_lookup_offset == -1 {
        // Need to wait until the end of compilation to know its index. 
        return();
    };
    
    fixups := symbol.fixups.items();
    new_got_reloc: ?Fixup = .None;
    fix := self.target.fixup;
    each fixups { fixup | 
        fix(self, symbol, fixup, new_got_reloc&);
    };
    if new_got_reloc { it | // avoided reallocating while iterating. rustc would be proud.
        push_fixup(self, symbol, it);
    };
}

fn ensure_got_slot(self: *QbeModule, symbol: *SymbolInfo, jit_address: rawptr) ?Fixup = {
    @debug_assert(!self.fixups_locked);
    new_got_reloc: ?Fixup = .None;
    if symbol.got_lookup_offset == -1 {
        // need to reserve a new __got entry.  :GotAtConstantDataBase 
        @if(use_threads) pthread_mutex_lock(self.icache_mutex&).unwrap();
        symbol.got_lookup_offset = self.got&.len();
        got := self.got&.reserve(rawptr);  // :TodoErrorMessage
        @if(use_threads) pthread_mutex_unlock(self.icache_mutex&).unwrap();
        if self.debug["T".char()] {
            @fmt_write(self.debug_out, "# reserve __got[%] = %\n", symbol.got_lookup_offset / 8, symbol.name);
        };
        new_got_reloc = (Some = (patch_at = ptr_cast_unchecked(rawptr, u8, got), type = (DataAbsolute = (increment = 0))));
        if !(self.got_indirection_instead_of_patches && jit_address.is_null()) { // TODO: remove this condition
            got[] = jit_address; // because we don't loop again to handle `new_got_reloc`
        }
    };
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    got := ptr_cast_unchecked(u8, rawptr, got);
    if self.got_indirection_instead_of_patches {
        if jit_address.is_null() {
            //@println("missing jit import? % % % %", symbol.name, symbol.jit_addr, got[], symbol.got_lookup_offset);
        } else {
            got[] = jit_address;
        };
    } else {
        @debug_assert_eq(got[], jit_address, "ensure_got_slot with different address");
    };
    new_got_reloc
}

fn emit_fn(f: *Qbe.Fn) void = {
    m := f.globals;
    code := m.segments&[.Code]&;
    start_offset := code.len();
    name_id := f.lnk.id;
    
    // We know we're emitting the function right here so we can do old fixups now. 
    maybe_add_export(m, name_id, f.lnk.export);
    m.do_jit_fixups(name_id, .Code, start_offset);
    
    use_symbol(m, name_id) { s |
        start_debug_info(m, s);
    };
    {m.target.emit_fn}(f);
    size := code.len() - start_offset;
    @assert_lt(code.len(), code.mmapped.len, "too much code");
    
    // :FuncSizeLimit 
    // b.cond gives you 19 bits, but encoded/4, but signed so that's 20 bits.  
    // you can remove this limit on the function but then have to add the check when emitting each branch. 
    @assert_lt(size, 1.shift_left(20), "we don't allow a function larger than one arm64 instruction can jump");
    
    code_bytes := code.mmapped.subslice(start_offset, size);
    finish_function(m, code_bytes, name_id);
    
    if m.debug["D".char()] {
        @fmt(m.debug_out, "# disassembly of '%' (% bytes)\n", m.str(f.lnk.id), code_bytes.len);
        dis := m.dis;
        dis(m.debug_out, code_bytes, m.goal.arch);
    };
}

fn llvm_mc_dis(out: *List(u8), code: []u8, arch: Arch) void = {
    arch_name := @match(arch) {
        fn x86_64()  => "--arch=x86-64";
        fn aarch64() => "--arch=aarch64";
        fn wasm32()  => "--arch=wasm32";
    };
    hex: List(u8) = list(code.len * 5, temp());
    for code { byte |
        hex&.push_prefixed_hex_byte(byte);
        hex&.push_all(" ");
    };
    file := open_temp_file();
    file.fd&.write(hex.items());
    DIS :: "llvm-mc";
    // varient 1 means intel syntax. ignored when doing arm. 
    args := @slice(arch_name, "--disassemble", "-output-asm-variant=1", "--show-encoding", file&.s_name());
    _ok, o, e := exec_and_catch(DIS, args, temp());
    file.remove();
    out.push_all(o.items());
    out.push_all(e.items());
}

fn push_fixup(m: *QbeModule, symbol: *SymbolInfo, fix: Fixup) void #inline = {
    @if(use_threads) pthread_mutex_lock(m.intern_mutex&).unwrap();
    a := m.forever&.borrow();
    symbol.fixups&.push(fix, a);
    @if(use_threads) pthread_mutex_unlock(m.intern_mutex&).unwrap();
}

//////////////////////////////////
// Shared by native AOT targets //
//////////////////////////////////

fn main_entry_point_vaddr(m: *QbeModule) i64 = {
    @debug_assert_ne(m.goal.arch, .wasm32, "only makes sense for native targets");
    @debug_assert_ne(m.goal.type, .Dynamic, "shared libraries don't have an entry point");
    @debug_assert(m.fixups_locked, "this is not threadsafe");
    symbol := m.get_symbol_info(m.intern("main"));
    assert(symbol.kind == .Local, "no exported main function?");
    @debug_assert_ge(symbol.offset, COMMANDS_SIZE_GUESS + SIZE_OF_STUBS);
    symbol.offset
}

fn patch_pending_symbols(m: *QbeModule) void = {
    @debug_assert_ne(m.goal.arch, .wasm32, "only makes sense for native targets");
    // fill_from_libc is wrong because you might be cross compiling and disagree about what things exist. 
    for_symbols m { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending && symbol.fixups.len != 0 {
            m.imports&.push(id); 
            symbol.kind = .DynamicPatched;
            m.do_fixups(u8.ptr_from_int(0), symbol);
        };
    };
}

fn debug_log_byte_size(m: *QbeModule) void = {
    // TODO: remove
    code_count := m.segments&[.Code]&.len() - COMMANDS_SIZE_GUESS - SIZE_OF_STUBS;
    data_count := m.segments&[.MutableData]&.len();
    @if(show_backend_stats())
    @eprintln(">>> % bytes of code, % bytes of data.", code_count, data_count); 
}

fn live_segment_part(m: *QbeModule, s: SegmentType) []u8 = {
    s := m.segments&[s]&;
    s.mmapped.slice(0, ptr_diff(s.mmapped.ptr, s.next))
}

fn declare_bytes(m: *QbeModule, bytes: []u8, name: Str) u32 = {
    dat := Qbe.Dat.zeroed();
    lnk := Qbe.Lnk.zeroed();
    dat.lnk = lnk&;
    dat.lnk.id = m.intern(name);
    dat.type = .DStart;
    m.new_emit_data(dat&);
    dat.type = .DB;
    dat.u.str = bytes;
    dat.is_str = true;
    m.new_emit_data(dat&);
    dat.type = .DEnd;
    m.new_emit_data(dat&);
    dat.lnk.id
}

fn seal_debug_info(m: *QbeModule, debug_source_code: []u8, include: bool, files_bytes: []u8) void = {
    dat := Qbe.Dat.zeroed();
    lnk := Qbe.Lnk.zeroed();
    dat.lnk = lnk&;
    dat.lnk.id = m.intern("__franca_aot_debug_info");
    if !include {
        dat.u.num = size_of(AotDebugInfo);
        m.new_zeroed_symbol(dat&);
        return();
    };
    
    // TODO: don't force having a name for these. 
    code_p    := m.intern("__franca_code_segment");
    m.do_jit_fixups(code_p, .Code, text_padding);
    header_p  := m.declare_bytes(m.debug_headers.items(), "__franca_debug_headers");
    payload_p := m.declare_bytes(m.debug_data.items(), "__franca_debug_data");
    source_p  := m.declare_bytes(debug_source_code, "__franca_debug_source");
    files_p   := m.declare_bytes(files_bytes, "__franca_debug_files");
    
    dat.type = .DStart;
    m.new_emit_data(dat&);
    code := m.segments&.index(.Code);
    ::assert_eq(size_of(AotDebugInfo), size_of(i64) * 12);
    dat.type = .DL; dat.u.num = 1; m.new_emit_data(dat&);  // version
    append_slice(m, dat&, code_p, ptr_diff(code.mmapped.ptr, code.next) - text_padding);
    append_slice(m, dat&, header_p, m.debug_headers.len);
    append_slice(m, dat&, payload_p, m.debug_data.len);
    append_slice(m, dat&, source_p, debug_source_code.len);
    append_slice(m, dat&, files_p, files_bytes.len);
    dat.type = .DL; dat.u.num = 0; m.new_emit_data(dat&);  // nullable_cached_codemap
    dat.type = .DEnd;
    m.new_emit_data(dat&);
}

// requires including codemap.fr
fn encode_debug_files(files: []File) Ty([]u8, []u8) = {
    source := u8.list(temp());
    headers := u8.list(temp());
    leb128_unsigned(headers&, files.len);
    for files { file | 
        source&.push_all(file.content);
        leb128_unsigned(headers&, file.content.len);
        leb128_unsigned(headers&, file.name.len);
        push_all(headers&, file.name);
    };
    (source.items(), headers.items())
}

fn append_slice(m: *QbeModule, dat: *Qbe.Dat, ptr: u32, len: i64) void = {
    dat.type = .DL;
    dat.u.ref = (id = ptr, off = 0);
    dat.is_ref = true;
    m.new_emit_data(dat);
    dat.is_ref = false;
    dat.type = .DL;
    dat.u.num = len;
    m.new_emit_data(dat);
}

fn start_debug_info(m: *QbeModule, s: *SymbolInfo) void = {
    leb128_unsigned(m.debug_headers&, m.debug_data.len);
    leb128_unsigned(m.debug_data&, s.name.len);
    push_all(m.debug_data&, s.name);
    m.debug_last_off_func = 0;
    m.debug_last_off_source = 0;
}

fn end_debug_info(m: *QbeModule, s: *SymbolInfo) void = {
    leb128_unsigned(m.debug_headers&, s.size);
    // TODO: length prefix but that's annoying
    leb128_unsigned(m.debug_data&, 0);
    leb128_unsigned(m.debug_data&, 0);
}

fn add_debug_info(m: *QbeModule, i: *Qbe.Ins, off_in_func: i64) void = {
    hi, lo := (rsval(i.arg&[0]).intcast(), rsval(i.arg&[1]).intcast());
    offset_in_source := hi.shift_left(16).bit_or(lo);
    leb128_unsigned(m.debug_data&, off_in_func - m.debug_last_off_func);
    m.debug_last_off_func = off_in_func;
    leb128_signed(m.debug_data&, offset_in_source - m.debug_last_off_source);
    m.debug_last_off_source = offset_in_source;
}
