SegmentCursor :: @struct(
    mmapped: []u8,
    next: *u8,
);

SegmentType :: @enum(i64) (
    Code,
    ConstantData,
    MutableData,
);

SymbolBucket :: @struct(
    n: u32,
    data: QList(SymbolInfo),
);

SymbolInfo :: @struct(
    name: CStr,
    inline: *Qbe.Fn, // nullable
    fixups: RsVec(Fixup),
    // or -1 if not referenced yet. only relevant for imported functions. 
    // this gets repurposed when fixups_locked. 
    got_lookup_offset: i64,
    segment: SegmentType, // only valid when kind == Local
    //  local: offset in the segment
    // import: offset in stubs to trampoline (if function)
    offset: i64,
    kind: SymbolKind,
    local_but_marked_for_relocation: bool,
    is_thread_local: bool,
);

::enum_basic(SymbolKind);

SymbolKind :: @enum(i64) (
    // We haven't compiled this yet so we've emitted references as lookups into a table in the data section. 
    // 
    // It might also be a dynamic import. 
    // - For jit, references still go through the table because that means we didn't have to make the code memory writeable again if it already started executing. 
    // - For aot, you may want to go through the fixups again and translate them to use a GOT section. 
    //   That way you don't make the loader waste time patching two tables. 
    //
    // For llvm we still track this and emit forward declarations for anything that's still Pending at the end. 
    //
    Pending,
    // We've already compiled this and we know its close to the code and addressable by a static offset. 
    Local,
    // This is an import and we've already patched it in the jitted code. 
    DynamicPatched,
);

FixupType :: @tagged(
    //  local: adrp,add to the address directly
    // import: ??
    InReg: @struct(r: i64, increment: i64),
    //  local: bl to the address
    // import: bl to a trampoline in __stubs which adrp,ldr,br from __got
    //         but currently its adrp,add,br to the address directly
    Call,
    //  local: rebase.
    //         It's a local symbol but since we want its absolute address in memory, we need the loader to add our base address.  
    // import: bind.
    //         We want the absolute address of an import to be in memory. ie. __got uses this. 
    DataAbsolute,
    InRegThreadLocal: @struct(r: i64, increment: i64),
);

Fixup :: @struct(
    patch_at: *u8,
    type: FixupType,
);

fn reserve(s: *SegmentCursor, $T: Type) *T #generic = {
    space_left := s.mmapped.len - s.len(); 
    @assert_ge(space_left, T.size_of(), "not enough space in segment for reserve");
    ptr := s.next;
    @debug_assert(u8.int_from_ptr(ptr).mod(T.align_of()) == 0, "unaligned reserve");
    s.next = s.next.offset(T.size_of());
    ptr_cast_unchecked(u8, T, ptr)
}

fn len(s: *SegmentCursor) i64 = 
    ptr_diff(s.mmapped.ptr, s.next);

// TODO: i dont trust calling this multiple times on the same module... thats important to fix. 
fn make_exec(self: *QbeModule) void = {
    // TODO: this doesn't work with the new way i want to do stubs. shit. -- Oct 24
    segment := SegmentType.Code;
    prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
    segment := self.segments&.index(segment);
    end := segment.next&;
    len := self.start_of_writable_code.ptr_diff(end[]);
    if(len == 0, => return());
    res := mprotect(u8.raw_from_ptr(self.start_of_writable_code), len, prot); 
    assert(res.value.eq(0), "mprotect failed");
    clear_instruction_cache(u8.raw_from_ptr(self.start_of_writable_code), u8.raw_from_ptr(end[]));
        
    PAGE_SIZE :: 16384; // TODO: ask the os for this. we're wasting hella memory on linux. 
    page_start := u8.int_from_ptr(end[]) / PAGE_SIZE * PAGE_SIZE;
    end[] = u8.ptr_from_int(page_start + PAGE_SIZE);
    self.start_of_writable_code = end[];
}

fn get_addr(self: *QbeModule, name: Str) ?rawptr = {
    name := name.maybe_borrow_cstr(temp());
    self.get_addr(name)
}

fn get_addr(self: *QbeModule, name: CStr) ?rawptr = {
    assert(!self.fixups_locked, "you can't get a jit pointer after emitting for aot becuase it stomps data with relocation info.");
    assert(self.tls_vars&.len() == 0, "we don't handle thread locals while jitting.");
    id := self.intern(name);
    symbol := self.get_symbol_info(id);
    get_addr(self, symbol)
}

fn get_addr(self: *QbeModule, symbol: *SymbolInfo) ?rawptr = {
    if(symbol.kind != .Local, => return(.None));
    segment := self.segments&[symbol.segment];
    addr    := segment.mmapped.ptr.offset(symbol.offset);
    (Some = u8.raw_from_ptr(addr))
}

// TODO: be able to specify mutable or constant
fn new_emit_data(self: *QbeModule, dat: *Qbe.Dat) void = {
    ::enum(@type dat.type);
    
    if self.debug["P".char()] {
        print_dat(dat, self.debug_out);
    };
    
    if self.want_text_asm {
        @if(LINK_QBE_C, {
            fn emitdat(_0: *Qbe.Dat, _1: *FILE) void #import("qbe"); // :TodoPort
            emitdat(dat, self.outf);
        }, panic("LINK_QBE_C disabled: cannot emitdat for want_text_asm"));
    } else {
        self.emit_data_to_segment(dat);
    };
}

fn maybe_add_export(self: *QbeModule, id: u32, export: bool) void = {
    if self.goal == .MachoRelocatable && export {
        self.exports&.push(id);
    };
}

// Used when doing our own bytes. Not needed for emitting text asm/llvm. 
fn emit_data_to_segment(self: *QbeModule, dat: *Qbe.Dat) void = {
    assert(!self.fixups_locked, "you cannot add more data after emitting for aot");
    @assert(!dat.lnk.common, "i don't handle `common` symbols");
    @assert(dat.lnk.sec.ptr.is_null() && dat.lnk.secf.ptr.is_null(), "i don't handle custom sections");
    data_segment := self.segments&.index(.MutableData);
    segment := @if(dat.lnk.thread, self.tls_init&, data_segment);
    next := segment.next&;
    @match(dat.type) {
        fn DStart() => {
            segment.align_to(8);
            id := self.intern(dat.name);
            maybe_add_export(self, id, dat.lnk.export);
            off := if dat.lnk.thread {
                @match(self.goal) {
                    fn JitOnly() => panic("TODO: jit tls");
                    fn AsmText() => panic("how did we get here?");
                    @default => {  // for now this is only mach-o
                        // reserve a tls descriptor for this variable. 
                        // the symbol references this descriptor instead of the initialization data. 
                        // arm-apple-isel knows to convert addr to a load and call of the thunk. 
                        ::assert(DONT_FORGET_OTHER_TLS_ARCH, "DONT_FORGET_OTHER_TLS_ARCH");
                        desc_off := data_segment.mmapped.ptr.ptr_diff(self.tls_vars.next);
                        dist := self.distance_between(.MutableData, self.tls_vars&.len() + 16, .MutableData, self.tls_init&.len());
                        descriptor := self.tls_vars&.reserve(TlsDescriptor);
                        TlsDescriptor :: @struct(thunk: i64, key: i64, address: *u8);
                        desc_off
                    };
                }
            } else {
                segment.mmapped.ptr.ptr_diff(next[])
            };
            self.do_jit_fixups(id, .MutableData, off);
        }
        fn DEnd() => {
            @assert(ptr_diff(segment.mmapped.ptr, next[]) < segment.mmapped.len, "too much data");  
        };
        fn DZ() => {
            range(0, dat.u.num) { _ |
                next[][] = 0;
                next[] = next[].offset(1);
            };
        }
        fn DB() => {
            @debug_assert(!dat.isref, "can't have a byte sized pointer");
            len := if dat.isstr {
                s := dat.u.str;
                if dat.has_quotes_and_escapes {
                    // When building ir manually you'll probably just give us the raw bytes.
                    // But this makes it easy to run Qbe's text based tests. Really the parser should handle this. 
                    q :: "\"".ascii();
                    @debug_assert(s[0] == q && s[s.len - 1] == q, "strings are quoted");
                    s = s.slice(1, s.len - 1); // quotes
                    space_left := segment.mmapped.len - ptr_diff(segment.mmapped.ptr, next[]);
                    @assert(space_left > 0, "data segment full");
                    dest: List(u8) = (maybe_uninit = (ptr = next[], len = space_left), len = 0, gpa = panicking_allocator);
                    expand_string_escapes(s, dest&);
                    dest.len
                } else {
                    (@as([]u8) (ptr = next[], len = s.len)).copy_from(s);
                    s.len
                }
            } else {
                next[][] = dat.u.num.trunc();
                1
            };
            next[] = next[].offset(len);
        }
        fn DL() => {
            @assert_eq(u8.int_from_ptr(next[]).mod(8), 0, "sorry can't do unaligned constants yet");
            ptr := ptr_cast_unchecked(u8, i64, next[]);
            if dat.isref {
                // TODO: need to record this location and request a rebase reloc for AOT
                @assert(dat.u.ref.off == 0, "sorry can't do offset refs yet");
                name := dat.u.ref.name;
                
                id     := self.intern(name);
                symbol := self.get_symbol_info(id);
                if self.get_addr(symbol) { value |
                    ptr[] = int_from_rawptr(value);
                };
                // Since we need the absolute address in memory, even local symbols need a relocation. 
                symbol.fixups&.push((patch_at = next[], type = .DataAbsolute), self.gpa);
                if self.goal == .MachoRelocatable && !symbol.local_but_marked_for_relocation {
                    self.local_needs_reloc&.push(id);
                    symbol.local_but_marked_for_relocation = true;
                };
            } else {
                ptr[] = dat.u.num;
            };
            next[] = next[].offset(8);
        }
        fn DW() =>  {
            @debug_assert(!dat.isref, "can't have a w sized pointer");
            @assert_eq(u8.int_from_ptr(next[]).mod(4), 0, "sorry can't do unaligned constants yet");
            ptr_cast_unchecked(u8, u32, next[])[] = dat.u.num.trunc();
            next[] = next[].offset(4);
        }
        fn DH() =>  {
            @debug_assert(!dat.isref, "can't have a h sized pointer");
            @assert_eq(u8.int_from_ptr(next[]).mod(2), 0, "sorry can't do unaligned constants yet");
            ptr_cast_unchecked(u8, u16, next[])[] = dat.u.num.trunc();
            next[] = next[].offset(2);
        }
    } // returns
}

fn add_code_bytes(self: *QbeModule, name: CStr, bytes: []u8) void = {
    name_id := self.intern(name);
    symbol := self.get_symbol_info(name_id);
    code := self.segments&[.Code]&;
    code.align_to(4);  // todo: only for arm
    start_offset := ptr_diff(code.mmapped.ptr, code.next);
    self.do_jit_fixups(name_id, .Code, start_offset);
    dest := slice(code.next, bytes.len);
    dest.copy_from(bytes);
    code.next = code.next.offset(bytes.len);
    
    if self.want_text_asm {
        // TODO: the align4 is only required for arm
        @fmt_write(self.outf, ".text\n.balign 4\n.globl _%\n_%:\n", name, name);
        if bytes.len.mod(4) == 0 && u8.int_from_ptr(bytes.ptr).mod(4) == 0 { 
            // will always be true for arm.
            // doing it as bytes would still work, lets just make it a bit easier for the assembler because why not. 
            words: []u32 = (ptr = ptr_cast_unchecked(u8, u32, bytes.ptr), len = bytes.len / 4);
            for words { inst |
                v := @as(i64) inst.zext();
                @fmt_write(self.outf, "   .word %\n", v);
            };
        } else {
            for bytes { inst |
                v := @as(i64) inst.zext();
                @fmt_write(self.outf, "   .byte %\n", v);
            };
        }
    };
}

fn align_to(s: *SegmentCursor, align: i64) void = {
    extra := u8.int_from_ptr(s.next).mod(align);
    if extra != 0 {
        s.next = s.next.offset(align - extra); 
    };
}

fn put_jit_addr(self: *QbeModule, name: CStr, addr: rawptr) void = {
    symbol := self.get_symbol_info(self.intern(name));
    @assert(symbol.kind == .Pending, "Redeclared symbol %", name);
    symbol.kind = .DynamicPatched;
    self.do_fixups(u8.ptr_from_raw(addr), symbol);
}

// :SLOW insanely inefficient way of doing this. especially when someone tries to add multiple libs. 
// TODO: store a list of the pending ones so don't have to iterate them all. 
fn fill_pending_dynamic_imports(self: *QbeModule, lib: DlHandle) void = {
    for_symbols self { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending {
            self.imports&.push(id); 
            found := lib.dlsym(symbol.name);
            if !found.is_null() {
                symbol.kind = .DynamicPatched;
                self.do_fixups(u8.ptr_from_raw(found), symbol);
            };
        };
    };
}

// note: don't push symbols in the body!
fn for_symbols(m: *QbeModule, $body: @Fn(id: u32, s: *SymbolInfo) void) void = {
    enumerate m.symbols& { h, bucket | 
        bucket := bucket.data.slice(0, bucket.n.zext());
        enumerate bucket { i, symbol | 
            id := h + i.shift_left(IBits);
            body(trunc(id), symbol);
        };
    };
}

fn fill_from_libc(self: *QbeModule) void = {
    if find_os_libc_dylib() { libc_path | 
        libc := dlopen(libc_path, .Lazy);
        if !libc.lib.is_null() { 
            self.fill_pending_dynamic_imports(libc);
        };
    };
}
