SegmentCursor :: @struct(
    mmapped: []u8,
    next: *u8,
);

SegmentType :: @enum(i64) (
    Code,
    ConstantData,
    MutableData,
);

SymbolBucket :: @struct(
    n: u32,
    data: QList(SymbolInfo),
);

// TODO: make this less chunky
SymbolInfo :: @struct(
    name: Str,
    inline: *Qbe.Fn, // nullable
    fixups: RawList(Fixup),
    // or -1 if not referenced yet. valid: DynamicPatched
    // this gets repurposed when fixups_locked. 
    // when targetting wasm this is the index of the function (need to offset by import count if imported)
    got_lookup_offset: i64,
    segment: SegmentType, // valid: Local
    //  local: offset in the segment
    // import: offset in stubs to trampoline (if function)
    offset: i64,
    kind: SymbolKind,
    local_but_marked_for_relocation: bool,
    is_thread_local: bool,  
    referenced: bool,  // took address or called before knowing it was inlinable so you must actually emit the function.
    jit_addr: rawptr, // valid when kind == .DynamicPatched and goal == .JitOnly
    size: i64,  // only set for functions currently. 
    shim_addr: rawptr, // TODO: it would be great if i didn't need this
    wasm_type_index: i32,
    //wasm_space: Wasm.ImportType, // TODO: chunky
);

::enum_basic(SymbolKind);

// TODO: deal with matching on an @enum(u8) on llvm
SymbolKind :: @enum(i64) (
    // We haven't compiled this yet so we've emitted references as lookups into a table in the data section. 
    // 
    // It might also be a dynamic import. 
    // - For aot, you may want to go through the fixups again and translate them to use a GOT section. 
    //   That way you don't make the loader waste time patching two tables. 
    //
    Pending,
    // We've already compiled this and we know its close to the code and addressable by a static offset. 
    Local,
    // This is an import and we've already patched it in the code. 
    DynamicPatched,
);

FixupType :: @tagged(
    //  local: adrp,add to the address directly
    // import: ??
    InReg: @struct(r: i64, increment: u32),
    //  local: bl to the address
    // import: bl to a trampoline in __stubs which adrp,ldr,br from __got
    //         but currently its adrp,add,br to the address directly
    Call,
    //  local: rebase.
    //         It's a local symbol but since we want its absolute address in memory, we need the loader to add our base address.  
    // import: bind.
    //         We want the absolute address of an import to be in memory. ie. __got uses this. 
    DataAbsolute,
    
    // amd64
    RipDisp32: @struct(increment: u32, call: bool), // TODO: add safety checks for size of Qbe.Con.bits.i
    
    // wasm
    WasmIndex,
    WasmAddr: @struct(increment: u32, wide: bool),
);

Fixup :: @struct(
    patch_at: *u8,
    type: FixupType,
);

fn reserve(s: *SegmentCursor, $T: Type) *T #generic = {
    space_left := s.mmapped.len - s.len(); 
    @assert_ge(space_left, T.size_of(), "not enough space in segment for reserve");
    ptr := s.next;
    @debug_assert(u8.int_from_ptr(ptr).mod(T.align_of()) == 0, "unaligned reserve");
    s.next = s.next.offset(T.size_of());
    ptr_cast_unchecked(u8, T, ptr)
}

fn len(s: *SegmentCursor) i64 = 
    ptr_diff(s.mmapped.ptr, s.next);

fn cap(s: *SegmentCursor) i64 = 
    s.mmapped.len;

// TODO: i dont trust calling this multiple times on the same module... thats important to fix. 
fn make_exec(self: *QbeModule) void = {
    // TODO: this doesn't work with the new way i want to do stubs. shit. -- Oct 24
    segment := SegmentType.Code;
    prot := bit_or(@as(i64) MapProt.Exec, @as(i64) MapProt.Read);
    segment := self.segments&.index(segment);
    end := segment.next&;
    len := self.start_of_writable_code.ptr_diff(end[]);
    if(len == 0, => return());
    res := mprotect(u8.raw_from_ptr(self.start_of_writable_code), len, prot); 
    assert(res.value.eq(0), "mprotect failed");
    clear_instruction_cache(u8.raw_from_ptr(self.start_of_writable_code), u8.raw_from_ptr(end[]));  // IMPORTANT
        
    PAGE_SIZE :: 16384; // TODO: ask the os for this. we're wasting hella memory on linux. 
    page_start := u8.int_from_ptr(end[]) / PAGE_SIZE * PAGE_SIZE;
    end[] = u8.ptr_from_int(page_start + PAGE_SIZE);
    self.start_of_writable_code = end[];
}

fn get_jit_addr(self: *QbeModule, id: u32) rawptr = {
    assert(!self.fixups_locked, "you can't get a jit pointer after emitting for aot becuase it stomps data with relocation info.");
    result := rawptr.zeroed();
    use_symbol(self, id) { symbol | 
        result = symbol.jit_addr;
    };
    result
}

fn get_addr(self: *QbeModule, id: u32) ?Ty(rawptr, i64) = {
    assert(!self.fixups_locked, "you can't get a jit pointer after emitting for aot becuase it stomps data with relocation info.");
    result: ?Ty(rawptr, i64) = .None;
    use_symbol(self, id) { symbol | 
        result = get_addr(self, symbol);
    };
    result
}

fn get_addr(self: *QbeModule, name: Str) ?rawptr = {
    if get_addr(self, self.intern(name)) { a |
        return(Some = a._0);
    };
    .None
}

fn get_addr(self: *QbeModule, symbol: *SymbolInfo) ?Ty(rawptr, i64) = {
    if(symbol.kind != .Local, => return(.None));
    segment := self.segments&[symbol.segment];
    addr    := segment.mmapped.ptr.offset(symbol.offset);
    (Some = (u8.raw_from_ptr(addr), symbol.size))
}

// TODO: be able to specify mutable or constant
fn new_emit_data(self: *QbeModule, dat: *Qbe.Dat) void = {
    ::enum(@type dat.type);
    
    if self.debug["P".char()] {
        print_dat(self, dat, self.debug_out);
    };
    self.emit_data_to_segment(dat);
}

fn maybe_add_export(self: *QbeModule, id: u32, export: bool) void = {
    if export && (self.goal.arch == .wasm32 || @is(self.goal.type, .Relocatable, .Dynamic)) {
        self.exports&.push(id);
    };
}

fn emit_data_to_segment(self: *QbeModule, dat: *Qbe.Dat) void = 
    emit_data_to_segment(self, dat, .MutableData);

fn emit_data_to_segment(self: *QbeModule, dat: *Qbe.Dat, seg: SegmentType) void = {
    assert(!self.fixups_locked, "you cannot add more data after emitting for aot");
    data_segment := self.segments&.index(seg);
    segment := data_segment;
    next := segment.next&;
    ::enum(@type dat.type);
    @match(dat.type) {
        fn DStart() => {
            segment.align_to(8);
            id := dat.lnk.id;
            maybe_add_export(self, id, dat.lnk.export);
            off := segment.mmapped.ptr.ptr_diff(next[]);
            self.do_jit_fixups(id, seg, off);
        }
        fn DEnd() => {
            @assert(ptr_diff(segment.mmapped.ptr, next[]) < segment.mmapped.len, "too much data");  
        };
        fn DZ() => {
            range(0, dat.u.num) { _ |
                next[][] = 0;
                next[] = next[].offset(1);
            };
        }
        fn DB() => {
            @debug_assert(!dat.is_ref, "can't have a byte sized pointer");
            len := if dat.is_str {
                s := dat.u.str;
                if dat.has_quotes_and_escapes {
                    // When building ir manually you'll probably just give us the raw bytes.
                    // But this makes it easy to run Qbe's text based tests. Really the parser should handle this. 
                    q :: "\"".ascii();
                    @debug_assert(s[0] == q && s[s.len - 1] == q, "strings are quoted");
                    s = s.slice(1, s.len - 1); // quotes
                    space_left := segment.mmapped.len - ptr_diff(segment.mmapped.ptr, next[]);
                    @assert(space_left > 0, "data segment full");
                    dest: List(u8) = (maybe_uninit = (ptr = next[], len = space_left), len = 0, gpa = panicking_allocator);
                    expand_string_escapes(s, dest&);
                    dest.len
                } else {
                    (@as([]u8) (ptr = next[], len = s.len)).copy_from(s);
                    s.len
                }
            } else {
                next[][] = dat.u.num.trunc();
                1
            };
            next[] = next[].offset(len);
        }
        fn DL() => {
            @assert_eq(u8.int_from_ptr(next[]).mod(8), 0, "sorry can't do unaligned constants yet");
            ptr := ptr_cast_unchecked(u8, i64, next[]);
            if dat.is_ref {
                // TODO: need to record this location and request a rebase reloc for AOT
                @assert(dat.u.ref.off == 0, "sorry can't do offset refs yet");
                id := dat.u.ref.id;
                use_symbol(self, id) { symbol | 
                    mark_referenced(self, id, symbol);
                    if self.get_addr(symbol) { value |
                        ptr[] = int_from_rawptr(value._0);
                    };
                    // Since we need the absolute address in memory, even local symbols need a relocation. 
                    push_fixup(self, symbol, (patch_at = next[], type = .DataAbsolute));
                    if self.goal.arch != .wasm32 && self.goal.type == .Relocatable && !symbol.local_but_marked_for_relocation {
                        self.local_needs_reloc&.push(id);
                        symbol.local_but_marked_for_relocation = true;
                    };
                };
            } else {
                ptr[] = dat.u.num;
            };
            next[] = next[].offset(8);
        }
        fn DW() =>  {
            @debug_assert(!dat.is_ref, "can't have a w sized pointer");
            @assert_eq(u8.int_from_ptr(next[]).mod(4), 0, "sorry can't do unaligned constants yet");
            ptr_cast_unchecked(u8, u32, next[])[] = dat.u.num.trunc();
            next[] = next[].offset(4);
        }
        fn DH() =>  {
            @debug_assert(!dat.is_ref, "can't have a h sized pointer");
            @assert_eq(u8.int_from_ptr(next[]).mod(2), 0, "sorry can't do unaligned constants yet");
            ptr_cast_unchecked(u8, u16, next[])[] = dat.u.num.trunc();
            next[] = next[].offset(2);
        }
    } // returns
}

// :ThreadSafety we can't have both threads fucking with self.segments
fn add_code_bytes(m: *QbeModule, name_id: u32, bytes: []u8) void = {
    code := m.segments&[.Code]&;
    code.align_to(4);  // todo: only for arm
    start_offset := ptr_diff(code.mmapped.ptr, code.next);
    dest := slice(code.next, bytes.len);
    dest.copy_from(bytes);
    code.next = code.next.offset(bytes.len);
    
    m.do_jit_fixups(name_id, .Code, start_offset);
    finish_function(m, dest, name_id);
}

// this sure does something. please run it so many times before trying to remove.
fn fence() void #asm #aarch64 = (
    @bits(0b11010101000000110011, 0b1111, 0b10111111),  // DMB SY
    ret()
);

fn fence() void #asm #x86_bytes = (fn(out) = @asm_x64(
    PrimaryOp.Ret,PrimaryOp.Ret
) out);

fn finish_function(m: *QbeModule, dest: []u8, name_id: u32) void #inline = {
    if m.got_indirection_instead_of_patches && m.goal.type == .JitOnly && m.goal.arch == .aarch64 {
        // :SLOW
        @if(use_threads) pthread_mutex_lock(m.icache_mutex&).unwrap();
        
        m.last_function_end = dest.ptr.offset(dest.len);
        use_symbol(m, name_id) { s |
            s.size = dest.len;
        };
        sys_dcache_flush(u8.raw_from_ptr(dest.ptr), dest.len);
        fence();
        
        //sys_icache_invalidate(u8.raw_from_ptr(dest.ptr), dest.len);
        @if(use_threads) pthread_mutex_unlock(m.icache_mutex&).unwrap();
    } else {
        m.last_function_end = dest.ptr.offset(dest.len);
        use_symbol(m, name_id) { s |
            s.size = dest.len;
        };
    };
}

fn align_to(s: *SegmentCursor, align: i64) void = {
    extra := s.len().mod(align);
    if extra != 0 {
        s.next = s.next.offset(align - extra); 
    };
}

fn put_jit_addr(self: *QbeModule, name: u32, addr: rawptr) void = {
    use_symbol(self, name) { symbol |
        @assert(symbol.kind == .Pending, "(put_jit_addr) redeclared symbol %: %", name, symbol.name);
        symbol.kind = .DynamicPatched;
        symbol.jit_addr = addr;
        self.do_fixups(u8.ptr_from_raw(addr), symbol);
    };
}

// :SLOW insanely inefficient way of doing this. especially when someone tries to add multiple libs. 
// TODO: store a list of the pending ones so don't have to iterate them all. 
// This is not what you want if you care about cross compiling!
fn fill_pending_dynamic_imports(self: *QbeModule, lib: DlHandle) void = {
    buf := u8.list(temp());
    for_symbols self { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending {
            self.imports&.push(id); 
            buf&.clear();
            buf&.push_all(symbol.name);
            buf&.push(0);
            s: CStr = (ptr = buf.maybe_uninit.ptr);
            found := lib.dlsym(s);
            if !found.is_null() {
                symbol.kind = .DynamicPatched;
                self.do_fixups(u8.ptr_from_raw(found), symbol);
            };
        };
    };
}

// note: don't push symbols in the body!
fn for_symbols(m: *QbeModule, $body: @Fn(id: u32, s: *SymbolInfo) void) void = {
    enumerate m.symbols& { h, bucket | 
        bucket := bucket.data.slice(0, bucket.n.zext());
        enumerate bucket { i, symbol | 
            id := h + i.shift_left(IBits);
            body(trunc(id), symbol);
        };
    };
}

// This is not what you want if you care about cross compiling!
fn fill_from_libc(self: *QbeModule) void = {
    if find_os_libc_dylib() { libc_path | 
        libc := dlopen(libc_path, .Lazy);
        if !libc.lib.is_null() { 
            self.fill_pending_dynamic_imports(libc);
        };
    };
}

// TODO: when doing aot we just have to ask for our segments to have the same spacing as they did now and then relative addressing will just work out. 
//       but for data constants with pointers we'll need to remember what relocations to ask the loader to do at runtime. 
//       would be great if my language had nice relative pointers and then you wouldn't have to deal with that as much
//
fn do_jit_fixups(self: *QbeModule, id: u32, segment: SegmentType, final_offset: i64) void = {
    @assert(final_offset >= 0 && final_offset < self.segments&[segment].mmapped.len, "thats not in the segment");
    use_symbol(self, id) { symbol | 
        @debug_assert(int_from_ptr(@type symbol[], symbol) != 0, "fixups for a null symbol! (forgot to set Fn.lnk.id?)"); // only catches it if you haven't had something hash to 0 yet. 
        @assert(symbol.kind != .Local, "(do_jit_fixups) redeclared symbol %: %", id, symbol.name);
        symbol.segment = segment;
        symbol.offset = final_offset;
        symbol.kind = .Local;
        dest := self.segment_address(symbol.segment, symbol.offset);
        fixups := symbol.fixups&;
        if self.goal.type == .JitOnly && self.got_indirection_instead_of_patches && !symbol.shim_addr.is_null() {
            // :InsaneCaches
            //@assert(fixups.len == 1); // not true when shim + bounce. 
            //fixups.drop(self.gpa); 
            fixups[] = zeroed(@type fixups[]);
            // don't return here. do_fixups sets jit_addr
        };
        self.do_fixups(dest, symbol);
        if self.goal.arch != .wasm32 {
            if self.goal.type == .JitOnly || self.goal.type == .Exe || self.goal.type == .Dynamic { //  || symbol.segment == .Code // :canireallynothavecodeberelative
                // Since it's a local symbol, we don't care about the relocations anymore. The linker never needs to know. 
                // TODO: actually thats also true for imports becuase calls go through __got. 
                // TODO: for jitonly you do need to keep __got for weak. 
                unordered_retain(fixups, fn(it) => it.type&.is(.DataAbsolute));
                //if(fixups.len == 0, => fixups.drop(self.gpa));
            } else {
                // you still care because the static linker wants to move things around. :track_got_reloc
                if !symbol.local_but_marked_for_relocation {
                    self.local_needs_reloc&.push(id);
                    symbol.local_but_marked_for_relocation = true;
                };
            }; 
        };
    };
}
::tagged(FixupType);

fn do_fixups(self: *QbeModule, address: *u8, symbol: *SymbolInfo) void = {
    assert(!self.fixups_locked, "you cannot add more fixups after emitting for aot");
    symbol.jit_addr = u8.raw_from_ptr(address);  
    if self.goal.type == .JitOnly {
        @debug_assert(!address.is_null(), "do_fixups jit null symbol %", symbol.name);
    };
    
    if self.goal.arch == .wasm32 && symbol.segment == .Code && symbol.got_lookup_offset == -1 {
        // Need to wait until the end of compilation to know its index. 
        return();
    };
    
    fixups := symbol.fixups.items();
    new_got_reloc: ?Fixup = .None;
    fix := self.target.fixup;
    each fixups { fixup | 
        fix(self, symbol, fixup, new_got_reloc&);
    };
    if new_got_reloc { it | // avoided reallocating while iterating. rustc would be proud.
        push_fixup(self, symbol, it);
    };
}

fn ensure_got_slot(self: *QbeModule, symbol: *SymbolInfo, jit_address: rawptr) ?Fixup = {
    @debug_assert(!self.fixups_locked);
    new_got_reloc: ?Fixup = .None;
    if symbol.got_lookup_offset == -1 {
        // need to reserve a new __got entry.  :GotAtConstantDataBase 
        symbol.got_lookup_offset = self.got&.len();
        got := self.got&.reserve(rawptr);  // :TodoErrorMessage
        if self.debug["T".char()] {
            @fmt_write(self.debug_out, "# reserve __got[%] = %\n", symbol.got_lookup_offset / 8, symbol.name);
        };
        new_got_reloc = (Some = (patch_at = ptr_cast_unchecked(rawptr, u8, got), type = .DataAbsolute));
        if !(self.got_indirection_instead_of_patches && jit_address.is_null()) { // TODO: remove this condition
            got[] = jit_address; // because we don't loop again to handle `new_got_reloc`
        }
    };
    got := self.segment_address(.ConstantData, symbol.got_lookup_offset);  // :GotAtConstantDataBase 
    got := ptr_cast_unchecked(u8, rawptr, got);
    if self.got_indirection_instead_of_patches {
        if jit_address.is_null() {
            //@println("missing jit import? % % % %", symbol.name, symbol.jit_addr, got[], symbol.got_lookup_offset);
        } else {
            got[] = jit_address;
        };
    } else {
        @debug_assert_eq(got[], jit_address, "ensure_got_slot with different address");
    };
    new_got_reloc
}

fn emit_fn(f: *Qbe.Fn) void = {
    m := f.globals;
    code := m.segments&[.Code]&;
    start_offset := code.len();
    name_id := f.lnk.id;
    
    // We know we're emitting the function right here so we can do old fixups now. 
    maybe_add_export(m, name_id, f.lnk.export);
    m.do_jit_fixups(name_id, .Code, start_offset);
    
    {m.target.emit_fn}(f);
    size := code.len() - start_offset;
    @assert_lt(code.len(), code.mmapped.len, "too much code");
    
    // :FuncSizeLimit 
    // b.cond gives you 19 bits, but encoded/4, but signed so that's 20 bits.  
    // you can remove this limit on the function but then have to add the check when emitting each branch. 
    @assert_lt(size, 1.shift_left(20), "we don't allow a function larger than one arm64 instruction can jump");
    
    code_bytes := code.mmapped.subslice(start_offset, size);
    finish_function(m, code_bytes, name_id);
    
    if f.globals.debug["D".char()] {
        arch_name := @match(f.globals.goal.arch) {
            fn x86_64()  => "--arch=x86-64";
            fn aarch64() => "--arch=aarch64";
            fn wasm32()  => "--arch=wasm32";
        };
        @fmt_write(f.globals.debug_out, "# disassembly of '%' (% bytes)\n", f.globals.str(f.lnk.id), code_bytes.len);
        asm := code_bytes;
        hex: List(u8) = list(asm.len * 5, temp());
        for asm { byte |
            hex&.push_prefixed_hex_byte(byte);
            hex&.push_all(" ");
        };
        out := open_temp_file();
        out.fd&.write(hex.items());
        DIS :: "llvm-mc";
        // TODO: have the caller pass in anything like this, 
        //       but this is just for debugging the compiler so its not a big deal. :driver_io
        // varient 1 means intel syntax. ignored when doing arm. 
        success := run_cmd_blocking(DIS, @slice(arch_name, "--disassemble", "-output-asm-variant=1", "--show-encoding", out&.s_name()));
        if(!success, => eprintln("ICE(debugging): couldn't disassemble"));
        out.remove();
    };
}

fn push_fixup(m: *QbeModule, symbol: *SymbolInfo, fix: Fixup) void #inline = {
    @if(use_threads) pthread_mutex_lock(m.intern_mutex&).unwrap();
    a := m.forever&.borrow();
    symbol.fixups&.push(fix, a);
    @if(use_threads) pthread_mutex_unlock(m.intern_mutex&).unwrap();
}

//////////////////////////////////
// Shared by native AOT targets //
//////////////////////////////////

fn main_entry_point_vaddr(m: *QbeModule) i64 = {
    @debug_assert_ne(m.goal.arch, .wasm32, "only makes sense for native targets");
    @debug_assert_ne(m.goal.type, .Dynamic, "shared libraries don't have an entry point");
    @debug_assert(m.fixups_locked, "this is not threadsafe");
    symbol := m.get_symbol_info(m.intern("main"));
    assert(symbol.kind == .Local, "no exported main function?");
    @debug_assert_ge(symbol.offset, COMMANDS_SIZE_GUESS + SIZE_OF_STUBS);
    symbol.offset
}

fn patch_pending_symbols(m: *QbeModule) void = {
    @debug_assert_ne(m.goal.arch, .wasm32, "only makes sense for native targets");
    // fill_from_libc is wrong because you might be cross compiling and disagree about what things exist. 
    for_symbols m { id, symbol |  // :SLOW  just keep list of pending symbols instead
        if symbol.kind == .Pending && symbol.fixups.len != 0 {
            m.imports&.push(id); 
            symbol.kind = .DynamicPatched;
            m.do_fixups(u8.ptr_from_int(0), symbol);
        };
    };
}

fn debug_log_byte_size(m: *QbeModule) void = {
    // TODO: remove
    code_count := m.segments&[.Code]&.len() - COMMANDS_SIZE_GUESS - SIZE_OF_STUBS;
    data_count := m.segments&[.MutableData]&.len();
    @if(show_backend_stats())
    @eprintln(">>> % bytes of code, % bytes of data.", code_count, data_count); 
}

fn live_segment_part(m: *QbeModule, s: SegmentType) []u8 = {
    s := m.segments&[s]&;
    s.mmapped.slice(0, ptr_diff(s.mmapped.ptr, s.next))
}
