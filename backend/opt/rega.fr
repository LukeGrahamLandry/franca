// Adapted from Qbe. MIT License. © 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//!
//! The overarching goal is to fulfil register constraints with as mov instructions inserted as possible. 
//! > Recall that the spill pass has already ensured we have enough registers for all the values live at any given time. 
//!
//! There are a few situations that require values to be in specific registers:
//! - The calling convention specifies where to put arguments and returns.
//! - Some ISAs have instructions that can only access certain registers (ie. x64 mul/div/shift). 
//! Before this pass those constraints are expressed as copies between our infinite tmps and the required registers. 
//! Now we try to arrange it so values are already where we need them so the copy can be removed.
//!
//! Similar for lowering phi nodes.
//!

RMap :: @struct(
    // parallel arrays. the tmp value t[i] is in register r[i]
    tmps: Array(i32, Qbe.Tmp0),
    regs: Array(i32, Qbe.Tmp0),
    live: Qbe.BSet,
    len: i64,  // numbers of registers used so far
);

PMove :: @struct(
    src:  Qbe.Ref, 
    dest: Qbe.Ref, 
    cls:  Qbe.Cls, 
);

RegaCtx :: @struct(
    f: *Qbe.Fn,
    regu: u64,       /* registers used */
    pm: List(PMove), /* parallel move constructed */
    added_moves:  u32, 
    added_blocks: u32,
    loads := 0,
    stores := 0,
);

fn hint(c: *RegaCtx, t: i64) *i32 #inline = 
    c.f.tmp[phi_dest(t, c.f)].hint.register&;

fn sethint(c: *RegaCtx, t: i64, r: i64) void = {
    h := hint(c, t);
    if h[] == -1 {
        h[] = r.intcast();
    }
}

fn rcopy(dest: *RMap, src: *RMap) void = {
    n := src.len;
    dest.len = n;
    dest.tmps&.items().slice(0, n).copy_from(src.tmps&.items().slice(0, n));
    dest.regs&.items().slice(0, n).copy_from(src.regs&.items().slice(0, n));
    bscopy(dest.live&, src.live&);
}

fn rfind(m: *RMap, t: i64) ?i32 = {  
    range(0, m.len) { i | 
        if m.tmps&[i] == t.intcast() {
            r := m.regs&[i];
            @debug_assert(t >= Qbe.Tmp0 || r.intcast() == t, "not allocated to itself");
            return(Some = r);
        }
    };
    .None
}

fn rref(c: *RegaCtx, m: *RMap, t: i64) Qbe.Ref = {
    ::if_opt(i32, Qbe.Ref);
    if rfind(m, t) { r |
        TMP(r)
    } else {
        s := c.f.tmp[t].slot;
        @debug_assert(s != -1, "should have spilled TMP(%)", t);
        SLOT(s)
    }
}

fn radd(c: *RegaCtx, m: *RMap, t: i64, r: i64) void = {
    T := c.f.globals.target&;
    @debug_assert(t >= Qbe.Tmp0 || t == r, "invalid temporary");
    is_int   := T.gpr.bit_is_set(r);
    is_float := T.fpr.bit_is_set(r);
    @debug_assert(is_int || is_float, "invalid register %", r);
    @debug_assert(!bshas(m.live&, t), "temporary has mapping");
    @debug_assert(!bshas(m.live&, r), "register already allocated");
    @debug_assert(m.len.intcast() <= T.ngpr+T.nfpr, "too many mappings");
    
    bsset(m.live&, t);
    bsset(m.live&, r);
    m.tmps&[m.len] = t.intcast();
    m.regs&[m.len] = r.intcast();
    m.len += 1;
    c.regu = c.regu.bit_or(BIT(r));
}

fn ralloctry(c: *RegaCtx, m: *RMap, t: i32, try: bool) Qbe.Ref = {
    t := t.intcast();
    if t < Qbe.Tmp0 {   // this is a register
        @debug_assert(bshas(m.live&, t), "reg must be allocated to itself");
        return(TMP(t));
    };
    if bshas(m.live&, t) {  // we're already in a register
        r := rfind(m, t).expect("thought we allocated but don't know where");
        return(TMP(r));
    };
    r_hinted := c.hint(t)[];
    r_allocated := if r_hinted != -1 && !bshas(m.live&, r_hinted.zext()) {
        r_hinted.intcast()
    } else {
        if try {
            return(QbeNull);
        };
        to_avoid := c.f.tmp[phi_dest(t, c.f)].hint.avoid;
        
        T := c.f.globals.target&;
        of_kind := @if(c.f.tmp[t].cls.is_int(), T.gpr, T.fpr);
        allow := of_kind.bit_and(bit_not m.live.t[]);
        
        // at least try not to get a bad place
        prefer := allow.bit_and(bit_not to_avoid);
        if prefer == 0 {
            // ok fine, anything will do
            prefer = allow;
        };
        @assert(prefer != 0, "no register free for % in %. this should be unreachable becuase of the spill pass.", c.f.tmp.index(t).name(), c.f.name());
        prefer.bitcast().trailing_zeros()
    };
    c.radd(m, t, r_allocated);
    c.sethint(t, r_allocated);
    TMP(r_allocated)
}

fn ralloc(c: *RegaCtx, m: *RMap, t: i32) Qbe.Ref #inline = 
    c.ralloctry(m, t, false);

fn bit_is_set(s: u64, i: i64) bool #inline = 
    s.bit_and(BIT(i)) != 0;

// this changes the order in regs/tmps
fn rfree(c: *RegaCtx, m: *RMap, t: i64) i64 = {
    T := c.f.globals.target&;
    @debug_assert(t >= Qbe.Tmp0 || !T.rglob.bit_is_set(t), "reg non-global");
    if !bshas(m.live&, t) {
        return(-1);
    };
    i := 0;
    while => m.tmps&[i].intcast() != t {
        @debug_assert(i+1 < m.len, "reg not found");
        i += 1;
    };
    rfree_at_index(c, m, t, i)
}

fn rfree_at_index(c: *RegaCtx, m: *RMap, t: i64, i: i64) i64 = {
    T := c.f.globals.target&;
    @debug_assert(i < m.len && m.tmps&[i].intcast() == t, "invalid rfree_at_index");
    r := m.regs&[i].intcast();
    bsclr(m.live&, t);
    bsclr(m.live&, r);
    m.len -= 1;
    m.regs&[i] = m.regs&[m.len];
    m.tmps&[i] = m.tmps&[m.len];
    @debug_assert(t >= Qbe.Tmp0 || t == r, "invalid rfree_at_index.");
    r
}

fn pmadd(c: *RegaCtx, src: Qbe.Ref, dst: Qbe.Ref, k: Qbe.Cls) void #inline = {
    c.loads += int(rtype(src) == .RSlot);
    c.stores += int(rtype(dst) == .RSlot);
    c.pm&.push(src = src, dest = dst, cls = k);
}

PMStat :: @enum(i64) (ToMove, Moving, Moved);

// not exactly the same (we do it backwards and with swap instead of a temporary) but similar: 
// Rideau, L., Serpette, B.P. & Leroy, X. Tilting at Windmills with Coq: Formal Verification of a Compilation Algorithm for Parallel Moves. J Autom Reasoning 40, 307–326 (2008).
// https://xavierleroy.org/publi/parallel-move.pdf
//
// swap generates suboptimal code on arm/rv (3 instructions). 
// can do better with a temporary instead but its a tiny improvement and not worth the hassle. see devlog Sep 21, 2025
//
// note: k is a pointer. we mutate back up through levels of recursion. 
fn pmrec(c: *RegaCtx, status: []PMStat, i: i64, k: *Qbe.Cls) i64 = {
    /* note, this routine might emit
     * too many large instructions
     */
    if c.pm&[i].src == c.pm&[i].dest {
        status[i] = .Moved;
        return(-1);
    };
    @debug_assert(KBASE(c.pm&[i].cls) == KBASE(k[]), "bad kbase");
    ::assert(Qbe.Cls.Kw.raw().bit_or(Qbe.Cls.Kl.raw()) == Qbe.Cls.Kl.raw(), "Cls numbers");
    ::assert(Qbe.Cls.Ks.raw().bit_or(Qbe.Cls.Kd.raw()) == Qbe.Cls.Kd.raw(), "Cls numbers");
    k[] = @as(Qbe.Cls) @as(i32) k[].raw().bit_or(c.pm&[i].cls.raw());
    j := 0;
    while => j < c.pm.len && c.pm&[j].dest != c.pm&[i].src {
        j += 1;
    };
    cycle := -1;
    ::if(PMStat);
    s := if(j == c.pm.len, => PMStat.Moved, => status[j]);
    @match(s) {
        fn Moving() => {
            cycle = j; /* start of cycle */
            c.f.emit(.swap, k[], QbeNull, c.pm&[i].src, c.pm&[i].dest);
        }
        fn ToMove() => {
            status[i] = .Moving; // TODO: not insane error message if you forget the dot so it tries to reference the overload set created for the switch case above.  
            cycle = c.pmrec(status, j, k);
            if cycle == i {
                cycle = -1; /* end of cycle */
            } else {
                if cycle != -1 {
                    c.f.emit(.swap, k[], QbeNull, c.pm&[i].src, c.pm&[i].dest);
                } else {
                    cycle = -1;
                    c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx
                }
            };
        }
        fn Moved() => {
            cycle = -1;
            c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx same
        }
    };
    status[i] = .Moved;
    cycle
}

fn pmgen(c: *RegaCtx) void = {
    ::enum_basic(PMStat);
    status := temp().alloc_zeroed(PMStat, c.pm.len);
    ::assert(PMStat.zeroed() == .ToMove, "zero init is ToMove");
    
    range(0, c.pm.len) { i | 
        if status[i] == .ToMove {
            k := c.pm&[i].cls;
            c.pmrec(status, i, k&);
        };
    };
}

fn move(c: *RegaCtx, r: i64, to: Qbe.Ref, m: *RMap) void = {
    r1 := if(to == QbeNull, => -1, => c.rfree(m, to.val()));
    if bshas(m.live&, r) {
        /* r is used and not by to */
        @debug_assert(r1 != r, "already in reg");
        n := 0;
        while => m.regs&[n].intcast() != r {
            @debug_assert(n+1 < m.len, "reg not found.");
            n += 1;
        };
        t := m.tmps&[n];
        rfree_at_index(c, m, t.intcast(), n);
        bsset(m.live&, r);
        c.ralloc(m, t);
        bsclr(m.live&, r);
    };
    t := if(to == QbeNull, => r, => to.val());
    c.radd(m, t, r);
}

// when you have a group of copies from registers to tmps, 
// you need to make sure none of the dest tmps are allocated to registers later used as a src.
// it has to execute as though all the moves happened at the same time, without letting any stomp eachother. 
fn do_parallel_moves(c: *RegaCtx, b: *Qbe.Blk, last_move: *Qbe.Ins, m: *RMap) *Qbe.Ins = {
    T := c.f.globals.target&;
    m0 := m[]; /* okay since we don't use m0.b */
    m0.live.t = u64.ptr_from_int(0); // assert ^
    past_last_move := last_move.offset(1);
    first_move := {
        i := past_last_move;
        dowhile {
            i = i.offset(-1);
            c.move(i.arg&[0].val(), i.to, m);
            !i.identical(b.ins.ptr) && is_copy_from_reg(i.offset(-1))
        };
        i
    };
    
    // :RequireCopyAfterCall
    @debug_assert(m0.len <= m.len);
    is_after_call := !first_move.identical(b.ins.ptr) && @is(first_move.offset(-1).op(), .call);
    if is_after_call {
        discard := Array(i32, 2).ptr_from_int(0); // TODO: helper for this 
        call_abi_info := first_move.offset(-1)[].arg&[1];
        def := {T.retregs}(call_abi_info, discard).bit_or(T.rglob);
        to_move := T.caller_saved.bit_and(bit_not def);
        for_bits to_move { r |
            c.move(r, QbeNull, m);
        };
    };
    c.pm&.clear();
    range(0, m.len) { n |
        t  := m.tmps&[n].intcast();
        spill_slot  := c.f.tmp[t].slot;
        r1 := m.regs&[n];
        if rfind(m0&, t) { r |
            c.pmadd(TMP(r1), TMP(r), c.f.tmp[t].cls);
        } else {
            if spill_slot != -1 {
                c.pmadd(TMP(r1), SLOT(spill_slot), c.f.tmp[t].cls);
            }
        }
    };
    for(first_move, past_last_move) { ip |
        if ip.to != QbeNull {
            c.rfree(m, ip.to.val());
        };
        r := ip.arg&[0].val();
        if rfind(m, r).is_none() {
            c.radd(m, r, r);
        };
    };
    // until now, we've just been updating the RMap bookkeeping 
    // to match the goal register state (without any emits). 
    c.pmgen();
    first_move
}

fn do_block_instructions(c: *RegaCtx, b: *Qbe.Blk, cur: *RMap) void = {
    T := c.f.globals.target&;
    if rtype(b.jmp.arg) == .RTmp {
        b.jmp.arg = c.ralloc(cur, b.jmp.arg.val().intcast());
    };
    c.f.reset_scratch();
    for_insts_rev b { i1 |
        continue :: local_return;
        i := c.f.emit(i1[][]);
        just_freed := -1;
        
        @debug_assert(!@is(i.op(), .blit0, .blit1, .sel0, .sel1), "frontend op % in rega", i.op());
        @match(i.op()) {
            fn copy() => {
                if is_copy_from_reg(i) {
                    // consecutive copies from registers are handled seperatly. 
                    c.f.unemit(i);
                    i1[] = c.do_parallel_moves(b, i1[], cur);
                    // :LookAtLastInst but just for logging
                    c.added_moves += c.f.globals.curi.ptr_diff(i.offset(1)).trunc();
                    continue();
                };
                if isreg(i.to) && rtype(i.arg&[0]) == .RTmp {
                    c.sethint(i.arg&[0].val(), i.to.val());  // try to arrange so this copy is a nop.
                };
            }
            fn call() => {
                    discard := Array(i32, 2).ptr_from_int(0);
                    needed_at_call := {T.argregs}(i.arg&[1], discard).bit_or(T.rglob);
                    to_free := T.caller_saved.bit_and(bit_not needed_at_call);
                    
                    for_bits to_free { r |
                        c.rfree(cur, r);
                    };
            }
            @default => ();
        };
        
        if i.to != QbeNull {
            @debug_assert(i.op() != .call);
            @debug_assert(rtype(i.to) == .RTmp, "can only assign tmp");
            r := i.to.val();
            if r >= Qbe.Tmp0 || !T.rglob.bit_is_set(r) {
                goto_args :: local_return;
                just_freed = c.rfree(cur, r);
                if just_freed == -1 {
                    if i.op() == .cas1 { // HACK
                        i.to = QbeNull;
                        goto_args();
                    };
                    // DCE: if we didn't need the value later, we don't need the instruction that creates it. 
                    @debug_assert(!isreg(i.to), "DCE reg");
                    c.f.unemit(i);
                    continue();
                };
                i.to = TMP(just_freed);
                
                if i.op() == .copy && rtype(i.arg&[0]) == .RTmp {
                    c.sethint(i.arg&[0].val(), just_freed);  // try to arrange so this copy is a nop.
                }
            };
        };
        each i.arg& { arg | 
            @match(rtype(arg[])) {
                fn RMem() => {
                    m := c.f.get_memory(arg[]);
                    if rtype(m.base) == .RTmp {
                        m.base = c.ralloc(cur, m.base.val().intcast());
                    };
                    if rtype(m.index) == .RTmp {
                        m.index = c.ralloc(cur, m.index.val().intcast());
                    };
                }
                fn RTmp() => {
                    arg[] = c.ralloc(cur, arg[].val().intcast());
                }
                @default => ();
            };
        };
        was_nop_copy := i.op() == .copy && i.to == i.arg&[0];
        if was_nop_copy { 
            c.f.unemit(i);
        };
    };
    c.f.copy_instructions_from_scratch(b);
}

/* register allocation
 * depends on rpo, phi, (and obviously spill)
 */
fn register_allocation(f: *Qbe.Fn) void = {
    /* 1. setup */
    c: RegaCtx = (f = f, regu = 0, pm = PMove.list(temp()), added_moves = 0, added_blocks = 0); c := c&;
    rl := Array(i32, Qbe.Tmp0).zeroed(); rl := rl&.items();
    end := temp().alloc_zeroed(RMap, f.nblk.zext());
    beg := temp().alloc_zeroed(RMap, f.nblk.zext());
    each(end, fn(it) => bsinit(it.live&, f.ntmp.zext()));
    each(beg, fn(it) => bsinit(it.live&, f.ntmp.zext()));
    cur := temp().alloc_zeroed(RMap, 1).as_ptr();
    old := temp().alloc_zeroed(RMap, 1).as_ptr();
    bsinit(cur.live&, f.ntmp.zext());
    bsinit(old.live&, f.ntmp.zext());
    // (t.hint.register is setup in spill)
    
    /* 2. assign registers */
    for_blocks_rpo_forward f { b | 
        n: i64 = b.id.zext();
        cur.len = 0;
        bszero(cur.live&);
        
        // From spill pass, we know which tmps need to be in registers at the end of this block. 
        live_at_end_count := 0;
        for(b.out&, Qbe.Tmp0) { t | 
            j := live_at_end_count;
            live_at_end_count += 1;
            rl[j] = t.intcast();
        };
        T := f.globals.target&;
        @debug_assert(live_at_end_count.intcast() <= T.ngpr + T.nfpr, "too many live tmps");
        for_bits b.out.t[] { r |
            c.radd(cur, r, r);
        };
        // First give everyone a chance to get thier hinted register
        range(0, live_at_end_count) { j |
            c.ralloctry(cur, rl[j], true);
        };
        // Then the rest just get forced somewhere. 
        range(0, live_at_end_count) { j |
            c.ralloc(cur, rl[j]);
        };
        rcopy(end[n]&, cur);
    
        c.do_block_instructions(b, cur); 
        bscopy(b.in&, cur.live&);
        for_phi b { p | 
            if rtype(p.to) == .RTmp {
                bsclr(b.in&, p.to.val());
            }
        };
        rcopy(beg.index(n), cur);
    };

    // :Explain
    /* 3. emit copies shared by multiple edges
     * to the same block */
    for_blocks f { s |
        continue :: local_return;
        if(s.npred <= 1, => continue());
        m := beg[s.id.zext()]&;

        /* rl maps a register that is live at the
         * beginning of s to the one used in all
         * predecessors (if any, -1 otherwise) */
        rl.set_zeroed();

        /* to find the register of a phi in a
         * predecessor, we have to find the
         * corresponding argument */
        for_phi s { p |
            continue :: local_return;
            if(rtype(p.to) != .RTmp, => continue());
            r := rfind(m, p.to.val()).or(=> continue());
            r := r.intcast();
            range(0, p.narg.zext()) { u |
                continue :: local_return;
                b := p.blk[u];
                src := p.arg[u];
                if(rtype(src) != .RTmp, => continue());
                //@debug_assert(usecheck(src, p.cls, f), "phi cls mismatch % in $%", src, f.name());  // bad if arg was spilled to 4 byte slot load a 8 bytes expecting it to be zero extended.
                x := or rfind(end.index(b.id.zext()), src.val()) {
                    continue() /* spilled */
                };
                rl[r] = if(rl[r] == 0 || rl[r] == x, => x, => -1);
            };
            if rl[r] == 0 {
                rl[r] = -1;
            };
        };
        /* process non-phis temporaries */
        range(0, m.len) { j | 
            continue :: local_return;
            t := m.tmps&[j];
            r := m.regs&[j].intcast();
            if rl[r] != 0 || t < Qbe.Tmp0 /* todo, remove this */ {
                continue();
            };
            for_pred s { bp |
                continue :: local_return;
                x := or rfind(end.index(bp.id.zext()), t.intcast()) {
                    continue() /* spilled */
                };
                rl[r] = if(rl[r] == 0 || rl[r] == x, => x, => -1);
            };
            if rl[r] == 0 {
                rl[r] = -1;
            };
        };

        c.pm&.clear();
        range(0, m.len) { j |
            t := m.tmps&[j].intcast();
            r := m.regs&[j].intcast();
            x := rl[r].intcast();
            @debug_assert(x != 0 || t < Qbe.Tmp0 /* todo, ditto */);
            if x > 0 && !bshas(m.live&, x) {
                c.pmadd(TMP(x), TMP(r), f.tmp[t].cls);
                m.regs&[j] = x.intcast();
                bsset(m.live&, x);
            }
        };
        f.reset_scratch();
        c.pmgen();
        moves_inserted := f.len_scratch();
        if moves_inserted != 0 {
            c.added_moves += moves_inserted.trunc();
            i: RawList(Qbe.Ins) = init(temp(), s.ins.len + moves_inserted);
            i&.push_all_assume_capacity(f.globals.curi.slice(moves_inserted));  // :LookAtLastInst
            i&.push_all_assume_capacity(s.ins.items());
            s.ins = i;
        };
    };

    when_debug(f, .RegAlloc) { out |
        debug_dump(f, beg, end, out);
    };
    
    // :Explain
    /* 4. emit remaining copies in new blocks */
    emit_phi_blocks(c, beg, end);
    
    // phis have been lowered to moves so sanity assert that nobody tries to look at them after this pass. 
    for_blocks f { b |
        b.phi = Qbe.Phi.ptr_from_int(0);
    };
    f.reg = c.regu;

    when_debug(f, .RegAlloc) { out |
        if c.added_moves != 0 || c.added_blocks != 0 {
            // some of the loads/stores are double counted as moves but not all. 
            @fmt(out, "\n## Register allocation statistics:\n");
            @fmt(out, "#\t(moves: %, blocks: %, loads: %, stores: %)\n", c.added_moves, c.added_blocks, c.loads, c.stores);
        };
        @fmt(out, "\n## After register allocation:\n");
        printfn(f, out);
    };
}

fn emit_phi_blocks(c: *RegaCtx, beg: []RMap, end: []RMap) void = {
    blist := Qbe.Blk.ptr_from_int(0);
    f := c.f;
    for_blocks f { b | 
        @debug_assert(!b.s1.identical(b.s2) || b.s2.is_null(), "i think these are removed and my behaviour here differs if not"); // :SketchyIterTargets
        for_jump_targets_mut b { ps |
            s := ps[];
            c.pm&.clear();
            for_phi s { p |
                continue :: local_return;
                dst := p.to;
                @debug_assert(rtype(dst) == .RSlot || rtype(dst) == .RTmp);
                if rtype(dst) == .RTmp {
                    r := or rfind(beg.index(s.id.zext()), dst.val()) {
                        continue()
                    };
                    dst = TMP(r);
                };
                u := index_in_phi(b, p);
                src := p.arg[u];
                if rtype(src) == .RTmp {
                    //@debug_assert(usecheck(src, p.cls, f), "phi cls mismatch % in $%", src, f.name());
                    src = c.rref(end.index(b.id.zext()), src.val());
                };
                c.pmadd(src, dst, p.cls);
            };
            for(s.in&, Qbe.Tmp0) { t |
                src := c.rref(end.index(b.id.zext()), t);
                dst := c.rref(beg.index(s.id.zext()), t);
                c.pmadd(src, dst, f.tmp[t].cls);
            };
            f.reset_scratch();
            c.pmgen();
            moves_inserted := f.len_scratch();
            if moves_inserted != 0 {
                b1 := newblk();
                b1.loop = (b.loop + s.loop) / 2;
                b1.link = blist;
                blist = b1;
                f.nblk += 1;
                name_phi_block(f, b, s, b1);
                c.added_moves += moves_inserted.trunc();
                c.added_blocks += 1;
                f.copy_instructions_from_scratch(b1);
                b1.jmp.type = .jmp;
                b1.s1 = s;
                ps[] = b1;
            };
        };
        if b.link.is_null() {
            b.link = blist;
            return();
        }
    };
};

fn name_phi_block(f: *Qbe.Fn, source: *Qbe.Blk, dest: *Qbe.Blk, inserted: *Qbe.Blk) void #inline = {
    @if(f.track_ir_names()) {
        inserted.name = @tfmt("%_%", source.name(), dest.name());
    };
}
