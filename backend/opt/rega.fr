// Adapted from Qbe. MIT License. © 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//!
//! The overarching goal is to fulfil register constraints with as mov instructions inserted as possible. 
//! > Recall that the spill pass has already ensured we have enough registers for all the values live at any given time. 
//!
//! There are a few situations that require values to be in specific registers:
//! - The calling convention specifies where to put arguments and returns.
//! - Some ISAs have instructions that can only access certain registers (ie. x64 mul/div/shift). 
//! Before this pass those constraints are expressed as copies between our infinite tmps and the required registers. 
//! Now we try to arrange it so values are already where we need them so the copy can be removed.
//!
//! Similar for lowering phi nodes.
//!

RMap :: @struct(
    // parallel arrays. the tmp value t[i] is in register r[i]
    tmps: Array(i32, Qbe.Tmp0),
    regs: Array(i32, Qbe.Tmp0),
    wait_list: Array(i32, Qbe.Tmp0), /* for unmatched hints */ // [register] = temp that wants to be there.
    live: Qbe.BSet,
    len: i64,  // numbers of registers used so far
);

PMove :: @struct(
    src:  Qbe.Ref, 
    dest: Qbe.Ref, 
    cls:  Qbe.Cls, 
);

RegaCtx :: @struct(
    f: *Qbe.Fn,
    regu: u64,       /* registers used */
    pm: List(PMove), /* parallel move constructed */
    loop:  i32,      /* current loop level */
    added_moves:  u32, 
    added_blocks: u32,
);

fn hint(c: *RegaCtx, t: i64) *i32 #inline = 
    c.f.tmp[phi_dest(t, c.f)].hint.register&;

fn sethint(c: *RegaCtx, t: i64, r: i64) void = {
    p := c.f.tmp[phi_dest(t, c.f)]&;
    if p.hint.register == -1 || p.hint.weight > c.loop {
        p.hint.register = r.intcast();
        p.hint.weight = c.loop;
        c.f.tmp[t].visit = -1;
    }
}

fn rcopy(dest: *RMap, src: *RMap) void = {
    dest.tmps&.items().copy_from(src.tmps&.items());
    dest.regs&.items().copy_from(src.regs&.items());
    dest.wait_list&.items().copy_from(src.wait_list&.items());
    bscopy(dest.live&, src.live&);
    dest.len = src.len;
}

fn rfind(m: *RMap, t: i64) ?i32 = {  
    range(0, m.len) { i | 
        if m.tmps&[i] == t.intcast() {
            r := m.regs&[i];
            @debug_assert(t >= Qbe.Tmp0 || r.intcast() == t, "not allocated to itself");
            return(Some = r);
        }
    };
    .None
}

fn rref(c: *RegaCtx, m: *RMap, t: i64) Qbe.Ref = {
    ::if_opt(i32, Qbe.Ref);
    if rfind(m, t) { r |
        TMP(r)
    } else {
        s := c.f.tmp[t].slot;
        @debug_assert(s != -1, "should have spilled");
        SLOT(s)
    }
}

fn radd(c: *RegaCtx, m: *RMap, t: i64, r: i64) void = {
    T := c.f.globals.target&;
    @debug_assert(t >= Qbe.Tmp0 || t == r, "invalid temporary");
    is_int   := T.gpr.bit_is_set(r);
    is_float := T.fpr.bit_is_set(r);
    @debug_assert(is_int || is_float, "invalid register %", r);
    @debug_assert(!bshas(m.live&, t), "temporary has mapping");
    @debug_assert(!bshas(m.live&, r), "register already allocated");
    @debug_assert(m.len.intcast() <= T.ngpr+T.nfpr, "too many mappings");
    
    bsset(m.live&, t);
    bsset(m.live&, r);
    m.tmps&[m.len] = t.intcast();
    m.regs&[m.len] = r.intcast();
    m.len += 1;
    c.regu = c.regu.bit_or(BIT(r));
}

fn ralloctry(c: *RegaCtx, m: *RMap, t: i32, try: bool) Qbe.Ref = {
    t := t.intcast();
    if t < Qbe.Tmp0 {   // this is a register
        @debug_assert(bshas(m.live&, t), "reg must be allocated to itself");
        return(TMP(t));
    };
    if bshas(m.live&, t) {  // we're already in a register
        r := rfind(m, t).expect("thought we allocated but don't know where");
        return(TMP(r));
    };
    r_hinted := c.f.tmp[t].visit;
    if r_hinted == -1 || bshas(m.live&, r_hinted.intcast()) {
        r_hinted = c.hint(t)[];
    };
    T := c.f.globals.target&;
    r_allocated := if r_hinted != -1 && !bshas(m.live&, r_hinted.zext()) {
        r_hinted.intcast()
    } else {
        found :: local_return;
        if try {
            return(QbeNull);
        };
        to_avoid := c.f.tmp[phi_dest(t, c.f)].hint.avoid;
        
        ::if(Ty(i32, i32));
        of_kind := @if(c.f.tmp[t].cls.is_int(), T.gpr, T.fpr);
        allow := of_kind.bit_and(bit_not m.live.t[]);
        
        // at least try not to get a bad place
        prefer := allow.bit_and(bit_not to_avoid);
        for_bits prefer { r |
            found(r);
        };
        // ok fine, anything will do
        for_bits allow { r |
            found(r);
        };
        c.f.debug_dump(m, c.f.globals.debug_out);
        printfn(c.f, c.f.globals.debug_out);
        @panic("no register free for % in %. this should be unreachable becuase of the spill pass.", c.f.tmp.index(t).name(), c.f.name());
    };
    c.radd(m, t, r_allocated);
    c.sethint(t, r_allocated);
    c.f.tmp[t].visit = r_allocated.intcast(); // remember that we were allocated this register for later. 
    r_hinted := c.hint(t)[].intcast();
    if r_hinted != -1 && r_hinted != r_allocated {
        // if we didn't get where we wanted, remember that we should be swapped in if r_hinted frees up. 

        // TODO: the hinting sounds like a good idea but seems to generate worse code. 
        //       is there something weird about the shape of code i output that confuses it somehow? 
        // 1098396 -> 1003912
        // qbe does this but it makes the code size 10% bigger and not faster. 
        // it seems to generate a useless move to restore local variables after a call? 
        // T64 was allocated R20 but wants R1
        // @println("T% was allocated R% but wants R%", t, r_allocated, r_hinted);
        // which like does make sense but we have to somehow remember that it's already there or it inserts a bunch of useless stuff
        // but its hard to think about because we're going backwards
        // is it too dumb to add an extra pass that tries harder to remove redundant copies? probably. 
        // the other places hints are added are useful it seems.
        // -- Mar 18, 2025
        /*
        R20 =l copy R1
    	# R1 =l copy R1
    	call $no_epar, C33
    	R1 =l copy R20
    	R20 =l copy R1
    	# R1 =l copy R1
    	call $no_epar, C33
        ------
	
        R20 =l copy R1
		R1 =l copy R20  <-- still dumb
		call $no_epar, C33
		R1 =l copy R20
		call $no_epar, C33
        */
        // the situation that it really hurts is that i use the env parameter so much,
        // (it you do it on import_c without preserve_franca_env, it's much less dramatic), 
        // it remembers on entry that env wants to be in x9 and then does an extra instruction 
        // of shuffleing every time it gets stomped by a call. 
        // that only explains why removing it in set_param_copy_hints helps, not here, 
        // so i think the problem is more general, that just adds an argument preserved accross every single call so it's more obvious. 
        // code size numbers kinda agree with that intuition:  
        // both disabled:                 912928
        // ralloctry disabled:            919640
        // set_param_copy_hints disabled: 940832
        // both enabled:                 1043424
        // set_param_copy_hints disabled only for x9 on arm: 935592 (so env is 1/4 of the cost there)
        // -- Sep 21, 2025
        @if(ADD_TO_REG_WAIT_LIST) {
            m.wait_list&[r_hinted] = t.intcast();
        };
    };
    TMP(r_allocated)
}

fn ralloc(c: *RegaCtx, m: *RMap, t: i32) Qbe.Ref #inline = 
    c.ralloctry(m, t, false);

fn bit_is_set(s: u64, i: i64) bool #inline = 
    s.bit_and(BIT(i)) != 0;

// this changes the order in regs/tmps
fn rfree(c: *RegaCtx, m: *RMap, t: i64) i64 = {
    T := c.f.globals.target&;
    @debug_assert(t >= Qbe.Tmp0 || !T.rglob.bit_is_set(t), "reg non-global");
    if !bshas(m.live&, t) {
        return(-1);
    };
    i := 0;
    while => m.tmps&[i].intcast() != t {
        @debug_assert(i+1 < m.len, "reg not found");
        i += 1;
    };
    rfree_at_index(c, m, t, i)
}

fn rfree_at_index(c: *RegaCtx, m: *RMap, t: i64, i: i64) i64 = {
    T := c.f.globals.target&;
    @debug_assert(i < m.len && m.tmps&[i].intcast() == t, "invalid rfree_at_index");
    r := m.regs&[i].intcast();
    bsclr(m.live&, t);
    bsclr(m.live&, r);
    m.len -= 1;
    m.regs&[i] = m.regs&[m.len];
    m.tmps&[i] = m.tmps&[m.len];
    @debug_assert(t >= Qbe.Tmp0 || t == r, "invalid rfree_at_index.");
    r
}

fn pmadd(c: *RegaCtx, src: Qbe.Ref, dst: Qbe.Ref, k: Qbe.Cls) void #inline = 
    c.pm&.push(src = src, dest = dst, cls = k);

PMStat :: @enum(i64) (ToMove, Moving, Moved);

ADD_TO_REG_WAIT_LIST :: false;

// Rideau, L., Serpette, B.P. & Leroy, X. Tilting at Windmills with Coq: Formal Verification of a Compilation Algorithm for Parallel Moves. J Autom Reasoning 40, 307–326 (2008).
// https://xavierleroy.org/publi/parallel-move.pdf

// note: k is a pointer. we mutate back up through levels of recursion. 
fn pmrec(c: *RegaCtx, status: []PMStat, i: i64, k: *Qbe.Cls) i64 = {
    /* note, this routine might emit
     * too many large instructions
     */
    if c.pm&[i].src == c.pm&[i].dest {
        status[i] = .Moved;
        return(-1);
    };
    @debug_assert(KBASE(c.pm&[i].cls) == KBASE(k[]), "bad kbase");
    ::assert(Qbe.Cls.Kw.raw().bit_or(Qbe.Cls.Kl.raw()) == Qbe.Cls.Kl.raw(), "Cls numbers");
    ::assert(Qbe.Cls.Ks.raw().bit_or(Qbe.Cls.Kd.raw()) == Qbe.Cls.Kd.raw(), "Cls numbers");
    k[] = @as(Qbe.Cls) @as(i32) k[].raw().bit_or(c.pm&[i].cls.raw());
    j := 0;
    while => j < c.pm.len && c.pm&[j].dest != c.pm&[i].src {
        j += 1;
    };
    cycle := -1;
    ::if(PMStat);
    s := if(j == c.pm.len, => PMStat.Moved, => status[j]);
    @match(s) {
        fn Moving() => {
            cycle = j; /* start of cycle */
            c.f.emit(.swap, k[], QbeNull, c.pm&[i].src, c.pm&[i].dest);
        }
        fn ToMove() => {
            status[i] = .Moving; // TODO: not insane error message if you forget the dot so it tries to reference the overload set created for the switch case above.  
            cycle = c.pmrec(status, j, k);
            if cycle == i {
                cycle = -1; /* end of cycle */
            } else {
                if cycle != -1 {
                    c.f.emit(.swap, k[], QbeNull, c.pm&[i].src, c.pm&[i].dest);
                } else {
                    cycle = -1;
                    c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx
                }
            };
        }
        fn Moved() => {
            cycle = -1;
            c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx same
        }
    };
    status[i] = .Moved;
    cycle
}

// TODO: this is arm int only
fn pmrec_NEW(c: *RegaCtx, status: []PMStat, i: i64, k: *Qbe.Cls) void = {
    if c.pm&[i].src == c.pm&[i].dest {
        status[i] = .Moved;
        return();
    };
    @debug_assert(KBASE(c.pm&[i].cls) == KBASE(k[]), "bad kbase");
    ::assert(Qbe.Cls.Kw.raw().bit_or(Qbe.Cls.Kl.raw()) == Qbe.Cls.Kl.raw(), "Cls numbers");
    ::assert(Qbe.Cls.Ks.raw().bit_or(Qbe.Cls.Kd.raw()) == Qbe.Cls.Kd.raw(), "Cls numbers");
    k[] = @as(Qbe.Cls) @as(i32) k[].raw().bit_or(c.pm&[i].cls.raw());
    status[i] = .Moving;
    range(0, c.pm.len) { j |
        if c.pm&[i].dest == c.pm&[j].src {
            @match(status[j]) {
                fn Moving() => {
                    t := TMP(18);  @debug_assert(c.f.globals.goal.arch == .aarch64 && k[].is_int());
                    c.f.emit(.copy, c.pm&[j].cls, t, c.pm&[j].src, QbeNull); // xxx same
                    c.pm&[j].src = t;
                }
                fn ToMove() => c.pmrec_NEW(status, j, k);
                fn Moved() => ()
            };
        }
    };
    c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx same
    status[i] = .Moved;
}

SWAP_PM :: true;

fn pmgen(c: *RegaCtx) void = {
    ::enum_basic(PMStat);
    status := temp().alloc_zeroed(PMStat, c.pm.len);
    ::assert(PMStat.zeroed() == .ToMove, "zero init is ToMove");
    
    end := c.f.globals.curi;
    range(0, c.pm.len) { i | 
        if status[i] == .ToMove {
            k := c.pm&[i].cls;
            if SWAP_PM {
                c.pmrec(status, i, k&);
            } else {
                c.pmrec_NEW(status, i, k&);
            }
        };
    };
    if !SWAP_PM {
        start := c.f.globals.curi;
        ins := between(start, end);
        ins.reverse();
    }
}

fn move(c: *RegaCtx, r: i64, to: Qbe.Ref, m: *RMap) void = {
    r1 := if(to == QbeNull, => -1, => c.rfree(m, to.val()));
    if bshas(m.live&, r) {
        /* r is used and not by to */
        @debug_assert(r1 != r, "already in reg");
        n := 0;
        while => m.regs&[n].intcast() != r {
            @debug_assert(n+1 < m.len, "reg not found.");
            n += 1;
        };
        t := m.tmps&[n];
        rfree_at_index(c, m, t.intcast(), n);
        bsset(m.live&, r);
        c.ralloc(m, t);
        bsclr(m.live&, r);
    };
    t := if(to == QbeNull, => r, => to.val());
    c.radd(m, t, r);
}

// when you have a group of copies from registers to tmps, 
// you need to make sure none of the dest tmps are allocated to registers later used as a src.
// it has to execute as though all the moves happened at the same time, without letting any stomp eachother. 
fn do_parallel_moves(c: *RegaCtx, b: *Qbe.Blk, last_move: *Qbe.Ins, m: *RMap) *Qbe.Ins = {
    T := c.f.globals.target&;
    m0 := m[]; /* okay since we don't use m0.b */
    m0.live.t = u64.ptr_from_int(0); // assert ^
    past_last_move := last_move.offset(1);
    first_move := {
        i := past_last_move;
        dowhile {
            i = i.offset(-1);
            c.move(i.arg&[0].val(), i.to, m);
            !i.identical(b.ins.ptr) && is_copy_from_reg(i.offset(-1))
        };
        i
    };
    
    // :RequireCopyAfterCall
    @debug_assert(m0.len <= m.len);
    is_after_call := !first_move.identical(b.ins.ptr) && @is(first_move.offset(-1).op(), .call);
    if is_after_call {
        discard := Array(i32, 2).ptr_from_int(0); // TODO: helper for this 
        call_abi_info := first_move.offset(-1)[].arg&[1];
        def := {T.retregs}(call_abi_info, discard).bit_or(T.rglob);
        to_move := T.caller_saved.bit_and(bit_not def);
        for_bits to_move { r |
            c.move(r, QbeNull, m);
        };
    };
    c.pm&.clear();
    range(0, m.len) { n |
        t  := m.tmps&[n].intcast();
        spill_slot  := c.f.tmp[t].slot;
        r1 := m.regs&[n];
        if rfind(m0&, t) { r |
            c.pmadd(TMP(r1), TMP(r), c.f.tmp[t].cls);
        } else {
            if spill_slot != -1 {
                c.pmadd(TMP(r1), SLOT(spill_slot), c.f.tmp[t].cls);
            }
        }
    };
    for(first_move, past_last_move) { ip |
        if ip.to != QbeNull {
            c.rfree(m, ip.to.val());
        };
        r := ip.arg&[0].val();
        if rfind(m, r).is_none() {
            c.radd(m, r, r);
        };
    };
    // until now, we've just been updating the RMap bookkeeping 
    // to match the goal register state (without any emits). 
    c.pmgen();
    first_move
}

fn prio1(c: *RegaCtx, r1: Qbe.Ref, r2: Qbe.Ref) bool = {
    /* trivial heuristic to begin with,
     * later we can use the distance to
     * the definition instruction
     */
    c.hint(r1.val())[] != -1
}

fn insert(c: *RegaCtx, r: *Qbe.Ref, rs: *Array(*Qbe.Ref, 4), p: i64) void = {
    i := p;
    rs[i] = r;
    while => i > 0 && c.prio1(r[], rs[i - 1][]) {
        i -= 1;
        rs[i+1] = rs[i];
        rs[i] = r;
    }
}

fn do_block_instructions(c: *RegaCtx, b: *Qbe.Blk, cur: *RMap) void = { // #log_ir("RIS") = {
    T := c.f.globals.target&;
    reg_args := Array(*Qbe.Ref, 4).zeroed(); reg_args := reg_args&;

    if rtype(b.jmp.arg) == .RTmp {
        b.jmp.arg = c.ralloc(cur, b.jmp.arg.val().intcast());
    };
    c.f.reset_scratch();
    curi := c.f.globals.curi&; // :LookAtLastInst (but just to pop off dead code)
    for_insts_rev b { i1 |
        continue :: local_return;
        i := c.f.emit(i1[][]);
        just_freed := -1;
        other :: fn() => {
            if i.to != QbeNull  {
                @debug_assert(rtype(i.to) == .RTmp, "can only assign tmp");
                r := i.to.val();
                if r >= Qbe.Tmp0 || !T.rglob.bit_is_set(r) {
                    goto_args :: local_return;
                    just_freed = c.rfree(cur, r);
                    if just_freed == -1 {
                        if i.op() == .cas1 { // HACK
                            i.to = QbeNull;
                            goto_args();
                        };
                        // DCE: if we didn't need the value later, we don't need the instruction that creates it. 
                        @debug_assert(!isreg(i.to), "DCE reg");
                        curi[] = curi[].offset(1); // pop off the emit() from this iteration
                        continue();
                    };
                    i.to = TMP(just_freed);
                };
            };
        };
        
        @debug_assert(!@is(i.op(), .blit0, .blit1, .sel0, .sel1), "frontend op % in rega", i.op());
        @match(i.op()) {
            fn copy() => {
                if is_copy_from_reg(i) {
                    // consecutive copies from registers are handled seperatly. 
                    curi[] = curi[].offset(1);  // pop off the emit() from this iteration
                    i1[] = c.do_parallel_moves(b, i1[], cur);
                    c.added_moves += curi[].ptr_diff(i.offset(1)).trunc();
                    continue();
                };
                if isreg(i.to) && rtype(i.arg&[0]) == .RTmp {
                    c.sethint(i.arg&[0].val(), i.to.val());  // try to arrange so this copy is a nop.
                };
                other();
            }
            @default => {
                if (@is(i.op(), .call)) {
                    discard := Array(i32, 2).ptr_from_int(0);
                    needed_at_call := {T.argregs}(i.arg&[1], discard).bit_or(T.rglob);
                    to_free := T.caller_saved.bit_and(bit_not needed_at_call);
                    
                    for_bits to_free { r |
                        c.rfree(cur, r);
                    };
                } else {
                    other();
                };
            };
        };
        reg_args_count := 0;
        each i.arg& { arg | 
            @match(rtype(arg[])) {
                fn RMem() => {
                    m := c.f.get_memory(arg[]);
                    if rtype(m.base) == .RTmp {
                        c.insert(m.base&, reg_args, reg_args_count);
                        reg_args_count += 1;
                    };
                    if rtype(m.index) == .RTmp {
                        c.insert(m.index&, reg_args, reg_args_count);
                        reg_args_count += 1;
                    };
                }
                fn RTmp() => {
                    c.insert(arg, reg_args, reg_args_count);
                    reg_args_count += 1;
                }
                @default => ();
            };
        };
        range(0, reg_args_count) { r |
            reg_args[r][] = c.ralloc(cur, reg_args[r][].val().intcast());
        };
        was_nop_copy := i.op() == .copy && i.to == i.arg&[0];
        if was_nop_copy { 
            // :LookAtLastInst
            c.f.globals.curi = c.f.globals.curi.offset(1); // pop off the emit() from this iteration
        };

        /* try to change the register of a hinted
         * temporary if `just_freed` is available */
        if(just_freed == -1, => continue());
        t := cur.wait_list&[just_freed];
        if(t == 0, => continue());
        if(bshas(cur.live&, just_freed), => continue());
        r_hint := c.hint(t.intcast())[].intcast();
        if(r_hint != just_freed, => continue());
        rt := c.rfree(cur, t.intcast()).intcast();
        if(rt == -1, => continue());
        
        c.f.tmp[t.intcast()].visit = -1;
        c.ralloc(cur, t);
        @debug_assert(bshas(cur.live&, just_freed));
        c.f.emit(.copy, c.f.tmp[t.intcast()].cls, TMP(rt), TMP(just_freed), QbeNull);
        c.added_moves += 1;
        cur.wait_list&[just_freed] = 0;
        range(0, reg_args_count) { r | 
            if reg_args[r][] == TMP(rt) { // rT // TODO: rename. 
                reg_args[r][] = TMP(just_freed); 
            }
        };
        /* one could iterate this logic with
         * the newly freed rt, but in this case
         * the above loop must be changed */
    };
    c.f.copy_instructions_from_scratch(b);
}

/* comparison function to order temporaries
 * for allocation at the end of blocks */
fn prio2(c: *RegaCtx, t1: i64, t2: i32) i64 = {
    if c.f.tmp[t1].visit.intcast().bit_xor(c.f.tmp[t2.intcast()].visit.intcast()) < 0 { /* != signs */
        return(if(c.f.tmp[t1].visit != -1, => 1, => -1));
    };
    if c.hint(t1)[].intcast().bit_xor(c.hint(t2.intcast())[].intcast()) < 0 {
        return(if(c.hint(t1)[] != -1, => 1, => -1));
    };
    xxx := c.f.tmp[t1].cost;
    xxxx := c.f.tmp[t2.intcast()].cost;
    (xxx - xxxx).bitcast().intcast()
}

/* peel loop nests from inside out */
carve :: fn(ba: **Qbe.Blk, bb: **Qbe.Blk) bool = { 
    // TODO: flipping these conditions to >= generates less code but breaks backend/test/folding.fr
    @if(ba.loop == bb.loop, ba.id <= bb.id, ba.loop <= bb.loop)
};

/* register allocation
 * depends on rpo, phi, cost, (and obviously spill)
 */
fn register_allocation(f: *Qbe.Fn) void = {
    /* 1. setup */
    c: RegaCtx = (f = f, regu = 0, pm = PMove.list(temp()), loop = MAX_i32, added_moves = 0, added_blocks = 0); c := c&;
    rl := Array(i32, Qbe.Tmp0).zeroed(); rl := rl&.items();
    blk := temp().alloc_zeroed(*Qbe.Blk, f.nblk.zext());
    end := temp().alloc_zeroed(RMap, f.nblk.zext());
    beg := temp().alloc_zeroed(RMap, f.nblk.zext());
    each(end, fn(it) => bsinit(it.live&, f.ntmp.zext()));
    each(beg, fn(it) => bsinit(it.live&, f.ntmp.zext()));
    cur := temp().alloc_zeroed(RMap, 1).as_ptr();
    old := temp().alloc_zeroed(RMap, 1).as_ptr();
    bsinit(cur.live&, f.ntmp.zext());
    bsinit(old.live&, f.ntmp.zext());
    enumerate f.tmps() { t, it |
        ::if(i32);
        it.hint.register = if(t < Qbe.Tmp0, => t.intcast(), => -1);
        it.hint.weight = c.loop;
        it.visit = -1;
    };
    
    n := 0;
    for_blocks f { b |  
        blk[n] = b;
        n += 1;
    };
    c.set_param_copy_hints();
    sort :: quicksort(*Qbe.Blk, carve);
    blk.sort();
    
    /* 2. assign registers */
    for blk { b | 
        n: i64 = b.id.zext();
        loop := b.loop;
        cur.len = 0;
        bszero(cur.live&);
        cur.wait_list&.set_zeroed();
        
        // From spill pass, we know which tmps need to be in registers at the end of this block. 
        live_at_end_count := 0;
        for(b.out&, Qbe.Tmp0) { t | 
            j := live_at_end_count;
            live_at_end_count += 1;
            rl[j] = t.intcast();
            while => j > 0 && c.prio2(t, rl[j-1]) > 0 {
                j -= 1;
                rl[j+1] = rl[j];
                rl[j] = t.intcast();
            };
        };
        T := f.globals.target&;
        @debug_assert(live_at_end_count.intcast() <= T.ngpr + T.nfpr, "too many live tmps");
        for_bits b.out.t[] { r |
            c.radd(cur, r, r);
        };
        // First give everyone a chance to get thier hinted register
        range(0, live_at_end_count) { j |
            c.ralloctry(cur, rl[j], true);
        };
        // Then the rest just get forced somewhere. 
        range(0, live_at_end_count) { j |
            c.ralloc(cur, rl[j]);
        };
        rcopy(end[n]&, cur);
    
        c.do_block_instructions(b, cur); 
        bscopy(b.in&, cur.live&);
        for_phi b { p | 
            if rtype(p.to) == .RTmp {
                bsclr(b.in&, p.to.val());
            }
        };
        rcopy(beg.index(n), cur);
    };

    // :Explain
    /* 3. emit copies shared by multiple edges
     * to the same block */
    for_blocks f { s |
        continue :: local_return;
        if(s.npred <= 1, => continue());
        m := beg[s.id.zext()]&;

        /* rl maps a register that is live at the
         * beginning of s to the one used in all
         * predecessors (if any, -1 otherwise) */
        rl.set_zeroed();

        /* to find the register of a phi in a
         * predecessor, we have to find the
         * corresponding argument */
        for_phi s { p |
            continue :: local_return;
            if(rtype(p.to) != .RTmp, => continue());
            r := rfind(m, p.to.val()).or(=> continue());
            r := r.intcast();
            range(0, p.narg.zext()) { u |
                continue :: local_return;
                b := p.blk[u];
                src := p.arg[u];
                if(rtype(src) != .RTmp, => continue());
                x := or rfind(end.index(b.id.zext()), src.val()) {
                    continue() /* spilled */
                };
                rl[r] = if(rl[r] == 0 || rl[r] == x, => x, => -1);
            };
            if rl[r] == 0 {
                rl[r] = -1;
            };
        };
        /* process non-phis temporaries */
        range(0, m.len) { j | 
            continue :: local_return;
            t := m.tmps&[j];
            r := m.regs&[j].intcast();
            if rl[r] != 0 || t < Qbe.Tmp0 /* todo, remove this */ {
                continue();
            };
            for_pred s { bp |
                continue :: local_return;
                x := or rfind(end.index(bp.id.zext()), t.intcast()) {
                    continue() /* spilled */
                };
                rl[r] = if(rl[r] == 0 || rl[r] == x, => x, => -1);
            };
            if rl[r] == 0 {
                rl[r] = -1;
            };
        };

        c.pm&.clear();
        range(0, m.len) { j |
            t := m.tmps&[j].intcast();
            r := m.regs&[j].intcast();
            x := rl[r].intcast();
            @debug_assert(x != 0 || t < Qbe.Tmp0 /* todo, ditto */);
            if x > 0 && !bshas(m.live&, x) {
                c.pmadd(TMP(x), TMP(r), f.tmp[t].cls);
                m.regs&[j] = x.intcast();
                bsset(m.live&, x);
            }
        };
        f.reset_scratch();
        c.pmgen();
        moves_inserted := f.len_scratch();
        if moves_inserted != 0 {
            c.added_moves += moves_inserted.trunc();
            i: RawList(Qbe.Ins) = init(temp(), s.ins.len + moves_inserted);
            i&.push_all_assume_capacity(f.globals.curi.slice(moves_inserted));  // :LookAtLastInst
            i&.push_all_assume_capacity(s.ins.items());
            s.ins = i;
        };
    };

    when_debug(f, .RegAlloc) { out |
        debug_dump(f, beg, end, out);
    };
    
    // :Explain
    /* 4. emit remaining copies in new blocks */
    emit_phi_blocks(c, beg, end);
    
    // phis have been lowered to moves so sanity assert that nobody tries to look at them after this pass. 
    for_blocks f { b |
        b.phi = Qbe.Phi.ptr_from_int(0);
    };
    f.reg = c.regu;

    when_debug(f, .RegAlloc) { out |
        if c.added_moves != 0 || c.added_blocks != 0 {
            @fmt(out, "\n## Register allocation statistics:\n");
            @fmt(out, "#\tnew moves:  %\n", c.added_moves);
            @fmt(out, "#\tnew blocks: %\n", c.added_blocks);
        };
        @fmt(out, "\n## After register allocation:\n");
        printfn(f, out);
    };
}

fn emit_phi_blocks(c: *RegaCtx, beg: []RMap, end: []RMap) void = {
    blist := Qbe.Blk.ptr_from_int(0);
    f := c.f;
    for_blocks f { b | 
        @debug_assert(!b.s1.identical(b.s2) || b.s2.is_null(), "i think these are removed and my behaviour here differs if not"); // :SketchyIterTargets
        for_jump_targets_mut b { ps |
            s := ps[];
            c.pm&.clear();
            for_phi s { p |
                continue :: local_return;
                dst := p.to;
                @debug_assert(rtype(dst) == .RSlot || rtype(dst) == .RTmp);
                if rtype(dst) == .RTmp {
                    r := or rfind(beg.index(s.id.zext()), dst.val()) {
                        continue()
                    };
                    dst = TMP(r);
                };
                u := 0;
                while => !p.blk[u].identical(b) {
                    @debug_assert(u+1 < p.narg.zext());
                    u += 1;
                };
                src := p.arg[u];
                if rtype(src) == .RTmp {
                    src = c.rref(end.index(b.id.zext()), src.val());
                };
                c.pmadd(src, dst, p.cls);
            };
            for(s.in&, Qbe.Tmp0) { t |
                src := c.rref(end.index(b.id.zext()), t);
                dst := c.rref(beg.index(s.id.zext()), t);
                c.pmadd(src, dst, f.tmp[t].cls);
            };
            f.reset_scratch();
            c.pmgen();
            moves_inserted := f.len_scratch();
            if moves_inserted != 0 {
                b1 := newblk();
                b1.loop = (b.loop + s.loop) / 2;
                b1.link = blist;
                blist = b1;
                f.nblk += 1;
                name_phi_block(f, b, s, b1);
                c.added_moves += moves_inserted.trunc();
                c.added_blocks += 1;
                f.copy_instructions_from_scratch(b1);
                b1.jmp.type = .jmp;
                b1.s1 = s;
                ps[] = b1;
            };
        };
        if b.link.is_null() {
            b.link = blist;
            return();
        }
    };
};

fn name_phi_block(f: *Qbe.Fn, source: *Qbe.Blk, dest: *Qbe.Blk, inserted: *Qbe.Blk) void #inline = {
    @if(f.track_ir_names()) {
        inserted.name = @tfmt("%_%", source.name(), dest.name());
    };
}

// we want our parameters to be assgned to thier abi registers so we don't actually have to do a copy. 
fn set_param_copy_hints(c: *RegaCtx) void = {
    // turning this off makes x64 fail tests. encoding.ssa mem4.ssa 
    // which is scary cause it should be just an optimisation. 
    // so hopefully ive revealed some other subtle problem. TODO! 
    @if(!ADD_TO_REG_WAIT_LIST && c.f.globals.goal.arch == .aarch64) return();
    // 1004012 -> 981364
    
    for_insts_forward c.f.start { i |
        if i.op() != .copy || !isreg(i.arg&[0]) {
            return();
        };
        @debug_assert(rtype(i.to) == .RTmp, "can only copy to tmp");
        abi_reg := i.arg&[0].val();
        c.sethint(i.to.val(), abi_reg);
    };
}
