// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//!
//! The overarching goal is to fulfil register constraints with as mov instructions inserted as possible. 
//! > Recall that the spill pass has already ensured we have enough registers for all the values live at any given time. 
//!
//! There are a few situations that require values to be in specific registers:
//! - The calling convention specifies where to put arguments and returns.
//! - Some ISAs have instructions that can only access certain registers (ie. x64 mul/div/shift). 
//! Before this pass those constraints are expressed as copies between our infinite tmps and the required registers. 
//! Now we try to arrange it so values are already where we need them so the copy can be removed.
//!
//! Similar for lowering phi nodes.
//!

// TODO: this doesn't get identical results to Qbe. 

RMap :: @struct(
    // parallel arrays. the tmp value t[i] is in register r[i]
    tmps: Array(i32, Qbe.Tmp0),
    regs: Array(i32, Qbe.Tmp0),
    wait_list: Array(i32, Qbe.Tmp0), /* for unmatched hints */ // [register] = temp that wants to be there.
    live: Qbe.BSet,
    len: i64,  // numbers of registers used so far
);
  
RegaCtx :: @struct(
    f: *Qbe.Fn,
    regu: u64,          /* registers used */
    pm: Array(@struct(
        src:  Qbe.Ref, 
        dest: Qbe.Ref, 
        cls:  Qbe.Cls, 
    ), Qbe.Tmp0),       /* parallel move constructed */
    npm:   i64,         /* size of pm */
    loop:  i32,         /* current loop level */
    added_moves:  u32, 
    added_blocks: u32,
);

fn hint(c: *RegaCtx, t: i64) *i32 #inline = 
    c.f.tmp[phi_dest(t, c.f.tmp)].hint.register&;

fn sethint(c: *RegaCtx, t: i64, r: i64) void = {
    p := c.f.tmp[phi_dest(t, c.f.tmp)]&;
    if p.hint.register == -1 || p.hint.weight > c.loop {
        p.hint.register = r.intcast();
        p.hint.weight = c.loop;
        c.f.tmp[t].visit = -1;
    }
}

fn rcopy(dest: *RMap, src: *RMap) void = {
    dest.tmps&.items().copy_from(src.tmps&.items());
    dest.regs&.items().copy_from(src.regs&.items());
    dest.wait_list&.items().copy_from(src.wait_list&.items());
    bscopy(dest.live&, src.live&);
    dest.len = src.len;
}

fn rfind(m: *RMap, t: i64) ?i32 = {  
    range(0, m.len) { i | 
        if m.tmps&[i] == t.intcast() {
            r := m.regs&[i];
            @debug_assert(t >= Qbe.Tmp0 || r.intcast() == t, "not allocated to itself");
            return(Some = r);
        }
    };
    .None
}

::if_opt(i32, Qbe.Ref);
fn rref(c: *RegaCtx, m: *RMap, t: i64) Qbe.Ref = {
    if rfind(m, t) { r |
        TMP(r)
    } else {
        s := c.f.tmp[t].slot;
        @debug_assert(s != -1, "should have spilled");
        SLOT(s)
    }
}

fn radd(c: *RegaCtx, m: *RMap, t: i64, r: i64) void = {
    T := c.f.globals.target;
    @debug_assert(t >= Qbe.Tmp0 || t == r, "invalid temporary");
    is_int   := T.gpr0.intcast() <= r && r.intcast() < T.gpr0 + T.ngpr;
    is_float := T.fpr0.intcast() <= r && r.intcast() < T.fpr0 + T.nfpr;
    @debug_assert(is_int || is_float, "invalid register");
    @debug_assert(!bshas(m.live&, t), "temporary has mapping");
    @debug_assert(!bshas(m.live&, r), "register already allocated");
    @debug_assert(m.len.intcast() <= T.ngpr+T.nfpr, "too many mappings");
    
    bsset(m.live&, t);
    bsset(m.live&, r);
    m.tmps&[m.len] = t.intcast();
    m.regs&[m.len] = r.intcast();
    m.len += 1;
    c.regu = c.regu.bit_or(BIT(r));
}

fn ralloctry(c: *RegaCtx, m: *RMap, t: i32, try: bool) Qbe.Ref = {
    t := t.intcast();
    if t < Qbe.Tmp0 {   // this is a register
        @debug_assert(bshas(m.live&, t), "reg must be allocated to itself");
        return(TMP(t));
    };
    if bshas(m.live&, t) {  // we're already in a register
        r := rfind(m, t).expect("thought we allocated but don't know where");
        return(TMP(r));
    };
    r_hinted := c.f.tmp[t].visit;
    if r_hinted == -1 || bshas(m.live&, r_hinted.intcast()) {
        r_hinted = c.hint(t)[];
    };
    T := c.f.globals.target;
    r_allocated := if r_hinted != -1 && !bshas(m.live&, r_hinted.zext()) {
        r_hinted.intcast()
    } else {
        found :: local_return;
        if try {
            return(QbeNull);
        };
        to_avoid := c.f.tmp[phi_dest(t, c.f.tmp)].hint.avoid;
        to_avoid  = to_avoid.bit_or(m.live.t[]); // include those already allocated
        
        ::if(Ty(i32, i32));
        r0, count   := if(KBASE(c.f.tmp[t].cls) == 0, => (T.gpr0, T.ngpr), => (T.fpr0, T.nfpr));
        first, last := (r0.intcast(), intcast(r0 + count));
        
        // at least try not to get a bad place
        range(first, last) { r |
            if(!to_avoid.bit_is_set(r), => found(r));
        };
        // ok fine, anything will do
        range(first, last) { r |
            if(!bshas(m.live&, r), => found(r));
        };
        @panic("no more registers. this should be unreachable becuase of the spill pass");
    };
    c.radd(m, t, r_allocated);
    c.sethint(t, r_allocated);
    c.f.tmp[t].visit = r_allocated.intcast(); // remember that we were allocated this register for later. 
    r_hinted := c.hint(t)[].intcast();
    if r_hinted != -1 && r_hinted != r_allocated {
        // if we didn't get where we wanted, remember that we should be swapped in if r_hinted frees up. 
        m.wait_list&[r_hinted] = t.intcast();
    };
    TMP(r_allocated)
}

fn ralloc(c: *RegaCtx, m: *RMap, t: i32) Qbe.Ref #inline = 
    c.ralloctry(m, t, false);

fn bit_is_set(s: u64, i: i64) bool #inline = 
    s.bit_and(BIT(i)) != 0;

fn rfree(c: *RegaCtx, m: *RMap, t: i64) i64 = {
    T := c.f.globals.target;
    @debug_assert(t >= Qbe.Tmp0 || !T.rglob.bit_is_set(t));
    if !bshas(m.live&, t) {
        return(-1);
    };
    i := 0;
    while => m.tmps&[i].intcast() != t {
        @debug_assert(i+1 < m.len);
        i += 1;
    };
    r := m.regs&[i].intcast();
    bsclr(m.live&, t);
    bsclr(m.live&, r);
    m.len -= 1;
    m.tmps&.items().copy_overlapping(i, i + 1, m.len - i); // TODO: makes sure these offsets are right
    m.regs&.items().copy_overlapping(i, i + 1, m.len - i);
    @debug_assert(t >= Qbe.Tmp0 || t == r);
    r
}

fn pmadd(c: *RegaCtx, src: Qbe.Ref, dst: Qbe.Ref, k: Qbe.Cls) void = {
    @assert(c.npm < Qbe.Tmp0, "cannot have more moves than registers");
    m := c.pm&[c.npm]&;
    m.src = src;
    m.dest = dst;
    m.cls = k;
    c.npm += 1;
}

PMStat :: @enum(i64) (ToMove, Moving, Moved);

// :Explain
// note: k is a pointer. we mutate back up through levels of recursion. 
fn pmrec(c: *RegaCtx, status: []PMStat, i: i64, k: *Qbe.Cls) i64 = {
    /* note, this routine might emit
     * too many large instructions
     */
    if c.pm&[i].src == c.pm&[i].dest {
        status[i] = .Moved;
        return(-1);
    };
    @debug_assert(KBASE(c.pm&[i].cls) == KBASE(k[]));
    ::assert(Qbe.Cls.Kw.raw().bit_or(Qbe.Cls.Kl.raw()) == Qbe.Cls.Kl.raw(), "Cls numbers");
    ::assert(Qbe.Cls.Ks.raw().bit_or(Qbe.Cls.Kd.raw()) == Qbe.Cls.Kd.raw(), "Cls numbers");
    k[] = @as(Qbe.Cls) @as(i16) k[].raw().bit_or(c.pm&[i].cls.raw());
    j := 0;
    while => j < c.npm && c.pm&[j].dest != c.pm&[i].src {
        j += 1;
    };
    cycle := -1;
    ::if(PMStat);
    s := if(j == c.npm, => PMStat.Moved, => status[j]);
    @match(s) {
        fn Moving() => {
            cycle = j; /* start of cycle */
            c.f.emit(.swap, k[], QbeNull, c.pm&[i].src, c.pm&[i].dest);
        }
        fn ToMove() => {
            status[i] = .Moving; // TODO: not insane error message if you forget the dot so it tries to reference the overload set created for the switch case above.  
            cycle = c.pmrec(status, j, k);
            if cycle == i {
                cycle = -1; /* end of cycle */
            } else {
                if (cycle != -1) {
                    c.f.emit(.swap, k[], QbeNull, c.pm&[i].src, c.pm&[i].dest);
                } else {
                    cycle = -1;
                    c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx
                }
            };
        }
        fn Moved() => {
            cycle = -1;
            c.f.emit(.copy, c.pm&[i].cls, c.pm&[i].dest, c.pm&[i].src, QbeNull); // xxx same
        }
    };
    status[i] = .Moved;
    cycle
}
::enum_basic(PMStat);
fn pmgen(c: *RegaCtx) void = {
    status := temp().alloc_zeroed(PMStat, c.npm);
    @debug_assert(c.npm == 0 || status[c.npm-1] == .ToMove, "zero init is ToMove");
    range(0, c.npm) { i | 
        if status[i] == .ToMove {
            k := c.pm&[i].cls;
            c.pmrec(status, i, k&);
        };
    };
}

fn move(c: *RegaCtx, r: i64, to: Qbe.Ref, m: *RMap) void = {
    r1 := if(to == QbeNull, => -1, => c.rfree(m, to.val()));
    if bshas(m.live&, r) {
        /* r is used and not by to */
        @debug_assert(r1 != r);
        n := 0;
        while => m.regs&[n].intcast() != r {
            @debug_assert(n+1 < m.len);
            n += 1;
        };
        t := m.tmps&[n];
        c.rfree(m, t.intcast());
        bsset(m.live&, r);
        c.ralloc(m, t);
        bsclr(m.live&, r);
    };
    t := if(to == QbeNull, => r, => to.val());
    c.radd(m, t, r);
}

// when you have a group of copies from registers to tmps, 
// you need to make sure none of the dest tmps are allocated to registers later used as a src.
// it has to execute as though all the moves happened at the same time, without letting any stomp eachother. 
fn do_parallel_moves(c: *RegaCtx, b: *Qbe.Blk, last_move: *Qbe.Ins, m: *RMap) *Qbe.Ins = {
    T := c.f.globals.target;
    m0 := m[]; /* okay since we don't use m0.b */
    m0.live.t = u64.ptr_from_int(0); // assert ^
    past_last_move := last_move.offset(1);
    first_move := {
        i := past_last_move;
        dowhile {
            i = i.offset(-1);
            c.move(i.arg&[0].val(), i.to, m);
            !i.identical(b.ins) && is_copy_from_reg(i.offset(-1))
        };
        i
    };
    @debug_assert(m0.len <= m.len);
    is_after_call := !first_move.identical(b.ins) && first_move.offset(-1).op() == .call;
    if is_after_call {
        discard := Array(i32, 2).ptr_from_int(0); // TODO: helper for this 
        call_abi_info := first_move.offset(-1)[].arg&[1];
        def := {T.retregs}(call_abi_info, discard).bit_or(T.rglob);
        for_caller_saved T { r | 
            if !def.bit_is_set(r) {
                c.move(r, QbeNull, m);
            }
        };
    };
    c.npm = 0;
    range(0, m.len) { n |
        t  := m.tmps&[n].intcast();
        spill_slot  := c.f.tmp[t].slot;
        r1 := m.regs&[n];
        if rfind(m0&, t) { r |
            c.pmadd(TMP(r1), TMP(r), c.f.tmp[t].cls);
        } else {
            if spill_slot != -1 {
                c.pmadd(TMP(r1), SLOT(spill_slot), c.f.tmp[t].cls);
            }
        }
    };
    ip := first_move;
    while => Qbe.Ins.int_from_ptr(ip) < Qbe.Ins.int_from_ptr(past_last_move) {
        if ip.to != QbeNull {
            c.rfree(m, ip.to.val());
        };
        r := ip.arg&[0].val();
        if rfind(m, r).is_none() {
            c.radd(m, r, r);
        };
        ip = ip.offset(1);
    };
    c.pmgen();
    first_move
}

fn prio1(c: *RegaCtx, r1: Qbe.Ref, r2: Qbe.Ref) bool = {
    /* trivial heuristic to begin with,
     * later we can use the distance to
     * the definition instruction
     */
    c.hint(r1.val())[] != -1
}

fn insert(c: *RegaCtx, r: *Qbe.Ref, rs: *Array(*Qbe.Ref, 4), p: i64) void = {
    i := p;
    rs[i] = r;
    while => i > 0 && c.prio1(r[], rs[i - 1][]) {
        i -= 1;
        rs[i+1] = rs[i];
        rs[i] = r;
    }
}

fn do_block_instructions(c: *RegaCtx, b: *Qbe.Blk, cur: *RMap) void = {
    T := c.f.globals.target;
    reg_args := Array(*Qbe.Ref, 4).zeroed(); reg_args := reg_args&;

    if rtype(b.jmp.arg) == .RTmp {
        b.jmp.arg = c.ralloc(cur, b.jmp.arg.val().intcast());
    };
    c.f.reset_scratch();
    curi := c.f.globals.curi; // :LookAtLastInst (but just to pop off dead code)
    for_insts_rev b { i1 |
        continue :: local_return;
        i := c.f.emit(i1[][]);
        just_freed := -1;
        other :: fn() => {
            if i.to != QbeNull  {
                @debug_assert(rtype(i.to) == .RTmp, "can only assign tmp");
                r := i.to.val();
                if r >= Qbe.Tmp0 || !T.rglob.bit_is_set(r) {
                    just_freed = c.rfree(cur, r);
                    if just_freed == -1 {
                        // if we didn't need the value later, we don't need the instruction that creates it. 
                        @debug_assert(!isreg(i.to));
                        curi[] = curi[].offset(1); // pop off the emit() from this iteration
                        continue();
                    };
                    i.to = TMP(just_freed);
                };
            };
        };
        @match(i.op()) {
            fn call() => { 
                discard := Array(i32, 2).ptr_from_int(0);
                needed_at_call := {T.argregs}(i.arg&[1], discard).bit_or(T.rglob);
                for_caller_saved T { r |
                    if !needed_at_call.bit_is_set(r) {
                        c.rfree(cur, r);
                    }
                };
            }
            fn copy() => {
                if is_copy_from_reg(i) {
                    // consecutive copies from registers are handled seperatly. 
                    curi[] = curi[].offset(1);  // pop off the emit() from this iteration
                    i1[] = c.do_parallel_moves(b, i1[], cur);
                    c.added_moves += curi[].ptr_diff(i.offset(1)).trunc();
                    continue();
                };
                if isreg(i.to) && rtype(i.arg&[0]) == .RTmp {
                    c.sethint(i.arg&[0].val(), i.to.val());  // try to arrange so this copy is a nop.
                };
                other();
            }
            @default => other();
        };
        reg_args_count := 0;
        each i.arg& { arg | 
            @match(rtype(arg[])) {
                fn RMem() => {
                    m := c.f.get_memory(arg[]);
                    if rtype(m.base) == .RTmp {
                        c.insert(m.base&, reg_args, reg_args_count);
                        reg_args_count += 1;
                    };
                    if rtype(m.index) == .RTmp {
                        c.insert(m.index&, reg_args, reg_args_count);
                        reg_args_count += 1;
                    };
                }
                fn RTmp() => {
                    c.insert(arg, reg_args, reg_args_count);
                    reg_args_count += 1;
                }
                @default => ();
            };
        };
        range(0, reg_args_count) { r |
            reg_args[r][] = c.ralloc(cur, reg_args[r][].val().intcast());
        };
        was_nop_copy := i.op() == .copy && i.to == i.arg&[0];
        if was_nop_copy { 
            c.f.globals.curi[] = c.f.globals.curi[].offset(1); // pop off the emit() from this iteration
        };

        /* try to change the register of a hinted
         * temporary if `just_freed` is available */
        if(just_freed == -1, => continue());
        t := cur.wait_list&[just_freed];
        if(t == 0, => continue());
        if(bshas(cur.live&, just_freed), => continue());
        r_hint := c.hint(t.intcast())[].intcast();
        if(r_hint != just_freed, => continue());
        rt := c.rfree(cur, t.intcast()).intcast();
        if(rt == -1, => continue());
        
        c.f.tmp[t.intcast()].visit = -1;
        c.ralloc(cur, t);
        @debug_assert(bshas(cur.live&, just_freed));
        c.f.emit(.copy, c.f.tmp[t.intcast()].cls, TMP(rt), TMP(just_freed), QbeNull);
        c.added_moves += 1;
        cur.wait_list&[just_freed] = 0;
        range(0, reg_args_count) { r | 
            if reg_args[r][] == TMP(rt) { // rT // TODO: rename. 
                reg_args[r][] = TMP(just_freed); 
            }
        };
        /* one could iterate this logic with
            * the newly freed rt, but in this case
            * the above loop must be changed */
    };
    c.f.copy_instructions_from_scratch(b);
}

/* comparison function to order temporaries
 * for allocation at the end of blocks */
fn prio2(c: *RegaCtx, t1: i64, t2: i32) i64 = {
    if c.f.tmp[t1].visit.intcast().bit_xor(c.f.tmp[t2.intcast()].visit.intcast()) < 0 { /* != signs */
        return(if(c.f.tmp[t1].visit != -1, => 1, => -1));
    };
    if c.hint(t1)[].intcast().bit_xor(c.hint(t2.intcast())[].intcast()) < 0 {
        return(if(c.hint(t1)[] != -1, => 1, => -1));
    };
    xxx := c.f.tmp[t1].cost;
    xxxx := c.f.tmp[t2.intcast()].cost;
    (xxx - xxxx).bitcast().intcast()
}

/* peel loop nests from inside out */
carve :: fn(ba: **Qbe.Blk, bb: **Qbe.Blk) bool = { 
    /* todo, evaluate if this order is really
    * better than the simple postorder */
    if ba.loop == bb.loop {
        ba.id <= bb.id  // TODO: flipped?
    } else {
        ba.loop <= bb.loop // TODO: flipped?
    }
};

/* register allocation
 * depends on rpo, phi, cost, (and obviously spill)
 */
fn register_allocation(f: *Qbe.Fn) void = {
    /* 1. setup */
    c: RegaCtx = (f = f, regu = 0, pm = get_field_type(RegaCtx, @symbol pm).zeroed(), npm = 0, loop = MAX_i32, added_moves = 0, added_blocks = 0); c := c&;
    rl := Array(i32, Qbe.Tmp0).zeroed(); rl := rl&.items();
    blk := temp().alloc_zeroed(*Qbe.Blk, f.nblk.zext());
    end := temp().alloc_zeroed(RMap, f.nblk.zext());
    beg := temp().alloc_zeroed(RMap, f.nblk.zext());
    each(end, fn(it) => bsinit(it.live&, f.ntmp.zext()));
    each(beg, fn(it) => bsinit(it.live&, f.ntmp.zext()));
    cur := temp().alloc_zeroed(RMap, 1).as_ptr(); // TODO: these could go on the stack but my old backend can't do big stack frames
    old := temp().alloc_zeroed(RMap, 1).as_ptr();
    bsinit(cur.live&, f.ntmp.zext());
    bsinit(old.live&, f.ntmp.zext());
    enumerate f.tmps() { t, it |
        it.hint.register = if(t < Qbe.Tmp0, => t.intcast(), => -1);
        it.hint.weight = c.loop;
        it.visit = -1;
    };
    
    n := 0;
    for_blocks f { b |  
        blk[n] = b;
        n += 1;
    };
    c.set_param_copy_hints();
    sort :: quicksort(*Qbe.Blk, carve);
    blk.sort();
    
    /* 2. assign registers */
    for blk { b | 
        n: i64 = b.id.zext();
        loop := b.loop;
        cur.len = 0;
        bszero(cur.live&);
        cur.wait_list&.set_zeroed();
        
        // From spill pass, we know which tmps need to be in registers at the end of this block. 
        live_at_end_count := 0;
        for(b.out&, Qbe.Tmp0) { t | 
            j := live_at_end_count;
            live_at_end_count += 1;
            rl[j] = t.intcast();
            while => j > 0 && c.prio2(t, rl[j-1]) > 0 {
                j -= 1;
                rl[j+1] = rl[j];
                rl[j] = t.intcast();
            };
        };
        T := f.globals.target;
        @debug_assert(live_at_end_count.intcast() <= T.ngpr + T.nfpr, "too many live tmps");
        if true {
            break :: local_return; // TODO: this pattern is a language failing. 
            for b.out& { r | 
                if(r >= Qbe.Tmp0, => break());
                c.radd(cur, r, r);
            };
        };
        // First give everyone a chance to get thier hinted register
        range(0, live_at_end_count) { j |
            c.ralloctry(cur, rl[j], true);
        };
        // Then the rest just get forced somewhere. 
        range(0, live_at_end_count) { j |
            c.ralloc(cur, rl[j]);
        };
        rcopy(end[n]&, cur);
        c.do_block_instructions(b, cur); 
        bscopy(b.in&, cur.live&);
        for_phi b { p | 
            if rtype(p.to) == .RTmp {
                bsclr(b.in&, p.to.val());
            }
        };
        rcopy(beg.index(n), cur);
    };

    // :Explain
    /* 3. emit copies shared by multiple edges
     * to the same block */
    for_blocks f { s |
        continue :: local_return;
        if(s.npred <= 1, => continue());
        m := beg[s.id.zext()]&;

        /* rl maps a register that is live at the
         * beginning of s to the one used in all
         * predecessors (if any, -1 otherwise) */
        rl.set_zeroed();

        /* to find the register of a phi in a
         * predecessor, we have to find the
         * corresponding argument */
        for_phi s { p |
            continue :: local_return;
            if(rtype(p.to) != .RTmp, => continue());
            r := rfind(m, p.to.val()).or(=> continue());
            r := r.intcast();
            range(0, p.narg.zext()) { u |
                continue :: local_return;
                b := p.blk[u];
                src := p.arg[u];
                if(rtype(src) != .RTmp, => continue());
                x := or rfind(end.index(b.id.zext()), src.val()) {
                    continue() /* spilled */
                };
                rl[r] = if(rl[r] == 0 || rl[r] == x, => x, => -1);
            };
            if rl[r] == 0 {
                rl[r] = -1;
            };
        };
        /* process non-phis temporaries */
        range(0, m.len) { j | 
            continue :: local_return;
            t := m.tmps&[j];
            r := m.regs&[j].intcast();
            if rl[r] != 0 || t < Qbe.Tmp0 /* todo, remove this */ {
                continue();
            };
            for_pred s { bp |
                continue :: local_return;
                x := or rfind(end.index(bp.id.zext()), t.intcast()) {
                    continue() /* spilled */
                };
                rl[r] = if(rl[r] == 0 || rl[r] == x, => x, => -1);
            };
            if rl[r] == 0 {
                rl[r] = -1;
            };
        };

        c.npm = 0;
        range(0, m.len) { j |
            t := m.tmps&[j].intcast();
            r := m.regs&[j].intcast();
            x := rl[r].intcast();
            @debug_assert(x != 0 || t < Qbe.Tmp0 /* todo, ditto */);
            if x > 0 && !bshas(m.live&, x) {
                c.pmadd(TMP(x), TMP(r), f.tmp[t].cls);
                m.regs&[j] = x.intcast();
                bsset(m.live&, x);
            }
        };
        f.reset_scratch();
        c.pmgen();
        moves_inserted := f.len_scratch();
        if moves_inserted != 0 {
            c.added_moves += moves_inserted.trunc();
            s.nins += moves_inserted.trunc();
            i := temp().alloc(Qbe.Ins, s.nins.zext()).as_ptr();
            icpy(icpy(i, f.globals.curi[], moves_inserted), s.ins, s.nins.zext()-moves_inserted); // :LookAtLastInst
            s.ins = i;
        };
    };

    out := f.globals.debug_out;
    if f.globals.debug["R".char()] {
        write(out, "\n> Register mappings:\n");
        range(0, f.nblk.zext()) { n |
            b := f.rpo[n];
            @fmt_write(out, "\t% beg", f_pad(b.name(), 10, .After));
            f.debug_dump(beg.index(n));
            write(out, "\t           end");
            f.debug_dump(end.index(n));
        };
        write(out, "\n");
    };
    
    // :Explain
    /* 4. emit remaining copies in new blocks */
    blist := Qbe.Blk.ptr_from_int(0);
    if true {
        break :: local_return; // TODO: ugh. i should really make this less painful.
        for_blocks f { b | 
            // TODO
            //@debug_assert(!b.s1.identical(b.s2), "i think these are removed and my behaviour here differs if not"); // :SketchyIterTargets
            for_jump_targets_mut b { ps |
                s := ps[];
                c.npm = 0;
                for_phi s { p |
                    continue :: local_return;
                    dst := p.to;
                    @debug_assert(rtype(dst) == .RSlot || rtype(dst) == .RTmp);
                    if rtype(dst) == .RTmp {
                        r := or rfind(beg.index(s.id.zext()), dst.val()) {
                            continue()
                        };
                        dst = TMP(r);
                    };
                    u := 0;
                    while => !p.blk[u].identical(b) {
                        @debug_assert(u+1 < p.narg.zext());
                        u += 1;
                    };
                    src := p.arg[u];
                    if rtype(src) == .RTmp {
                        src = c.rref(end.index(b.id.zext()), src.val());
                    };
                    c.pmadd(src, dst, p.cls);
                };
                for(s.in&, Qbe.Tmp0) { t |
                    src := c.rref(end.index(b.id.zext()), t);
                    dst := c.rref(beg.index(s.id.zext()), t);
                    c.pmadd(src, dst, f.tmp[t].cls);
                };
                f.reset_scratch();
                c.pmgen();
                moves_inserted := f.len_scratch();
                if moves_inserted != 0 {
                    b1 := newblk();
                    b1.loop = (b.loop + s.loop) / 2;
                    b1.link = blist;
                    blist = b1;
                    f.nblk += 1;
                    // TODO: ugh. garbage. you just want to truncate if not enough space in the buffer.
                    l: List(u8) = (maybe_uninit = b1.name&.items(), len = 0, gpa = panicking_allocator); // :UnacceptablePanic
                    @fmt(l&, "%_%", b.name(), s.name());
                    b1.nins = moves_inserted.trunc();
                    c.added_moves += b1.nins;
                    c.added_blocks += 1;
                    f.copy_instructions_from_scratch(b1);
                    b1.jmp.type = .Jjmp;
                    b1.s1 = s;
                    ps[] = b1;
                };
            };
            if b.link.is_null() {
                b.link = blist;
                break();
            }
        };
    };
    
    // phis have been lowered to moves so sanity assert that nobody tries to look at them after this pass. 
    for_blocks f { b |
        b.phi = Qbe.Phi.ptr_from_int(0);
    };
    f.reg = c.regu;

    if f.globals.debug["R".char()] {
        write(out, "\n> Register allocation statistics:\n");
        @fmt_write(out, "\tnew moves:  %\n", c.added_moves);
        @fmt_write(out, "\tnew blocks: %\n", c.added_blocks);
        write(out, "\n> After register allocation:\n");
        printfn(f, out);
    };
}

// we want our parameters to be assgned to thier abi registers so we don't actually have to do a copy. 
fn set_param_copy_hints(c: *RegaCtx) void = {
    for_insts_forward c.f.start { i |
        if i.op() != .copy || !isreg(i.arg&[0]) {
            return();
        };
        @debug_assert(rtype(i.to) == .RTmp, "can only copy to tmp");
        abi_reg := i.arg&[0].val();
        c.sethint(i.to.val(), abi_reg);
    };
}
