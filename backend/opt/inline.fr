// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>
// This is from a patch on the qbe mailing list by Roland Paterson-Jones: https://lists.sr.ht/~mpu/qbe/patches/54822

//!
//! When you try to call a small function whose definition we've already seen, we paste its body into the caller instead. 
//! This avoids forcing locals to be spilled for a callee that isn't going to do much work anyway,  
//! and gives the later optimisation passes a more complete picture of the program. 
//! We try to catch the cases where the cruft introduced by following the abi would be larger than the function body itself. 
//! If (all calls are inlined + its address is never taken + it is not exported) we don't bother generating code for the function at all. 
//!
//! You could do much better with a more complicated heuristic and the ability to inline multiblock functions, 
//! but what we do already is extremely good bang for your buck.
//!
//! note: the franca front end sorts based on callgraph so increase chances of things being inlined. 
//!

MAX_INL_COST :: 4;  // remember zero is not the minimum if you want to disable inlining. 

::if(*i64);
// TODO: this is broken when inlining mismatching env

fn good4inl(f: *Qbe.Fn) bool = {
    //return(false);
    fillrpo(f);
    fillpreds(f);
    blkmerge(f);
    fillrpo(f);
    fillpreds(f);
    fill_use(f);
    
    if f.nblk != 1 || f.vararg || f.start.jmp.type == .hlt {
        return(false);
    };
    
    b := f.start;
    npar := 0;
    nins := 0;
    write := false;
    for_insts_forward b { i |
        continue :: local_return;
        o := i.op();
        if(@is(o, .nop, .dbgloc), => continue());
        if o == .blit0 || is_store(i.op()) || i.op() == .call {    
            write = true;
        };
        @debug_assert(o != .vastart);
        if(is_par(i.op()), => npar&, => nins&)[] += 1;
    };
    cost := nins - 2 * (npar + int(b.jmp.type != .ret0)) - 1 /*Ocall*/;
    f.read_only = !write;
    
    cost <= MAX_INL_COST
}

// Note: this goes in the inlining cache so it can't use temp() memory.
fn cloneblk(b: *Qbe.Blk) *Qbe.Blk = {
    inlb := libc_allocator.box_zeroed(Qbe.Blk); // :HardcodeAlloc
    inlb.jmp = b.jmp;
    
    msg :: "can only inline functions with one block";
    @debug_assert(inlb.phi.is_null(),  msg);
    @debug_assert(inlb.s1.is_null(),   msg);
    @debug_assert(inlb.s2.is_null(),   msg);
    @debug_assert(inlb.link.is_null(), msg);
    
    inlb.ins = new_long_life(b.nins.zext());
    idup(inlb, b.ins.first, b.nins.zext());
    inlb
}

// Note: this goes in the inlining cache so it can't use temp() memory.
fn clone(f: *Qbe.Fn) *Qbe.Fn = {
    inlfn := libc_allocator.box_zeroed(Qbe.Fn); // :HardcodeAlloc
    inlfn.retty = f.retty;
    inlfn.ntmp = f.ntmp;
    inlfn.ncon = f.ncon;
    inlfn.nblk = f.nblk;
    inlfn.leaf = f.leaf;
    inlfn.globals = f.globals;
    inlfn.lnk.id = f.lnk.id;
    inlfn.tmp = new_long_life(inlfn.ntmp.zext());
    inlfn.tmp.slice(0, f.ntmp.zext()).copy_from(f.tmp.slice(0, f.ntmp.zext()));
    range(Qbe.Tmp0, inlfn.ntmp.zext()) { i |
        inlfn.tmp[i].visit = -1; // used when remapping 
        inlfn.tmp[i].use.first = Qbe.Use.ptr_from_int(0);  // don't try to reuse memory in clear_usage_info if this function is suspended. 
    };
    inlfn.con = new_long_life(inlfn.ncon.zext());
    inlfn.con.slice(0, f.ncon.zext()).copy_from(f.con.slice(0, f.ncon.zext()));
    @debug_assert(f.nmem == 0, "shouldn't need to inline clone f.mem");
    @debug_assert(f.nblk == 1, "don't know how to inline that");
    inlfn.start = cloneblk(f.start);
    @if(TRACK_IR_NAMES) inlfn.name&.items().copy_from(f.name&.items()); // useless, just lazy way to make the debug logging better
    inlfn
}

::ptr_utils(Qbe.Fn);

fn inlsave(f: *Qbe.Fn) bool = {
    if(!good4inl(f), => return(false));
    
    inlfn  := clone(f);
    if f.globals.debug["G".char()] {
        @fmt_write(f.globals.debug_out, "\nSave for inlining:\n");
        printfn(inlfn, f.globals.debug_out);
    };
    m := f.globals;
    
    result := false;
    use_symbol(f.globals, f.lnk.id) { symbol |
        if f.lnk.export {
            symbol.referenced = true;
        };
        //@debug_assert(symbol.name.len > 0, "function with empty string as name. was that a mistake?");
        @debug_assert(symbol.inline.is_null(), "'%' already has an entry in the inline cache.", symbol.name);
        symbol.inline = inlfn;
        result = !symbol.referenced;
    };
    result
    
}

// TODO: if you want to have multiple threads compiling together can't use the visit field of the cached tmps. 
fn inlfind(f: *Qbe.Fn, r: Qbe.Ref) ?*Qbe.Fn = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type != .CAddr, => return(.None));
    if(c.sym.type != .SGlo, => return(.None));
    m := f.globals;
    result: ?*Qbe.Fn = .None;
    use_symbol(f.globals, c.sym.id) { symbol |
        if !symbol.inline.is_null() {
            result = (Some = symbol.inline);
        } else {
            mark_referenced(m, c.sym.id, symbol);
        };
    };
    result
}

fn getinlref(inlfn: *Qbe.Fn, inlr: Qbe.Ref, f: *Qbe.Fn, c: *InlineCtx) Qbe.Ref = {
    if inlr == QbeNull || rtype(inlr) == .RInt || rtype(inlr) == .RType {
        // some ref types are global
        return(inlr);
    };
    
    // it's a Tmp/Con, have we seen it before? 
    
    @match(rtype(inlr)) {
        fn RTmp() => {
            t := inlfn.tmp[inlr.val()]&;
            if t.visit == -1 {
                // first time seeing this ref: need to create an equivalent in the caller.  
                t.visit = f.newtmp(t.name().str() /*:SLOW*/, t.cls).val().intcast();
                c.to_clear&.push(t);  // after inlining this call, reset visit to -1 for next time we inline this function. 
            };
            TMP(t.visit)
        }
        fn RCon() => {
            i := inlr.val();
            r := c.remapped_consts[i];
            if r == QbeNull {
                // first time seeing this ref: need to create an equivalent in the caller.  
                r = f.newcon(inlfn.get_constant(inlr));
                c.remapped_consts[i] = r;
            };
            r
        }
        @default => panic("bad inline ref kind. RSlot/RMem are created by later passes.");
    }
}

fn inlargs(inlfn: *Qbe.Fn, f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins, c: *InlineCtx) void = {
    /* replace args with copies to par tmps */
    @debug_assert(inlfn.nblk == 1);
    i1 := inlfn.start.ins.first;
    if i.op() == .arge {
        if inlfn.start.nins == 0 || i1.op() != .pare {
            /* drop env arg because the callee does not take one */
            i.set_nop();
            i = i.offset(1);
        };
    } else {
        if inlfn.start.nins > 0 && i1.op() == .pare {
            @debug_assert(rtype(i1.to) == .RTmp, "pare target must be tmp");
            // callee wants an env arg but caller does not provide one. they get 0.
            rto := getinlref(inlfn, i1.to, f, c);
    		ins := make_ins(.copy, i1.cls(), rto, QbeConZero, QbeNull);
    		addins(c.vins&, c.nins&, ins&);
            i1 = i1.offset(1);
        };
    };
    while => i.op() != .call {
        @debug_assert(is_arg(i.op()), "expected 'argX' instructions before call. did we start in the wrong place somehow?");
        @debug_assert(is_par(i1.op()), "expected 'parX' instructions at beginning of function. did we start in the wrong place somehow?");
        @assert(i.cls() == i1.cls(), "cant inline call with wrong argument type. (callee = %, caller = %)", inlfn.name(), f.name());
        @debug_assert(rtype(i1.to) == .RTmp, "par target must be tmp");
        
        r := i.arg&[int(i.op() == .argc)];
		rto := getinlref(inlfn, i1.to, f, c);

		if i.op() == .argc {
			@debug_assert(i1.op() == .parc, "expected parc for argc");
			// If a function makes absolutely no writes to memory (or calls that might do so), 
			// we don't need to bother copying the argument because it can't be mutated in the callee. 
			// TODO: this could take advantage of the more sophisticated alias analysis stuff we do in other passes.
			if !inlfn.read_only {
				r = insert_blit(f, i.arg&[0], i1.arg&[0], r, c);
			}
		};

		ins := make_ins(.copy, i.cls(), rto, r, QbeNull);
		addins(c.vins&, c.nins&, ins&);

        i  = i.offset(1);
        i1 = i1.offset(1);
        @debug_assert(b.ins.index(b.nins.zext()).in_memory_after(i), "inlined oob");
        last_inl := inlfn.start.ins.index(inlfn.start.nins.zext() + 1); // TODO: why +1?
        @debug_assert(last_inl.in_memory_after(i1), "inlined oob");
    };
}

fn insert_blit(f: *Qbe.Fn, rty: Qbe.Ref, rty1: Qbe.Ref, r: Qbe.Ref, c: *InlineCtx) Qbe.Ref = {
    m := f.globals;
    @if(use_threads) pthread_mutex_lock(m.types_mutex&).unwrap();
    @debug_assert(rtype(rty) == .RType);
    ty := f.globals.get_type(rty.val());
    @debug_assert(rtype(rty1) == .RType);
    ty1 := f.globals.get_type(rty1.val());

    @assert(ty.align_log2 == ty1.align_log2 && ty.size == ty1.size, "tried to inlined function with mismatching aggragate types");

    // TODO: i shouldn't just copy pasta math i don't understand. 
    al := if(ty.align_log2 < 2, => 2, => if(ty.align_log2 > 4, => 4, => ty.align_log2.intcast()));
    sz := (ty.size + ((1.shift_left(al))-1)).bit_and(bit_not(1.shift_left(al)-1));

    ral := f.newtmp("inl", .Kl);

    f.emit(alloc_op(ty.align_log2), .Kl, ral, f.getcon(sz), QbeNull);
    ins := make_ins(.blit0, .Kw, QbeNull, r, ral);
    addins(c.vins&, c.nins&, ins&);
    ins := make_ins(.blit1, .Kw, QbeNull, INT(ty.size), QbeNull);
    addins(c.vins&, c.nins&, ins&);
    @if(use_threads) pthread_mutex_unlock(m.types_mutex&).unwrap();
    ral
}

fn inlbins(inlfn: *Qbe.Fn, f: *Qbe.Fn, c: *InlineCtx) void = {
    inlb := inlfn.start;
    /* skip par instructions */
    i := inlfn.find_past_last_param();
    last := inlb.ins.index(inlb.nins.zext());
    /* copy remaining instructions, translating refs to current fn */
    for(i, last) { i |
        addins(c.vins&, c.nins&, i);
        i1 := c.vins.index(c.nins - 1);
        i1.to = getinlref(inlfn, i1.to, f, c);
        i1.arg&[0] = getinlref(inlfn, i1.arg&[0], f, c);
        i1.arg&[1] = getinlref(inlfn, i1.arg&[1], f, c);
        
        /* hoist inlfn allocs to fn->start */
        // this lets them become fast locals (in $arch/isel) or be promoted to temporaries (in mem/promote).
        if is_alloc(i1.op()) {
            f.emit(i1[]);
            c.nins -= 1;  // i1.set_nop();
        } else {
            if i1.op() == .blit1 {
                @debug_assert(rtype(i1.arg&[0]) == .RInt);
                /* TODO - deeply dubious/overlapping */
                i1.arg&[0] = INT(abs(rsval(i1.arg&[0]).intcast())); // :FUCKED
            };
        };
    };
}

InlineCtx :: @struct(
    // for RCon this stores: [val() in callee] -> ref in caller. (QbeNull means not remapped yet).
    // for RTmp the mapping is in Tmp.visit field of the callee.      (-1 means not remapped yet).
    remapped_consts: []Qbe.Ref,
    vins: QList(Qbe.Ins), 
    nins := 0, 
    to_clear: List(*Qbe.Tmp),
);

/* return index of last inlined ins */
fn inlcall(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) i64 = {
    @debug_assert(ptr_diff(b.ins.first, i) >= 0 && ptr_diff(i, b.ins.index(b.nins.zext())) >= 0, "bad size for inlining");
    @debug_assert(i.op() == .call, "expected to be inlining call");
    inlfn := or inlfind(f, i.arg&[0]) {
        f.leaf = false;
        return(ptr_diff(b.ins.first, i))
    };
    f.leaf = f.leaf && inlfn.leaf;
    
    if f.globals.debug["G".char()] {
        @fmt_write(f.globals.debug_out, "    inlining call at @% ins % to simple function $%\n", b.name(), ptr_diff(b.ins.first, i), inlfn.name());
    };
    
    c: InlineCtx = (
        remapped_consts = temp().alloc_zeroed(Qbe.Ref, inlfn.ncon.zext()),
        vins  = new(b.nins.zext() + inlfn.start.nins.zext()),
        to_clear = list(inlfn.ntmp.zext().shift_right_logical(4), temp()),
    );
    
    /* go back to start of args */
    i1 := i;
    while => ptr_diff(b.ins.first, i1) > 0 && is_arg(i1.offset(-1).op()) {
        i1 = i1.offset(-1);
    };
    
    append(c.vins&, c.nins&, b.ins.first, i1);
    
    /* replace arg ins with copy to inline fn par tmps */
    inlargs(inlfn, f, b, i1, c&);
    
    @debug_assert(inlfn.nblk == 1, "don't know how to inline that");
    @debug_assert(is_ret(inlfn.start.jmp.type));
    
    /* splice inlined fn instructions */
    inlbins(inlfn, f, c&);
    
    /* replace call ins with copy for non-void fn's */
    if i.to == QbeNull {
        // They called like it was a void function. 
        // We allow this for scalars because it would work if it wasn't inlined. 
        // But if an aggragate would have been returned by reference, this would fault at runtime if we hadn't tried to inline it. 
        @assert(inlfn.retty == -1, "Tried to inline void call to function $% which returns an aggregate.", inlfn.name());
        @debug_assert(inlfn.start.jmp.type != .retc);
    } else {
        new_ref := getinlref(inlfn, inlfn.start.jmp.arg, f, c&);
        /* aggregate type pass-by-value is implicit copy */
        if inlfn.retty != -1 {
            @debug_assert(inlfn.start.jmp.type == .retc, "we save for inlining before lowering abi");
            // We have to do a copy because they might be returning something that wasn't on thier stack frame, 
            // TODO: we have aliasing info so should know if they're returning an alloca and don't need this copy. 
            new_ref = insert_blit(f, TYPE(inlfn.retty.intcast()), i.arg&[1], new_ref, c&);
        } else {
            @debug_assert(i.arg&[1] == QbeNull);
            @debug_assert(inlfn.start.jmp.type != .retc);
        };
        ins := make_ins(.copy, i.cls(), i.to, new_ref, QbeNull);
        addins(c.vins&, c.nins&, ins&);
    };
    nret := c.nins - 1;
    
    // TODO: copy chunks as we go so you don't recopy the end of the block for every inlined call :SLOW
    append(c.vins&, c.nins&, i.offset(1), b.ins.index(b.nins.zext()));
    idup(b, c.vins.first, c.nins);
    
    // tmps are sparse after being eliminated by other passes (61346/1150101 = 5% average for the compiler -- Nov 20). 
    // so only reset the ones that we actually messed with. 
    for c.to_clear& { t | 
        t.visit = -1;
    };
    
    nret
}

fn inlcalls(f: *Qbe.Fn) void = {
    if f.globals.debug["G".char()] {
        @fmt_write(f.globals.debug_out, "\nProcessing function $%:\n", f.name());
    };
    
    // during this pass we collect allocas for inlined arg/ret in the scratch buffer 
    // and splice them into f.start at the end. 
    f.reset_scratch();
    
    f.leaf = true;  // updated in inlcall()
    for_blocks f { b | 
        n := 0;
        while => n < b.nins.zext() {
            i := b.ins.index(n);
            if i.op() == .call {
                /* careful, this rewrites b.ins */
                n = inlcall(f, b, i); 
            } else {
                // This is unfortunate extra work I need to do becuase I want to skip emitting functions when all uses are inlined.
                // This doesn't see the instructions we've inlined into this function, 
                // but it's fine because they'd have been checked when we processed the callee the first time.  
                // note: need to add something to the case above if inlining heuristic gets complicated enough to make choices based on the caller not just the callee. 
                for i.arg&.items() { r | 
                    continue :: local_return;
                    if(rtype(r) != .RCon, => continue());
                    c := f.get_constant(r);
                    if(c.type != .CAddr, => continue());
                    use_symbol(f.globals, c.sym.id) { s | 
                        mark_referenced(f.globals, c.sym.id, s);
                    };
                };
            };
            n += 1;
        };
    };
    
    /* some new allocs in fn->start for retc, parc etc. */
    alloc_ins := f.slice_pending_scratch();
    if alloc_ins.len != 0 {
        // ins = ins[..<past_param] ++ alloc_ins ++ ins[past_param..<ins.end]
        i := f.find_past_last_param();
        vins: QList(Qbe.Ins) = new(f.start.nins.zext() + alloc_ins.len);
        nins := 0;
        append(vins&, nins&, f.start.ins.first, i);
        append(vins&, nins&, alloc_ins.ptr, alloc_ins.index_unchecked(alloc_ins.len));
        append(vins&, nins&, i, f.start.ins.index(f.start.nins.zext()));
        f.start.ins = vins;
        f.start.nins = nins.trunc();
    };
    
    if f.globals.debug["G".char()] {
        write(f.globals.debug_out, "\nAfter inlining calls:\n\n");
        printfn(f, f.globals.debug_out);
    };
}

fn mark_referenced(m: *QbeModule, id: u32, symbol: *SymbolInfo) void #inline = {
    if !symbol.inline.is_null() && !symbol.referenced {
        m.inlinable_but_referenced_later&.push(id);
    };
    symbol.referenced = true;
}

//
// This gets called at the end of compilation, 
// to emit all the functions that were,
// - declared before thier first use
// - and small enough to inline
// - but referenced by pointer later
//
fn emit_suspended_inlinables(m: *QbeModule) void = {
    log := m.debug["G".char()];
    for m.inlinable_but_referenced_later { id | 
        mark := mark_temporary_storage();
        f := @uninitialized Qbe.Fn;
        use_symbol(m, id) { s | 
            f = s.inline[]; // we stomp arrays to point at temp, but it would probably be fine because everyone's already gone through inlining. 
            f.globals = m;
            if log {
                @fmt_write(m.debug_out, "\nFinishing suspended function: %\n", s.name);
                printfn(f&, f.globals.debug_out);
            };
        };
        
        // we don't bother storing these in the inlining cache so recompute them
        // for now, its single block functions only so this is trivial. 
        fillrpo(f&);
        fillpreds(f&);
        fill_use(f&);
        
        finish_qbe_passes(f&);
        reset_temporary_storage(mark);
    };
    m.inlinable_but_referenced_later&.clear();
}
