// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>
// This is from a patch on the qbe mailing list by Roland Paterson-Jones: https://lists.sr.ht/~mpu/qbe/patches/54822

//!
//! When you try to call a small function whose definition we've already seen, we paste its body into the caller instead. 
//! This avoids forcing locals to be spilled for a callee that isn't going to do much work anyway,  
//! and gives the later optimisation passes a more complete picture of the program. 
//! We try to catch the cases where the cruft introduced by following the abi would be larger than the function body itself. 
//! If (all calls are inlined + its address is never taken + it is not exported) we don't bother generating code for the function at all. 
//!
//! You could do much better with a more complicated heuristic and the ability to inline multiblock functions, 
//! but what we do already is extremely good bang for your buck.
//!
//! note: the franca front end sorts based on callgraph so increase chances of things being inlined. 
//!

MAX_INL_COST :: 6;  // remember zero is not the minimum if you want to disable inlining. 

// TODO: this is broken when inlining mismatching env

// don't forget :ThisWillBreakIfYouMoveAstInliningToTheBackend (changes what set_dynamic_env means)
fn good4inl(f: *Qbe.Fn) bool = {
    fillrpo(f);
    fillpreds(f);
    blkmerge(f);
    fillrpo(f);
    fillpreds(f);
    // Disabling inlining makes self compile go from 1040ms to 1560ms -- May 30, 2025
    @if(!ENABLE_INLINING) return(false); // you can't return false before blkmerge :CreepyNop
    
    ::enum(Qbe.J);
    if f.nblk != 1 || f.vararg || f.start.jmp.type == .hlt {
        return(false);
    };
    
    b := f.start;
    npar := 0;
    nins := 0;
    write := false;
    for_insts_forward b { i |
        continue :: local_return;
        o := i.op();
        if(@is(o, .nop, .dbgloc), => continue());
        if o == .blit0 || is_store(i.op()) || i.op() == .call {    
            write = true;
        };
        @debug_assert(o != .vastart, "considering inlining $% which contains a vastart instruction but is not marked as variadic", f.name());
        if(is_par(i.op()), => npar&, => nins&)[] += 1;
    };
    cost := nins - 2 * (npar + int(b.jmp.type != .ret0)) - 1 /*Ocall*/;
    f.read_only = !write;
    
    cost <= MAX_INL_COST
}

// Note: this goes in the inlining cache so it can't use temp() memory.
fn cloneblk(m: *QbeModule, dest: []Qbe.Blk, b: *Qbe.Blk) void = {
    inlb := dest.index(b.id.intcast());
    a := m.forever&.borrow();
    inlb.ins = init(a, b.ins.len);
    inlb.jmp = b.jmp;
    inlb.ins.len = b.ins.len;
    inlb.ins.items().copy_from(b.ins.items());
    
    ::ptr_utils(@type inlb.phi[]);
    if dest.len == 1 {
        return();
    };

    if !b.phi.is_null() {
        phis := list(*Qbe.Phi, temp());
        for_phi b { p |
            d := a.box(Qbe.Phi);
            d[] = p[];
            d.blk = m.new_long_life(p.narg.zext());
            enumerate p.blk.slice(0, p.narg.zext()) { i, b |
                d.blk[i] = dest.index(b.id.intcast());
            };
            d.arg = m.new_copy_long_life(p.arg.slice(0, p.narg.zext()));
            phis&.push(d);
        };
        range(0, phis.len - 1) { i |
            phis[i].link = phis[i + 1].link;
        };
        inlb.phi = phis[0];
    };
    
    if !b.s1.is_null() {
        inlb.s1 = dest.index(b.s1.id.intcast());
    };
    if !b.s2.is_null() {
        inlb.s2 = dest.index(b.s2.id.intcast());
    };
    if !b.link.is_null() {
        inlb.link = dest.index(b.link.id.intcast());
    };
}

// Note: this goes in the inlining cache so it can't use temp() memory.
fn clone(f: *Qbe.Fn) *Qbe.InlineFn = {
    m := f.globals;
    inlfn := m.forever&.borrow().box_zeroed(Qbe.InlineFn);
    inlfn.retty = f.retty;
    inlfn.nblk = f.nblk;
    inlfn.leaf = f.leaf;
    inlfn.read_only = f.read_only;
    inlfn.lnk = f.lnk;
    inlfn.past_last_param_idx = ptr_diff(f.start.ins.ptr, f.find_past_last_param()).intcast();
    inlfn.ret_cls = f.ret_cls;
    
    n: i64 = f.ntmp.zext();
    inlfn.ntmp = f.ntmp;
    @if(f.track_ir_names) {
        inlfn.tmp_names = m.forever&.borrow().alloc_init(Str, f.ntmp.zext() - Qbe.Tmp0) { i |
            f.tmp[i + Qbe.Tmp0].name.shallow_copy(m.forever&.borrow())
        };
    };
    
    inlfn.ncon = f.ncon;
    inlfn.con = m.new_long_life(inlfn.ncon.zext());
    inlfn.con.slice(0, f.ncon.zext()).copy_from(f.con.slice(0, f.ncon.zext()));
    
    @debug_assert(f.nmem == 0, "shouldn't need to inline clone f.mem");
    inlfn.blocks = m.forever&.borrow().alloc_zeroed(Qbe.Blk, f.nblk.zext());
    
    f.set_block_id();
    for_blocks f { b |
        cloneblk(m, inlfn.blocks, b);
    };
    inlfn
}

// return true if we should stop processing because it's possible for all uses to be inlined. 
fn save_for_inlining(f: *Qbe.Fn) bool = {
    // good4inl mutates to try to reduce block count so it seems creepy to skip that when marked no_inline (especially :CreepyNop)
    no_inline := !good4inl(f) || f.lnk.no_inline;
    if no_inline {
        // TODO: a test where this matters
        use_symbol(f.globals, f.lnk.id) { symbol |
            if f.lnk.export {
                symbol.referenced = true;
            };
        };
        return false;
    };
    
    m := f.globals;
    ::ptr_utils(Qbe.Fn);
    
    inlfn := clone(f);
    when_debug(f, .Inlining) { out |
        @fmt(out, "\n## Save for inlining:\n");
        printfn(f, out);
    };
    m := f.globals;
    
    result := false;
    use_symbol(f.globals, f.lnk.id) { symbol |
        symbol.inlinable = !no_inline;
        if f.lnk.export {
            symbol.referenced = true;
        };
        ::ptr_utils(@type symbol.inline[]);
        //@debug_assert(symbol.name.len > 0, "function with empty string as name. was that a mistake?");
        @debug_assert(symbol.inline.is_null(), "'%' already has an entry in the inline cache. these should both look like pointers: % %", symbol.name, Qbe.Fn.raw_from_ptr(f), Qbe.InlineFn.raw_from_ptr(symbol.inline));
        symbol.inline = inlfn;
        result = !symbol.referenced;
    };
    
    result
}

fn inlfind(f: *Qbe.Fn, r: Qbe.Ref) ?*Qbe.InlineFn = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type() != .CAddr, => return(.None));
    m := f.globals;
    ::ptr_utils(Qbe.InlineFn);
    result: ?*Qbe.InlineFn = .None;
    use_symbol(f.globals, c.sym) { symbol |
        if symbol.inlinable {
            @debug_assert(!symbol.inline.is_null());
            result = (Some = symbol.inline);
        };
    };
    result
}

fn getinlref(inlfn: *Qbe.InlineFn, inlr: Qbe.Ref, f: *Qbe.Fn, c: *InlineCtx) Qbe.Ref = { 
    find :: fn(map: []Qbe.Ref, max: i64, $remap: @Fn(i: i64) Qbe.Ref) Qbe.Ref => {
        i := inlr.val();
        if(i < max, => return(inlr));
        t := map[i]&;
        if t[] == QbeNull {
            // first time seeing this ref: need to create an equivalent in the caller. 
            t[] = remap(i);
        };
        t[]
    };
    
    @match(rtype(inlr)) {
        fn RTmp() => find(c.remapped_tmp, Qbe.Tmp0) { i |
            name := @if(inlfn.tmp_names.len == 0, "", inlfn.tmp_names[i - Qbe.Tmp0]);
            f.newtmp(name, .Kw)
        };
        fn RCon() => find(c.remapped_consts, fixed_const_count) { i |
            f.newcon(inlfn.con.index(i))
        };
        // some ref types are global
        @default => {
            @debug_assert(@is(rtype(inlr), .RNull, .RInt, .RType), "bad inline ref kind. RSlot/RMem are created by later passes.");
            inlr
        };
    }
}

fn display(f: *Qbe.Fn, out: *List(u8)) void = {
    printfn(f, out);    
}

fn inlargs(inlfn: *Qbe.InlineFn, f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins /*dest*/, c: *InlineCtx) void = {
    /* replace args with copies to par tmps */
    @debug_assert(inlfn.nblk == 1);
    src_ins := inlfn.start()[].ins&;
    i1 := src_ins.ptr;
    if i.op() == .arge {
        if src_ins.len == 0 || i1.op() != .pare {
            /* drop env arg because the callee does not take one */
            i.set_nop();
            i = i.offset(1);
        };
    } else {
        if src_ins.len > 0 && i1.op() == .pare {
            @debug_assert(rtype(i1.to) == .RTmp, "pare target must be tmp");
            // callee wants an env arg but caller does not provide one. they get 0.
            rto := getinlref(inlfn, i1.to, f, c);
    		ins := make_ins(.copy, i1.cls(), rto, QbeConZero, QbeNull);
    		addins(c.vins&, ins&);
            i1 = i1.offset(1);
        };
    };
    callee_name := f.globals.str(inlfn.lnk.id);
    while => i.op() != .call {
        @debug_assert(is_arg(i.op()), "expected 'argX' instructions before call. did we start in the wrong place somehow? called with wrong signeture?");
        @debug_assert(i.op() != .argv, "trying to inline a va-call (which means the def was not va)");
        @debug_assert(is_par(i1.op()), "expected 'parX' instructions at beginning of function. did we start in the wrong place somehow? caller=%, callee=%", f.name(), callee_name);
        @assert(i.cls() == i1.cls() || (i1.cls() == .Kw && i.cls() == .Kl), "cant inline call with wrong argument type. (callee = %, caller = %)", callee_name, f.name());
        @debug_assert(rtype(i1.to) == .RTmp, "par target must be tmp");
        
        r := i.arg&[int(i.op() == .argc)];
		rto := getinlref(inlfn, i1.to, f, c);

		if i.op() == .argc {
			@debug_assert(i1.op() == .parc, "expected parc for argc");
			// If a function makes absolutely no writes to memory (or calls that might do so), 
			// we don't need to bother copying the argument because it can't be mutated in the callee. 
			// TODO: this could take advantage of the more sophisticated alias analysis stuff we do in other passes.
			if !inlfn.read_only {
				r = insert_blit(f, i.arg&[0], i1.arg&[0], r, c);
			};
		};

		ins := make_ins(.copy, i.cls(), rto, r, QbeNull);
		addins(c.vins&, ins&);

        i  = i.offset(1);
        i1 = i1.offset(1);
        @debug_assert(b.ins.index_unchecked(b.ins.len).in_memory_after(i), "inlined oob");
        last_inl := src_ins[].index_unchecked(src_ins.len);
        @debug_assert(last_inl.in_memory_after(i1) || i.op() == .call, "inlined oob");// callee=%", callee_name);
    };
}

fn display(a: *Qbe.Ref, out: *List(u8)) void = a[].display(out);

fn insert_blit(f: *Qbe.Fn, rty: Qbe.Ref, rty1: Qbe.Ref, r: Qbe.Ref, c: *InlineCtx) Qbe.Ref = {
    m := f.globals;
    lock(m.types_mutex&);
    ty := m.get_type(rty.val())[].header;
    ty1 := m.get_type(rty1.val())[].header;
    
    msg :: "tried to inlined function with mismatching aggragate types (CALLER %)";
    @assert_eq(ty.size, ty1.size, msg, m.str(f.lnk.id));
    @assert_eq(ty.align_log2, ty1.align_log2, msg, m.str(f.lnk.id));

    // TODO: i shouldn't just copy pasta math i don't understand. 
    al := if(ty.align_log2 < 2, => 2, => if(ty.align_log2 > 4, => 4, => ty.align_log2.zext()));
    sz := (ty.size + ((1.shift_left(al))-1)).bit_and(bit_not(1.shift_left(al)-1));

    ral := f.newtmp("inl", .Kl);

    f.emit(alloc_op(zext ty.align_log2), .Kl, ral, f.getcon(zext sz), QbeNull);
    ins := make_ins(.blit0, .Kw, QbeNull, r, ral);
    addins(c.vins&, ins&);
    ins := make_ins(.blit1, .Kw, QbeNull, INT(zext ty.size), QbeNull);
    addins(c.vins&, ins&);
    unlock(m.types_mutex&);
    ral
}

fn inlbins(inlfn: *Qbe.InlineFn, f: *Qbe.Fn, c: *InlineCtx) void = {
    inlb := inlfn.start();
    /* skip par instructions */
    i := inlb.ins.index_unchecked(inlfn.past_last_param_idx.zext());
    last := inlb.ins.index_unchecked(inlb.ins.len);
    /* copy remaining instructions, translating refs to current fn */
    for(i, last) { i |
        addins(c.vins&, i);
        i1 := c.vins.index(c.vins.len - 1);
        i1.to = getinlref(inlfn, i1.to, f, c);
        i1.arg&[0] = getinlref(inlfn, i1.arg&[0], f, c);
        i1.arg&[1] = getinlref(inlfn, i1.arg&[1], f, c);
        
        /* hoist inlfn allocs to fn->start */
        // this lets them become fast locals (in $arch/isel) or be promoted to temporaries (in mem/promote).
        if is_alloc(i1.op()) {
            f.emit(i1[]);
            c.vins.len -= 1;  // i1.set_nop();
        } else {
            if i1.op() == .blit1 {
                @debug_assert(rtype(i1.arg&[0]) == .RInt);
                /* TODO - deeply dubious/overlapping */
                i1.arg&[0] = INT(abs(rsval(i1.arg&[0]).intcast()));
            };
        };
    };
}

InlineCtx :: @struct(
    // for RCon this stores: [val() in callee] -> ref in caller. (QbeNull means not remapped yet).
    remapped_consts: []Qbe.Ref,
    // for RTmp this stores: [val() in callee] -> ref in caller. (QbeNull means not remapped yet).
    remapped_tmp: []Qbe.Ref,
    vins: RawList(Qbe.Ins), 
);


/* return index of last inlined ins */
fn inlcall(f: *Qbe.Fn, b: *Qbe.Blk, i: *Qbe.Ins) i64 = {
    @debug_assert(ptr_diff(b.ins.ptr, i) >= 0 && ptr_diff(i, b.ins.index_unchecked(b.ins.len)) >= 0, "bad size for inlining");
    @debug_assert(i.op() == .call, "expected to be inlining call");
    inlfn := or inlfind(f, i.arg&[0]) {
        f.leaf = false;
        return(ptr_diff(b.ins.ptr, i))
    };
    f.leaf = f.leaf && inlfn.leaf;
    
    when_debug(f, .Inlining) { out |
        @fmt(out, "#    inlining call at @% ins % to simple function $%\n", b.name(), ptr_diff(b.ins.ptr, i), f.globals.str(inlfn.lnk.id));
    };
    
    @debug_assert_eq(inlfn.blocks.len, 1);
    c: InlineCtx = (
        remapped_consts = temp().alloc_zeroed(Qbe.Ref, inlfn.ncon.zext()),
        remapped_tmp = temp().alloc_zeroed(Qbe.Ref, inlfn.ntmp.zext()),
        vins  = init(temp(), b.ins.len + inlfn.start()[].ins.len),
    );
    
    /* go back to start of args */
    i1 := i;
    while => ptr_diff(b.ins.ptr, i1) > 0 && is_arg(i1.offset(-1).op()) {
        i1 = i1.offset(-1);
    };
    
    append(c.vins&, b.ins.ptr, i1);
    
    /* replace arg ins with copy to inline fn par tmps */
    inlargs(inlfn, f, b, i1, c&);
    
    @debug_assert(inlfn.nblk == 1, "don't know how to inline that");
    jmp := inlfn.start()[].jmp&;
    @debug_assert(is_ret(jmp.type));
    
    /* splice inlined fn instructions */
    inlbins(inlfn, f, c&);
    
    /* replace call ins with copy for non-void fn's */
    if i.to == QbeNull {
        // They called like it was a void function. 
        // We allow this for scalars because it would work if it wasn't inlined. 
        // But if an aggragate would have been returned by reference, this would fault at runtime if we hadn't tried to inline it. 
        @assert(inlfn.retty == QbeNull, "Tried to inline void call to function $% which returns an aggregate.", f.globals.str(inlfn.lnk.id));
        @debug_assert(jmp.type != .retc);
    } else {
        new_ref := getinlref(inlfn, jmp.arg, f, c&);
        /* aggregate type pass-by-value is implicit copy */
        if inlfn.retty != QbeNull {
            @debug_assert(jmp.type == .retc, "we save for inlining before lowering abi");
            // We have to do a copy because they might be returning something that wasn't on thier stack frame, 
            // TODO: we have aliasing info so should know if they're returning an alloca and don't need this copy. 
            new_ref = insert_blit(f, i.arg&[1], inlfn.retty, new_ref, c&);
        } else {
            @debug_assert(i.arg&[1] == QbeNull);
            @debug_assert(jmp.type != .retc);
            if new_ref == QbeNull {
                // it was a void function but they called it like it returned a scalar. 
                // again, this is probably a frontend bug but it work would if i didn't 
                // do inlining so i guess it should be allowed. 
                new_ref = QbeConZero;
            };
        };
        ins := make_ins(.copy, i.cls(), i.to, new_ref, QbeNull);
        addins(c.vins&, ins&);
    };
    nret := c.vins.len - 1;
    
    // TODO: copy chunks as we go so you don't recopy the end of the block for every inlined call :SLOW
    append(c.vins&, i.offset(1), b.ins.index_unchecked(b.ins.len));
    idup(b, c.vins.items());
    
    nret
}

fn inlcalls(f: *Qbe.Fn) void = {
    when_debug(f, .Inlining, fn(out) => @fmt(out, "\n## Processing function $%:\n", f.name()));
    
    // during this pass we collect allocas for inlined arg/ret in the scratch buffer 
    // and splice them into f.start at the end. 
    f.reset_scratch();
    
    f.leaf = true;  // updated in inlcall()
    for_blocks f { b | 
        n := 0;
        while => n < b.ins.len {
            i := b.ins.index(n);
            if i.op() == .call {
                /* careful, this rewrites b.ins */
                n = inlcall(f, b, i); 
            };
            n += 1;
        };
    };
    
    /* some new allocs in fn->start for retc, parc etc. */
    alloc_ins := f.slice_pending_scratch();
    if alloc_ins.len != 0 {
        // ins = ins[..<past_param] ++ alloc_ins ++ ins[past_param..<ins.end]
        i := f.find_past_last_param();
        vins: RawList(Qbe.Ins) = init(temp(), f.start.ins.len + alloc_ins.len);
        append(vins&, f.start.ins.ptr, i);
        append(vins&, alloc_ins.ptr, alloc_ins.index_unchecked(alloc_ins.len));
        append(vins&, i, f.start.ins.index_unchecked(f.start.ins.len));
        f.start.ins = vins;
        f.reset_scratch();
    };
    
    when_debug_printfn(f, .Inlining, "\n## After inlining calls:\n");
}

fn mark_referenced(m: *QbeModule, id: Qbe.Sym, symbol: *SymbolInfo) void #inline = {
    ::ptr_utils(@type symbol.inline[]);
    if symbol.inlinable && !symbol.referenced {
        m.inlinable_but_referenced_later&.push(id);
    };
    symbol.referenced = true;
}

//
// This gets called at the end of compilation, 
// to emit all the functions that were,
// - declared before thier first use
// - and small enough to inline
// - but referenced by pointer later
//
fn emit_suspended_inlinables(m: *QbeModule) void = {
    flush_debug(m);
    
    for m.inlinable_but_referenced_later& { id | 
        mark := mark_temporary_storage();
        f := @uninitialized Qbe.Fn;
        use_symbol(m, id) { s | 
            load_inline_fn(m, f&, s.inline); // we stomp arrays to point at temp, but it would probably be fine because everyone's already gone through inlining. 
            s.inline = Qbe.InlineFn.ptr_from_int(0); // for good luck
            f.globals = m;
            when_debug(m, .Inlining) { out |
                @fmt(out, "\n# Finishing suspended function: %\n", s.name);
                printfn(f&, out);
            };
        };
        before := m.inlinable_but_referenced_later.len;
        
        // we don't bother storing these in the inlining cache so recompute them
        // for now, its single block functions only so this is trivial. 
        fillrpo(f&);
        fillpreds(f&);
        fill_use(f&);
        
        {m.target.finish_passes}(f&);
        flush_debug(m);
        reset_temporary_storage(mark);
        
        // save_for_inlining() is after symbols in the function are marked as referenced, 
        // so don't have to worry about new ones being found in this loop. 
        @debug_assert_eq(before, m.inlinable_but_referenced_later.len);
    };
    m.inlinable_but_referenced_later&.clear();
    
    if m.need_static_memmove {
        import("@/backend/opt/simplify.fr")'emit_static_memmove(m);
    };
    
    // TODO: it would be a good idea to always do a debug check for unfilled non-weak symbols. 
    //       only matters when jitting until i stop defaulting to libc as import source. 
    //       the whole treatment of finalizing symbols in this compiler could use a refactor. 
    //       i just keep adding shit until it kinda works. 
    // TODO: this is a dumb place to put this. it just needs to be at the end before you try to call anything. 
    ::enum(@type m.goal.type); 
    ::enum(@type m.goal.arch);
    // :UnfilledGotAccessStillNeedsPatch
    if !m.goal.link_libc && !m.goal.got_indirection_instead_of_patches && m.goal.type == .JitOnly && m.goal.arch != .x86_64 {
        // TODO: keep a list of unfilled brks in loadaddr_bits :SLOW 
        for_symbols m { _, s |
            ::ptr_utils(@type s.inline[]);
            ::enum(@type s.kind);
            if s.inline.is_null() && @is(s.kind, .Pending) {
                @assert(!s.strong, "importing non-weak symbol '%' in static binary", s.name);
                for s.fixups { it |
                    ::tagged(@type it.type);
                    @match(it.type&) {
                        fn DataAbsolute() => {
                            // TODO: decide if this is the policy. 
                            //       the world is a better place if weak symbol pointers are null when not available 
                            //       but im inconsistant about doing that. 
                            i64.ptr_from_raw(it.patch_at)[] = 0;
                        };
                        fn InReg(info) => @match(m.goal.arch) {
                            fn aarch64() => {
                                // filling this patch matters because emit leaves it as brk. 
                                // it's legal to produce the address of a missing weak symbol, you just can't try to use it. 
                                reg := @as(u5) info.r;
                                inst: []u32 = (ptr = u32.ptr_from_raw(it.patch_at), len = 2);
                                inst[0] = movz(.X64, reg, 0, .Left0);
                                inst[1] = Arm.arm_nop;
                            };
                            fn rv64() => todo();
                            @default => unreachable();
                        };
                        @default => ();
                    };
                };
            }
        };
    };
}

fn load_inline_fn(m: *Qbe.Module, f: *Qbe.Fn, src: *Qbe.InlineFn) void = {
    default_init(f, m);
    f.start = src.start();
    f.nblk = src.nblk;
    f.con = src.con;
    f.ncon = src.ncon;
    f.retty = src.retty;
    f.lnk = src.lnk;
    f.leaf = src.leaf;
    f.read_only = src.read_only;
    f.ret_cls = src.ret_cls;
    range(Qbe.Tmp0, src.ntmp.zext()) { i |
        name := @if(src.tmp_names.len == 0, "", src.tmp_names[i - Qbe.Tmp0]);
        _ := f.newtmp(name, .Kw);
    };
}

fn start(f: *Qbe.InlineFn) *Qbe.Blk = 
    f.blocks.index(0);

#use("@/backend/lib.fr");
