// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

mix :: fn(x0: u32, x1: u32) u32 = 
    x0 + 17*x1;

rhash :: fn(r: Qbe.Ref) u32 = {
    t := r.type3_val29.bit_and(1.shift_left(3) - 1);
    v := r.type3_val29.shift_right_logical(3);
    mix(t, v)
}

ihash :: fn(i: *Qbe.Ins) u32 = {
    i.op().raw().bitcast()
    .mix(i.cls().raw().bitcast())
    .mix(rhash(i.arg&[0]))
    .mix(rhash(i.arg&[1]))
}

ieq :: fn(ia: *Qbe.Ins, ib: *Qbe.Ins) bool = 
    ia.op30_cls2 == ib.op30_cls2 && ia.arg&[0] == ib.arg&[0] && ia.arg&[1] == ib.arg&[1];

Gvn :: @struct {
    con01: Array(Qbe.Ref, 2);
    table: []*Qbe.Ins;
    copy_of: []Qbe.Ref;
    
    // These are just used inside simplify_phi (and cleared every time), we just want to reuse thier memory. 
    t_s: Qbe.BSet;
    a_s: Qbe.BSet;
    stk: List(*Qbe.Use);
};

gvndup :: fn(c: *Gvn, i: *Qbe.Ins, insert: bool) ?*Qbe.Ins = {
    idx := ihash(i).zext().mod(c.table.len);
    n := 1;
    loop {
        ii := c.table[idx];
        if ii.is_null() {
            if insert {
                c.table[idx] = i;
            }
            return(.None);
        }
        if ieq(i, ii) {
            return(Some = ii);
        }
        idx += 1;
        if c.table.len <= idx {
            idx = 0
        }
    }
}

replaceuse :: fn(f: *Qbe.Fn, u: *Qbe.Use, r1: Qbe.Ref, r2: Qbe.Ref) void = {
    b := f.rpo[u.bid];
    refs := @match(u.type) {
        fn UPhi() => u.u.phi.arg();
        fn UIns() => u.u.ins.arg&.items();
        fn UJmp() => b.jmp.arg&.slice(1);
        fn UXXX() => unreachable();
    };
    each refs { a |
        if a[] == r1 {
            a[] = r2;
        }
    };
    f.adduse(r2, u.type, b, u.u.r);
}

replaceuses :: fn(f: *Qbe.Fn, g: *Gvn, r1: Qbe.Ref, r2: Qbe.Ref) void = {
    @if(::safety_check_enabled(.DebugAssertions)) 
    if r1 == r2 {
        // happens after propagating dead edges in dedupjmp. 
        // i guess its fine, i think a phi=itself (with narg=1) can only happen in unreachable blocks.
        // TODO: make a more minimal test than tcc/collect_sections()
        r2 = Qbe.Undef;
    }
    t1 := f.get_temporary(r1);
    g.copy_of[r1.val()] = r2;
    each t1.uses() { u |
        replaceuse(f, u, r1, r2);
    };
    t1.nuse = 0;
}

dedupphi :: fn(f: *Qbe.Fn, g: *Gvn, b: *Qbe.Blk) void = {
    filter_phis b { p |
        phicopyref :: import("@/backend/opt/copy2.fr").phicopyref;
        r := phicopyref(f, b, p, g);
        keep := r == Qbe.Null;
        if !keep {
            replaceuses(f, g, p.to, r);
            p.to = Qbe.Null;
        };
        keep
    };
}

rcmp :: fn(a: Qbe.Ref, b: Qbe.Ref) i32 #inline = {
    if rtype(a) != rtype(b) {
        return rtype(a).raw() - rtype(b).raw();
    };
    (a.val() - b.val()).intcast()
}

maskcon :: fn(f: *Qbe.Fn, r: *Qbe.Ref, mask: i64) void = {
    if f.get_int(r[]) { v |
        if v.bit_and(mask) != v {
            r[] = f.getcon(v.bit_and(mask));
        }
    };
}

normins :: fn(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    if i.op().is_shift() {
        mask := 1.shift_left(5 + i.cls().is_wide().int()) - 1;
        f.maskcon(i.arg&.index(1), mask);
    }

    /* truncate constant bits to
     * 32 bits for s/w uses */
    range(0, 2) { n |
        if !is_wide(argcls(i, n)) {
            f.maskcon(i.arg&.index(n), MAX_u32);
        }
    }
    
    /* order arg[0] <= arg[1] for
     * commutative ops, preferring
     * RTmp in arg[0] */
    if OpTab'get(i.op(), .commutative) && rcmp(i.arg&[0], i.arg&[1]) > 0 {
        i.arg&.items().swap(0, 1);
    }
}

negcon :: fn(cls: Qbe.Cls, c: *Qbe.Con) bool = {
    z := @static(Qbe.Con) con(Qbe.no_symbol_S, 0);
    if try_fold(.sub, cls, z, c) { result |
        c[] = result;
        return true;
    };
    false
}

assoccon :: fn(f: *Qbe.Fn, b: *Qbe.Blk, i1: *Qbe.Ins) void = {
    o := i1.op();
    if o == .sub {
        o = .add;
    }

    if !OpTab'get(o, .associative) || is_float(i1.cls()) || rtype(i1.arg&[1]) != .RCon {
        return()
    }
    t2 := f.get_tmp(i1.arg&[0]) || return();
    if t2.alias.slot != Qbe.Null {
        // computing the address of a stack slot is pulled to usage site by isel.
        return();
    }
    
    c1 := f.get_constant(i1.arg&[1])[];
    if t2.def.is_null() {
        return();
    }
    i2 := t2.def;
    
    if rtype(i2.arg&[1]) != .RCon || o != @if(i2.op() == .sub, .add, i2.op()) {
        return();
    }
    c2 := f.get_constant(i2.arg&[1])[];

    @debug_assert(is_int(i2.cls()));
    @debug_assert(is_wide(i2.cls()).int() >= is_wide(i1.cls()).int());

    if i1.op() == .sub && !negcon(i1.cls(), c1&) {
        return();
    }
    if i2.op() == .sub && !negcon(i2.cls(), c2&) {
        return();
    }
    
    c := try_fold(o, i1.cls(), c1&, c2&) || return();

    if o == .add && c&.type() == .CBits {
        @debug_assert(i1.cls().is_int());
        b := @if(i1.cls() == .Kl, c&.bits(), c&.bits().sign_extend_low32());
        if b < 0 {
            ok := negcon(i1.cls(), c&);
            @debug_assert(ok);
            o = .sub;
        }
    }

    i1.set_op(o);
    i1.arg&[0] = i2.arg&[0];
    i1.arg&[1] = f.newcon(c&);
    f.adduse(i1.arg&[0], .UIns, b, Qbe.Ins, i1);
}

killins :: fn(f: *Qbe.Fn, g: *Gvn, i: *Qbe.Ins, r: Qbe.Ref) void = {
    replaceuses(f, g, i.to, r);
    i.set_nop();
}

dedupins :: fn(f: *Qbe.Fn, g: *Gvn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    normins(f, i);
    
    if i.op() == .sel1 {
        sel := igroup(b, i);
        if truthiness(f, b, i.arg&[0] == i.arg&[1], sel[0].arg&[0]) { cond |
            sel[0]&.set_nop();
            killins(f, g, i, i.arg&[cond.not().int()]);
            return();
        };
        g.copy_of[i.to.val()] = i.to;
        return();
    };
    
    @if(i.op() == .nop || f.pinned(i)) return();

    @debug_assert_ne(i.to, Qbe.Null, "tried to number inst without output: % in $%", i.op(), f.name());
    @if(ENABLE_CONST_FOLD) assoccon(f, b, i);

    copyref :: import("@/backend/opt/copy2.fr").copyref;
    r := copyref(f, b, i, g);
    if r != Qbe.Null {
        killins(f, g, i, r);
        return();
    }
    
    @if(ENABLE_CONST_FOLD) {
        r := foldref(f, i);
        if r != Qbe.Null {
            killins(f, g, i, r);
            return();
        }
    };
    
    alias := f.get_temporary(i.to)[].alias&;
    if alias.slot != Qbe.Null {
        // computing the address of a stack slot is pulled to usage site by isel.
        g.copy_of[i.to.val()] = i.to;
        return();
    }
    
    if g.gvndup(i, true) { i1 |
        killins(f, g, i, i1.to);
        return();
    }
    
    g.copy_of[i.to.val()] = i.to;
}

cmpeqz :: fn(f: *Qbe.Fn, r: Qbe.Ref) ?Ty(Qbe.Ref, Qbe.Cls, i64) = {
    t := f.get_tmp(r) || return(.None);
    i := t.def;
    if i.is_null() || !OpTab'get(i.op(), .cmpeqwl) || i.arg&[1] != Qbe.ConZero {
        return(.None);
    };
    (Some = (i.arg&[0], argcls(i, 0), OpTab'get(i.op(), .eqval).int()))
}

branchdom :: fn(f: *Qbe.Fn, bif: *Qbe.Blk, bbr1: *Qbe.Blk, bbr2: *Qbe.Blk, b: *Qbe.Blk) bool = {
    @debug_assert(bif.jmp.type == .jnz);
    !b.identical(bif) && dom(bbr1, b) && !reachesnotvia(f, bbr2, b, bif)
}

domzero :: fn(f: *Qbe.Fn, d: *Qbe.Blk, b: *Qbe.Blk) ?i64 = {
    @if(branchdom(f, d, d.s1, d.s2, b)) return(Some = 0);
    @if(branchdom(f, d, d.s2, d.s1, b)) return(Some = 1);
    .None
}

/* infer 0/non-0 value from dominating jnz */
zeroval :: fn(f: *Qbe.Fn, b: *Qbe.Blk, r: Qbe.Ref, cls: Qbe.Cls) ?i64 = {
    d := b.idom;
    while => !d.is_null() {
        continue :: local_return;
        if d.jmp.type != .jnz {
            d = d.idom;  // :defer
            continue();
        }
        
        @if(r == d.jmp.arg && cls == .Kw)
        if domzero(f, d, b) { z | 
            return(Some = z);
        };
        if cmpeqz(f, d.jmp.arg) { arg, cls1, eqval |
            @if(r == arg && cls == cls1)
            if domzero(f, d, b) { z |
                return(Some = z.bit_xor(eqval));
            }
        }
        d = d.idom;  // :defer
    };
    .None
}

usecls :: fn(u: *Qbe.Use, r: Qbe.Ref, cls: Qbe.Cls) Qbe.Cls = {
    @match(u.type) {
        fn UIns() => {
            k := Qbe.Cls.Kx;  /* widest use */
            if u.u.ins.arg&[0] == r {
                k = argcls(u.u.ins, 0);
            }
            if u.u.ins.arg&[1] == r && (k == .Kx || !is_wide(k)) {
                k = argcls(u.u.ins, 1);
            }
            @if(k == .Kx, cls, k)
        }
        fn UPhi() => {
            eliminated := u.u.phi.to == Qbe.Null;
            @if(eliminated, cls, u.u.phi.cls)
        }
        fn UJmp() => .Kw;
        fn UXXX() => unreachable();
    }
}

propjnz0 :: fn(f: *Qbe.Fn, bif: *Qbe.Blk, s0: *Qbe.Blk, snon0: *Qbe.Blk, r: Qbe.Ref, cls: Qbe.Cls) void = {
    @if(s0.npred != 1) return();
    t := f.get_tmp(r) || return();
    each t.uses() { u |
        b := f.rpo[u.bid];
        /* we may compare an l temp with a w
        * comparison; so check that the use
        * does not involve high bits */
        if usecls(u, r, cls) == cls && branchdom(f, bif, s0, snon0, b) {
            replaceuse(f, u, r, Qbe.ConZero);
        }
    }
}

dedupjmp :: fn(f: *Qbe.Fn, b: *Qbe.Blk) void = {
    @if(b.jmp.type != .jnz) return();

    /* propagate jmp arg as 0 through s2 */
    propjnz0(f, b, b.s2, b.s1, b.jmp.arg, .Kw);
    /* propagate cmp eq/ne 0 def of jmp arg as 0 */
    if cmpeqz(f, b.jmp.arg) { arg, cls, eqval |
        ps := @slice(b.s1, b.s2);
        propjnz0(f, b, ps[eqval.bit_xor(1)], ps[eqval], arg, cls);
    }

    /* collapse trivial/constant jnz to jmp */
    if truthiness(f, b, b.s1.identical(b.s2), b.jmp.arg) { cond |
        if !cond {
            tmp := b.s1;
            b.s1 = b.s2;
            b.s2 = tmp;
        };
        /* we later move active ins out of dead blks */
        dest := edgedel(b, b.s2&);  // this makes sure you don't have dead preds in the phi so blkmerge and write_phi work
        b.jmp = (type = .jmp, arg = Qbe.Null);
        
        if dest.npred == 0 {
            // propagate removal of edges to create more foldable phis in live blocks. 
            stack := list(*Qbe.Blk, temp());
            stack&.push(dest);
            while => stack&.pop() { b |
                for_jump_targets_mut b { s |
                    dest := edgedel(b, s);
                    if dest.npred == 0 {
                        stack&.push(dest);
                    }
                };
            };
        };
    };
}

// :jnz_is_Kw
truthiness :: fn(f: *Qbe.Fn, b: *Qbe.Blk, force: bool, cond: Qbe.Ref) ?bool = {
    if force {
        return(Some = true);
    };
    if f.get_int(cond) { v |
        return(Some = v.bit_and(MAX_u32) != 0);
    };
    if zeroval(f, b, cond, .Kw) { z |
        return(Some = z == 0);
    };
    .None
}

rebuildcfg :: fn(f: *Qbe.Fn, g: *Gvn) void = {
    rpo := f.rpo.slice(0, f.nblk.zext()).shallow_copy(temp());
    fillcfg(f);

    /* move instructions that were in
     * killed blocks and may be active
     * in the computation in the start
     * block */
    for rpo { b |
        continue :: local_return;
        @if(b.id != MAX_u32.bitcast()) continue();
        /* blk unreachable after GVN */
        @debug_assert(!b.identical(f.start));
        for_insts_forward b { i |
            if !OpTab'get(i.op(), .pinned) {
                if g.gvndup(i, false) { i2 |
                    if i.identical(i2) {
                        f.start.push(i[]);
                    }
                };
            }
        }
    }
}

gvn :: fn(f: *Qbe.Fn) void = {
    f.requires(.Rpo); f.requires(.Pred); f.requires(.Use); f.requires(.Ssa);

    /* copy.c uses the visit bit */
    for_blocks f { b |
        for_phi b { p |
            p.visit = 0;
        };
    };

    nins := 0;
    for_blocks f { b |
        b.visit = 0;
        nins += b.ins.len;
    }

    g: Gvn = (
        table = temp().alloc_zeroed(*Qbe.Ins, nins + nins/2),
        con01 = (f.getcon(0), f.getcon(1)),
        copy_of = temp().alloc_zeroed(Qbe.Ref, f.ntmp.zext()),
        t_s = init_bitset(f.ntmp.zext()),
        a_s = init_bitset(f.ntmp.zext()),
        stk = list(*Qbe.Use, 10, temp()),
    );
    for_blocks_rpo_forward f { b |
        if b.npred != 0 || b.identical(f.start) {
            dedupphi(f, g&, b);
            for_insts_forward b { i | 
                dedupins(f, g&, b, i);
            };
            dedupjmp(f, b);
        } else {
            // block was dead before we got there, 
            // so none of its instructions can be live in table, 
            // and rebuildcfg doesn't have to bother. 
            b.ins.len = 0;
        }
    }
    rebuildcfg(f, g&);

    when_debug_printfn(f, .GCM, "\n## After GVN:\n");
    
    // :SLOW do this in there somewhere
    seen := init_bitset(f.ncon.zext());
    for_blocks f { b |
        refcon :: fn(r: Qbe.Ref) => @if(rtype(r) == .RCon) bsset(seen&, r.val());
        for_phi b { p |        
            for(p.arg(), refcon);
        };
        for_insts_forward b { i | 
            for(i.arg&, refcon);
        };
        refcon(b.jmp.arg);
    };
    mark_seen(f, seen&);
    
    f.breaks(.Ssa); f.breaks(.Use); f.breaks(.UseCounts); f.breaks(.Dom);
}

fn mark_seen(f: *Qbe.Fn, seen: *Qbe.BSet) void = {
    range(fixed_const_count, f.ncon.zext()) { i |
        c := f.con[i]&;
        @if(c.type() == .CAddr)
        if bshas(seen, i) { 
            // record that we referenced the symbol so it will be emitted even if it was suspended for inlining. 
            use_symbol(f.globals, c.sym) { s | 
                mark_referenced(f.globals, c.sym, s);  // required!
                
                // TODO: this was supposed to be just to help deduplication (which i got rid of), 
                //       but now removing it breaks examples/bf/bf2ir.fr (i assume because of the declare_alias changes)
                if s.alias != Qbe.no_symbol_S {
                    c.sym = s.alias;
                };
            };
        } else {
            // prevent unreferenced constants showing up in .frc files (as `$foo is Invalid` spam). 
            // happens when all uses were (inlined) or (dead after folding). 
            c.sym = Qbe.no_symbol_S;  // optional
        }
    };
}

fn foldref(f: *Qbe.Fn, i: *Qbe.Ins) Qbe.Ref = {
    @if(rtype(i.to) != .RTmp) return(Qbe.Null);
    @if(!OpTab'get(i.op(), .can_fold)) return(Qbe.Null);
    @if(rtype(i.arg&[0]) != .RCon) return(Qbe.Null);
    cl := f.get_constant(i.arg&[0]);
    rr := i.arg&[1];
    if rr == Qbe.Null {
        rr = Qbe.ConZero;
    };
    @if(rtype(rr) != .RCon) return(Qbe.Null);
    cr := f.get_constant(rr);
    c := try_fold(i.op(), i.cls(), cl, cr) || return(Qbe.Null);
    f.newcon(c&)
}

fn try_fold(op: Qbe.O, cls: Qbe.Cls, cl: *Qbe.Con, cr: *Qbe.Con) ?Qbe.Con = {
    c := Qbe.Con.zeroed();
    c.sym.id = Qbe.no_symbol;  // .CBits
    if cl.type() != .CBits || cr.type() != .CBits {
        if(!cls.is_int() || !(@is(op, .add, .sub)), => return(.None));
        // This is used for constant expression evaluation in import_c.
        // TODO: but sometimes you'd be better off doing this in isel when you could have the offsets as part of the memory access and create fewer relocations. 
        if cr.type() == .CAddr {
            if op == .add {
                t := cl; cl = cr; cr = t;
            };
            if(cr.type() == .CAddr, => return(.None));
        };
        c.sym = cl.sym;  // .CAddr
    };
    
    w := cls.is_wide();
    if cls.is_int() && (@is(op, .div, .rem, .udiv, .urem)) {
        if(iscon(cr, w, 0), => return(.None));
        if (@is(op, .div, .rem)) {
            x := if(w, => MIN_i64, => MIN_i32);
            if(iscon(cr, w, bitcast(-1)) && iscon(cl, w, x.bitcast()), => return(.None));
        };
    };
    
    r := Precompiled'do_fold(op, cls, cl.bits(), cr.bits());
    c&.set_bits(r);
    (Some = c)
}

fn iscon(c: *Qbe.Con, w: bool, k: u64) bool = {
    c.type() == .CBits 
    && (!w || (@as(u64) c.bits().bitcast()) == k) 
    && ( w || (@as(u32) c.bits().trunc())   == k.trunc())
}

#use("@/backend/opt/gcm.fr");
#use("@/backend/lib.fr");
