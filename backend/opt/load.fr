// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! After promote() has removed the easy cases, this pass deals with 
//! the fact that aliasable memory access is more expressive than simple assignment.
//! Memory gives you two special powers:
//! - Splicing: You can store two ints and load them back as a long,
//!             or copy individual bytes around within a value, etc.
//!             We can convert these to bitshifts and masking to mix values in registers. 
//! -  Casting: You can store a float and load an int from the same address.
//!             We can convert that to a bitcast. 

Loc :: @struct(
    type : @enum(u32) (
        LRoot,   /* right above the original load */
        LLoad,   /* inserting a load is allowed */
        LNoLoad, /* only scalar operations allowed */
    ),
    index_in_block: u32,
    blk: *Qbe.Blk,
);
::enum_basic(get_field_type(Loc, @symbol type));
::enum_basic(Aliasing);

MSlice :: @struct(
    ref: Qbe.Ref,
    off: i32,
    size: i32, // this was an i16 but that doesn't make Insert any smaller
    cls: Qbe.Cls, /* load class */
);

Insert :: @struct(
    is_phi: bool, // Qbe had this as a bit field in `num` but that doesn't save space because of alignment padding for the pointer in `new`
    num: u32,
    bid: u32,
    index_in_block: u32,
    new: @union(
        ins: Qbe.Ins,
        phi: @struct (
            m: MSlice,
            p: *Qbe.Phi,
        ),
    ),
);

LoadCtx :: @struct(
    f: *Qbe.Fn,
    inum: u32,            /* current insertion number */
    ilog: QList(Insert),  /* global insertion log */
    nlog: i64,            /* number of entries in the log */
);

/* require rpo ssa alias */
loadopt :: fn(f: *Qbe.Fn) void = {
    c: LoadCtx = (f = f, ilog = new(0), nlog = 0, inum = 0); c := c&;
    for_blocks f { b |
        for_insts_forward b { i |
            if maybe_load(i) { size |
                sl: MSlice = (ref = i.arg&[0], off = 0, size = size, cls = i.cls());
                l: Loc = (type = .LRoot, index_in_block = ptr_diff(b.ins.ptr, i).trunc(), blk = b);
                @debug_assert(i.arg&[1] == QbeNull, "load pass runs after isel so my arm thingy wont mess with it");
                i.arg&[1] = c.def(sl, ones_mask_of_size(size), b, (Some = i), l&);
            }
        }
    };
    
    emit_insertions(f, c.ilog, c.nlog);
    
    when_debug_printfn(f, .Memory, "\n## After load elimination:\n");
};

emit_insertions :: fn(f: *Qbe.Fn, ilog: QList(Insert), nlog: i64) void = {
    // Sort them so we can just scan through the blocks in order. 
    // All insertions for a block together, phis first, insts sequential. 
    sort :: quicksort(Insert, icmp);
    sort(ilog.slice(0, nlog));
    icmp :: fn(a: *Insert, b: *Insert) bool = {
        if(a.bid != b.bid, => return(a.bid < b.bid));
        if(a.is_phi && b.is_phi, => return(true));
        if(a.is_phi, => return(true));
        if(b.is_phi, => return(false));
        if(a.index_in_block != b.index_in_block, => return(a.index_in_block < b.index_in_block));
        a.num <= b.num
    }; // TODO: should be able to pass this as an argument when its an overload set and do the resolve. 
    
    ilog&.grow(nlog + 1);
    ilog[nlog].bid = f.nblk.bitcast(); /* add a sentinel */
    
    // TODO: explain more
    ib: RawList(Qbe.Ins) = init(temp(), 0);
    next_insertion := ilog.first;
    range(0, f.nblk.zext()) { current_bid |
        move_to_next_block :: local_return; 
        b := f.rpo[current_bid];
        changed := next_insertion.bid == current_bid.trunc();
        while => next_insertion.bid == current_bid.trunc() && next_insertion.is_phi {
            next_insertion.new.phi.p.link = b.phi;
            b.phi = next_insertion.new.phi.p;
            next_insertion = next_insertion.offset(1);
        };
        index_in_old_block := 0;
        ib.len = 0;
        loop {
            ::if(*Qbe.Ins);
            next_goes_here := next_insertion.bid == current_bid.trunc() && next_insertion.index_in_block == index_in_old_block.trunc();
            i := if next_goes_here {
                i := next_insertion.new.ins&;
                next_insertion = next_insertion.offset(1);
                i
            } else {
                if b.ins.len == index_in_old_block {
                    if changed {
                        idup(b, ib.items());
                    } else {
                        @debug_assert_eq(b.ins.len, ib.len, "load ins count");
                    };
                    move_to_next_block();
                };
                i := b.ins.index(index_in_old_block);
                index_in_old_block += 1;
                if is_load(i.op()) && i.arg&[1] != QbeNull {
                    convert_load_to_move(i);
                    changed = true;
                };
                i
            };
            push(ib&, i[], temp());
        };
    };
    free(ilog);
};

convert_load_to_move :: fn(i: *Qbe.Ins) void = {
    ::if(Qbe.O);
    ext := rebase(i.op(), .extsb, .loadsb);
    o := @if_else {
        @if(i.op() == .load) => Qbe.O.copy;
        @if(i.op() == .loadsw || i.op() == .loaduw) => 
            if(i.cls() == .Kl, => ext, => .copy);
        @else => ext;
    };
    i.set_op(o);
    i.arg&[0] = i.arg&[1];
    i.arg&[1] = QbeNull;
};

/* returns a ref containing the contents of the slice
 * passed as argument, all the bits set to 0 in the
 * requested_bits argument are zeroed in the result;
 * the returned ref has an integer class when the
 * mask does not cover all the bits of the slice,
 * otherwise, it has class sl.cls
 * the procedure returns R when it fails */
def :: fn(c: *LoadCtx, sl: MSlice, requested_bits: u64, b: *Qbe.Blk, i: ?*Qbe.Ins, il: *Loc) Qbe.Ref = {
    /* invariants:
     * -1- b dominates il.blk; so we can use
     *     temporaries of b in il.blk
     * -2- if il.type != LNoLoad, then il.blk
     *     postdominates the original load; so it
     *     is safe to load in il.blk
     * -3- if il.type != LNoLoad, then b
     *     postdominates il.blk (and by 2, the
     *     original load)
     */
    @debug_assert(dom(b, il.blk));
    oldl := c.nlog;
    oldt := c.f.ntmp;
    give_up_and_do_load :: fn() Never => {
        // Unwind any insertions we queued while trying to remove the load. 
        c.f.ntmp = oldt;
        c.nlog = oldl;
        
        if(il.type != .LLoad, => return(QbeNull));
        return(c.load_masked(sl, requested_bits, il))
    };
    i    := i || b.ins.index_unchecked(b.ins.len);
    out_int_cls  := if(sl.size > 4, => Qbe.Cls.Kl, => .Kw);
    bits_of_slice := ones_mask_of_size(sl.size);

    // First scan through the instructions in the block before the load being replaced. 
    // 
    while => ptr_diff(b.ins.ptr, i) > 0 {
        continue :: local_return;
        i = i.offset(-1);
        if c.killsl(i.to, sl) {
            // We've reached where our address was computed without seeing a store to it. 
            // So we have no idea what might have stored to that memory and have to bail. 
            // Simple cases (copy, adding a constant) have already been accounted for by alias.fr so this is not as conservative as it seems. 
            give_up_and_do_load();
        };
        is_call := @is(i.op(), .call);
        if is_call && escapes(sl.ref, c.f) {
            // We're calling some other function whose code we can't analyze,
            // and since this memory escapes the current function at some point, 
            // we have to assume it might be modified by the callee. 
            give_up_and_do_load();
        };
        is_read := is_load(i.op()); 
        ::if(i32);
        value, address, size := @if_else {
            @if(is_read)          => (i.to,      i.arg&[0],  size_of_load(i));
            @if(is_store(i.op())) => (i.arg&[0], i.arg&[1], size_of_store(i));
            @if(i.op() == .blit1) => {
                @debug_assert(rtype(i.arg&[0]) == .RInt, "blit needs const arg");
                size := abs(rsval(i.arg&[0]));
                @debug_assert(ptr_diff(b.ins.ptr, i) > 0, "malformed blit");
                i = i.offset(-1);
                @debug_assert(i.op() == .blit0, "malformed blit");
                (QbeNull, i.arg&[1], size)
            };
            @if(i.op() == .cas0) => (QbeNull, i.arg&[0], @as(i32) if(i.cls().is_wide(), => 8, => 4));
            @else => continue();  // if its not a memory op, it can't affect this load.
        };
        off: i32 = 0;  // the distance between the memory we want to load and the memory being affected by `i`. 
        @match(alias(sl.ref, sl.off, sl.size, address, size, off&, c.f)) {
            // If its a write that might be to our location but we're not sure, give up. 
            fn MayAlias() => if(is_read, => continue(), give_up_and_do_load);
            fn NoAlias()  => continue();
            fn MustAlias() => {
                if(i.op() == .cas0, give_up_and_do_load);  // we don't know if another thread modified the memory
                blit_source := sl;
                if i.op() == .blit0 {
                    blit_source.ref = i.arg&[0];
                    if off >= 0 {
                        @debug_assert(off < size);
                        blit_source.off = off;
                        size -= off;
                        off = 0;
                    } else {
                        blit_source.off = 0;
                        blit_source.size += off;
                    };
                    if size > blit_source.size {
                        size = blit_source.size;
                    };
                    @debug_assert(size <= 8);
                    blit_source.size = size;
                };
                @debug_assert(off <= 8, "they must alias and size is <= 8 so shifting below is ok.");
                bits_affected := ones_mask_of_size(size);
                ::if(Qbe.O);
                shift_op := if off < 0 {
                    off = -off;
                    bits_affected = bits_affected.shift_left(8*off.intcast());
                    Qbe.O.shl
                } else {
                    bits_affected = bits_affected.shift_right_logical(8*off.intcast());
                    .shr
                };
                bits_affected = bits_affected.bit_and(bits_of_slice);
                bits_overlap := bits_affected.bit_and(requested_bits);
                if(bits_overlap == 0, => continue());
                if i.op() == .blit0 {
                    // We found a memcpy to the memory we're trying to load.
                    // So repeat the process as through we were loading from the source of the copy instead.
                    value = c.def(blit_source, ones_mask_of_size(size), b, (Some = i), il);
                    if(value == QbeNull, give_up_and_do_load);
                };
                if off != 0 {
                    cls1 := out_int_cls;
                    if shift_op == .shr && off + sl.size > 4 {
                        cls1 = .Kl;
                    };
                    c.cast(value&, cls1, il);
                    r1 := c.f.getcon(8 * off.intcast());
                    value = c.iins(cls1, shift_op, value, r1, il);
                };
                if bits_overlap != bits_affected || off + size < sl.size {
                    c.mask(out_int_cls, value&, bits_overlap, il);
                };
                
                extra_bits_needed := requested_bits.bit_and(bit_not(bits_affected));
                if extra_bits_needed != 0 {
                    // `i` did not cover the whole region of memory being loaded, 
                    // so continue the process looking for the section we don't know yet,
                    // then merge the two parts.
                    r1 := c.def(sl, extra_bits_needed, b, (Some = i), il);
                    if(r1 == QbeNull, give_up_and_do_load);
                    value = c.iins(out_int_cls, .or, value, r1, il);
                };
                if requested_bits == bits_of_slice {
                    c.cast(value&, sl.cls, il);
                };
                return(value);
            };
        };
    };
    
    // If we get this far, 
    // - the address base wasn't created in this block
    // - the address didn't escape in this block
    // - we didn't see a potentially aliasing write
    
    // We might have already inserted a phi for this memory that we can reuse. 
    each c.ilog.slice(0, c.nlog) { ist | 
        m := ist.new.phi.m;
        if ist.is_phi
        && ist.bid == b.id.bitcast()
        && m.ref   == sl.ref
        && m.off   == sl.off
        && m.size  == sl.size {
            r := ist.new.phi.p.to;
            if requested_bits != bits_of_slice {
                c.mask(out_int_cls, r&, requested_bits, il);
            } else {
                c.cast(r&, sl.cls, il);
            };
            return(r);
        };
    };

    for_phi b { p |
        if c.killsl(p.to, sl) {
            /* scanning predecessors in that
             * case would be unsafe */
            give_up_and_do_load();
        };
    };

    if(b.npred == 0, give_up_and_do_load); // if this is the start block, we don't know where the memory came from. 
    if b.npred == 1 {
        // If we know what block came before the load, repeat the process there. 
        predecesor := b.pred[0];
        @debug_assert(predecesor.loop >= il.blk.loop);
        l := il[];
        if !predecesor.s2.is_null() {
            l.type = .LNoLoad;
        };
        value := c.def(sl, requested_bits, predecesor, .None, l&);
        if(value == QbeNull, give_up_and_do_load);
        return(value);
    };
    
    // If we could have branched here from multiple places,
    // repeat the process for each option and insert a phi in the current block. 
    // If any pred can't have its load eliminated, abandon the whole process. 
    // Don't want to bloat the code by uselessly splating out a single load into many earlier loads. 
    
    final_value := c.f.newtmp("ld", sl.cls);
    p := temp().boxed(Qbe.Phi, (
        to = final_value, 
        cls = sl.cls, 
        narg = b.npred, 
        arg = new(b.npred.zext()),
        blk = new(b.npred.zext()),
        link = zeroed(*Qbe.Phi),
    ));
    push(c.ilog&, c.nlog&, (
        is_phi = true, 
        bid = b.id.bitcast(), 
        new = (phi = (m = sl, p = p)),
        index_in_block = 0, num = 0, // these two don't matter
    ));
    range(0, b.npred.zext()) { np |
        bp    := b.pred[np];
        can_insert_load  := bp.s2.is_null() && il.type != .LNoLoad && bp.loop < il.blk.loop;
        l: Loc = (type = @if(can_insert_load, .LLoad, .LNoLoad), blk  = bp, index_in_block = trunc bp.ins.len);
        r1    := c.def(sl, bits_of_slice, bp, .None, l&);
        if(r1 == QbeNull, give_up_and_do_load);
        p.arg[np] = r1;
        p.blk[np] = bp;
    };
    if requested_bits != bits_of_slice {
        c.mask(out_int_cls, final_value&, requested_bits, il);
    };
    final_value
};

mask :: fn(c: *LoadCtx, cls: Qbe.Cls, r: *Qbe.Ref, msk: u64, l: *Loc) void ={
    c.cast(r, cls, l);
    con := getcon(c.f, msk.bitcast());
    r[] = c.iins(cls, .and, r[], con, l);
};

cast :: fn(c: *LoadCtx, r: *Qbe.Ref, cls: Qbe.Cls, l: *Loc) void = {
    if(rtype(r[]) == .RCon, => return());
    @debug_assert(rtype(r[]) == .RTmp);
    cls0 := c.f.tmp[r[].val()].cls;
    if(cls0 == cls || (cls == .Kw && cls0 == .Kl), => return());
    if !is_wide(cls0) && is_wide(cls) {
        if cls0 == .Ks {
            r[] = c.iins(.Kw, .cast, r[], QbeNull, l);
        };
        r[] = c.iins(.Kl, .extuw, r[], QbeNull, l);
        if cls == .Kd {
            r[] = c.iins(.Kd, .cast, r[], QbeNull, l);
        }
    } else {
        if cls0 == .Kd && cls != .Kl {
            r[] = c.iins(.Kl, .cast, r[], QbeNull, l);
        };
        if cls0 != .Kd || cls != .Kw { 
            r[] = c.iins(cls, .cast, r[], QbeNull, l);
        }
    }
};

iins :: fn(c: *LoadCtx, cls: Qbe.Cls, op: Qbe.O, a0: Qbe.Ref, a1: Qbe.Ref, l: *Loc) Qbe.Ref = {
    r := c.f.newtmp("ld", cls);
    push(c.ilog&, c.nlog&, (
        is_phi = false,
        num = c.inum,
        bid = l.blk.id.bitcast(),
        index_in_block = l.index_in_block,
        new = (ins = make_ins(op, cls, r, a0, a1)),
    ));
    c.inum += 1;
    r
};

load_masked :: fn(c: *LoadCtx, sl: MSlice, requested_bits: u64, l: *Loc) Qbe.Ref = {
    loads_by_size :: {
        m := ast_alloc().alloc_zeroed(Qbe.O, 9);
        m[1] = .loadub;
        m[2] = .loaduh;
        m[4] = .loaduw;
        m[8] = .load;
        m
    };

    all := requested_bits == ones_mask_of_size(sl.size);
    cls := if(all, => sl.cls, => if(sl.size > 4, => .Kl, => .Kw));
    address := sl.ref;
    /* sl.ref might not be live here,
     * but its alias base ref will be
     * (see killsl() below) */
    if rtype(address) == .RTmp {
        a := c.f.get_temporary(address)[].alias;
        if a.type == .ACon || a.type == .ASym {
            address = c.f.symcon(a.u.sym, a.offset);
        } else { // ALoc, AEsc, AUnk
            @debug_assert(a.type != .ABot);
            address = TMP(a.base);
            if a.offset != 0 {
                r1 := c.f.getcon(a.offset);
                address = c.iins(.Kl, .add, address, r1, l);
            };
        };
    };
    ld  := loads_by_size[sl.size.int()];
    address = c.iins(cls, ld, address, QbeNull, l);
    if(!all, => c.mask(cls, address&, requested_bits, l));
    address
};

killsl :: fn(c: *LoadCtx, r: Qbe.Ref, sl: MSlice) bool #inline = {
    if(rtype(sl.ref) != .RTmp, => return(false));
    t := c.f.get_temporary(sl.ref);
    a := t.alias&;
    if a.type == .ACon || a.type == .ASym {
        false
    } else { // ALoc, AEsc, AUnk
        @debug_assert(a.type != .ABot, "ICE: confused about alising of %%. trying to load from an undeclared variable?", "%", t.name());
        TMP(a.base) == r
    }
};

// 1 -> 0xFF, 2 -> 0xFFFF, etc
ones_mask_of_size :: fn(bytes: i32) u64 #inline = (BIT(8*(bytes.intcast())-1)*2-1); /* must work when bytes==8 */

#use("@/backend/lib.fr");
