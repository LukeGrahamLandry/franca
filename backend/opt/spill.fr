// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! The ISA gives us a limited number of registers to work with but programs can use an arbitrary number of temporaries. 
//! This pass limits the number of values live at any moment to the number of registers allowed,
//! by inserting loads and stores to stack slots as needed. Ensuring that a value is live when needed by an instruction. 
//! We also try to be smart about not spilling things that are used a bunch in a loop. 
//! After this pass we know which tmps will be in registers but not which registers they'll be in.  

fn aggreg(hd: *Qbe.Blk, b: *Qbe.Blk) void = {
    /* aggregate looping information at
     * loop headers */
    bsunion(hd.gen&, b.gen&);
    range(0, 2) { k | 
        if b.nlive&[k] > hd.nlive&[k] {
            hd.nlive&[k] = b.nlive&[k];
        }
    }
}

// TODO: "unhandled mutual recursion" is a useless error message if you forget the return type here.
fn tmpuse(f: *Qbe.Fn, r: Qbe.Ref, is_use: bool, loop: i32) void = {
    if rtype(r) == .RMem {
        m := f.mem[r.val()]&;
        f.tmpuse(m.base, true, loop);
        f.tmpuse(m.index, true, loop);
        return();
    };
    
    if rtype(r) == .RTmp && r.val() >= Qbe.Tmp0 {
        t := f.get_temporary(r.val());
        t.nuse += int(is_use).trunc();
        t.ndef += int(!is_use).trunc();
        t.cost += loop.bitcast();
    };
}

/* evaluate spill costs of temporaries,
 * this also fills usage information
 * requires rpo, preds
 */
fn fillcost(f: *Qbe.Fn) void = {
    loopiter(f, aggreg);
    when_debug(f, .Spill, fn(out) => dump_loop_cost(f, out));
    
    range(0, f.ntmp.zext()) { i |
        t := f.get_temporary(i);
        is_reg := i < Qbe.Tmp0;
        ::if(u32);
        t.cost = if(is_reg, => MAX_u32, => 0); 
        t.nuse = 0;
        t.ndef = 0;
    };
    
    for_blocks f { b |
        for_phi b { p |
            t := f.get_temporary(p.to);
            f.tmpuse(p.to, false, 0);
            range(0, p.narg.zext()) { a | 
                n := p.blk[a].loop;
                t.cost += n.bitcast();
                f.tmpuse(p.arg[a], true, n);
            };
        };
        n := b.loop;
        for_insts_forward b { i |
            f.tmpuse(i.to, false, n);
            f.tmpuse(i.arg&[0], true, n);
            f.tmpuse(i.arg&[1], true, n);
        };
        f.tmpuse(b.jmp.arg, true, n);
    };
    
    ::FmtPad(CStr); ::FmtPad(i32); ::FmtPad(Str);
    when_debug(f, .Spill) { out |
        @fmt(out, "\n## Spill costs:\n");
        range(Qbe.Tmp0, f.ntmp.zext()) { n | 
            t := f.tmp[n]&;
            if t.cost != 0 {
                @fmt(out, "#\t%.% %\n", f_pad(t.name(), 10, .Before), n, t.cost);
            }
        };
        @fmt(out, "\n");
    };
}

SpillPass :: @struct(   
    f: *Qbe.Fn,
    slot4: i32,                /* next slot of 4 bytes */
    slot8: i32,                /* ditto, 8 bytes */
    class_mask: Array(Qbe.BSet, 2), 
    limit_buf: []i32 = empty(),
    // these are used in Limit_In_Reg and Do_Parallel_Moves respectively. 
    // nothing persists between calls, i just want to reuse the memory (and save rezeroing in bsinit)
    scratch_lir: Qbe.BSet,
    scratch_dpm: Qbe.BSet,
    loads := 0,
    stores := 0,
    slots := 0,
);

// Get the stack slot assigned for spilling t. 
// Reserving a new one if this is the first time we've spilled t.
fn slot(pass: *SpillPass, t: i64) Qbe.Ref = {
    @debug_assert_ge(t, Qbe.Tmp0, "cannot spill register");
    s := pass.f.tmp[t].slot;
    if s == -1 {
        pass.slots += 1;
        /* specific to NAlign == 3 */
        /* nice logic to pack stack slots
         * on demand, there can be only
         * one hole and slot4 points to it
         *
         * invariant: slot4 <= slot8
         */
        if is_wide(pass.f.tmp[t].cls) {
            s = pass.slot8;
            if pass.slot4 == pass.slot8 {
                pass.slot4 += 2*4;
            };
            pass.slot8 += 2*4;
        } else {
            s = pass.slot4;
            if pass.slot4 == pass.slot8 {
                pass.slot8 += 2*4;
                pass.slot4 += 1*4;
            } else {
                pass.slot4 = pass.slot8;
            }
        };
        s += pass.f.slot;
        pass.f.tmp[t].slot = s;
    };
    SLOT(s.zext())
}

/* restricts b to hold at most k
 * temporaries, preferring those
 * present in `prioritize` (if given), then
 * those with the largest spill cost.
 */
fn limit(pass: *SpillPass, in_reg: *Qbe.BSet, max_in_reg: i32, prioritize: ?*Qbe.BSet) void = {
    @debug_assert(max_in_reg >= 0, "negative allowed in registers??");
    f := pass.f;
    // Setup the list of tmps live right now,
    buf := pass.limit_buf&; // reuse memory because we really spam this function. 
    live_count: i64 = bscount(in_reg).zext();
    if live_count <= max_in_reg.intcast() {
        // easy, we have enough registers for everyone so we're done.
        return(); 
    };
    if live_count > buf.len { 
        buf[] = temp().alloc_uninit(i32, live_count);
    };
    live_tmps := buf[].slice(0, live_count);
    i := 0;
    for in_reg { t |
        bsclr(in_reg, t);
        live_tmps[i] = t.intcast();
        i += 1;
    };
    
    SContext :: @struct(f: *Qbe.Fn, prioritize: *Qbe.BSet);
    tcmp0 :: fn(pa: *i32, pb: *i32, f: *Qbe.Fn) bool = {
        ca := f.tmp[pa[].zext()].cost;
        cb := f.tmp[pb[].zext()].cost;
        cb <= ca 
    };
    tcmp1 :: fn(pa: *i32, pb: *i32, ctx: SContext) bool = {
        bb := bshas(ctx.prioritize, pb[].zext());
        aa := bshas(ctx.prioritize, pa[].zext());
        if aa == bb {
            tcmp0(pa, pb, ctx.f)
        } else {
            bb.int() <= aa.int()
        }
    };

    // then sort them by priority.
    if live_tmps.len > 1 {
        // TODO: none of the .ssa tests get here :MakeATestThatFails
        if prioritize { prioritize |
            sort :: quicksort(SContext, i32, tcmp1);
            live_tmps.sort(f = f, prioritize = prioritize);
        } else {
            sort :: quicksort(*Qbe.Fn, i32, tcmp0);
            live_tmps.sort(f);
        };
    };
    in_reg_count := max_in_reg.intcast();
    
    for live_tmps.slice(0, in_reg_count) { t |
        bsset(in_reg, t.zext());
    };
    for live_tmps.rest(in_reg_count) { t |
        pass.slot(t.zext());
    };
}

/* spills temporaries to fit the
 * target limits using the same
 * preferences as limit(); 
 */
fn limit_in_reg(pass: *SpillPass, in_reg: *Qbe.BSet, is_for_call: bool, prioritize: ?*Qbe.BSet) void = {
    allowed_ints, allowed_floats := (pass.f.globals.target.ngpr, pass.f.globals.target.nfpr);
    if is_for_call { 
        // at the call we're allowed to have ((total - caller saved) = callee saved) registers live
        T := pass.f.globals.target&;
        allowed_ints -= T.nrsave&[0];
        allowed_floats -= T.nrsave&[1];
    };
    floats := pass.scratch_lir; pass.scratch_lir = zeroed(Qbe.BSet) /*for good luck*/;
    bscopy(floats&, in_reg);
    bsinter(in_reg, pass.class_mask&.index(0)); // now in_reg is just ints
    bsinter(floats&, pass.class_mask&.index(1));
    pass.limit(in_reg, allowed_ints, prioritize);
    pass.limit(floats&, allowed_floats, prioritize);
    bsunion(in_reg, floats&); // return both to the caller
    pass.scratch_lir = floats;
}

fn hint_to_avoid(f: *Qbe.Fn, u: *Qbe.BSet, r: u64) void = {
    for(u, Qbe.Tmp0) { t |
        idx := phi_dest(t, f); 
        hint := f.tmp[idx].hint.avoid&;
        hint[] = hint[].bit_or(r);
    };
}

// note: rega rref() also looks at t.slot when creating copies to/from RSlot, 
//       so spill() can create stores that look dead but are used later. 
//       the times that spill() calls slot() without using the result are 
//       when it detects that it won't be in a register when rega needs to rref() it. 

/* reloads temporaries in `required_in_reg` that are
 * not in `already_in_reg` from their slots
 */
fn reload_spilled(pass: *SpillPass, required_in_reg: *Qbe.BSet, already_in_reg: *Qbe.BSet) void = {
    for(required_in_reg, Qbe.Tmp0) { t | 
        if !bshas(already_in_reg, t) {
            pass.f.emit(.load, pass.f.tmp[t].cls, TMP(t), pass.slot(t), QbeNull);
            pass.loads += 1;
        }
    };
}

fn maybe_store(pass: *SpillPass, r: Qbe.Ref) void #inline = {
    t := pass.f.get_temporary(r);
    if t.slot != -1 {
        // TODO: debug_assert(t.alias.slot != qbe.null) but that doesn't work because of fake_alloca
        f := pass.f;
        op := Qbe.O.storew.raw().zext() + t.cls.raw().zext();
        op := @as(Qbe.O) @as(i32) op.intcast();
        f.emit(op, .Kw, QbeNull, r, SLOT(t.slot.zext()));
        pass.stores += 1;
    };
}

fn is_copy_from_reg(i: *Qbe.Ins) bool #inline = 
    i.op() == .copy && isreg(i.arg&[0]);

fn do_parallel_moves(pass: *SpillPass, b: *Qbe.Blk, last_copy: *Qbe.Ins, in_reg: *Qbe.BSet) *Qbe.Ins = {
    f := pass.f;
    T := f.globals.target&;
    /* consecutive copies from
     * registers need to be handled
     * as one large instruction
     *
     * fixme: there is an assumption
     * that calls are always followed
     * by copy instructions here, this
     * might not be true if previous
     * passes change
     */
     // :RequireCopyAfterCall
     
    // Scan backwards to find the first in the group of consecutive copies.
    first_copy := {
        i := last_copy.offset(1);
        dowhile {
            i = i.offset(-1);
            t := i.to.val();
            if i.to != QbeNull && bshas(in_reg, t) {
                bsclr(in_reg, t);
                pass.maybe_store(i.to);
            };
            bsset(in_reg, i.arg&[0].val());
            !i.identical(b.ins.ptr) && is_copy_from_reg(i.offset(-1))
        };
        i
    };
    
    final_in_reg := pass.scratch_dpm; pass.scratch_dpm = zeroed(Qbe.BSet) /*for good luck*/;
    bscopy(final_in_reg&, in_reg);
    is_after_call := !first_copy.identical(b.ins.ptr) && @is(first_copy.offset(-1).op(), .call);
    avoid_registers := if is_after_call { 
        discard := Array(i32, 2).ptr_from_int(0);
        rcall := first_copy.offset(-1)[].arg&[1];
        retreg := ({T.retregs}(rcall, discard));
        @debug_assert(in_reg.t[].bit_and(retreg) == retreg, "we should already know returns are live");
        bs_clrlow(in_reg, retreg); // retregs arn't live before the call becuase we're making them now
        pass.limit_in_reg(in_reg, true, .None);
        arg_reg := {T.argregs}(rcall, discard);
        bs_setlow(in_reg, arg_reg); // the arguments are live before because we need them at the call
        T.caller_saved
    } else {
        pass.limit_in_reg(in_reg, false, .None);
        in_reg.t[]
    };
    f.hint_to_avoid(in_reg, avoid_registers);
    pass.reload_spilled(final_in_reg&, in_reg);
    pass.scratch_dpm = final_in_reg;
    
    copies := first_copy.between(last_copy.offset(1));
    // order doesn't matter because rega does parallel move
    each_rev copies { i |
        pass.f.emit(i);
    };
    first_copy
}

fn merge(f: *Qbe.Fn, u: *Qbe.BSet, bu: *Qbe.Blk, v: *Qbe.BSet, bv: *Qbe.Blk) void = {
    ::if(u64);

    if (bu.loop <= bv.loop) {
        bsunion(u, v);
    } else {
        for v { t |
            if f.tmp[t].slot == -1 {
                bsset(u, t);
            }
        };
    }
}

/* spill code insertion
 * requires spill costs, rpo, liveness
 *
 * Note: this will replace liveness
 * information (in, out) with temporaries
 * that must be in registers at block
 * borders
 *
 * Be careful with:
 * - Ocopy instructions to ensure register
 *   constraints
 */

fn spill(f: *Qbe.Fn) void = {
    T := f.globals.target&;
    ntmp: i64 = f.ntmp.zext();
    pass: SpillPass = (
        f = f, slot4 = 0, slot8 = 0, 
        class_mask = @array(init_bitset(ntmp), init_bitset(ntmp)), 
        scratch_lir = init_bitset(ntmp),
        scratch_dpm = init_bitset(ntmp),
    );
    
    // in this pass t.slot is its spill address. 
    // -1 means hasn't been spilled yet. 
    // on entry, all used tmps will have slot=-1 because of newtmp() and :IselRSlotGetsNewTmp
    range(0, ntmp) { t | 
        it := f.tmp[t]&;
        it.hint.register = @if(t < Qbe.Tmp0, t.intcast(), -1);  // for rega pass, avoids iterating the tmps twice
        is_float := !is_int(it.cls);  // don't need to check (t >= Qbe.Tmp0), util.fr/default_init sets tmp.cls correctly. 
        bsset(pass.class_mask&.index(int(is_float)), t);
    };

    // These are not used between blocks, we just want to reuse the memory. 
    all_needed_later := init_bitset(ntmp);
    in_reg := init_bitset(ntmp);
    prioritize := init_bitset(ntmp);
    
    for_blocks_rpo_rev f { b |
        /* invariant: all blocks with bigger rpo got
         * their in,out updated. */

        /* 1. find temporaries in registers at
         * the end of the block (put them in `in_reg`) */
        f.globals.curi = Qbe.Ins.ptr_from_int(0); // i guess this is just asserting that we don't try to emit until we reset it below?
        backwards: ?*Qbe.Blk = .None;
        for_jump_targets(b) { s |
            if s.id <= b.id && (backwards.is_none() || s.id >= backwards.Some.id) {
                backwards = (Some = s);
            };
        };
        if backwards { backwards |
            /* back-edge */
            bszero(in_reg&);
            bs_setlow(backwards.gen&, T.rglob); /* don't spill registers */
            range(0, 2) { k |
                n := if(k == 0, => T.ngpr, => T.nfpr);
                bscopy(all_needed_later&, b.out&);
                bsinter(all_needed_later&, pass.class_mask&.index(k));
                bscopy(prioritize&, all_needed_later&);
                bsinter(all_needed_later&, backwards.gen&);
                bsdiff(prioritize&, backwards.gen&);
                if bscount(all_needed_later&).bitcast() < n {
                    j: i32 = bscount(prioritize&).bitcast(); /* live through */
                    l := backwards.nlive&[k];
                    pass&.limit(prioritize&, max(0, n - (l - j)), .None);
                    bsunion(all_needed_later&, prioritize&);
                } else {
                    pass&.limit(all_needed_later&, n, .None);
                };
                bsunion(in_reg&, all_needed_later&);
            }
        } else {
            if !b.s1.is_null() {
                /* avoid reloading temporaries
                * in the middle of loops */
                bszero(in_reg&);
                live_on_edge(prioritize&, b, b.s1);
                f.merge(in_reg&, b, prioritize&, b.s1);
                if !b.s2.is_null() {
                    live_on_edge(all_needed_later&, b, b.s2);
                    f.merge(in_reg&, b, all_needed_later&, b.s2);
                    bsinter(prioritize&, all_needed_later&);
                };
                pass&.limit_in_reg(in_reg&, false, (Some = prioritize&));
            } else {
                bscopy(in_reg&, b.out&);
                if rtype(b.jmp.arg) == .RCall {
                    @debug_assert(b.jmp.type == .ret0, "expected return");
                    return_registers := {T.retregs}(b.jmp.arg, Array(i32, 2).ptr_from_int(0));
                    bs_setlow(in_reg&, return_registers);
                };
            };
        };
        
        f.globals.curi = f.scratch_start();
            
        /* 2. process the block instructions */
        if rtype(b.jmp.arg) == .RTmp && b.jmp.type != .hlt {
            // abi removes ret with value and non-rv-isel replaces jnz with cmp+jmpCC
            @debug_assert(f.globals.goal.arch == .rv64);
            t := b.jmp.arg.val();
            @debug_assert(f.tmp[t].cls.is_int(), "can only jump on int");
            bsset(in_reg&, t);
            bscopy(all_needed_later&, in_reg&);
            bszero(prioritize&); 
            bsset(prioritize&, t);
            pass&.limit_in_reg(in_reg&, false, (Some = prioritize&));
            @debug_assert(bshas(in_reg&, t), "jump arg must not be spilled"); 
            pass&.reload_spilled(all_needed_later&, in_reg&);
        };
        
        for(b.out&, Qbe.Tmp0) { t | 
            if !bshas(in_reg&, t) {
                pass&.slot(t);
            };
        };
        
        bscopy(b.out&, in_reg&);
        
        for_insts_rev b { i |
            continue :: local_return;
            if(i[].op() == .nop, => continue());
            if is_copy_from_reg(i[]) {
                i[] = pass&.do_parallel_moves(b, i[], in_reg&);
                continue();
            };
            
            bszero(prioritize&);
            if i.to != QbeNull {
                @debug_assert(rtype(i.to) == .RTmp, "can only assign tmp");
                t := i.to.val();
                if bshas(in_reg&, t) {
                    // we know we had t in a register after this, and we're creating t now, so its not in a register before this. 
                    bsclr(in_reg&, t);
                } else {                    
                    dead := pass.f.tmp[t].slot == -1 && !b.out&.bshas(t) && !@is(i[].op(), .cas1, .sel1, .salloc);
                    if dead {
                        // - late_copy_elim doesn't remove dead copies
                        // - isel inserts fake copies to Qbe.Null to keep flag setting result live
                        // - (most) instructions with outputs don't have side effects
                        // - this changes behaviour by removing potentially trapping loads/divisions
                        // - not for correctness. helps rega speed a bit and makes the ir more readable
                        @debug_assert(!is_store(i[].op()) && i[].op() != .call);
                        // don't emit() this instruction 
                        continue();
                    };
                    
                    // Either not used later in the block (but maybe in another block) or spilled later in the block.
                    /* make sure we have a reg
                     * for the result */
                    @debug_assert_ge(t, Qbe.Tmp0, "dead reg");
                    bsset(in_reg&, t);
                    bsset(prioritize&, t);
                };
            };
            j := 0;
            if f.globals.goal.arch == .x86_64 {
                // TODO: should deal with this properly but im not in the mood for intel hellscape today
                if i[].cls().is_int() {
                    j = amd64_memargs(i[].op());
                };
                for i.arg& { r |
                    j -= int(rtype(r) == .RMem);
                };
            };
            arg_used_again := Array(bool, 2).zeroed();
            range(0, 2) { n | 
                @match(rtype(i.arg&[n])) {
                    fn RMem() => {
                        t := i.arg&[n].val();
                        m := f.mem[t]&;
                        if rtype(m.base) == .RTmp {
                            bsset(in_reg&, m.base.val());
                            bsset(prioritize&, m.base.val());
                        };
                        if rtype(m.index) == .RTmp {
                            bsset(in_reg&, m.index.val());
                            bsset(prioritize&, m.index.val());
                        };
                    }
                    fn RTmp() => {
                        t := i.arg&[n].val();
                        arg_used_again&[n] = bshas(in_reg&, t);
                        bsset(in_reg&, t); // instructions want arguments in registers
                        if j <= 0 {
                            bsset(prioritize&, t);
                        };
                        j -= 1;
                    }
                    @default => ();
                };
            };
            bscopy(all_needed_later&, in_reg&);
            pass&.limit_in_reg(in_reg&, false, (Some = prioritize&));
            range(0, 2) { n | 
                if rtype(i.arg&[n]) == .RTmp {
                    t := i.arg&[n].val();
                    if !bshas(in_reg&, t) {
                        /* do not reload if the
                         * argument is dead
                         */
                        if !arg_used_again&[n] {
                            bsclr(all_needed_later&, t);
                        };
                        i.arg&[n] = pass&.slot(t);
                    }
                }
            };
            pass&.reload_spilled(all_needed_later&, in_reg&);
            if i.to != QbeNull {
                t := i.to.val();
                pass&.maybe_store(i.to);
                if t >= Qbe.Tmp0 {
                    /* in case i.to was a
                     * dead temporary */
                    bsclr(in_reg&, t);
                }
            };
            f.emit(i[][]);
            r := in_reg.t[]; /* Tmp0 is NBit */
            if r != 0 {
                f.hint_to_avoid(in_reg&, r);
            };
        };
        if b.identical(f.start) {
            @debug_assert_eq(in_reg.t[], T.rglob.bit_or(f.reg), "global + callee saved % % in $%", T.rglob, f.reg, f.name());
        } else {
            @debug_assert_eq(in_reg.t[], T.rglob, "just global");
        };

        for_phi b { p |
            @debug_assert(rtype(p.to) == .RTmp, "can only assign tmp");
            t := p.to.val();
            if bshas(in_reg&, t) {
                bsclr(in_reg&, t);
                pass&.maybe_store(p.to);
            } else {
                if bshas(b.in&, t) {
                    /* only if the phi is live */
                    p.to = pass&.slot(p.to.val());
                }
            }
        };
        bscopy(b.in&, in_reg&);
        f.copy_instructions_from_scratch(b);
    };

    /* specific to NAlign == 3 */
    f.slot += pass.slot8.align_to(16);
    
    when_debug(f, .Spill, fn(out) => pass&.dump_spill(out));
}

// 
// Note: explicit memory access like load/store/cas HAS NO MEM ARGS HERE. 
// The number returned here is WHICH (not how many) arguments are allowed to be accessed 
// indirectly from thier spill slot in memory if there's a lot of register pressure. 
// 0 means none. 1 means only the first. 2 means either (but not both in the same instruction). 
// TODO: this happens a grand total of 27 times in the compiler. why bother? 
//
amd64_memargs :: fn(o: Qbe.O) i64 = {
    OpTab.tables[intcast @as(i32) o].shift_right_logical(24).bit_and(2)
};

#use("@/backend/lib.fr");
