// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! The ISA gives us a limited number of registers to work with but programs can use an arbitrary number of temporaries. 
//! This pass limits the number of values live at any moment to the number of registers allowed,
//! by inserting loads and stores to stack slots as needed. Ensuring that a value is live when needed by an instruction. 
//! We also try to be smart about not spilling things that are used a bunch in a loop. 
//! After this pass we know which tmps will be in registers but not which registers they'll be in.  

fn aggreg(hd: *Qbe.Blk, b: *Qbe.Blk) void = {
    /* aggregate looping information at
     * loop headers */
    bsunion(hd.gen&, b.gen&);
    range(0, 2) { k | 
        if b.nlive&[k] > hd.nlive&[k] {
            hd.nlive&[k] = b.nlive&[k];
        }
    }
}

// TODO: "unhandled mutual recursion" is a useless error message if you forget the return type here.
fn tmpuse(f: *Qbe.Fn, r: Qbe.Ref, is_use: bool, loop: i32) void = {
    if rtype(r) == .RMem {
        m := f.mem[r.val()]&;
        f.tmpuse(m.base, true, loop);
        f.tmpuse(m.index, true, loop);
        return();
    };
    
    if rtype(r) == .RTmp && r.val() >= Qbe.Tmp0 {
        t := f.get_temporary(r.val());
        t.nuse += int(is_use).trunc();
        t.ndef += int(!is_use).trunc();
        t.cost = trunc(@as(i64) t.cost.zext() + loop.intcast()); // :Casts
    };
}

/* evaluate spill costs of temporaries,
 * this also fills usage information
 * requires rpo, preds
 */
fn fillcost(f: *Qbe.Fn) void = {
    loopiter(f, aggreg);
    
    out := f.globals.debug_out;
    if f.globals.debug["S".char()] {
        write(out, "\n> Loop information:\n");
        for_blocks f { b |
            a := 0;
            while => a < b.npred.zext() && b.id > b.pred[a].id {
                a += 1;
            };
            
            if a != b.npred.zext() {
                @fmt_write(out, "\t%", f_pad(b.name(), 10, .After));
                @fmt_write(out, " (% ", f_pad(b.nlive&[0], 3, .Before));
                @fmt_write(out, "%) ", f_pad(b.nlive&[1], 3, .Before));
                dumpts(b.gen&, f.tmp, out);
            }
        }
    };
    
    range(0, f.ntmp.zext()) { i |
        t := f.get_temporary(i);
        is_reg := i < Qbe.Tmp0;
        ::if(u32);
        // TODO: this can't be all ones because i compare u32 like they were i32
        //       fascinating to me that i only broke with new_comptime_jit
        t.cost = if(is_reg, => 1.shift_left(31), => 0); 
        t.nuse = 0;
        t.ndef = 0;
    };
    
    for_blocks f { b |
        for_phi b { p |
            t := f.get_temporary(p.to);
            f.tmpuse(p.to, false, 0);
            range(0, p.narg.zext()) { a | 
                n := p.blk[a].loop;
                t.cost = trunc(@as(i64) t.cost.zext() + n.intcast()); // :Casts
                f.tmpuse(p.arg[a], true, n);
            };
        };
        n := b.loop;
        for_insts_forward b { i |
            f.tmpuse(i.to, false, n);
            f.tmpuse(i.arg&[0], true, n);
            f.tmpuse(i.arg&[1], true, n);
        };
        f.tmpuse(b.jmp.arg, true, n);
    };
    
    ::FmtPad(CStr); ::FmtPad(i32); ::FmtPad(Str);
    if f.globals.debug["S".char()] {
        write(out, "\n> Spill costs:\n");
        range(Qbe.Tmp0, f.ntmp.zext()) { n | 
            @fmt_write(out, "\t% %\n", f_pad(f.tmp[n]&.name(), 10, .After), f.tmp[n].cost);
        };
        write(out, "\n");
    };
}

SpillPass :: @struct(   
    f: *Qbe.Fn,
    locs: i32,                 /* stack size used by locals */
    slot4: i32,                /* next slot of 4 bytes */
    slot8: i32,                /* ditto, 8 bytes */
    class_mask: Array(Qbe.BSet, 2), 
    limit_buf: []i32 = empty(),
    // these are used in Limit_In_Reg and Do_Parallel_Moves respectively. 
    // nothing persists between calls, i just want to reuse the memory (and save rezeroing in bsinit)
    scratch_lir: Qbe.BSet,
    scratch_dpm: Qbe.BSet,
);

// Get the stack slot assigned for spilling t. 
// Reserving a new one if this is the first time we've spilled t.
fn slot(pass: *SpillPass, t: i64) Qbe.Ref = {
    @debug_assert_ge(t, Qbe.Tmp0, "cannot spill register");
    s := pass.f.tmp[t].slot;
    if s == -1 {
        /* specific to NAlign == 3 */
        /* nice logic to pack stack slots
         * on demand, there can be only
         * one hole and slot4 points to it
         *
         * invariant: slot4 <= slot8
         */
        if is_wide(pass.f.tmp[t].cls) {
            s = pass.slot8;
            if pass.slot4 == pass.slot8 {
                pass.slot4 += 2*4;
            };
            pass.slot8 += 2*4;
        } else {
            s = pass.slot4;
            if pass.slot4 == pass.slot8 {
                pass.slot8 += 2*4;
                pass.slot4 += 1*4;
            } else {
                pass.slot4 = pass.slot8;
            }
        };
        s += pass.locs;
        pass.f.tmp[t].slot = s;
    };
    SLOT(s.zext())
}

/* restricts b to hold at most k
 * temporaries, preferring those
 * present in `prioritize` (if given), then
 * those with the largest spill cost.
 */
fn limit(pass: *SpillPass, in_reg: *Qbe.BSet, max_in_reg: i32, prioritize: ?*Qbe.BSet) void = {
    @debug_assert(max_in_reg >= 0, "negative allowed in registers??");
    f := pass.f;
    // Setup the list of tmps live right now,
    buf := pass.limit_buf&; // reuse memory because we really spam this function. 
    live_count: i64 = bscount(in_reg).zext();
    if live_count <= max_in_reg.intcast() {
        // easy, we have enough registers for everyone so we're done.
        return(); 
    };
    if live_count > buf.len { 
        buf[] = temp().alloc(i32, live_count);
    };
    live_tmps := buf[].slice(0, live_count);
    i := 0;
    for in_reg { t |
        bsclr(in_reg, t);
        live_tmps[i] = t.intcast();
        i += 1;
    };
    
    SContext :: @struct(f: *Qbe.Fn, prioritize: *Qbe.BSet);
    tcmp0 :: fn(pa: *i32, pb: *i32, f: *Qbe.Fn) bool = {
        ca := f.tmp[pa[].zext()].cost;
        cb := f.tmp[pb[].zext()].cost;
        cb <= ca 
    };
    tcmp1 :: fn(pa: *i32, pb: *i32, ctx: SContext) bool = {
        bb := bshas(ctx.prioritize, pb[].zext());
        aa := bshas(ctx.prioritize, pa[].zext());
        if aa == bb {
            tcmp0(pa, pb, ctx.f)
        } else {
            bb.int() <= aa.int()
        }
    };

    // then sort them by priority.
    if live_tmps.len > 1 {
        // TODO: none of the .ssa tests get here :MakeATestThatFails
        if prioritize { prioritize |
            sort :: quicksort(SContext, i32, tcmp1);
            live_tmps.sort(f = f, prioritize = prioritize);
        } else {
            sort :: quicksort(*Qbe.Fn, i32, tcmp0);
            live_tmps.sort(f);
        };
    };
    in_reg_count := max_in_reg.intcast();
    
    for live_tmps.slice(0, in_reg_count) { t |
        bsset(in_reg, t.zext());
    };
    for live_tmps.slice(in_reg_count, live_tmps.len) { t |
        pass.slot(t.zext());
    };
}

/* spills temporaries to fit the
 * target limits using the same
 * preferences as limit(); 
 */
fn limit_in_reg(pass: *SpillPass, in_reg: *Qbe.BSet, is_for_call: bool, prioritize: ?*Qbe.BSet) void = {
    allowed_ints, allowed_floats := (pass.f.globals.target.ngpr, pass.f.globals.target.nfpr);
    if is_for_call { 
        // at the call we're allowed to have ((total - caller saved) = callee saved) registers live
        T := pass.f.globals.target&;
        allowed_ints -= T.nrsave&[0];
        allowed_floats -= T.nrsave&[1];
    };
    floats := pass.scratch_lir; pass.scratch_lir = zeroed(Qbe.BSet) /*for good luck*/;
    bscopy(floats&, in_reg);
    bsinter(in_reg, pass.class_mask&.index(0)); // now in_reg is just ints
    bsinter(floats&, pass.class_mask&.index(1));
    pass.limit(in_reg, allowed_ints, prioritize);
    pass.limit(floats&, allowed_floats, prioritize);
    bsunion(in_reg, floats&); // return both to the caller
    pass.scratch_lir = floats;
}

fn hint_to_avoid(f: *Qbe.Fn, u: *Qbe.BSet, r: u64) void = {
    for(u, Qbe.Tmp0) { t |
        idx := phi_dest(t, f); 
        hint := f.tmp[idx].hint.avoid&;
        hint[] = hint[].bit_or(r);
    };
}

/* reloads temporaries in `required_in_reg` that are
 * not in `already_in_reg` from their slots
 */
fn reload_spilled(pass: *SpillPass, required_in_reg: *Qbe.BSet, already_in_reg: *Qbe.BSet) void = {
    for(required_in_reg, Qbe.Tmp0) { t | 
        if !bshas(already_in_reg, t) {
            pass.f.emit(.load, pass.f.tmp[t].cls, TMP(t), pass.slot(t), QbeNull);
        }
    };
}

fn maybe_store(f: *Qbe.Fn, r: Qbe.Ref, slot: i32) void #inline = {
    if slot != -1 {
        op := Qbe.O.storew.raw().zext() + f.get_temporary(r)[].cls.raw().zext();
        op := @as(Qbe.O) @as(i32) op.intcast();
        f.emit(op, .Kw, QbeNull, r, SLOT(slot.zext()));
    };
}

fn is_copy_from_reg(i: *Qbe.Ins) bool #inline = 
    i.op() == .copy && isreg(i.arg&[0]);

fn do_parallel_moves(pass: *SpillPass, b: *Qbe.Blk, last_copy: *Qbe.Ins, in_reg: *Qbe.BSet) *Qbe.Ins = {
    f := pass.f;
    T := f.globals.target&;
    /* consecutive copies from
     * registers need to be handled
     * as one large instruction
     *
     * fixme: there is an assumption
     * that calls are always followed
     * by copy instructions here, this
     * might not be true if previous
     * passes change
     */
     
    // Scan backwards to find the first in the group of consecutive copies.
    first_copy := {
        i := last_copy.offset(1);
        dowhile {
            i = i.offset(-1);
            t := i.to.val();
            if i.to != QbeNull && bshas(in_reg, t) {
                bsclr(in_reg, t);
                f.maybe_store(i.to, f.tmp[t].slot);
            };
            bsset(in_reg, i.arg&[0].val());
            !i.identical(b.ins.ptr) && is_copy_from_reg(i.offset(-1))
        };
        i
    };
    
    final_in_reg := pass.scratch_dpm; pass.scratch_dpm = zeroed(Qbe.BSet) /*for good luck*/;
    bscopy(final_in_reg&, in_reg);
    is_after_call := !first_copy.identical(b.ins.ptr) && @is(first_copy.offset(-1).op(), .call, .syscall);
    avoid_registers := if is_after_call { 
        discard := Array(i32, 2).ptr_from_int(0);
        retreg := ({T.retregs}(first_copy.offset(-1)[].arg&[1], discard));
        @debug_assert(in_reg.t[].bit_and(retreg) == retreg, "we should already know returns are live");
        bs_clrlow(in_reg, retreg); // retregs arn't live before the call becuase we're making them now
        pass.limit_in_reg(in_reg, true, .None);
        arg_reg := {T.argregs}(first_copy.offset(-1)[].arg&[1], discard);
        bs_setlow(in_reg, arg_reg); // the arguments are live before because we need them at the call
        T.caller_saved
    } else {
        pass.limit_in_reg(in_reg, false, .None);
        in_reg.t[]
    };
    f.hint_to_avoid(in_reg, avoid_registers);
    pass.reload_spilled(final_in_reg&, in_reg);
    pass.scratch_dpm = final_in_reg;
    
    // Now emit all those copies
    i1 := last_copy.offset(1);
    dowhile {
        i1 = i1.offset(-1);
        pass.f.emit(i1[]);
        !i1.identical(first_copy)
    };
    first_copy
}


fn merge(f: *Qbe.Fn, u: *Qbe.BSet, bu: *Qbe.Blk, v: *Qbe.BSet, bv: *Qbe.Blk) void = {
    ::if(u64);

    if (bu.loop <= bv.loop) {
        bsunion(u, v);
    } else {
        for v { t |
            if f.tmp[t].slot == -1 {
                bsset(u, t);
            }
        };
    }
}

/* spill code insertion
 * requires spill costs, rpo, liveness
 *
 * Note: this will replace liveness
 * information (in, out) with temporaries
 * that must be in registers at block
 * borders
 *
 * Be careful with:
 * - Ocopy instructions to ensure register
 *   constraints
 */

fn spill(f: *Qbe.Fn) void = {
    T := f.globals.target&;
    // TODO: are we copying these because we might resize the array or can i inline them?
    tmp := f.tmp;
    ntmp: i64 = f.ntmp.zext();
    pass: SpillPass = (
        f = f, locs = f.slot, slot4 = 0, slot8 = 0, 
        class_mask = init(@slice(init_bitset(ntmp), init_bitset(ntmp))), 
        scratch_lir = init_bitset(ntmp),
        scratch_dpm = init_bitset(ntmp),
    );
    range(0, ntmp) { t | 
        f.tmp[t].slot = -1; // TODO: remove. should be fine. just making sure.
        
        is_float := if t >= Qbe.Tmp0 {
            !is_int(tmp[t].cls)
        } else {
            t >= T.fpr0.zext() && t < T.fpr0.zext() + T.nfpr.zext()
        };
        bsset(pass.class_mask&.index(int(is_float)), t);
    };

    // These are not used between blocks, we just want to reuse the memory. 
    all_needed_later := init_bitset(ntmp);
    in_reg := init_bitset(ntmp);
    prioritize := init_bitset(ntmp);
    
    bp := f.rpo.index_unchecked(f.nblk.zext());
    while => !bp.identical(f.rpo.first) {
        bp = bp.offset(-1);
        b := bp[];
        
        /* invariant: all blocks with bigger rpo got
         * their in,out updated. */

        /* 1. find temporaries in registers at
         * the end of the block (put them in `in_reg`) */
        f.globals.curi = Qbe.Ins.ptr_from_int(0); // i guess this is just asserting that we don't try to emit until we reset it below?
        backwards: ?*Qbe.Blk = .None;
        for_jump_targets(b) { s |
            if s.id <= b.id && (backwards.is_none() || s.id >= backwards.Some.id) {
                backwards = (Some = s);
            };
        };
        if backwards { (backwards:*Qbe.Blk) |  // TODO: why does this need a type annotation after adding meta/parse.fr? -- Nov 12
            /* back-edge */
            bszero(in_reg&);
            bs_setlow(backwards.gen&, T.rglob); /* don't spill registers */
            range(0, 2) { k |
                n := if(k == 0, => T.ngpr, => T.nfpr);
                bscopy(all_needed_later&, b.out&);
                bsinter(all_needed_later&, pass.class_mask&.index(k));
                bscopy(prioritize&, all_needed_later&);
                bsinter(all_needed_later&, backwards.gen&);
                bsdiff(prioritize&, backwards.gen&);
                if bscount(all_needed_later&).bitcast() < n {
                    j: i32 = bscount(prioritize&).bitcast(); /* live through */
                    l := backwards.nlive&[k];
                    //TODO: this is super fucking sketchy?
                    //i must be making a mistake elsewhere
                    //this is what made it think it was out of registers -- Nov 25
                    //but qbe seems to pass negative here as well it just doesn't choke on it as hard
                    //because its for loop just ends immediately as tho it was 0. 
                    pass&.limit(prioritize&, max(0, n - (l - j)), .None);
                    bsunion(all_needed_later&, prioritize&);
                } else {
                    pass&.limit(all_needed_later&, n, .None);
                };
                bsunion(in_reg&, all_needed_later&);
            }
        } else {
            if !b.s1.is_null() {
                /* avoid reloading temporaries
                * in the middle of loops */
                s := get_jump_targets(b);
                bszero(in_reg&);
                live_on_edge(prioritize&, b, s[0]);
                f.merge(in_reg&, b, prioritize&, s[0]);
                if s.len > 1 {
                    for s.rest(1) { s |
                        live_on_edge(all_needed_later&, b, s);
                        f.merge(in_reg&, b, all_needed_later&, s);
                        bsinter(prioritize&, all_needed_later&);
                    };
                };
                pass&.limit_in_reg(in_reg&, false, (Some = prioritize&));
            } else {
                bscopy(in_reg&, b.out&);
                if rtype(b.jmp.arg) == .RCall {
                    @debug_assert(b.jmp.type == .ret0, "expected return");
                    return_registers := {T.retregs}(b.jmp.arg, Array(i32, 2).ptr_from_int(0));
                    bs_setlow(in_reg&, return_registers);
                };
            };
        };
        
        for(b.out&, Qbe.Tmp0) { t | 
            if !bshas(in_reg&, t) {
                pass&.slot(t);
            };
        };
        
        f.globals.curi = f.scratch_start();
            
        /* 2. process the block instructions */
        if rtype(b.jmp.arg) == .RTmp && b.jmp.type != .hlt {
            // abi removes ret with value and non-rv-isel replaces jnz with cmp+jmpCC
            @debug_assert(f.globals.goal.arch == .rv64);  // on other arches 
            t := b.jmp.arg.val();
            @debug_assert(tmp[t].cls.is_int(), "can only jump on int");
            bsset(in_reg&, t);
            bscopy(all_needed_later&, in_reg&);
            bszero(prioritize&); 
            bsset(prioritize&, t);
            pass&.limit_in_reg(in_reg&, false, (Some = prioritize&));
            @debug_assert(bshas(in_reg&, t), "jump arg must not be spilled"); 
            pass&.reload_spilled(all_needed_later&, in_reg&);
        };
        bscopy(b.out&, in_reg&);
        
        for_insts_rev b { i |
            continue :: local_return;
            if(i[].op() == .nop, => continue());
            if is_copy_from_reg(i[]) {
                i[] = pass&.do_parallel_moves(b, i[], in_reg&);
                continue();
            };
            
            bszero(prioritize&); // :SLOW don't do this on the codepath where dead=true so we just continue() 
                                 // but can't do it after this block because we set something in the other case
            if i.to != QbeNull {
                @debug_assert(rtype(i.to) == .RTmp, "can only assign tmp");
                t := i.to.val();
                if bshas(in_reg&, t) {
                    // we know we had t in a register after this, and we're creating t now, so its not in a register before this. 
                    bsclr(in_reg&, t);
                } else {
                    // Either not used later in the block (but maybe in another block) or spilled later in the block.
                    dead := pass.f.tmp[t].slot == -1 && !b.out&.bshas(t);
                    if dead && !(@is(i[].op(), .salloc, .cas1)) { 
                        // my fuse_addressing in arm isel doesn't update usage counts correctly so i end up with a lot of dead pointer arithmetic. 
                        // rega would remove the dead code anyway but we can skip doing some work and reduce false register pressure. 
                        // A less hacky solution would be to fix it in fuse_addressing instead. that would prevent us from incorrectly thinking the args are live until getting here. 
                        // saves ~70ms. This happens 150k times with fuse_addressing on and 6k times with it off which mostly reassures me that i understand how we get here. -- Nov 23
                        // TODO: are there other bitsets im supposed to update?
                        
                        // TODO: need to do more if mem arg?
                        // :clear_dead_args
                        range(0, 2) { n | 
                            if rtype(i.arg&[n]) == .RTmp {
                                t := i.arg&[n].val();
                                if !bshas(in_reg&, t) {
                                    // if this instruction was the reason we though we cared about this argument, we don't anymore. 
                                    bsclr(all_needed_later&, t);
                                }
                            }
                        };
                        
                        // invariant: instructions that have outputs do not have side effects. (not salloc tho) 
                        //   amd64/isel tries to reuse flags a little (like {c := a+b; jnz c;} becomes {c := a+b; jmpCC;} not {c := a+b; cmp c; jmpCC;}), 
                        //   but it's careful to emit a fake copy if that's the last use so it won't get here. 
                        @debug_assert(!is_store(i[].op()) && i[].op() != .call && i[].op() != .syscall, "sideeffect");
                        // don't emit() this instruction 
                        continue();
                    };
                    /* make sure we have a reg
                     * for the result */
                    @debug_assert_ge(t, Qbe.Tmp0, "dead reg");
                    bsset(in_reg&, t);
                    bsset(prioritize&, t);
                };
            };
            j := {T.memargs}(i[].op());
            // TODO: should deal with this properly but im not in the mood for intel hellscape today
            if !i[].cls().is_int() {
                j = 0;
            };
            for i.arg& { r |
                if rtype(r) == .RMem {
                    j -= 1;
                };
            };
            
            arg_used_again := Array(bool, 2).zeroed();
            range(0, 2) { n | 
                @match(rtype(i.arg&[n])) {
                    fn RMem() => {
                        t := i.arg&[n].val();
                        m := f.mem[t]&;
                        if rtype(m.base) == .RTmp {
                            bsset(in_reg&, m.base.val());
                            bsset(prioritize&, m.base.val());
                        };
                        if rtype(m.index) == .RTmp {
                            bsset(in_reg&, m.index.val());
                            bsset(prioritize&, m.index.val());
                        };
                    }
                    fn RTmp() => {
                        t := i.arg&[n].val();
                        arg_used_again&[n] = bshas(in_reg&, t);
                        bsset(in_reg&, t); // instructions want arguments in registers
                        if j <= 0 {
                            bsset(prioritize&, t);
                        };
                        j -= 1;
                    }
                    @default => ();
                };
            };
            bscopy(all_needed_later&, in_reg&);
            pass&.limit_in_reg(in_reg&, false, (Some = prioritize&));
            // :clear_dead_args
            range(0, 2) { n | 
                if rtype(i.arg&[n]) == .RTmp {
                    t := i.arg&[n].val();
                    if !bshas(in_reg&, t) {
                        /* do not reload if the
                         * argument is dead
                         */
                        if !arg_used_again&[n] {
                            bsclr(all_needed_later&, t);
                        };
                        i.arg&[n] = pass&.slot(t);
                    }
                }
            };
            pass&.reload_spilled(all_needed_later&, in_reg&);
            if i.to != QbeNull {
                t := i.to.val();
                f.maybe_store(i.to, f.tmp[t].slot);
                if t >= Qbe.Tmp0 {
                    /* in case i.to was a
                     * dead temporary */
                    bsclr(in_reg&, t);
                }
            };
            f.emit(i[][]);
            r := in_reg.t[]; /* Tmp0 is NBit */
            if r != 0 {
                f.hint_to_avoid(in_reg&, r);
            };
        };
        if b.identical(f.start) {
            @debug_assert_eq(in_reg.t[], T.rglob.bit_or(f.reg), "global + callee saved % % in $%", T.rglob, f.reg, f.name());
        } else {
            @debug_assert_eq(in_reg.t[], T.rglob, "just global");
        };

        for_phi b { p |
            @debug_assert(rtype(p.to) == .RTmp, "can only assign tmp");
            t := p.to.val();
            if bshas(in_reg&, t) {
                bsclr(in_reg&, t);
                f.maybe_store(p.to, f.tmp[t].slot);
            } else {
                if bshas(b.in&, t) {
                    /* only if the phi is live */
                    p.to = pass&.slot(p.to.val());
                }
            }
        };
        bscopy(b.in&, in_reg&);
        f.copy_instructions_from_scratch(b);
    };

    /* align the locals to a 16 byte boundary */
    /* specific to NAlign == 3 */
    pass.slot8 += pass.slot8.zext().div(4).bit_and(3).intcast() * 4;
    f.slot += pass.slot8;

    if f.globals.debug["S".char()] {
        out := f.globals.debug_out;
        write(out, "\n> Block information:\n");
        for_blocks f { b |
            ::FmtPad(i32);
            @fmt_write(out, "\t% (%) ", f_pad(b.name(), 10, .After), f_pad(b.loop, 5, .Before));
            dumpts(b.out&, f.tmp, out);
        };
        write(out, "\n> After spilling:\n");
        printfn(f, out);
    };
}

fn bytes_to_bit_mask(bytes: []u8) u64 = {
    r: u64 = 0; 
    for bytes { b | 
        r = r.bit_or(BIT(b));
    };
    @debug_assert_eq(bytes.len, r.count_ones().zext(), "bytes_to_bit_mask expected unique entries");
    r
}

#use("@/backend/lib.fr");
