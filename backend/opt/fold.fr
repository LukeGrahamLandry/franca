// Adapted from Qbe. MIT License. © 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! This is the SCC algorithm described in 
//! Mark N. Wegman and F. Kenneth Zadeck. 1991. Constant propagation with conditional branches. 
//! ACM Transactions on Programming Languages and Systems, 13, 2 (1991), 181–210. https://doi.org/10.1145/103135.103136

// this process is split two phases: collecting constants and making changes. 
// first we go through all the blocks and call visit* and keep adding things to the work lists. 
// once the work lists are empty, we know all the constants, then we move to the next phase. 
// in the second phase we iterate all the live blocks once and replaces uses of any newly constant refs. 
// the first phase (visit* functions) are not allowed to make changes to the instructions! 
// since we start by assuming edges are dead, don't always visit the predecessors of a block before the block itself, 
// the first time a phi node is processed it might incorrectly treat some of its sources as dead and thus itself as a constant. 
// this will trigger a whole cascade of incorrect constant folding, but since no changes are being made yet, this is fine, 
// we're just putting junk in the val list. but when we eventually visit the predecesors of that phi, 
// we notice the mistake, and readd everything that depended on it to the work list and do it all again.  
// this seems more confusing than necessary, surely the point of calling it a lattice is that you only move down? 
// TODO: also surely all that extra work makes it slower. can we not? 
// -- Jan 23, 2025

// TODO: the places where i iscon($$, 0) assumes symbols are nonzero. 
//       this will need to change if i allow weak dynamic linking where a symbol might actually be null at runtime. 

fold_status_runtime :i32: -1;
fold_status_pending :i32:  0;

Edge :: @rec @struct(dest: i32, dead: bool);

FoldCtx :: @struct(
    f: *Qbe.Fn,
    val: []i32,
    edges: []Array(Edge, 2),
    flow_work: List(*Edge),
    use_work: List(*Qbe.Use),
    folded_sel := false,
);

// not memory and not special inserted later by abi/isel :OrderMatters
fn can_fold(o: Qbe.O) bool = {
    if(o == .extsb || o == .extsh, => return(false)); // TODO
    o.between(.add, .cuod) || o.between(.extsb, .cast) || o == .sel1 
}

/* require rpo, use, pred */
fn fold_constants(f: *Qbe.Fn) void = {
    start := Edge.zeroed();
    f: FoldCtx = (
        f      = f,
        val    = temp().alloc_zeroed(i32, f.ntmp.zext()),
        edges  = temp().alloc_zeroed(Array(Edge, 2), f.nblk.zext()), 
        use_work = list(0, temp()),
        flow_work = list(5, temp()),
    ); f := f&;
    f.flow_work&.push(start&);
    
    // val = alloc_zeroed so don't have to `range(f.f.ntmp.zext()) f.val[t] = fold_status_pending;`
    ::assert_eq(fold_status_pending, 0); 
    
    // :ExprLevelAsm
    ::assert_eq(fold_status_runtime.intcast(), -1);
    memset(i32.raw_from_ptr(f.val.ptr), 255, size_of(@type f.val[0]) * Qbe.Tmp0);
    
    range(0, f.f.nblk.zext()) { n | 
        b := f.f.rpo[n];
        b.visit = 0;
        initedge(f.edge(n, 0), b.s1);
        initedge(f.edge(n, 1), b.s2);
        @debug_assert_eq(b.id.intcast(), n);
    };
    initedge(start&, f.f.start);
    
    /* 1. find out constants and dead cfg edges */
    dowhile {
        continue :: local_return;
        if f.flow_work&.pop() { e |
            if e.dest == -1 || !e.dead {
                continue(true);
            };
            e.dead = false;
            n := e.dest;
            b := f.f.rpo[n.zext()];
            for_phi b { p | 
                f.visitphi(p, n);
            };
            if b.visit == 0 {
                for_insts_forward b { i |
                    f.visitins(b, i);
                };
                f.visitjmp(b);
            };
            @debug_assert(
                b.jmp.type != .jmp
                || !f.edge(n, 0)[].dead
                || identical(f.flow_work[f.flow_work.len - 1], f.edge(n, 0)
            ), "live direct jump should be queued");
            b.visit += 1;
            continue(true);
        };
        if f.use_work&.pop() { u |
            b := f.f.rpo[u.bid.zext()];
            if(b.visit == 0, => continue(true));
            @match(u.type) {
                fn UPhi() => f.visitphi(u.u.phi, u.bid);
                fn UIns() => f.visitins(b, u.u.ins);
                fn UJmp() => f.visitjmp(b);
                @default => panic("invalid fold use type");
            };
            continue(true);
        };
        false
    };
    
    /* 2. trim dead code, replace constants */
    any_dead_blocks := false;
    pb := f.f.start&;
    while => !pb[].is_null() { 
        continue :: local_return;
        b := pb[];
        if b.visit == 0 {
            any_dead_blocks = true;
            edgedel(b, b.s1&);
            edgedel(b, b.s2&);
            pb[] = b.link;
            continue();
        };
        pp := b.phi&;
        while => !pp[].is_null() { 
            p := pp[];
            if f.val[p.to.val()] != fold_status_runtime {
                // remove this phi node. 
                pp[] = p.link;
            } else {
                range(0, p.narg.zext()) { a | 
                    if !f.deadedge(p.blk[a].id, b.id) {
                        f.renumber(p.arg.index(a));
                    }
                };
                pp = p.link&;
            };
        };
        
        for_insts_forward b { i |
            if(i.op() == .sel1, => f.fold_sel_part2(i));
            if f.renumber(i.to&) {
                i.set_nop();
            } else {
                range(0, 2) { n | 
                    f.renumber(i.arg&.index(n))
                };
                if is_store(i.op()) && i.arg&[0] == QbeUndef {
                    i.set_nop();
                };
            };
        };
        f.renumber(b.jmp.arg&);
        
        if b.jmp.type == .jnz && rtype(b.jmp.arg) == .RCon {
            cond_is_false := iscon(f.f.get_constant(b.jmp.arg), false, 0);  // :jnz_is_Kw
            if cond_is_false {
                edgedel(b, b.s1&);
                b.s1 = b.s2;
                b.s2 = Qbe.Blk.ptr_from_int(0);
            } else {
                edgedel(b, b.s2&);
            };
            b.jmp.type = .jmp;
            b.jmp.arg = QbeNull;
        };
        pb = b.link&;
    };
    
    if f.f.globals.debug["F".char()] {
        dump_folding(f.f, f.val, f.f.globals.debug_out, f.folded_sel);
    };
}

fn fold_sel_part2(f: *FoldCtx, isel1: *Qbe.Ins) void = {
    isel0 := isel1.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    // do this first so we notice if they folded to the same constant. 
    range(0, 2, fn(n) => f.renumber(isel1.arg&.index(n)));
    
    // If the args are the same, the condition doesn't matter so pretend it's constant.  
    cond := if(isel1.arg&[0] == isel1.arg&[1], => QbeConZero, => isel0.arg&[0]);
    if rtype(cond) == .RCon {
        cond_is_false := iscon(f.f.get_constant(cond), false, 0);  // :jnz_is_Kw
        a := isel1.arg&[int(cond_is_false)];
        isel1[] = make_ins(.copy, isel1.cls(), isel1.to, a, QbeNull);
        isel0.set_nop();
        f.folded_sel = true;  // for logging (dump_folding)
    };
}

fn iscon(c: *Qbe.Con, w: bool, k: u64) bool = {
    c.type == .CBits 
    && (!w || (@as(u64) c.bits.i.bitcast()) == k) 
    && ( w || (@as(u32) c.bits.i.trunc())   == k.trunc())
}

fn latval(f: *FoldCtx, r: Qbe.Ref) i32 = @match(rtype(r)) {
    fn RTmp() => f.val[r.val()];
    fn RCon() => r.val().intcast();
    @default => @panic("latval of invalid %", r);
};

fn latmerge(v: i32, m: i32) i32 = {
    ::if(i32);
    if(m == fold_status_pending, => return(v));
    if(v == fold_status_pending || v == m, => m, => fold_status_runtime)
}

fn update(f: *FoldCtx, t: i32, m: i32) void = {
    t := t.zext();
    m := latmerge(f.val[t], m); 
    if(m == f.val[t], => return()); 
    // it changed; queue it to be reprocessed.
    uses := f.f.get_temporary(t).uses();
    f.use_work&.reserve(uses.len);
    each(uses, fn(u) => f.use_work&.push_assume_capacity(u));
    f.val[t] = m;
}

fn deadedge(f: *FoldCtx, s: i32, d: i32) bool = {
    e := f.edges.index(s.zext());
    if(e[0].dest == d && !e[0].dead, => return(false));
    if(e[1].dest == d && !e[1].dead, => return(false));
    true
}

fn visitphi(f: *FoldCtx, p: *Qbe.Phi, bid: i32) void = {
    v: i32 = fold_status_pending;
    // since a dead edge might just not have been visited yet, 
    // the latval after this loop might be wrong! but if it is, 
    // it will be fixed when the other edges are added to the work list. 
    range(0, p.narg.zext()) { a |
        if !f.deadedge(p.blk[a].id, bid) { 
            v = latmerge(v, f.latval(p.arg[a]));
        };
    };
    f.update(p.to.val().intcast(), v);
}

fn visitins(f: *FoldCtx, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    if(i.op() == .sel0, => f.fold_sel_part1(i));
    if(rtype(i.to) != .RTmp, => return());
    
    v := if(!can_fold(i.op()), => fold_status_runtime) {
        l := f.latval(i.arg&[0]);
        r := if(i.arg&[1] != QbeNull, => f.latval(i.arg&[1]), => QbeConZero.val().intcast());
        @if_else {
            @if(l == fold_status_runtime || r == fold_status_runtime) => fold_status_runtime;
            @if(l == fold_status_pending || r == fold_status_pending) => fold_status_pending;
            @else => f.f.opfold(i.op(), i.cls(), l, r);
        }
    };
    
    f.update(i.to.val().intcast(), v);
}

fn visitjmp(f: *FoldCtx, b: *Qbe.Blk) void = {
    n := b.id;
    @match(b.jmp.type) {
        fn jnz() => {
            cond := f.latval(b.jmp.arg);
            @debug_assert_ne(cond, fold_status_pending, "jnz(pending)");
            @if_else {
                @if(cond == fold_status_runtime) => {
                    f.flow_work&.push(f.edge(n, 1));
                    f.flow_work&.push(f.edge(n, 0));
                };
                @if(iscon(f.f.con.index(cond.zext()), false, 0)) => {  // cond = false  // :jnz_is_Kw
                    @debug_assert(f.edge(n, 0)[].dead, "jnz(0) s1 should be dead");
                    f.flow_work&.push(f.edge(n, 1));
                };
                @else => {  // cond = true
                    @debug_assert(f.edge(n, 1)[].dead, "jnz(1) s2 should be dead");
                    f.flow_work&.push(f.edge(n, 0));
                };
            };
        }
        fn jmp() => f.flow_work&.push(f.edge(n, 0));
        fn hlt() => ();
        @default => @assert(is_ret(b.jmp.type), "bad block terminator for fold");
    };
}

fn fold_sel_part1(f: *FoldCtx, isel0: *Qbe.Ins) void = {
    isel1 := isel0.offset(1);
    @debug_assert_eq(isel1.op(), .sel1);
    cond := f.latval(isel0.arg&[0]);
    if(cond <= 0, => return());
    want_right_value := iscon(f.f.con.index(cond.zext()), false, 0);  // :jnz_is_Kw
    a := isel1.arg&[int(want_right_value)];
    f.update(isel1.to.val().intcast(), f.latval(a));
}

fn initedge(e: *Edge, s: *Qbe.Blk) void = {
    e.dest = if(!s.is_null(), => s.id.bitcast(), => -1);
    e.dead = true;
}

fn renumber(f: *FoldCtx, r: *Qbe.Ref) bool = {
    if rtype(r[]) == .RTmp {
        v := f.val[r[].val()];
        if v != fold_status_runtime {
            r[] = CON(v.zext());
            return(true);
        }
    };
    false
}

fn edge(f: *FoldCtx, i: i32, j: i64) *Edge = f.edge(i.zext(), j);
fn edge(f: *FoldCtx, i: i64, j: i64) *Edge = 
    f.edges.index(i).index(j);

fn opfold(f: *Qbe.Fn, op: Qbe.O, cls: Qbe.Cls, l: i32, r: i32) i32 = {
    if op == .sel1 { 
        return(if(l == r, => l, => fold_status_runtime));
    };
    cl, cr := (f.con.index(l.zext()), f.con.index(r.zext()));
    if(cl.type != .CBits || cr.type != .CBits, => return(fold_status_runtime));
    
    c := Qbe.Con.zeroed();
    c.type = .CBits;
    if cls == .Kw || cls == .Kl {
        if foldint(c&, op, cls == .Kl, cl, cr) {
            return(fold_status_runtime);
        };
    } else {
        foldflt(c&, op, cls == .Kd, cl, cr);
    };
    if !is_wide(cls) {
        c.bits.i = c.bits.i.bit_and(0xffffffff);
    };
    r := f.newcon(c&);
    // TODO: there are tests that trigger this assertion -- Nov 12
    //@debug_assert(!(cls == .Ks || cls == .Kd) || c.flt != 0, "wrong fold type");
    r.val().intcast()
}

/* boring folding code */
// 
// I could do a fancy meta-programming thing where i generate these match statements because 
// i have to have the mapping of op to intrinsic anyway, but also that would just 
// end up more complicated than just pasting this here and never touching it again, 
// decisions, decisions!       -- Nov 7
//

BitcastPrim :: @union(s: i64, u: u64, fs: f32, fd: f64, sw: i32, uw: u32);

// return true on fail
fn foldint(res: *Qbe.Con, op: Qbe.O, w: bool, cl: *Qbe.Con, cr: *Qbe.Con) bool = {
    l: BitcastPrim = (s = cl.bits.i);
    r: BitcastPrim = (s = cr.bits.i);

    if (@is(op, .div, .rem, .udiv, .urem)) {
        if(iscon(cr, w, 0), => return(true));
        if (@is(op, .div, .rem)) {
            x := if(w, => MIN_i64, => MIN_i32);
            if(iscon(cr, w, bitcast(-1)) && iscon(cl, w, x.bitcast()), => return(true));
        };
    };
    
    shift_mask := 31.bit_or(w.int().shift_left(5));
    x := @match(op) {
        // signed overflow is not undefined for me (i hope)
        fn add()  => l.s + r.s;
        fn sub()  => l.s - r.s;
        fn neg()  => -l.s;
        // for 32 bit this relies on union putting all fields starting at the beginning and numbers being little endian. 
        // TODO: some of these must be wrong becuase the old backends don't do real 32 bit or unsigned operations. :FUCKED
        fn div()  => if(w, => l.s / r.s, => zext(l.sw / r.sw));
        fn rem()  => if(w, => l.s.mod(r.s), => l.sw.mod(r.sw).zext());
        fn udiv() => if(w, => bitcast(l.u / r.u), => zext(l.uw / r.uw));
        fn urem() => if(w, => l.u.mod(r.u).bitcast(), => l.uw.mod(r.uw).zext());
        fn mul()  => l.s * r.s;
        fn and()  => l.s.bit_and(r.s);
        fn or()   => l.s.bit_or(r.s);
        fn xor()  => l.s.bit_xor(r.s);
        fn sar() => if(w, => l.s, => sign_extend_low32(l.s)).shift_right_arithmetic(r.s.bit_and(shift_mask));
        fn shr() => if(w, => l.s, => bit_and(l.s, 1.shift_left(32) - 1)).shift_right_logical(r.s.bit_and(shift_mask));
        fn shl() => l.s.shift_left(r.s.bit_and(shift_mask));
        //fn Oextsb() => (int8_t)l.u;   // TODO
        fn extub() => l.s.bit_and(1.shift_left(8) - 1);
        //fn Oextsh() => (int16_t)l.u; // TODO 
        fn extuh() => l.s.bit_and(1.shift_left(16) - 1);
        fn extsw() => sign_extend_low32(l.s); 
        fn extuw() => l.s.bit_and(1.shift_left(32) - 1);
        fn stosi() => if(w, => l.fs.cast().int(), => l.fs.cast().int().sign_extend_low32());  // TODO: probably wrong rounding to just upcast. 
        fn stoui() => if(w, => l.fs.cast().int(), => l.fs.cast().int().sign_extend_low32());  // ^+ TODO: probably unsigned has to do something different to fit bigger number?
        //fn Ostoui() => w ? (uint64_t)cl.bits.s : (uint32_t)cl.bits.s;
        fn dtosi() => l.fd.int();
        fn dtoui() => l.fd.int(); // TODO: probably wide unsigned has to do something different to fit bigger number?
        //fn Odtoui() => w ? (uint64_t)cl.bits.d : (uint32_t)cl.bits.d;
        fn cast() => {
            if cl.type == .CAddr {
                res.type = .CAddr;
                res.sym = cl.sym;
            };
            l.s    
        }
        @default => fold_cmp(op, l&, r&).int();
    };
    res.bits = (i = x);
    false
}

Ocmpw :: Qbe.O.ceqw;
Ocmpl :: Qbe.O.ceql;

// TODO: can't pass @union by value
fn fold_cmp(op: Qbe.O, l: *BitcastPrim, r: *BitcastPrim) bool = {
    if op.between(Ocmpw, .cultw)  {
        l.s = sign_extend_low32(l.s);
        r.s = sign_extend_low32(r.s);
        op = rebase(op, Ocmpl, Ocmpw);
    };
    @match(op) {
        // i64
        fn culel() => l.u <= r.u; 
        fn cultl() => l.u < r.u;
        fn cslel() => l.s <= r.s; 
        fn csltl() => l.s < r.s;
        fn csgtl() => l.s > r.s;
        fn csgel() => l.s >= r.s;
        fn cugtl() => l.u > r.u;
        fn cugel() => l.u >= r.u;
        fn ceql()  => l.u == r.u;
        fn cnel()  => l.u != r.u;
        
        // f32
        fn cles() => l.fs <= r.fs;
        fn clts() => l.fs < r.fs; 
        fn cgts() => l.fs > r.fs; 
        fn cges() => l.fs >= r.fs;
        fn cnes() => l.fs != r.fs;
        fn ceqs() => l.fs == r.fs;
        fn cos() => l.fs < r.fs || l.fs >= r.fs;
        fn cuos() => !(l.fs < r.fs || l.fs >= r.fs);
        
        // f64
        fn cled() => l.fd <= r.fd;
        fn cltd() => l.fd < r.fd; 
        fn cgtd() => l.fd > r.fd; 
        fn cged() => l.fd >= r.fd;
        fn cned() => l.fd != r.fd;
        fn ceqd() => l.fd == r.fd;
        fn cod() => l.fd < r.fd || l.fd >= r.fd;
        fn cuod() => !(l.fd < r.fd || l.fd >= r.fd);
        @default => @panic("expected fold_cmp");
    }
}

fn foldflt(res: *Qbe.Con, op: Qbe.O, w: bool, cl: *Qbe.Con, cr: *Qbe.Con) void = {
    // TODO: this is wrong! I can't do real f32 ops on old backends.
    //       so instead i promote them to f64 and then cast the result back. 
    //       which is very similar but i assume not bit-for-bit the same. 
    
    ld := @if(w, cl.bits.d, cl.bits.s.s.cast());
    rd := @if(w, cr.bits.d, cl.bits.s.s.cast());
    
    x := @match(op) {
        fn add() => ld + rd;
        fn sub() => ld - rd;
        fn neg() => -ld;
        fn div() => ld / rd;
        fn mul() => ld * rd;
        // for the casts, we want to read as integer not float!
        fn swtof() => cl.bits.i.sign_extend_low32().float();
        fn uwtof() => cl.bits.i.bit_and(1.shift_left(32) - 1).float();
        fn sltof() => cl.bits.i.float();
        fn ultof() => {
            @assert(cl.bits.i >= 0, "TODO: implement ultof for numbers larger than MAX_i64");
            cl.bits.i.float()
        }
        fn exts()   => cl.bits.s.s.cast(); // w
        fn truncd() => cl.bits.d;          // !w
        fn cast()   => {
            res[] = cl[]; // don't touch the bits at all
            return()
        };
        @default => @panic("expected fold_float %", op.get_name());
    };
    if w {
        res.bits.d = x;
    } else {
        res.bits.s.s = x.cast();
    };
    res.flt = @if(w, 2, 1);
}
