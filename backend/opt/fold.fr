// Adapted from Qbe. MIT License. © 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! This is the SCC algorithm described in 
//! Mark N. Wegman and F. Kenneth Zadeck. 1991. Constant propagation with conditional branches. 
//! ACM Transactions on Programming Languages and Systems, 13, 2 (1991), 181–210. https://doi.org/10.1145/103135.103136

// this process is split two phases: collecting constants and making changes. 
// first we go through all the blocks and call visit* and keep adding things to the work lists. 
// once the work lists are empty, we know all the constants, then we move to the next phase. 
// in the second phase we iterate all the live blocks once and replaces uses of any newly constant refs. 
// the first phase (visit* functions) are not allowed to make changes to the instructions! 
// since we start by assuming edges are dead, don't always visit the predecessors of a block before the block itself, 
// the first time a phi node is processed it might incorrectly treat some of its sources as dead and thus itself as a constant. 
// this will trigger a whole cascade of incorrect constant folding, but since no changes are being made yet, this is fine, 
// we're just putting junk in the val list. but when we eventually visit the predecesors of that phi, 
// we notice the mistake, and readd everything that depended on it to the work list and do it all again.  
// this seems more confusing than necessary, surely the point of calling it a lattice is that you only move down? 
// TODO: also surely all that extra work makes it slower. can we not? 
// -- Jan 23, 2025

fold_status_runtime :i32: -1;
fold_status_pending :i32:  0;

Edge :: @rec @struct(dest: i32, dead: bool);

FoldCtx :: @struct(
    f: *Qbe.Fn,
    val: []i32,
    edges: []Array(Edge, 2),
    flow_work: List(*Edge),
    use_work: List(*Qbe.Use),
    folded_sel := false,
);

// not memory and not special inserted later by abi/isel :OrderMatters
fn can_fold(o: Qbe.O) bool = {
    argcls := import("@/backend/meta/ops.fr").tables;
    flags := argcls[intcast @as(i32) o];
    ops_table_canfold(flags)
}

/* require rpo, use, pred */
fn fold_constants(f: *Qbe.Fn) void = {
    @if(!ENABLE_CONST_FOLD) {
        mark_seen(f, zeroed(*Qbe.BSet), true);
        return();
    };
    
    start := Edge.zeroed();
    f: FoldCtx = (
        f      = f,
        val    = temp().alloc_zeroed(i32, f.ntmp.zext()),
        edges  = temp().alloc_zeroed(Array(Edge, 2), f.nblk.zext()), 
        use_work = list(0, temp()),
        flow_work = list(5, temp()),
    ); f := f&;
    f.flow_work&.push(start&);
    
    // val = alloc_zeroed so don't have to `range(f.f.ntmp.zext()) f.val[t] = fold_status_pending;`
    ::assert_eq(fold_status_pending, 0); 
    
    // :ExprLevelAsm
    ::assert_eq(fold_status_runtime.intcast(), -1);
    f.val.slice(0, Qbe.Tmp0).interpret_as_bytes().set_bytes(255);
    
    range(0, f.f.nblk.zext()) { n | 
        b := f.f.rpo[n];
        b.visit = 0;
        initedge(f.edge(n, 0), b.s1);
        initedge(f.edge(n, 1), b.s2);
        @debug_assert_eq(b.id.intcast(), n);
    };
    initedge(start&, f.f.start);
    
    /* 1. find out constants and dead cfg edges */
    dowhile {
        continue :: local_return;
        if f.flow_work&.pop() { e |
            if e.dest == -1 || !e.dead {
                continue(true);
            };
            e.dead = false;
            n := e.dest;
            b := f.f.rpo[n.zext()];
            for_phi b { p | 
                f.visitphi(p, n);
            };
            if b.visit == 0 {
                for_insts_forward b { i |
                    f.visitins(b, i);
                };
                f.visitjmp(b);
            };
            @debug_assert(
                b.jmp.type != .jmp
                || !f.edge(n, 0)[].dead
                || identical(f.flow_work[f.flow_work.len - 1], f.edge(n, 0)
            ), "live direct jump should be queued");
            b.visit += 1;
            continue(true);
        };
        if f.use_work&.pop() { u |
            b := f.f.rpo[u.bid.zext()];
            if(b.visit == 0, => continue(true));
            @match(u.type) {
                fn UPhi() => f.visitphi(u.u.phi, u.bid);
                fn UIns() => f.visitins(b, u.u.ins);
                fn UJmp() => f.visitjmp(b);
                @default => panic("invalid fold use type");
            };
            continue(true);
        };
        false
    };
    
    /* 2. trim dead code, replace constants */
    any_dead_blocks := false;
    pb := f.f.start&;
    seen   := init_bitset(f.f.ncon.zext());
    refcon :: fn(r) => @if(rtype(r) == .RCon) bsset(seen&, r.val());
    while => !pb[].is_null() { 
        continue :: local_return;
        b := pb[];
        if b.visit == 0 {
            any_dead_blocks = true;
            edgedel(b, b.s1&);
            edgedel(b, b.s2&);
            pb[] = b.link;
            continue();
        };
        pp := b.phi&;
        while => !pp[].is_null() { 
            p := pp[];
            if f.val[p.to.val()] != fold_status_runtime {
                // remove this phi node. 
                pp[] = p.link;
            } else {
                range(0, p.narg.zext()) { a | 
                    if !f.deadedge(p.blk[a].id, b.id) {
                        f.renumber(p.arg.index(a));
                        refcon(p.arg[a]);
                    }
                };
                pp = p.link&;
            };
        };
        
        for_insts_forward b { i |
            if(i.op() == .sel1, => f.fold_sel_part2(i));
            if f.renumber(i.to&) {
                i.set_nop();
            } else {
                range(0, 2) { n | 
                    f.renumber(i.arg&.index(n));
                };
                if is_store(i.op()) && i.arg&[0] == QbeUndef {
                    i.set_nop();
                };
            };
            for(i.arg&.items(), fn(r) => refcon(r));
        };
        f.renumber(b.jmp.arg&);
        
        @if(b.jmp.type == .jnz) if strong_const(f.f, b.jmp.arg) { c |
            cond_is_false := iscon(c, false, 0);  // :jnz_is_Kw
            if cond_is_false {
                edgedel(b, b.s1&);
                b.s1 = b.s2;
                b.s2 = Qbe.Blk.ptr_from_int(0);
            } else {
                edgedel(b, b.s2&);
            };
            b.jmp.type = .jmp;
            b.jmp.arg = QbeNull;
        };
        refcon(b.jmp.arg);
        pb = b.link&;
    };
    
    if f.f.globals.debug["F".char()] {
        dump_folding(f.f, f.val, f.f.globals.debug_out, f.folded_sel);
    };
    
    mark_seen(f.f, seen&, false);
}

fn mark_seen(f: *Qbe.Fn, seen: *Qbe.BSet, $force: bool) void = {
    range(fixed_const_count, f.ncon.zext()) { i |
        c := f.con[i]&;
        @if(c.type() == .CAddr)
        if force || bshas(seen, i) { 
            // record that we referenced the symbol so it will be emitted even if it was suspended for inlining. 
            use_symbol(f.globals, c.sym) { s | 
                mark_referenced(f.globals, c.sym, s);  // required!
            };
        } else {
            // prevent unreferenced constants showing up in .frc files (as `$foo is Invalid` spam). 
            // happens when all uses were (inlined) or (dead after folding). 
            c.sym = Qbe.no_symbol_S;  // optional
        }
    };
}

fn fold_sel_part2(f: *FoldCtx, isel1: *Qbe.Ins) void = {
    isel0 := isel1.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    // do this first so we notice if they folded to the same constant. 
    range(0, 2, fn(n) => f.renumber(isel1.arg&.index(n)));
    
    // If the args are the same, the condition doesn't matter so pretend it's constant.  
    cond := if(isel1.arg&[0] == isel1.arg&[1], => QbeConZero, => isel0.arg&[0]);
    if strong_const(f.f, cond) { c |
        cond_is_false := iscon(c, false, 0);  // :jnz_is_Kw
        a := isel1.arg&[int(cond_is_false)];
        isel1[] = make_ins(.copy, isel1.cls(), isel1.to, a, QbeNull);
        isel0.set_nop();
        f.folded_sel = true;  // for logging (dump_folding)
    };
}

fn strong_const(f: *Qbe.Fn, r: Qbe.Ref) ?*Qbe.Con #inline = {
    if(rtype(r) != .RCon, => return(.None));
    c := f.get_constant(r);
    if(c.type() == .CBits, => return(Some = c));
    // TODO: write a test that gets here
    strong := false;
    use_symbol(f.globals, c.sym) { s |
        strong = s.kind == .Local || s.pledge_local || s.strong;
    };
    @if(strong, (Some = c), .None)
}

fn iscon(c: *Qbe.Con, w: bool, k: u64) bool = {
    c.type() == .CBits 
    && (!w || (@as(u64) c.bits().bitcast()) == k) 
    && ( w || (@as(u32) c.bits().trunc())   == k.trunc())
}

fn latval(f: *FoldCtx, r: Qbe.Ref) i32 = @match(rtype(r)) {
    fn RTmp() => f.val[r.val()];
    fn RCon() => r.val().intcast();
    @default => @panic("latval of invalid %", r);
};

fn latmerge(v: i32, m: i32) i32 = {
    ::if(i32);
    if(m == fold_status_pending, => return(v));
    if(v == fold_status_pending || v == m, => m, => fold_status_runtime)
}

fn update(f: *FoldCtx, t: i32, m: i32) void = {
    t := t.zext();
    m := latmerge(f.val[t], m); 
    if(m == f.val[t], => return()); 
    // it changed; queue it to be reprocessed.
    uses := f.f.get_temporary(t).uses();
    f.use_work&.reserve(uses.len);
    each(uses, fn(u) => f.use_work&.push_assume_capacity(u));
    f.val[t] = m;
}

fn deadedge(f: *FoldCtx, s: i32, d: i32) bool = {
    e := f.edges.index(s.zext());
    if(e[0].dest == d && !e[0].dead, => return(false));
    if(e[1].dest == d && !e[1].dead, => return(false));
    true
}

fn visitphi(f: *FoldCtx, p: *Qbe.Phi, bid: i32) void = {
    v: i32 = fold_status_pending;
    // since a dead edge might just not have been visited yet, 
    // the latval after this loop might be wrong! but if it is, 
    // it will be fixed when the other edges are added to the work list. 
    range(0, p.narg.zext()) { a |
        if !f.deadedge(p.blk[a].id, bid) { 
            v = latmerge(v, f.latval(p.arg[a]));
        };
    };
    f.update(p.to.val().intcast(), v);
}

fn visitins(f: *FoldCtx, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    if(i.op() == .sel0, => f.fold_sel_part1(i));
    if(rtype(i.to) != .RTmp, => return());
    
    ::if(i32);
    v := if(!can_fold(i.op()), => fold_status_runtime) {
        l := f.latval(i.arg&[0]);
        r := if(i.arg&[1] != QbeNull, => f.latval(i.arg&[1]), => QbeConZero.val().intcast());
        @if_else {
            @if(l == fold_status_runtime || r == fold_status_runtime) => fold_status_runtime;
            @if(l == fold_status_pending || r == fold_status_pending) => fold_status_pending;
            @else => f.f.opfold(i.op(), i.cls(), l, r);
        }
    };
    
    f.update(i.to.val().intcast(), v);
}

fn visitjmp(f: *FoldCtx, b: *Qbe.Blk) void = {
    n := b.id;
    @match(b.jmp.type) {
        fn jnz() => {
            cond := f.latval(b.jmp.arg);
            @debug_assert_ne(cond, fold_status_pending, "jnz(pending)");
            @if_else {
                @if(cond == fold_status_runtime) => {
                    f.flow_work&.push(f.edge(n, 1));
                    f.flow_work&.push(f.edge(n, 0));
                };
                @if(iscon(f.f.con.index(cond.zext()), false, 0)) => {  // cond = false  // :jnz_is_Kw
                    @debug_assert(f.edge(n, 0)[].dead, "jnz(0) s1 should be dead");
                    f.flow_work&.push(f.edge(n, 1));
                };
                @else => {  // cond = true
                    @debug_assert(f.edge(n, 1)[].dead, "jnz(1) s2 should be dead");
                    f.flow_work&.push(f.edge(n, 0));
                };
            };
        }
        fn jmp() => f.flow_work&.push(f.edge(n, 0));
        fn hlt() => ();
        @default => @debug_assert(is_ret(b.jmp.type), "bad block terminator for fold");
    };
}

fn fold_sel_part1(f: *FoldCtx, isel0: *Qbe.Ins) void = {
    isel1 := isel0.offset(1);
    @debug_assert_eq(isel1.op(), .sel1);
    cond := f.latval(isel0.arg&[0]);
    if(cond <= 0, => return());
    want_right_value := iscon(f.f.con.index(cond.zext()), false, 0);  // :jnz_is_Kw
    a := isel1.arg&[int(want_right_value)];
    f.update(isel1.to.val().intcast(), f.latval(a));
}

fn initedge(e: *Edge, s: *Qbe.Blk) void = {
    e.dest = if(!s.is_null(), => s.id, => -1);
    e.dead = true;
}

fn renumber(f: *FoldCtx, r: *Qbe.Ref) bool = {
    if rtype(r[]) == .RTmp {
        v := f.val[r[].val()];
        if v != fold_status_runtime {
            r[] = CON(v.zext());
            return(true);
        }
    };
    false
}

fn edge(f: *FoldCtx, i: i32, j: i64) *Edge = f.edge(i.zext(), j);
fn edge(f: *FoldCtx, i: i64, j: i64) *Edge = 
    f.edges.index(i).index(j);

fn opfold(f: *Qbe.Fn, op: Qbe.O, cls: Qbe.Cls, l: i32, r: i32) i32 = {
    if op == .sel1 { 
        return(if(l == r, => l, => fold_status_runtime));
    };
    cl, cr := (f.con.index(l.zext()), f.con.index(r.zext()));
    c := try_fold(op, cls, cl, cr) || return(fold_status_runtime);
    r := f.newcon(c&);
    // TODO: there are tests that trigger this assertion -- Nov 12
    //@debug_assert(!(cls == .Ks || cls == .Kd) || c.flt != 0, "wrong fold type");
    r.val().intcast()
}

fn try_fold(op: Qbe.O, cls: Qbe.Cls, cl: *Qbe.Con, cr: *Qbe.Con) ?Qbe.Con = {
    c := Qbe.Con.zeroed();
    c.sym.id = Qbe.no_symbol;  // .CBits
    ::enum(Qbe.ConType); ::enum(Qbe.O);
    if cl.type() != .CBits || cr.type() != .CBits {
        if(!cls.is_int() || !(@is(op, .add, .sub)), => return(.None));
        // This is used for constant expression evaluation in import_c.
        // TODO: but sometimes you'd be better off doing this in isel when you could have the offsets as part of the memory access and create fewer relocations. 
        if cr.type() == .CAddr {
            if op == .add {
                t := cl; cl = cr; cr = t;
            };
            if(cr.type() == .CAddr, => return(.None));
        };
        c.sym = cl.sym;  // .CAddr
    };
    
    w := cls.is_wide();
    if cls.is_int() && (@is(op, .div, .rem, .udiv, .urem)) {
        if(iscon(cr, w, 0), => return(.None));
        if (@is(op, .div, .rem)) {
            x := if(w, => MIN_i64, => MIN_i32);
            if(iscon(cr, w, bitcast(-1)) && iscon(cl, w, x.bitcast()), => return(.None));
        };
    };
    
    r  := 0;
    do_fold(op, cls, i64.raw_from_ptr(r&), cl.bits(), cr.bits());
    if !is_wide(cls) {
        r = r.bit_and(0xffffffff);  // TODO: should be redundant?
    };
    c&.set_bits(r);
    (Some = c)
}

//
// For each valid combintion of op + output type, just generate the code to do the thing. 
// This is more magic but also less verbose and less error prone than just typing out a massive switch statement.
// This is more verbose than using `inline_for` and `#ir` on individual functions but it compiles much faster (saves ~40ms, Jun 2025). 
//
fn do_fold(rt_o: Qbe.O, rt_k: Qbe.Cls, out: rawptr, a0: i64, a1: i64) void = {
    impl :: gen_do_fold_impl();
    if rt_o.iscmp() {
        // cmp returning Kw vs Kl doesn't matter, it's always 0 or 1
        rt_k = .Kl;
    };
    rt_o := intcast @as(i32) rt_o;  // waste. should allow J.switch on Cls.Kw
    i := @match(rt_k) {
        fn Kw() => impl'Kw(rt_o, a0, a1).zext();
        fn Kl() => impl'Kl(rt_o, a0, a1);
        fn Ks() => impl'Ks(rt_o, a0, a1).bitcast().zext();
        fn Kd() => impl'Kd(rt_o, a0, a1).bitcast();
        @default => unreachable();
    };
    i64.ptr_from_raw(out)[] = i;
}

// TODO: make the frcmodule api less painful for small useages like this
fn gen_do_fold_impl() ScopeId = {
    a := temp();
    fr := current_compiler_context();
    m := a.box(QbeModule);
    // TODO: "need to be consistant about how to handle modules like this that don't actually compile anything"
    init_default_module_dyn(m, fr.vtable, (arch = query_current_arch(), os = query_current_os(), type = .CachedEarly));
    
    argcls := import("@/backend/meta/ops.fr").tables;
    
    decls := Incremental'Fld.list(temp());
    writer := m.save.unwrap();
    // TODO: making everyone remember to do this is kinda lame
    range(0, Incremental.FTy.COUNT) { _ |
        writer.fty&.push(zeroed Incremental.FTy);   // skip builtin types so offsets work out
    };
    
    cls :: @const_slice(Qbe.Cls.Kw, .Kl, .Ks, .Kd);
    for cls { k |
        f := a.box(Qbe.Fn);
        default_init(f, m);
        ::enum(Qbe.Cls);
        f.lnk = (id = m.intern(@tfmt("%", k)), export = true);
        
        next := f.start&;
        b := a.box_zeroed(Qbe.Blk); next[] = b; next = b.link&; f.nblk += 1;
        f.start = b;
       
        r_o, r_a0, r_a1 := (f.newtmp("", .Kl), f.newtmp("", .Kl), f.newtmp("", .Kl));
        push(b, make_ins(.par, .Kl, r_o, QbeNull, QbeNull));
        push(b, make_ins(.par, .Kl, r_a0, QbeNull, QbeNull));
        push(b, make_ins(.par, .Kl, r_a1, QbeNull, QbeNull));
        
        fail := a.box_zeroed(Qbe.Blk); next[] = fail; next = fail.link&; f.nblk += 1;
        fail.jmp.type = .hlt;
        end := a.box_zeroed(Qbe.Blk); next[] = end; next = end.link&; f.nblk += 1;
        
        s: Qbe.SwitchPayload = (
            cases = new(20),
            case_count = 0,
            default = fail,
            inspect = r_o,
            src = b,
        );
        b.jmp = (type = .switch, arg = INT(0));
        phi := f.new_phi(end, k, 20);
        ::enum(Qbe.J);
        end.jmp = (type = k.retk(), arg = phi.to);
        
        r_a0s, r_a0d, r_a1s, r_a1d := (f.newtmp("", .Ks), f.newtmp("", .Kd), f.newtmp("", .Ks), f.newtmp("", .Kd));
        push(b, make_ins(.cast, .Ks, r_a0s, r_a0, QbeNull));
        push(b, make_ins(.cast, .Kd, r_a0d, r_a0, QbeNull));
        push(b, make_ins(.cast, .Ks, r_a1s, r_a1, QbeNull));
        push(b, make_ins(.cast, .Kd, r_a1d, r_a1, QbeNull));
        a0 := @slice(r_a0, r_a0, r_a0s, r_a0d, QbeNull, QbeNull);
        a1 := @slice(r_a1, r_a1, r_a1s, r_a1d, QbeNull, QbeNull);

        range(0, Qbe.O.enum_count()) { i |
            continue :: local_return;
            o     := @as(Qbe.O) @as(i32) intcast i;
            flags := argcls[i];
            k0    := ops_table_argscls(flags, k, 0);
            k1    := ops_table_argscls(flags, k, 1);
            allow := ops_table_canfold(flags) && k0 != .Ke && o != .sel1;
            if(!allow, => continue());
            // cmp returning Kw vs Kl doesn't matter, it's always 0 or 1
            if(o.iscmp() && k == .Kw, => continue());
            
            // cast pars to the right types for this instruction (compiles to int vs float registers)
            a0 := a0[intcast @as(i32) k0];
            a1 := a1[intcast @as(i32) k1];
            
            b  := a.box_zeroed(Qbe.Blk); next[] = b; next = b.link&; f.nblk += 1;
            r  := f.newtmp("", k);
            push(b, make_ins(o, k, r, a0, a1));
            push(s.cases&, s.case_count&, (b, i));
            push(phi, b, r);
            
            b.jmp.type = .jmp;
            b.s1 = end;
        };
        
        f.switches = new(1);
        f.switch_count = 1;
        f.switches[0] = s;
        
        // making people remember to do this is also a bit annoying
        i := 0;
        for_blocks f { b |
            b.id = intcast i; i += 1;
        };
        
        // and this is kinda lame, the frontend could just do this itself if you don't provide a root_scope
        // (it's just annoying because i don't want to mutate the module bytes so can't just abi_function_type in the compiler)
        decls&.push(
            name = writer.push(m.str(f.lnk.id)),
            type = abi_function_type(writer, f),
            payload = (offset = trunc writer.map_sym(m, f.lnk.id))
        );
        
        fr'vtable'run_qbe_passes(Qbe.Fn.raw_from_ptr(f));
    };
    
    meta: Incremental'Meta = (
        arch_os = 0x0F0F, 
        root_scope = save_fields(writer, .Scope, decls.items()),
        debug_name = writer.push("backend_do_fold"),
    );
    b := writer.to_bytes(meta, m);
    bytes := concat(b, ast_alloc());
    fr'vtable'drop_qbe_module(QbeModule.raw_from_ptr(m));
    result := fr'vtable'import_frc(fr.data, bytes);
    result.or(fn(err) => fr.report_error(err))
}

#use("@/backend/lib.fr");
