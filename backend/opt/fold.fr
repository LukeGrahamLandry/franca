// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! "Sparse Conditional Constant Propagation"
//! Which is a fancy way of saying `while changed: changed = any(try_fold(i) for i in instructions)`

//!
//! This is kinda tedious and doesn't help me much because my frontend has to do aggressive constant evaluation anyway. 
//!     TODO: I think it would be much faster if the frontend just tracked what 
//!           should be constant, and only materialized it upon seeing a 
//!           non-arithmetic #fold, and otherwise just defer until the backend.  
//! But it still catches a few stragglers that became constant after slot promotion, inlining, etc.
//!
// TODO: some are sketchy because old backends only fully implment i64/f64 while f32/i32/u32/u64 are just kinda hacked in with casts. 
// TODO: fuse sequential add/sub with one const arg. 

fold_status_runtime :i32: -1;
fold_status_dead :i32:  0;

FoldCtx :: @struct(
    f: *Qbe.Fn,
    val: []i32,
    flowrk: List(i32), // bid
    nuse := 0,
    usewrk: QList(*Qbe.Use),
);

// not memory and not special inserted later by abi/isel :OrderMatters
fn can_fold(o: Qbe.O) bool = {
    if(o == .extsb || o == .extsh, => return(false)); // TODO
    o.between(.add, .cuod) || o.between(.extsb, .cast)
}

/* require rpo, use, pred */
fn fold_constants(f: *Qbe.Fn) void = {
    // never gonna happen but if there's no constants at all, there's nothing to fold. 
    if(f.ncon == @as(i32) FIXED_CONST_COUNT, => return());
    
    f: FoldCtx = (
        f      = f,
        val    = temp().alloc_zeroed(i32, f.ntmp.zext()),
        usewrk = new(0),
        flowrk = list(5, temp()),
    ); f := f&;
    
    // val = alloc_zeroed so don't have to `range(f.f.ntmp.zext()) f.val[t] = fold_status_dead;`
    ::assert_eq(fold_status_dead, 0); 
    
    // :ExprLevelAsm
    ::assert_eq(fold_status_runtime.intcast(), -1);
    memset(i32.raw_from_ptr(f.val.ptr), 255, size_of(@type f.val[0]) * Qbe.Tmp0);
    
    range(0, f.f.nblk.zext()) { n | 
        b := f.f.rpo[n];
        b.visit = 0;
    };
    f.flowrk&.push(f.f.start.id);
    
    /* 1. find out constants and dead cfg edges */
    dowhile {
        continue :: local_return;
        if f.flowrk.len > 0 {
            bid := f.flowrk&.pop().unwrap();
            b := f.f.rpo[bid.zext()];
            if b.visit != 0 {
                continue(true);
            };
            
            n := b.id;
            for_phi b { p | 
                f.visitphi(p, n);
            };
            if b.visit == 0 {
                for_insts_forward b { i |
                    f.visitins(i);
                };
                f.visitjmp(b, n);
            };
            b.visit += 1;
            continue(true);
        };
        if f.nuse != 0 {
            f.nuse -= 1;
            u := f.usewrk[f.nuse];
            n := u.bid;
            b := f.f.rpo[n.zext()];
            if(b.visit == 0, => continue(true));
            @match(u.type) {
                fn UPhi() => f.visitphi(u.u.phi, u.bid);
                fn UIns() => f.visitins(u.u.ins);
                fn UJmp() => f.visitjmp(b, n);
                @default => unreachable();
            };
            continue(true);
        };
        false
    };

    /* 2. trim dead code, replace constants */
    any_dead_blocks := false;
    pb := f.f.start&;
    while => !pb[].is_null() { 
        continue :: local_return;
        b := pb[];
        if b.visit == 0 {
            any_dead_blocks = true;
            for_jump_targets_mut b { s |
                edgedel(b, s);
            };
            pb[] = b.link;
            continue();
        };
        pp := b.phi&;
        while => !pp[].is_null() { 
            p := pp[];
            if f.val[p.to.val()] != fold_status_runtime {
                pp[] = p.link;
            } else {
                range(0, p.narg.zext()) { a | 
                    //if !f.deadedge(p.blk[a].id, b.id) {
                        f.renumber(p.arg.index(a));
                    //}
                };
                pp = p.link&;
            };
        };
        for_insts_forward b { i |
            if(f.renumber(i.to&), => i.set_nop()) {
                range(0, 2) { n | 
                    f.renumber(i.arg&.index(n))
                };
                if is_store(i.op()) && i.arg&[0] == QbeUndef {
                    i.set_nop();
                }
            }
        };
        f.renumber(b.jmp.arg&);
        
        if constant_conditional(f, b) { dest |
            convert_to_jmp(b, dest);
        };
        pb = b.link&;
    };
    
    if f.f.globals.debug["F".char()] {
        dump_folding(f.f, f.val, f.f.globals.debug_out);
    };
}

fn constant_conditional(f: *FoldCtx, b: *Qbe.Blk) ?*Qbe.Blk = {
    if(b.is_null() || !@is(b.jmp.type, .jnz, .switch), => return(.None));
    c := f.latval(b.jmp.arg);
    if(c == fold_status_runtime, => return(.None));
    c := f.f.con[c.zext()]&;
    // TODO: should we assume cmp to a symbol is nonnull? or are you allowed to do weak dynamic linking or something? 
    if(c.type != .CBits, => return(.None));
    
    dest := @match(b.jmp.type) {
        fn jnz() => @if(c.bits.i == 0, b.s2, b.s1);
        fn switch() => {
            targets := get_jump_targets(b);
            table_index := if(c.bits.i < 0 || c.bits.i >= targets.len - 1, => 0, => c.bits.i + 1);
            targets[table_index]
        }
        @default => unreachable();
    };
    (Some = dest)
}

fn convert_to_jmp(b: *Qbe.Blk, dest: *Qbe.Blk) void = {
    for_jump_targets_mut b { s |
        if !identical(s[], dest) {
            edgedel(b, s);
        };
    };
    b.s1 = dest;
    b.s2 = Qbe.Blk.ptr_from_int(0);
    b.jmp = (type = .jmp, arg = QbeNull);
}

fn iscon(c: *Qbe.Con, w: bool, k: u64) bool = {
    c.type == .CBits 
    && (!w || (@as(u64) c.bits.i.bitcast()) == k) 
    && ( w || (@as(u32) c.bits.i.trunc())  == k.trunc())
}

fn latval(f: *FoldCtx, r: Qbe.Ref) i32 = {
    @match(rtype(r)) {
        fn RTmp() => f.val[r.val()];
        fn RCon() => r.val().intcast();
        @default => unreachable();
    }
}
::if(i32);
fn latmerge(v: i32, m: i32) i32 = {
    if(m == fold_status_dead, => return(v));
    if(v == fold_status_dead || v == m, => m, => fold_status_runtime)
}

fn update(f: *FoldCtx, t: i32, m: i32) void = {
    t := t.zext();
    m := latmerge(f.val[t], m);
    // if it changed, queue it to be reprocessed. 
    if m != f.val[t] {
        tmp := f.f.get_temporary(t);
        each tmp.uses() { u |
            f.nuse += 1;
            grow(f.usewrk&, f.nuse);
            f.usewrk[f.nuse - 1] = u;
        };
        f.val[t] = m;
    }
}

fn deadedge(f: *FoldCtx, s: i32, d: i32) bool = {
    e := f.edge.index(s.zext());
    !(e[0].dest == d && !e[0].dead) && 
    !(e[1].dest == d && !e[1].dead)
}

fn visitphi(f: *FoldCtx, p: *Qbe.Phi, bid: i32) void = {
    v: i32 = fold_status_dead;
    range(0, p.narg.zext()) { a |
        // TODO: this feels important. there's a test in fold2.ssa that Qbe can do but i can't. bring this back!
        //if !f.deadedge(p.blk[a].id, bid) { 
            v = latmerge(v, f.latval(p.arg[a]));
        //};
    };
    f.update(p.to.val().intcast(), v);
}

fn visitins(f: *FoldCtx, i: *Qbe.Ins) void = {
    if(rtype(i.to) != .RTmp, => return());
    v := if(!can_fold(i.op()), => fold_status_runtime) {
        l := f.latval(i.arg&[0]);
        r := if(i.arg&[1] != QbeNull, => f.latval(i.arg&[1]), => QbeConZero.val().intcast());
        @if_else {
            @if(l == fold_status_runtime || r == fold_status_runtime) => fold_status_runtime;
            @if(l == fold_status_dead || r == fold_status_dead) => fold_status_dead;
            @else => f.f.opfold(i.op(), i.cls(), f.f.con.index(l.zext()), f.f.con.index(r.zext()));
        }
    };
    
    /* fprintf(stderr, "\nvisiting %s (%p)", optab[i.op].name, (void *)i); */
    f.update(i.to.val().intcast(), v);
}

fn visitjmp(f: *FoldCtx, b: *Qbe.Blk, n: i32) void = {
    @debug_assert_eq(n, b.id);  // TODO: don't pass two arguments then? 
    
    if f.constant_conditional(b) { dest |
        f.flowrk&.push(dest.id);
    } else {
        for_jump_targets b { s |
            f.flowrk&.push(s.id);
        };
    };
}

fn renumber(f: *FoldCtx, r: *Qbe.Ref) bool = {
    if rtype(r[]) == .RTmp {
        l := f.val[r[].val()];
        if l != fold_status_runtime {
            r[] = CON(l.zext());
            return(true);
        }
    };
    false
}

fn opfold(f: *Qbe.Fn, op: Qbe.O, cls: Qbe.Cls, cl: *Qbe.Con, cr: *Qbe.Con) i32 = {
    c := Qbe.Con.zeroed();
    if cls == .Kw || cls == .Kl {
        if foldint(c&, op, cls == .Kl, cl, cr) {
            return(fold_status_runtime);
        };
    } else {
        foldflt(c&, op, cls == .Kd, cl, cr);
    };
    if !is_wide(cls) {
        c.bits.i = c.bits.i.bit_and(0xffffffff);
    };
    r := f.newcon(c&);
    // TODO: there are tests that trigger this assertion -- Nov 12
    //@debug_assert(!(cls == .Ks || cls == .Kd) || c.flt != 0, "wrong fold type");
    r.val().intcast()
}

/* boring folding code */
// 
// I could do a fancy meta-programming thing where i generate these match statements because 
// i have to have the mapping of op to intrinsic anyway, but also that would just 
// end up more complicated than just pasting this here and never touching it again, 
// decisions, decisions!       -- Nov 7
//

BitcastPrim :: @union(s: i64, u: u64, fs: f32, fd: f64, sw: i32, uw: u32);

// return true on fail
fn foldint(res: *Qbe.Con, op: Qbe.O, w: bool, cl: *Qbe.Con, cr: *Qbe.Con) bool = {
    l: BitcastPrim = (s = cl.bits.i);
    r: BitcastPrim = (s = cr.bits.i);
    res.type = .CBits;
    @if_else {
        @if(op == .add) => {
            if cl.type == .CAddr {
                if(cr.type == .CAddr, => return(true));
                res.type = .CAddr;
                res.sym = cl.sym;
            } else {
                if cr.type == .CAddr {
                    res.type = .CAddr;
                    res.sym = cr.sym;
                }
            }
        };
        @if(op == .sub) => {
            if cl.type == .CAddr {
                if cr.type != .CAddr {
                    res.type = .CAddr;
                    res.sym = cl.sym;
                } else {
                    if(!(cl.sym == cr.sym), => return(true));
                }
            } else {
                if(cr.type == .CAddr, => return(true));
            };
        };
        @if(cl.type == .CAddr || cr.type == .CAddr) => return(true);
        @else => ();
    };
    if (@is(op, .div, .rem, .udiv, .urem)) {
        return(true); // :TodoAmdFoldOps `TODO: set slot`
        if(iscon(cr, w, 0), => return(true));
        if (@is(op, .div, .rem)) {
            x := if(w, => MIN_i64, => MIN_i32);
            if(iscon(cr, w, bitcast(-1)) && iscon(cl, w, x.bitcast()), => return(true));
        };
    };
    
    shift_mask := 31.bit_or(w.int().shift_left(5));
    x := @match(op) {
        // signed overflow is not undefined for me (i hope)
        fn add()  => l.s + r.s;
        fn sub()  => l.s - r.s;
        fn neg()  => -l.s;
        // for 32 bit this relies on union putting all fields starting at the beginning and numbers being little endian. 
        // TODO: some of these must be wrong becuase the old backends don't do real 32 bit or unsigned operations. :FUCKED
        fn div()  => if(w, => l.s / r.s, => zext(l.sw / r.sw));
        fn rem()  => if(w, => l.s.mod(r.s), => l.sw.mod(r.sw).zext());
        fn udiv() => if(w, => bitcast(l.u / r.u), => zext(l.uw / r.uw));
        fn urem() => if(w, => l.u.mod(r.u).bitcast(), => l.uw.mod(r.uw).zext());
        fn mul()  => l.s * r.s;
        fn and()  => l.s.bit_and(r.s);
        fn or()   => l.s.bit_or(r.s);
        fn xor()  => l.s.bit_xor(r.s);
        fn sar() => if(w, => l.s, => sign_extend_low32(l.s)).shift_right_arithmetic(r.s.bit_and(shift_mask));
        fn shr() => if(w, => l.s, => bit_and(l.s, 1.shift_left(32) - 1)).shift_right_logical(r.s.bit_and(shift_mask));
        fn shl() => l.s.shift_left(r.s.bit_and(shift_mask));
        //fn Oextsb() => (int8_t)l.u;   // TODO
        fn extub() => l.s.bit_and(1.shift_left(8) - 1);
        //fn Oextsh() => (int16_t)l.u; // TODO 
        fn extuh() => l.s.bit_and(1.shift_left(16) - 1);
        fn extsw() => sign_extend_low32(l.s); 
        fn extuw() => l.s.bit_and(1.shift_left(32) - 1);
        fn stosi() => if(w, => l.fs.cast().int(), => l.fs.cast().int().sign_extend_low32());  // TODO: probably wrong rounding to just upcast. 
        fn stoui() => if(w, => l.fs.cast().int(), => l.fs.cast().int().sign_extend_low32());  // ^+ TODO: probably unsigned has to do something different to fit bigger number?
        //fn Ostoui() => w ? (uint64_t)cl.bits.s : (uint32_t)cl.bits.s;
        fn dtosi() => l.fd.int();
        fn dtoui() => l.fd.int(); // TODO: probably wide unsigned has to do something different to fit bigger number?
        //fn Odtoui() => w ? (uint64_t)cl.bits.d : (uint32_t)cl.bits.d;
        fn cast() => {
            if cl.type == .CAddr {
                res.type = .CAddr;
                res.sym = cl.sym;
            };
            l.s    
        }
        @default => fold_cmp(op, l&, r&).int();
    };
    res.bits = (i = x);
    return(false)
}

Ocmpw :: Qbe.O.ceqw;
Ocmpl :: Qbe.O.ceql;

// TODO: can't pass @union by value
fn fold_cmp(op: Qbe.O, l: *BitcastPrim, r: *BitcastPrim) bool = {
    if op.between(Ocmpw, .cultw)  {
        l.s = sign_extend_low32(l.s);
        r.s = sign_extend_low32(r.s);
        op = rebase(op, Ocmpl, Ocmpw);
    };
    @match(op) {
        // i64
        fn culel() => l.u <= r.u; 
        fn cultl() => l.u < r.u;
        fn cslel() => l.s <= r.s; 
        fn csltl() => l.s < r.s;
        fn csgtl() => l.s > r.s;
        fn csgel() => l.s >= r.s;
        fn cugtl() => l.u > r.u;
        fn cugel() => l.u >= r.u;
        fn ceql()  => l.u == r.u;
        fn cnel()  => l.u != r.u;
        
        // f32
        fn cles() => l.fs <= r.fs;
        fn clts() => l.fs < r.fs; 
        fn cgts() => l.fs > r.fs; 
        fn cges() => l.fs >= r.fs;
        fn cnes() => l.fs != r.fs;
        fn ceqs() => l.fs == r.fs;
        fn cos() => l.fs < r.fs || l.fs >= r.fs;
        fn cuos() => !(l.fs < r.fs || l.fs >= r.fs);
        
        // f64
        fn cled() => l.fd <= r.fd;
        fn cltd() => l.fd < r.fd; 
        fn cgtd() => l.fd > r.fd; 
        fn cged() => l.fd >= r.fd;
        fn cned() => l.fd != r.fd;
        fn ceqd() => l.fd == r.fd;
        fn cod() => l.fd < r.fd || l.fd >= r.fd;
        fn cuod() => !(l.fd < r.fd || l.fd >= r.fd);
        @default => @panic("expected fold_cmp");
    }
}

fn foldflt(res: *Qbe.Con, op: Qbe.O, w: bool, cl: *Qbe.Con, cr: *Qbe.Con) void = {
    @assert(cl.type == .CBits && cr.type == .CBits, "invalid address operand for %", op.get_name());
    
    // TODO: this is wrong! I can't do real f32 ops on old backends.
    //       so instead i promote them to f64 and then cast the result back. 
    //       which is very similar but i assume not bit-for-bit the same. 
    
    ld := @if(w, cl.bits.d, cl.bits.s.s.cast());
    rd := @if(w, cr.bits.d, cl.bits.s.s.cast());
    
    x := @match(op) {
        fn add() => ld + rd;
        fn sub() => ld - rd;
        fn neg() => -ld;
        fn div() => ld / rd;
        fn mul() => ld * rd;
        // for the casts, we want to read as integer not float!
        fn swtof() => cl.bits.i.sign_extend_low32().float();
        fn uwtof() => cl.bits.i.bit_and(1.shift_left(32) - 1).float();
        fn sltof() => cl.bits.i.float();
        fn ultof() => {
            @assert(cl.bits.i >= 0, "TODO: implement ultof for numbers larger than MAX_i64");
            cl.bits.i.float()
        }
        fn exts() => cl.bits.s.s.cast(); // w
        fn truncd() => cl.bits.d; // !w
        fn cast() => {
            res[] = cl[]; // don't touch the bits at all
            return()
        };
        @default => @panic("expected fold_float %", op.get_name());
    };
    res.type = .CBits;
    if w {
        res.bits.d = x;
    } else {
        res.bits.s.s = x.cast();
    };
    res.flt = @if(w, 2, 1);
}
