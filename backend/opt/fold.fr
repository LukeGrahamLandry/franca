// Adapted from Qbe. MIT License. © 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

//! This is the SCC algorithm described in 
//! Mark N. Wegman and F. Kenneth Zadeck. 1991. Constant propagation with conditional branches. 
//! ACM Transactions on Programming Languages and Systems, 13, 2 (1991), 181–210. https://doi.org/10.1145/103135.103136

// this process is split two phases: collecting constants and making changes. 
// first we go through all the blocks and call visit* and keep adding things to the work lists. 
// once the work lists are empty, we know all the constants, then we move to the next phase. 
// in the second phase we iterate all the live blocks once and replaces uses of any newly constant refs. 
// the first phase (visit* functions) are not allowed to make changes to the instructions! 
// since we start by assuming edges are dead, don't always visit the predecessors of a block before the block itself, 
// the first time a phi node is processed it might incorrectly treat some of its sources as dead and thus itself as a constant. 
// this will trigger a whole cascade of incorrect constant folding, but since no changes are being made yet, this is fine, 
// we're just putting junk in the val list. but when we eventually visit the predecesors of that phi, 
// we notice the mistake, and readd everything that depended on it to the work list and do it all again.  
// this seems more confusing than necessary, surely the point of calling it a lattice is that you only move down? 
// TODO: also surely all that extra work makes it slower. can we not? 
// -- Jan 23, 2025

// TODO: the places where i iscon($$, 0) assumes symbols are nonzero. 
//       this will need to change if i allow weak dynamic linking where a symbol might actually be null at runtime. 

fold_status_runtime :i32: -1;
fold_status_pending :i32:  0;

Edge :: @rec @struct(dest: i32, dead: bool);

FoldCtx :: @struct(
    f: *Qbe.Fn,
    val: []i32,
    edges: []Array(Edge, 2),
    flow_work: List(*Edge),
    use_work: List(*Qbe.Use),
    folded_sel := false,
);

// not memory and not special inserted later by abi/isel :OrderMatters
fn can_fold(o: Qbe.O) bool = {
    o.between(.add, .cuod) || o.between(.extsb, .cast) || o == .sel1 || o.between(.byteswap, .max)
}

/* require rpo, use, pred */
fn fold_constants(f: *Qbe.Fn) void = {
    start := Edge.zeroed();
    f: FoldCtx = (
        f      = f,
        val    = temp().alloc_zeroed(i32, f.ntmp.zext()),
        edges  = temp().alloc_zeroed(Array(Edge, 2), f.nblk.zext()), 
        use_work = list(0, temp()),
        flow_work = list(5, temp()),
    ); f := f&;
    f.flow_work&.push(start&);
    
    // val = alloc_zeroed so don't have to `range(f.f.ntmp.zext()) f.val[t] = fold_status_pending;`
    ::assert_eq(fold_status_pending, 0); 
    
    // :ExprLevelAsm
    ::assert_eq(fold_status_runtime.intcast(), -1);
    f.val.slice(0, Qbe.Tmp0).interpret_as_bytes().set_bytes(255);
    
    range(0, f.f.nblk.zext()) { n | 
        b := f.f.rpo[n];
        b.visit = 0;
        initedge(f.edge(n, 0), b.s1);
        initedge(f.edge(n, 1), b.s2);
        @debug_assert_eq(b.id.intcast(), n);
    };
    initedge(start&, f.f.start);
    
    /* 1. find out constants and dead cfg edges */
    dowhile {
        continue :: local_return;
        if f.flow_work&.pop() { e |
            if e.dest == -1 || !e.dead {
                continue(true);
            };
            e.dead = false;
            n := e.dest;
            b := f.f.rpo[n.zext()];
            for_phi b { p | 
                f.visitphi(p, n);
            };
            if b.visit == 0 {
                for_insts_forward b { i |
                    f.visitins(b, i);
                };
                f.visitjmp(b);
            };
            @debug_assert(
                b.jmp.type != .jmp
                || !f.edge(n, 0)[].dead
                || identical(f.flow_work[f.flow_work.len - 1], f.edge(n, 0)
            ), "live direct jump should be queued");
            b.visit += 1;
            continue(true);
        };
        if f.use_work&.pop() { u |
            b := f.f.rpo[u.bid.zext()];
            if(b.visit == 0, => continue(true));
            @match(u.type) {
                fn UPhi() => f.visitphi(u.u.phi, u.bid);
                fn UIns() => f.visitins(b, u.u.ins);
                fn UJmp() => f.visitjmp(b);
                @default => panic("invalid fold use type");
            };
            continue(true);
        };
        false
    };
    
    /* 2. trim dead code, replace constants */
    any_dead_blocks := false;
    pb := f.f.start&;
    seen   := init_bitset(f.f.ncon.zext());
    refcon :: fn(r) => @if(rtype(r) == .RCon) bsset(seen&, r.val());
    while => !pb[].is_null() { 
        continue :: local_return;
        b := pb[];
        if b.visit == 0 {
            any_dead_blocks = true;
            edgedel(b, b.s1&);
            edgedel(b, b.s2&);
            pb[] = b.link;
            continue();
        };
        pp := b.phi&;
        while => !pp[].is_null() { 
            p := pp[];
            if f.val[p.to.val()] != fold_status_runtime {
                // remove this phi node. 
                pp[] = p.link;
            } else {
                range(0, p.narg.zext()) { a | 
                    if !f.deadedge(p.blk[a].id, b.id) {
                        f.renumber(p.arg.index(a));
                        refcon(p.arg[a]);
                    }
                };
                pp = p.link&;
            };
        };
        
        for_insts_forward b { i |
            if(i.op() == .sel1, => f.fold_sel_part2(i));
            if f.renumber(i.to&) {
                i.set_nop();
            } else {
                range(0, 2) { n | 
                    f.renumber(i.arg&.index(n));
                };
                if is_store(i.op()) && i.arg&[0] == QbeUndef {
                    i.set_nop();
                };
            };
            for(i.arg&.items(), fn(r) => refcon(r));
        };
        f.renumber(b.jmp.arg&);
        
        if b.jmp.type == .jnz && rtype(b.jmp.arg) == .RCon {
            cond_is_false := iscon(f.f.get_constant(b.jmp.arg), false, 0);  // :jnz_is_Kw
            if cond_is_false {
                edgedel(b, b.s1&);
                b.s1 = b.s2;
                b.s2 = Qbe.Blk.ptr_from_int(0);
            } else {
                edgedel(b, b.s2&);
            };
            b.jmp.type = .jmp;
            b.jmp.arg = QbeNull;
        };
        refcon(b.jmp.arg);
        pb = b.link&;
    };
    
    if f.f.globals.debug["F".char()] {
        dump_folding(f.f, f.val, f.f.globals.debug_out, f.folded_sel);
    };
    
    f := f.f;
    range(fixed_const_count, f.ncon.zext()) { i |
        c := f.con[i]&;
        @if(c.type() == .CAddr)
        if bshas(seen&, i) { 
            // record that we referenced the symbol so it will be emitted even if it was suspended for inlining. 
            use_symbol(f.globals, c.sym) { s | 
                mark_referenced(f.globals, c.sym, s);  // required!
            };
        } else {
            // prevent unreferenced constants showing up in .frc files (as `$foo is Invalid` spam). 
            // happens when all uses were (inlined) or (dead after folding). 
            c.sym = Qbe.no_symbol_S;  // optional
        }
    };
}

fn fold_sel_part2(f: *FoldCtx, isel1: *Qbe.Ins) void = {
    isel0 := isel1.offset(-1);
    @debug_assert_eq(isel0.op(), .sel0);
    
    // do this first so we notice if they folded to the same constant. 
    range(0, 2, fn(n) => f.renumber(isel1.arg&.index(n)));
    
    // If the args are the same, the condition doesn't matter so pretend it's constant.  
    cond := if(isel1.arg&[0] == isel1.arg&[1], => QbeConZero, => isel0.arg&[0]);
    if rtype(cond) == .RCon {
        cond_is_false := iscon(f.f.get_constant(cond), false, 0);  // :jnz_is_Kw
        a := isel1.arg&[int(cond_is_false)];
        isel1[] = make_ins(.copy, isel1.cls(), isel1.to, a, QbeNull);
        isel0.set_nop();
        f.folded_sel = true;  // for logging (dump_folding)
    };
}

fn iscon(c: *Qbe.Con, w: bool, k: u64) bool = {
    c.type() == .CBits 
    && (!w || (@as(u64) c.bits().bitcast()) == k) 
    && ( w || (@as(u32) c.bits().trunc())   == k.trunc())
}

fn latval(f: *FoldCtx, r: Qbe.Ref) i32 = @match(rtype(r)) {
    fn RTmp() => f.val[r.val()];
    fn RCon() => r.val().intcast();
    @default => @panic("latval of invalid %", r);
};

fn latmerge(v: i32, m: i32) i32 = {
    ::if(i32);
    if(m == fold_status_pending, => return(v));
    if(v == fold_status_pending || v == m, => m, => fold_status_runtime)
}

fn update(f: *FoldCtx, t: i32, m: i32) void = {
    t := t.zext();
    m := latmerge(f.val[t], m); 
    if(m == f.val[t], => return()); 
    // it changed; queue it to be reprocessed.
    uses := f.f.get_temporary(t).uses();
    f.use_work&.reserve(uses.len);
    each(uses, fn(u) => f.use_work&.push_assume_capacity(u));
    f.val[t] = m;
}

fn deadedge(f: *FoldCtx, s: i32, d: i32) bool = {
    e := f.edges.index(s.zext());
    if(e[0].dest == d && !e[0].dead, => return(false));
    if(e[1].dest == d && !e[1].dead, => return(false));
    true
}

fn visitphi(f: *FoldCtx, p: *Qbe.Phi, bid: i32) void = {
    v: i32 = fold_status_pending;
    // since a dead edge might just not have been visited yet, 
    // the latval after this loop might be wrong! but if it is, 
    // it will be fixed when the other edges are added to the work list. 
    range(0, p.narg.zext()) { a |
        if !f.deadedge(p.blk[a].id, bid) { 
            v = latmerge(v, f.latval(p.arg[a]));
        };
    };
    f.update(p.to.val().intcast(), v);
}

fn visitins(f: *FoldCtx, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    if(i.op() == .sel0, => f.fold_sel_part1(i));
    if(rtype(i.to) != .RTmp, => return());
    
    v := if(!can_fold(i.op()), => fold_status_runtime) {
        l := f.latval(i.arg&[0]);
        r := if(i.arg&[1] != QbeNull, => f.latval(i.arg&[1]), => QbeConZero.val().intcast());
        @if_else {
            @if(l == fold_status_runtime || r == fold_status_runtime) => fold_status_runtime;
            @if(l == fold_status_pending || r == fold_status_pending) => fold_status_pending;
            @else => f.f.opfold(i.op(), i.cls(), l, r);
        }
    };
    
    f.update(i.to.val().intcast(), v);
}

fn visitjmp(f: *FoldCtx, b: *Qbe.Blk) void = {
    n := b.id;
    @match(b.jmp.type) {
        fn jnz() => {
            cond := f.latval(b.jmp.arg);
            @debug_assert_ne(cond, fold_status_pending, "jnz(pending)");
            @if_else {
                @if(cond == fold_status_runtime) => {
                    f.flow_work&.push(f.edge(n, 1));
                    f.flow_work&.push(f.edge(n, 0));
                };
                @if(iscon(f.f.con.index(cond.zext()), false, 0)) => {  // cond = false  // :jnz_is_Kw
                    @debug_assert(f.edge(n, 0)[].dead, "jnz(0) s1 should be dead");
                    f.flow_work&.push(f.edge(n, 1));
                };
                @else => {  // cond = true
                    @debug_assert(f.edge(n, 1)[].dead, "jnz(1) s2 should be dead");
                    f.flow_work&.push(f.edge(n, 0));
                };
            };
        }
        fn jmp() => f.flow_work&.push(f.edge(n, 0));
        fn hlt() => ();
        @default => @assert(is_ret(b.jmp.type), "bad block terminator for fold");
    };
}

fn fold_sel_part1(f: *FoldCtx, isel0: *Qbe.Ins) void = {
    isel1 := isel0.offset(1);
    @debug_assert_eq(isel1.op(), .sel1);
    cond := f.latval(isel0.arg&[0]);
    if(cond <= 0, => return());
    want_right_value := iscon(f.f.con.index(cond.zext()), false, 0);  // :jnz_is_Kw
    a := isel1.arg&[int(want_right_value)];
    f.update(isel1.to.val().intcast(), f.latval(a));
}

fn initedge(e: *Edge, s: *Qbe.Blk) void = {
    e.dest = if(!s.is_null(), => s.id, => -1);
    e.dead = true;
}

fn renumber(f: *FoldCtx, r: *Qbe.Ref) bool = {
    if rtype(r[]) == .RTmp {
        v := f.val[r[].val()];
        if v != fold_status_runtime {
            r[] = CON(v.zext());
            return(true);
        }
    };
    false
}

fn edge(f: *FoldCtx, i: i32, j: i64) *Edge = f.edge(i.zext(), j);
fn edge(f: *FoldCtx, i: i64, j: i64) *Edge = 
    f.edges.index(i).index(j);

fn opfold(f: *Qbe.Fn, op: Qbe.O, cls: Qbe.Cls, l: i32, r: i32) i32 = {
    if op == .sel1 { 
        return(if(l == r, => l, => fold_status_runtime));
    };
    cl, cr := (f.con.index(l.zext()), f.con.index(r.zext()));
    c := try_fold(op, cls, cl, cr) || return(fold_status_runtime);
    r := f.newcon(c&);
    // TODO: there are tests that trigger this assertion -- Nov 12
    //@debug_assert(!(cls == .Ks || cls == .Kd) || c.flt != 0, "wrong fold type");
    r.val().intcast()
}

fn try_fold(op: Qbe.O, cls: Qbe.Cls, cl: *Qbe.Con, cr: *Qbe.Con) ?Qbe.Con = {
    c := Qbe.Con.zeroed();
    c.sym.id = Qbe.no_symbol;  // .CBits
    if cl.type() != .CBits || cr.type() != .CBits {
        if(!cls.is_int() || !(@is(op, .add, .sub)), => return(.None));
        // This is used for constant expression evaluation in import_c.
        // TODO: but sometimes you'd be better off doing this in isel when you could have the offsets as part of the memory access and create fewer relocations. 
        if cr.type() == .CAddr {
            if op == .add {
                t := cl; cl = cr; cr = t;
            };
            if(cr.type() == .CAddr, => return(.None));
        };
        c.sym = cl.sym;  // .CAddr
    };
    
    w := cls.is_wide();
    if cls.is_int() && (@is(op, .div, .rem, .udiv, .urem)) {
        if(iscon(cr, w, 0), => return(.None));
        if (@is(op, .div, .rem)) {
            x := if(w, => MIN_i64, => MIN_i32);
            if(iscon(cr, w, bitcast(-1)) && iscon(cl, w, x.bitcast()), => return(.None));
        };
    };
    
    r  := 0;
    do_fold(op, cls, i64.raw_from_ptr(r&), cl.bits(), cr.bits());
    if !is_wide(cls) {
        r = r.bit_and(0xffffffff);  // TODO: should be redundant?
    };
    c&.set_bits(r);
    (Some = c)
}

// For each valid combintion of op + output type, just generate the code to do the thing. 
// This is more magic but also less verbose and less error prone. 
fn do_fold(rt_o: Qbe.O, rt_k: Qbe.Cls, out: rawptr, a0: i64, a1: i64) void = {
    ops :: Qbe.O.get_cases();
    cls :: @const_slice(Qbe.Cls.Kw, .Kl, .Ks, .Kd);
    get_type :: fn(k: Qbe.Cls) Type = @match(k) {
        fn Kw() => u32;
        fn Kl() => u64;
        fn Ks() => f32;
        fn Kd() => f64;
        @default => void;  // :DoFoldInvalidArity
    };
    
    :: enable_franca_ir_types(Ty(Qbe.O, Qbe.Cls));
    hack :: fn($R: Type, $A0: Type, $A1: Type, $o: Qbe.O, $k: Qbe.Cls) FuncId = {
        // HACK: arg of #ir needs to be one expression or it will try to just look at the identifiers and try to find them in the enum.
        cast :: fn(a0: A0, a1: A1) R #ir({ (o, k) });  
        cast
    };
    
    // This is an interesting test for the overhead of immediate_eval_expr. 
    // The total overhead of do_fold using comptime vs just return(false) at the top of this function
    // is ~200ms (1150 -> 1350, tho not too long ago i was getting 1s so maybe i broke something else too), 
    // which is clearly too much. compiler needs to get better!
    // Factoring this expression out into a function that gets compiled once brings out back under 1200ms. 
    // (easy to experiment with: just move the definition of `allow` down to its callsite and see it get slower). 
    // tracy says it's 80ms (like this) vs 200 ms (with allow() manually inlined into its callsite). 
    // based on the same idea, it's better to calculate k0 twice than to hoist it out of the if and
    // do a seperate constant evaluation for it even when the op isn't foldable which is unintuitive.  
    // -- Feb 3, 2025
    allow :: fn(ct_o: Qbe.O, ct_k: Qbe.Cls) bool = 
        can_fold(ct_o) && ct_o != .sel1 && !@is(argcls(ct_o, ct_k, 0), .Kx, .Ke);

    inline_for cls { $k |
        ct_k :: k[];
        R    :: get_type(ct_k);
        @if(rt_k == ct_k) {
            inline_for ops { $o |
                ct_o :: o[];
                @if(::allow(ct_o, ct_k)) {
                    k0    :: argcls(ct_o, ct_k, 0);
                    k1    :: argcls(ct_o, ct_k, 1);
                    A0    :: get_type(k0);
                    A1    :: get_type(k1);
                    @if(rt_o == ct_o) {
                        // TODO: this should work without this extra indirection 
                        // cast :: fn(a0: A0, a1: A1) R #ir({ (ct_o, ct_k) });
                        cast :: hack(R, A0, A1, ct_o, ct_k);
                        
                        // we need to put the args in the right type of register for the instruction (float vs int). 
                        // passing these in as values and then just using a load to do a bitcast down here,
                        // avoids actually generating code for the loads on every branch. 
                        // for 32 bit (Kw, Ks) this relies on union putting all fields starting at the beginning and numbers being little endian. 
                        a0 := ptr_cast_unchecked(i64, A0, a0&)[];
                        a1 := ptr_cast_unchecked(i64, A1, a1&)[];
                        R.ptr_from_raw(out)[] = cast(a0, a1);
                        return();
                    };
                };
            };
        };
    };
}
