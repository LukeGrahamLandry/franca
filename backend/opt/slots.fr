//
// We do the first mem/load opts before lowering abi, 
// so you end up with redundant store to a stack slot and immediately load it back again
// when an aggragate arg/ret is passed in registers. 
// after isel, load/store can reference RSlot directly, and we know which stack slots don't escape.
// so just scan though each block and convert redundant load-store pairs to copies. 
//

// requires rpo, isel
// breaks use counts
elide_abi_slots :: fn(f: *Qbe.Fn) void = {
    @if(!ENABLE_ELIDE_SLOTS) return();

    SlotEntry :: @struct(value: Qbe.Ref, size: i32, k: Qbe.Cls);
    slot_contains: HashMap(i64, SlotEntry) = init(temp());
    StoreEntry :: @struct(ins: RawList(*Qbe.Ins), nins: i64);
    stores: HashMap(i64, StoreEntry) = init(temp());
    live_stack := init_bitset(f.escaping_slots.nt.zext() * 64);
    bscopy(live_stack&, f.escaping_slots&);
    constants: HashMap(u32, Ty(Qbe.Ref, Qbe.Cls)) = init(temp());
    copy_of := temp().alloc_zeroed(Qbe.Ref, f.ntmp.zext());
    
    dead_loads  := 0;
    dead_stores := 0;
    dead_consts := 0;
    
    made_addr_escape_mistake := false;
    for_blocks_rpo_forward f { b |
        for_phi b { p |
            each p.arg() { r |
                f.replace_copy(r, copy_of);
            };
        };
        for_insts_forward b { i |
            continue :: local_return;
            
            each i.arg& { r |
                f.replace_copy(r, copy_of);
            };
            
            if maybe_load(i) { size |
                addr := i.arg&[0];
                if rtype(i.arg&[0]) == .RMem {  // amd64 only
                    m := f.get_memory(i.arg&[0]);
                    if m.offset&.bits() != 0 || m.index != QbeNull {
                        // TODO: need to escape if something's slot if i try to remove stores?
                        continue();
                    };
                    addr = m.base;
                };
                
                if local_slot(f, addr) { slot | 
                    if size < 4 {
                        bsset(live_stack&, slot);
                        continue();
                    };
                    size /= 4;
                    
                    @debug_assert(i.arg&[1] == QbeNull, "need to deal with this if i want to allow slot + offset.");  // arm64 only
                    if slot_contains&.get(slot) { entry | 
                        // since this is after isel, we can't generate invalid instructions. 
                        is_con_to_float := entry.value.rtype() == .RCon && !i.cls().is_int();
                        
                        if entry.size == size && !is_con_to_float && i.op() == .load /*not extending*/ {
                            // TODO: share this logic with load.fr
                            o: Qbe.O = @if(entry.k.is_int() == i.cls().is_int(), .copy, .cast);
                            i[] = make_ins(o, i.cls(), i.to, entry.value, QbeNull);
                            f.add_copy(i, copy_of);
                            dead_loads += 1;
                            continue();
                        };
                    };
                    range(0, size.intcast()) { i |
                        bsset(live_stack&, slot + i);
                    };
                };
                continue();
            };
            
            if maybe_store(i) { size |
                a := i.arg&[1];
                @debug_assert(!i.arg&[0].isreg(), "store should not use register directly before rega. copy to an ssa tmp first.");
                slot := local_slot(f, a) || {
                    rtype(a) == .RMem || continue();
                    m := f.get_memory(a);
                    if m.offset&.bits() != 0 || m.index != QbeNull {
                        // TODO: need to escape if something's slot if i try to remove stores?
                        continue();
                    };
                    slot := local_slot(f, m.base) || continue();
                    slot
                };
                if size < 4 {
                    bsset(live_stack&, slot);
                    slot_contains&.remove(slot);
                    if slot > 0 {
                        slot_contains&.remove(slot-1);
                    };
                    continue();
                };
                size /= 4;
                // TODO: ugly ugly ugly
                range(0, 2) { idx |
                    check := slot - idx;
                    if check >= 0 {
                        if slot_contains&.get_ptr(check) { info |
                            total_overlap := idx == 0 && info.size == size;
                            if total_overlap && info.value == i.arg&[0] {
                                i.set_nop();
                                dead_stores += 1;
                                continue();
                            };
                            if check + info.size.intcast() >= slot && !total_overlap {
                                range(0, 4) { idx |
                                    check := slot - idx + 2;
                                    if check >= 0 {
                                        bsset(live_stack&, check);
                                        slot_contains&.remove(check);
                                    };
                                };
                                
                                continue();
                            };
                        };
                    };
                };
                
                slot_contains&.insert(slot, (value = i.arg&[0], size = size, k = argcls(i, 0)));
                
                if !bshas(live_stack&, slot) {
                    // TODO: get_or_insert
                    entry := stores&.get_or_insert(slot, => (ins = empty(), nins = 0));
                    push(entry.ins&, i, temp());
                };
                continue();
            };
            
            // Now you'd hope rega would put both sides of the copy in the same register so it disappears.
            // its not super reliable at the moment, but this is still a codesize improvement. 
            if i.op() == .addr || (i.op() == .copy && rtype(i.arg&[0]) == .RCon) {
                a0 := i.arg&[0];
                if constants&.get_ptr(a0.type3_val29) { prev |
                    if !prev._1.is_wide() && i.cls().is_wide() {
                        // creating the constant as Kw truncates so that value can't be used as a Kl. 
                        c := f.get_constant(a0);
                        if c.type() == .CAddr || c.bits._1 != 0 {
                            continue();
                        }
                    };
                    dead_consts += 1;
                    o: Qbe.O = @if(prev._1.is_int() == i.cls().is_int(), .copy, .cast);
                    i[] = make_ins(o, i.cls(), i.to, prev._0, QbeNull);
                } else {
                    if i.op() == .addr {
                        if local_slot(f, a0) { slot |
                            // This happens when the slots weren't aligned right 
                            // or were too far to be folded into the instruction in arm isel.
                            // or when an address is computed but not used so didn't get folded 
                            // in amd isel. 
                            made_addr_escape_mistake = true;
                            slot_contains&.remove(slot);
                            bsset(live_stack&, slot);
                        };
                    };
                    
                    if !isreg(i.to) {
                        constants&.insert(a0.type3_val29, (i.to, i.cls()));
                    };
                };
            };
            
            if i.op() == .call {
                // don't increase register pressure by making locals to save values that are cheap to reproduce. 
                slot_contains&.clear();
                constants&.clear();
            };
            
            f.add_copy(i, copy_of);
            @debug_assert(i.op() != .blit1, "TODO: wasm blit1 mark memory escaping");
            @debug_assert(i.op() != .blit0, "no blits!");
        };
        f.replace_copy(b.jmp.arg&, copy_of);
        
        // TODO: We know block dominators so we could do something across blocks maybe?
        slot_contains&.clear();
        constants&.clear();
    };
    
    // TODO
    @if(!made_addr_escape_mistake)
    each stores& { slot, e |
        for e.ins { i |
            continue :: local_return;
            size := i.size_of_store() /4;
            range(0, size.zext()) { i |
                if bshas(live_stack&, slot + i) {
                    continue();
                };
            };
            i.set_nop();
            dead_stores += 1;
        };
    };
    
    when_debug(f, .Memory) { out | 
        changed := dead_loads + dead_stores + dead_consts != 0;
        if changed {
            @fmt(out, "\n## After late slot promotion:\n");
            @fmt(out, "# Killed (% loads, % stores, % consts)\n", dead_loads, dead_stores, dead_consts);
            printfn(f, out);
        } else {
            write(out, "\n## After late slot promotion: (no change)\n");
        };
    };
};

local_slot :: fn(f: *Qbe.Fn, a: Qbe.Ref) ?i64 = {
    rtype(a) == .RSlot                  || return(.None);
    slot := rsval(a).intcast();
    (slot >= 0 && slot < f.slot.zext()) || return(.None);
    s := slot / 4;
    !bshas(f.escaping_slots&, s)     || return(.None);
    (Some = s)
};

// careful not to break the isel reg constraint copies. 
add_copy :: fn(f: *Qbe.Fn, i: *Qbe.Ins, copy_of: []Qbe.Ref) void = @if(LATE_COPY_ELIM) {
    if i.op() != .copy || rtype(i.to) != .RTmp || i.to.isreg() || rtype(i.arg&[0]) != .RTmp || i.arg&[0].isreg() {
        return();
    };
    copy_of[i.to.val()] = i.arg&[0];
};

replace_copy :: fn(f: *Qbe.Fn, r: *Qbe.Ref, copy_of: []Qbe.Ref) void =  @if(LATE_COPY_ELIM) {
    if rtype(r[]) == .RMem {
        m := f.get_memory(r[]);
        f.replace_copy(m.base&, copy_of);
        f.replace_copy(m.index&, copy_of);
        return();
    };
    if rtype(r[]) != .RTmp {
        return();
    };
    r2 := copy_of[r[].val()];
    if r2 == Qbe.Null {
        return();
    };
    @debug_assert(!r[].isreg() && !r2.isreg());
    r[] = r2;
};

#use("@/backend/lib.fr");
