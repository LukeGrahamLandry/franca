// Adapted from Qbe. MIT License. Â© 2015-2024 Quentin Carbonneaux <quentin@c9x.me>

fn promote(f: *Qbe.Fn) void = {
    @if(!ENABLE_PROMOTE) return(); 
    f.requires(.Use); f.breaks(.Ssa);
    
    /* promote uniform stack slots to temporaries */
    b := f.start;
    
    when_debug(f, .Memory, fn(out) => @fmt(out, "\n# promotions: ("));
    for_insts_forward b { i |
        skip :: local_return;
        if(!is_alloc(i.op()), => skip());
        // Could skip anything with size>8 but this way lets us promote if you ask for a large slot but only use one word at the beginning. meh. 
        
        /* specific to NAlign == 3 */
        r := i.to;
        @debug_assert(rtype(r) == .RTmp);
        t := f.get_temporary(r);
        if(t.ndef != 1, => skip());
        k: ?Qbe.Cls = .None;
        s: ?i32 = .None;
        each t.uses() { u |
            if u.type != .UIns || !is_uniform_access(f, u.u.ins, k&, s&, r) {
                skip();
            };
        };
        /* get rid of the alloc and replace uses */
        i.set_nop();
        @debug_assert(t.def.identical(i));
        t.def = Qbe.Ins.ptr_from_int(0);
        t.ndef -= 1;
        k := or k {
            @assert(t.nuse == 0, "slot %% is read but never stored to", "%", t.name());
            skip()
        };
        t.cls = k;
        uses := t.uses();
        each uses { u |
            replace_access_with_copy(f, t, u, k, r);
        };
        @debug_assert(t.ndef == 0 || !t.def.is_null());
        
        // maintain usage info to save a fill_use() call
        unordered_retain(uses&, fn(u) => u.type != .UXXX);
        @debug_assert_eq(uses.len, t.nuse.zext());
        
        when_debug(f, .Memory, fn(out) => @fmt(out, "%%, ", "%", t.name()));
    };
    when_debug_printfn(f, .Memory, ")\n## After slot promotion:\n");
}

fn is_uniform_access(f: *Qbe.Fn, l: *Qbe.Ins, k: *?Qbe.Cls, s: *?i32, address: Qbe.Ref) bool #once = {
    if maybe_load(l) { size |
        if none_or(s[], fn(s) => s == size) {
            s[] = (Some = size);
            return(true);
        };
    };
    if maybe_store(l) { size |
        if address == l.arg&[1] 
        && address != l.arg&[0]
        && none_or(s[], fn(s) => s == size)
        && none_or(k[], fn(k) => k == argcls(l, 0)) {
            s[] = (Some = size);
            k[] = (Some = argcls(l, 0));
            return(true);
        };
    };
    
    if l.op() == .blit0 {
        break :: local_return;
        blit1 := l.offset(1);
        @debug_assert(blit1.op() == .blit1);
        size := blit1.arg&[0].rsval().abs();
        if(size > 8, => break());
        src, dest := (l.arg&[0], l.arg&[1]);
        @if(address != src && address != dest) break();
        @if(src == dest) break(); // TODO: just kill it
        legal_size := @match(s[]) {
            fn Some(s) => s == size;
            fn None() => ispow2(size.intcast());
        }; 
        @if(!legal_size) break();
        s[] = (Some = size);
        if k.is_none() {
            // we don't care about the class but if you only access with blits we don't want to crash so assume int, 
            // but that maens that if something could have been promoted as a float but first use was a blit we won't see it.
            k[] = (Some = @if(size == 8, .Kl, .Kw));
        };
        return(true);
    };
    
    false
}

store_by_size :: @const_slice(Qbe.O.Oxxx, .storeb, .storeh, .Oxxx, .storew, .Oxxx, .Oxxx, .Oxxx, .storel);

fn replace_access_with_copy(f: *Qbe.Fn, t: *Qbe.Tmp, u: *Qbe.Use, k: Qbe.Cls, address: Qbe.Ref) void #once = {  
    l := u.u.ins;
    make_def :: fn(o: Qbe.O) => {
        l[] = make_ins(o, k, address, l.arg&[0], QbeNull);
        t.def = l;
        t.defining_block = u.bid;
        t.nuse -= 1;
        t.ndef += 1;
        u.type = .UXXX;
    };
    if is_store(l.op()) {
        @debug_assert(address == l.arg&[1]);
        make_def(.copy);
        return();
    };
    if l.op() == .blit0 {
        src, dest := (l.arg&[0], l.arg&[1]);
        blit1 := l.offset(1);
        @debug_assert(blit1.op() == .blit1);
        size: i64 = blit1.arg&[0].rsval().intcast().abs();
        // l needs to be the one converted to memory access so usable info stays valid when we try to replace it later.
        if src == address {
            o := @match(k) {
                fn Kd() => .stored;
                fn Ks() => .stores;
                @default => store_by_size[size];
            };
            l[] = make_ins(o, .Kw, QbeNull, src, dest);
        } else {
            @debug_assert(dest == address);
            ops := @const_slice(Qbe.O.Oxxx, .loadub, .loaduh, .Oxxx, .loaduw, .Oxxx, .Oxxx, .Oxxx, .load);
            o := @if(k == .Ks, Qbe.O.load, ops[size]);
            make_def(o);
        };
        blit1.set_nop();
        return();
    };
    
    @debug_assert(address == l.arg&[0] && is_load(l.op()));
     
    // HACK: examples/import_c has a habit of reading from uninitialized memory 
    // but not using the result which confuses ssa_check later. 
    // it feels weird to disallow reading uninitialized memory because if we'd done no 
    // optimisations, the program would run, you'd just get some arbitrary number. 
    // there is also the MADV_DONTNEED problem (you can read twice and get different values). 
    if f.get_temporary(l.to)[].nuse == 0 {
        l.arg&[0] = QbeUndef;
        l.set_op(.copy);
        t.nuse -= 1;
        u.type = .UXXX;
        return();
    };
    
    /* try to turn loads into copies so we
     * can eliminate them later */
    ext_op := rebase(l.op(), .extsb, .loadsb);
    if l.op() == .loadsw || l.op() == .loaduw {
        if k == .Kl {
            l.set_op(ext_op);
            return();
        };
    } else {
        if l.op() != .load {
            l.set_op(ext_op);
            return();
        };
    };
    l.set_op(if(KBASE(k) != KBASE(l.cls()), => .cast, => .copy));
}

/* [a, b) with 0 <= a */
Range :: @struct(a: i32, b: i32);
Store :: @struct(clock: i32, i: *Qbe.Ins);
Slot  :: @rec @struct(
    t: i32,
    sz: i32,
    ever_written_bits: u64,
    unknown_bits: u64,
    live_range: Range,
    replacement: *Slot,
    stores: RawList(Store),
);

fn rin(r: Range, n: i32) bool #inline =
    r.a <= n && n < r.b;

fn rovlap(r0: Range, r1: Range) bool #inline =
    r0.b != 0 && r1.b != 0 && r0.a < r1.b && r1.a < r0.b;

fn radd(r: *Range, n: i32) void = {
    @if_else {
        @if(r.b == 0) => {
            r[] = (a = n, b = n + 1);
        };
        @if(n < r.a) => {
            r.a = n;
        };
        @if(n >= r.b) => {
            r.b = n + 1;
        };
        @else => ();
    };
}

// ?(slot, offset)
fn slot(r: Qbe.Ref, f: *Qbe.Fn, sl: []Slot) ?Ty(*Slot, i64) = {
    a := Qbe.Alias.zeroed();
    getalias(a&, r, f);
    if a.type != .ALoc {
        @debug_assert(a.base.zext() < Qbe.Tmp0 || f.get_temporary(a.base.zext())[].visit == -1, "bad base");
        return(.None);  // not a local stack slot
    };
    @debug_assert_ge(a.base.zext(), Qbe.Tmp0);
    t := f.get_temporary(a.base.zext());
    if t.visit < 0 {
        return(.None);  // skipped by collect_local_slots for some other reason. 
    };
    @debug_assert_lt(a.offset, 64); // i think? otherwise the shifting doesn't work. TODO: find where its clamped -- Oct 21
    (Some = (sl.index(t.visit.zext()), a.offset))
}

fn coalesce(f: *Qbe.Fn) void = {
    f.requires(.Rpo); f.consumes(.Use); f.requires(.Alias);
    /* minimize the stack usage
     * by coalescing slots
     */
    sl := f.collect_local_slots();

    /* one-pass liveness analysis */
    for_blocks f { b |
        b.loop = -1;
    };
    loopiter f { (hd: *Qbe.Blk, b: *Qbe.Blk) | 
        xx := b.id;
        if hd.loop < xx {
            hd.loop = xx;
        };
    };
    blit_instructions := list(*Qbe.Ins, temp());
    block_ranges := temp().alloc_zeroed(Range, f.nblk.zext());
    clock := MAX_i32 - 1;
    range_rev(0, f.nblk.zext()) { n |
        b := f.rpo[n];
        block_ranges[n].b = clock.intcast();
        clock -= 1;
        each sl { s | 
            s.unknown_bits = 0;
            for_jump_targets b { ps | // :SketchyIterTargets
                m: i64 = ps.id.intcast();
                if m > n && rin(s.live_range, block_ranges[m].a) {
                    // this slot is live in `ps` so we care about its value at the end of `b`
                    s.unknown_bits = s.ever_written_bits;
                    radd(s.live_range&, clock.intcast());
                };
            };
        };
        if b.jmp.type == .retc {
            clock -= 1;
            load(b.jmp.arg, bitcast(-1), clock.intcast(), f, sl);
        };
        for_insts_rev b { i |
            if record_memory_liveness(f, i[], sl, clock.intcast(), blit_instructions&) {
                clock -= 1;
            };
        };
        each sl { s |
            if s.unknown_bits != 0 {
                // we care about bits that were set outside this block so its live at the beginning of the block
                radd(s.live_range&, clock.intcast());
                if b.loop != -1 {
                    @debug_assert(b.loop.intcast() >= n);
                    radd(s.live_range&, block_ranges[b.loop.zext()].b - 1);
                };
            };
        };
        block_ranges[n].a = clock.intcast();
    };

    kill_dead_stores(sl);

    /* kill slots with an empty live range */
    total: i32 = 0;
    freed: i32 = 0;
    dead_slot_stack := list(i32, temp());
    last_valid_slot := sl.ptr;
    each sl { s |
        total += s.sz;
        if s.live_range.b == 0 {
            dead_slot_stack&.push(s.t);
            freed += s.sz;
        } else {
            last_valid_slot[] = s[];
            last_valid_slot = last_valid_slot.offset(1);
        };
    };
    sl.len = ptr_diff(sl.ptr, last_valid_slot);
    when_debug(f, .Memory, fn(out) => debug_dump_dead_slots(f, dead_slot_stack.items(), out));
    while => dead_slot_stack&.pop() { t |
        kill_slot_uses(f, t.zext(), dead_slot_stack&);
    };

    /* fuse slots by decreasing size */
    scmp :: fn(a: *Slot, b: *Slot) bool = {
        if a.sz != b.sz {
            b.sz <= a.sz
        } else {
            a.live_range.a <= b.live_range.a
        }
    };
    sort :: quicksort(Slot, scmp);
    sort(sl);
    fused: i32 = 0;
    range(0, sl.len) { n |
        continue :: local_return;
        s0 := sl.index(n);
        if(!s0.replacement.is_null(), => continue());
        s0.replacement = s0;
        r := s0.live_range;
        range(n+1, sl.len) { si |
            skip :: local_return;
            s := sl.index(si);
            if(!s.replacement.is_null() || s.live_range.b == 0, => skip());
            if rovlap(r, s.live_range) {
                /* O(n); can be approximated
                 * by 'goto skip;' if need be
                 */
                range(n, si) { m |
                    if sl[m].replacement.identical(s0) && rovlap(sl[m].live_range, s.live_range) {
                        skip();
                    };
                };
            };
            radd(r&, s.live_range.a);
            radd(r&, s.live_range.b - 1);
            s.replacement = s0;
            fused += s.sz;
        }
    };

    /* substitute fused slots */
    enumerate sl { idx, s |
        t := f.tmp.index(s.t.intcast());
        /* the visit link is stale,
         * reset it before the slot()
         * calls in fix_overlapping_blit
         */
        t.visit = idx.intcast();
        @debug_assert(t.ndef == 1 && !t.def.is_null());
        if !s.replacement.identical(s) {
            f.substitute_fused_slot(s);
        };
    };

    /* fix newly overlapping blits */
    for blit_instructions& { i |
        if i.op() == .blit0 {
            f.fix_overlapping_blit(i, sl);
        };
    };

    when_debug(f, .Memory) { out |
        debug_dump_fused_slots(f, sl, clock, out);
        @fmt(out, "#\tsums %/%/% (killed/fused/total)\n\n", freed, fused, total);
        printfn(f, out);
    };
}

fn collect_local_slots(f: *Qbe.Fn) []Slot #once = {
    sl := Slot.list(temp()); 
    range(Qbe.Tmp0, f.ntmp.zext()) { n |
        t := f.get_temporary(n);
        t.visit = -1;
        if t.alias.type == .ALoc
        && t.alias.slot.val() == n
        && t.defining_block == f.start.id
        && t.alias.u.loc.sz != -1 {
            t.visit = sl.len.intcast();
            sl&.push(
                t = n.intcast(),
                sz = t.alias.u.loc.sz,
                ever_written_bits = t.alias.u.loc.m,
                replacement = Slot.ptr_from_int(0),
                stores = empty(),
                live_range = (a = 0, b = 0),
                unknown_bits = 0,
            );
        };
    };
    sl.items()
}

fn record_memory_liveness(f: *Qbe.Fn, i: *Qbe.Ins, sl: []Slot, clock: i32, blit_instructions: *List(*Qbe.Ins)) bool #once = {
    arg := i.arg&;
    if i.op() == .argc {
        // passing a memory argument is like the callee loading it. 
        @debug_assert(rtype(arg[0]) == .RType);
        load(arg[1], bitcast(-1), clock - 1, f, sl);
        return(true);
    };
    if maybe_load(i) { size |
        x := BIT(size.intcast()) - 1;
        load(arg[0], x, clock - 1, f, sl);
        return(true);
    };
    if is_store(i.op()) {
        x := BIT(size_of_store(i).intcast()) - 1;
        store(arg[1], x, clock, i, f, sl);
        return(true);
    };
    if i.op() == .blit0 {
        // copy is equivilent to a load followed by a store. 
        @debug_assert(i.offset(1).op() == .blit1);
        sz := i.offset(1)[].arg&[0];
        @debug_assert(rtype(sz) == .RInt);
        sz := abs(rsval(sz));
        x: u64 = if(sz >= 64, => bitcast(@as(i64) -1), => BIT(sz.intcast()) - 1);
        store(arg[1], x, clock, i, f, sl);
        load(arg[0], x, clock - 1, f, sl);
        blit_instructions.push(i);
        return(true);
    };
    false
}

fn load(r: Qbe.Ref, affected_bits: u64, clock: i32, f: *Qbe.Fn, sl: []Slot) void = {
    s, off := slot(r, f, sl) || return();  // if not local we don't care
    // now as we continue backwards in the block, we know we care about this section of memory. 
    s.unknown_bits = s.unknown_bits.bit_or(affected_bits.shift_left(off));
    s.unknown_bits = s.unknown_bits.bit_and(s.ever_written_bits);
    if s.unknown_bits != 0 {
        radd(s.live_range&, clock);
    };
}

fn store(r: Qbe.Ref, affected_bits: u64, clock: i32, i: *Qbe.Ins, f: *Qbe.Fn, sl: []Slot) void = {
    s, off := slot(r, f, sl) || return(); // if not local we don't care
    if s.unknown_bits != 0 {
        radd(s.live_range&, clock);
        // since we're writing these bits now, we don't care about thier value earlier in the block.
        s.unknown_bits = s.unknown_bits.bit_and(bit_not(affected_bits.shift_left(off)));
    } else {
        push(s.stores&, (clock = clock, i = i), temp());
    };
}

fn kill_dead_stores(sl: []Slot) void #once = {
    each sl { s |
        for s.stores { it |
            if !rin(s.live_range, it.clock) {
                i := it.i;
                if i.op() == .blit0 {
                    i.offset(1).set_nop();
                };
                i.set_nop();
            };
        };
    };
}

fn kill_slot_uses(f: *Qbe.Fn, dead_slot_tmp_index: i64, dead_slot_stack: *List(i32)) void #once = {
    t := f.tmp.index(dead_slot_tmp_index);
    @debug_assert(t.ndef == 1 && !t.def.is_null());
    i := t.def;
    if is_load(i.op()) {
        i.set_op(.copy);
        i.arg&[0] = QbeUndef;
        return();
    };
    i.set_nop();
    each t.uses() { u | 
        continue :: local_return;
        if u.type == .UJmp {
            b := f.rpo[u.bid.zext()];
            @debug_assert(is_ret(b.jmp.type));
            b.jmp.type = .ret0;
            b.jmp.arg = QbeNull;
            continue();
        };
        @debug_assert(u.type == .UIns);
        i := u.u.ins;
        @if_else {
            @if(i.to != QbeNull) => {
                // since live ranges are based on alias.base, any values derived from a dead slot must also be dead. 
                @debug_assert(rtype(i.to) == .RTmp);
                dead_slot_stack.push(i.to.val().intcast());
            };
            @if(is_arg(i.op())) => {
                // TODO: this confuses me and i don't have a test that gets here
                @debug_assert(i.op() == .argc);
                i.arg&[1] = QbeConZero;  /* crash */
            };
            @else => {
                if i.op() == .blit0 {
                    i.offset(1).set_nop();
                };
                i.set_nop();
            };
        };
    };
}

// Replace all references to `s.t` with `s.replacement.t`.
fn substitute_fused_slot(f: *Qbe.Fn, s: *Slot) void #once = {
    @debug_assert(s.t != s.replacement.t);
    old := f.tmp.index(s.t.intcast());
    old.def.set_nop();
    new := f.tmp.index(s.replacement.t.intcast());
    @debug_assert(old.defining_block == new.defining_block);
    if new.def.in_memory_after(old.def) {
        /* make sure the slot we
        * selected has a def that
        * dominates its new uses
        */
        old.def[] = new.def[];
        new.def.set_nop();
        new.def = old.def;
    };
    for old.uses() { u | 
        if u.type == .UJmp {
            b := f.rpo[u.bid.zext()];
            b.jmp.arg = TMP(s.replacement.t);
        } else {
            @debug_assert(u.type == .UIns);
            each u.u.ins.arg& { a | 
                if a[] == TMP(s.t) {
                    a[] = TMP(s.replacement.t);
                }
            };
        };
    };
}

fn fix_overlapping_blit(f: *Qbe.Fn, i: *Qbe.Ins, sl: []Slot) void #once = {
    s0, off_src  := slot(i.arg&[0], f, sl)   || return();
    s1, off_dest := slot(i.arg&[1], f, sl)   || return();
    s0.replacement.identical(s1.replacement) || return();
    // after coalescing, `i` copies a stack slot to itself. 
    
    size_i := i.offset(1);
    if off_src < off_dest {
        // Copy backwards to avoid stomping (like memcpy vs memmove)
        // 1 2 3 x [before]
        //  \ \ \
        // 1 1 2 3 [after]
        // See test/mem1.ssa for an ir example of this situation. 
        
        size_arg := size_i.arg&.index(0);
        sz := rsval(size_arg[]);
        @debug_assert(sz >= 0);
        size_arg[] = INT(-sz.intcast());
    };
    if off_src == off_dest {
        i.set_nop();
        size_i.set_nop();
    };
}

#use("@/backend/lib.fr");
