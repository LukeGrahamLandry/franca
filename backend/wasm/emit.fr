//! - Wasm is a bit annoying because it's designed to be compact and fast to load, not to be output in one pass. 
//!   For example, you can't know the index of any functions until you know how many imports there are. 

EmitWasmFn :: @struct(
    m: *QbeModule,
    f: *Qbe.Fn,
    // not just using the tmp's index because it's sparse (most ssa vars shouldn't need a local)
    // also we want to reorder them for run length encoding in pack_locals
    locals: []i32 = empty(),
    a: List(u8),
    block_stack: List(*Qbe.Blk),
    
    // These are just for a debug assertion. TODO: maybe remove them. 
    local_def := zeroed Qbe.BSet,
    local_use := zeroed Qbe.BSet,
);

fn record_wasm_function_index(m: *Qbe.Module, id: Qbe.Sym) void = {
    export := false;
    use_symbol(m, id) { s |
        s.referenced = true;
        export = s.export;
        if m.goal.type == .JitOnly {
            s.wasm_module_function_index = m.local_needs_reloc.len.intcast();
        }; // else, AOT: there can be imports so real index can't be assigned yet. 
    };
    m.local_needs_reloc&.push(id);
    
    
    // :WasmJitExportAllFunctions 
    @if(TODOWASM) @if(!export) m.exports&.push(id);
}

fn emit_func_wasm32(f: *Qbe.Fn) []Ty(Fixup, Qbe.Sym) = {
    code := f.globals.segments&[.Code]&;
    e: EmitWasmFn = (
        f = f,
        m = f.globals,
        a = fixed_list(ptr = code.next, len = code.cap() - code.len()),
        block_stack = list(temp()), 
    ); e := e&;
    
    record_wasm_function_index(e.m, f.lnk.id);
    e.pad(WASM_INDEX_PADDING); // reserve space for length of code. 
    
    pack_locals(e);
    
    last_jmp := Qbe.J.Jxxx;
    for_blocks e.f { b |
        emit_block(e, b);
        last_jmp = b.jmp.type;
    };
    @match(last_jmp) {
        fn hlt()  => ();
        fn ret0() => {
            e.a.len -= 1;
        };
        // keep the verifier happy (without this the implicit return needs to typecheck)
        @default => e.op(.Unreachable);  
    };
    e.op(.End);
    code.next = code.next.offset(e.a.len());
    
    @debug_assert_eq({ bsdiff(e.local_use&, e.local_def&); e.local_use&.bscount() }, 0, "use without def");
    
    size_of_func := e.a.len;
    e.a.len = 0;
    leb128_unsigned(e.a&, size_of_func - WASM_INDEX_PADDING, WASM_INDEX_PADDING); 
    // TODO: fix -d D llvm-mc dis needs to not include the locals part
    
    empty()
}

// im so impressed that this works
fn pack_wasm_result_type(m: *QbeModule, $iter_args: @Fn(yield: @Fn(k: Qbe.Cls) void) void, $iter_ret: @Fn(yield: @Fn(k: Qbe.Cls) void) void) i32 = {
    s := m.wasm_types&;
    prev_len := s.len();    
    s.reserve(Wasm.ValType)[] = .ResultStart;
    body :: fn($iter) => {
        count := s.reserve(u8);
        n := 0;
        iter { k |
            s.reserve(Wasm.ValType)[] = to_wasm_type(k);
            n += 1;
            //@print("%,", k);
        };
        @debug_assert_ult(n, 128, "wasm_result_type too large (todo: handle multi-byte leb here)"); 
        count[] = n.trunc();
    };
    //print("type (");
    body(iter_args);
    //print(") -> (");
    body(iter_ret);
    
    i := pack_wasm_result_type_outlined(m, prev_len);
    //@println(") = %", i);
    
    i
}

pack_wasm_result_type_outlined :: fn(m: *Qbe.Module, prev_len: i64) i32 = {
    s := m.wasm_types&;
    c := m.wasm_function_type_count&;
    
    bytes := s.mmapped.slice(prev_len, s.len());
    if m.wasm_types_cache&.get(bytes) { i |
        s.next = s.next.offset(-bytes.len());  // take it back now yall
        return i;
    };
    m.wasm_types_cache&.insert(bytes, c[]);
    
    c[] += 1;
    c[] - 1
};

fn pack_locals(e: *EmitWasmFn) void = {
    // 0-4 uleb fits in one byte so this patch is easy. 
    patch := e.a.index_unchecked(e.a.len);
    e.pad(1);
   
    e.locals = temp().alloc_uninit(i32, e.f.ntmp.zext());
    e.locals.interpret_as_bytes().set_bytes(0xFF);  // -1
    e.local_def = init_bitset(e.locals.len);
    e.local_use = init_bitset(e.locals.len);
    
    total_locals: i32 = 0;
    for_pars e.f { i |
        e.locals[i.to.val()] = total_locals;
        e.local_def&.bsset(i.to.val());
        total_locals += 1;
        i.set_nop();
    };
    // TODO: this probably isn't worth the time. 
    // Declaration of locals uses run-length-encoding so group by type to shrink the module. 
    k_ :: @const_slice(Qbe.Cls.Kw, .Kl, .Ks, .Kd);
    entries := 0;
    range(0, 4) { i |
        k := k_[i];
        wasm_type := to_wasm_type(k);
        prev_locals := total_locals;
        range(Qbe.Tmp0, e.f.ntmp.zext()) { i |
            t := e.f.tmp.index(i);
            // TODO: get smarter about removing unused locals and then don't assign one if it has a def but no use. 
            if e.locals[i].intcast() == -1 && { t.nuse > 0 || t.ndef > 0 } && t.cls == k {
                e.locals[i] = total_locals;
                total_locals += 1;
            };
        };
        n := total_locals - prev_locals;
        if n > 0 {
            leb128_unsigned(e.a&, n.zext());
            ::enum(@type wasm_type);
            e.a&.push(wasm_type.raw());
            entries += 1;
        };
    };
    
    patch[] = entries.trunc();
}

fn to_wasm_type(k: Qbe.Cls) Wasm.ValType = {
    wasm_type :: @const_slice(Wasm.ValType.I32, .I64, .F32, .F64);
    wasm_type;  // HACK FIXME :FUCKED
    xxx: i64 = k.raw().zext();
    wasm_type[xxx]
}

fn emit_block(e: *EmitWasmFn, b: *Qbe.Blk) void = {
    for_insts_forward b { i |
        if i.op() == .flow {
            e.emit_flow(b, i);
        } else {
            emit(e, i);
        }
    };
    
    @match(b.jmp.type) {
        fn ret0() => e.op(.Return);
        fn hlt()  => e.op(.Unreachable);
        // These are represented as O.flow instructions 
        fn jmp() => ();
        fn jnz() => ();
        @default => @panic("invalid terminator");
    };
}

fn emit_flow(e: *EmitWasmFn, b: *Qbe.Blk, i: *Qbe.Ins) void = {
    F :: import("@/backend/wasm/isel.fr")'Flow;
    flow := @as(F) @as(i64) i.arg&[0].rsval().intcast();
    e.op(@match(flow) {  // TODO: this is silly
        fn block() => .Block;
        fn loop() => .Loop;
        fn if() => .If;
        fn else() => .Else;
        fn end() => .End;
        fn br() => .Br;
        fn br_if() => .BrIf;
    });
    s := @if(i.arg&[1] == QbeNull, Qbe.Blk.ptr_from_int(0), e.f.rpo[i.arg&[1].rsval().intcast()]);
    
    // my loops never produce an output (because they never fall through, you break out to a block). 
    // blocks output whatever types thier end block expects. 
    if @is(flow, .block, .loop) {
        type := pack_wasm_result_type e.f.globals { $yield_arg #duplicated |
            for_phi_rev_TODOSLOW b { p |
                yield_arg(p.cls);
            };
        } and { $yield_ret |
            @if(flow == .block)
            for_phi_rev_TODOSLOW s { p |
                yield_ret(p.cls);
            };
        };
        leb128_signed(e.a&, type.intcast());
        e.block_stack&.push(s);
    };
    
    if @is(flow, .br, .br_if) {
        break :: local_return;
        depth := 0;
        for_rev e.block_stack& { check |
            if check.identical(s) {
                leb128_unsigned(e.a&, depth);
                break();
            };
            depth += 1;
        };
        @panic("missing % target @% -> @%\n", flow, b.name(), s.name());
    };
    
    if flow == .end {
        s1 := e.block_stack&.pop().unwrap();
        if !identical(s, s1) {
            printfn(e.f, e.f.globals.debug_out);
        };
        @debug_assert(identical(s, s1), "ICE: bad nesting. expected to end @% but found @%\n", s.name(), s1.name());
    };
}

fn emit(e: *EmitWasmFn, i: *Qbe.Ins) void = {
    if Wasm'lookup_wasm_encoding(i.op(), i.cls()) { byte |
        e.a&.push(byte);
        if maybe_load(i) { size |
            @debug_assert(rtype(i.arg&[0]) == .RInt);
            off := i.arg&[0].rsval().intcast();
            e.a&.push(@as(u8) size.zext().trailing_zeros().trunc());  // align hint
            leb128_unsigned(e.a&, off);
        };
        if maybe_store(i) { size | 
            @debug_assert(rtype(i.arg&[0]) == .RInt);
            off := i.arg&[0].rsval().intcast();
            e.a&.push(@as(u8) size.zext().trailing_zeros().trunc());  // align hint
            leb128_unsigned(e.a&, off);
        };
        return();
    };
    
    @match(i.op()) {
        fn call() => {
            // TODO: the way im doing jit modules, direct calls to an import don't work. 
            e.op(.Call);
            @debug_assert(i.arg&[0].rtype() == .RCon, "use call_indirect for non-constant callee");
            c := e.f.get_constant(i.arg&[0]);
            @debug_assert(c.type() == .CAddr && c.bits() == 0, "wasm can only call symbols");
            e.index_patch(c.sym, .Func);
        }
        fn call_indirect() => {
            e.m.wasm_has_any_indirect_calls = true;
            
            // Lookup a function reference in a table and call it. 
            e.op(.CallIndirect);
            @debug_assert_eq(rtype(i.arg&[1]), .RCall);
            type_index := i.arg&[1].rsval().intcast();
            @debug_assert_ge(type_index, 0);
            leb128_unsigned(e.a&, type_index); // type index
            leb128_unsigned(e.a&, 0);  // table index
        }
        fn push() => @match(rtype(i.arg&[0])) {
            fn RCon() => {
                c := e.f.get_constant(i.arg&[0]);
                @match(c.type()) {
                    fn CBits() => e.push_bits(c, i.cls());
                    fn CAddr() => {
                        use_symbol(e.m, c.sym) { symbol | 
                            break :: local_return;
                            e.op(@if(i.cls().is_wide(), .I64_Const, .I32_Const));
                            
                            is_function := symbol.wasm_type_index != -1;
                            if e.m.goal.type == .JitOnly && is_function {
                                if symbol.got_lookup_offset == -1 {
                                    // reserve an index in the table early. it will be filled in by make_exec();
                                    symbol.got_lookup_offset = table_grow(1).intcast();
                                    @debug_assert_ne(symbol.got_lookup_offset, -1, "failed to grow table");
                                    //@println("reserve T[%] = %", symbol.got_lookup_offset, symbol.name);
                                };
                                @debug_assert(c.bits() == 0 && symbol.segment == .Code);
                                leb128_signed(e.a&, symbol.got_lookup_offset);
                                break();
                            };
                            
                            // if it's a function address we have to wait until the end to know its index in the table. 
                            if symbol.kind == .Local && symbol.segment == .MutableData {
                                base := @if(e.m.goal.type == .JitOnly, symbol.jit_addr.int_from_rawptr(), symbol.offset);
                                leb128_signed(e.a&, base + c.bits());
                            } else {
                                @debug_assert(symbol.segment != .ConstantData, "TODO: ConstantData");
                                patch_at := u8.raw_from_ptr(e.a.maybe_uninit.ptr.offset(e.a.len));
                                push_fixup(e.m, symbol, (
                                    patch_at = patch_at, 
                                    type = (WasmAddr = (increment = c.bits().trunc())),
                                ));
                                e.pad(WASM_INDEX_PADDING);
                            };
                        };
                    }
                };
            }
            fn RTmp() => {
                e.local_use&.bsset(i.arg&[0].val());
                local_index := e.locals[i.arg&[0].val()];
                @debug_assert_ne(local_index, -1, "read from tmp without assigned local");
                e.op(.LocalGet);
                leb128_unsigned(e.a&, local_index.zext());
            }
            fn RSlot() => @panic("unlowered RSlot");
            @default => {
                printfn(e.f, e.m.debug_out);
                @panic("invalid argument for op push %", i.arg&[0]);
            };
        };
        fn pop() => {
            if i.to == QbeNull {
                e.op(.Drop);
                return();
            };
            @debug_assert(rtype(i.to) == .RTmp, "pop to tmp");
            local_index := e.locals[i.to.val()];
            e.local_def&.bsset(i.to.val());
            @debug_assert_ne(local_index, -1, "write to % without assigned local", i.to);
            e.op(.LocalSet);
            leb128_unsigned(e.a&, local_index.zext());
        }
        fn local_tee() => {
            @debug_assert(rtype(i.to) == .RTmp, "pop to tmp");
            e.local_def&.bsset(i.to.val());
            local_index := e.locals[i.to.val()];
            @debug_assert_ne(local_index, -1, "write to % without assigned local", i.to);
            e.op(.LocalTee);
            leb128_unsigned(e.a&, local_index.zext());
        }
        fn global_set() => {
            id, off := e.f.get_sym(i.arg&[0]) || @panic("global_set arg0 must be a symbol");
            @debug_assert_eq(off, 0);
            @debug_assert(i.arg&[1] == QbeNull, "should be on stack, not in a local");
            e.op(.GlobalSet);
            e.push_global_index(id);
        }
        fn global_get() => {
            id, off := e.f.get_sym(i.arg&[0]) || @panic("global_get arg0 must be a symbol");
            @debug_assert_eq(off, 0);
            e.op(.GlobalGet);
            e.push_global_index(id);
        }
        fn nop()    => ();
        fn dbgloc() => ();
        fn sel1()   => {
            e.op(.Select);
            leb128_unsigned(e.a&, 1);  // vector of types
            ::enum(Wasm.ValType);
            push(e.a&, to_wasm_type(i.cls()).raw()); 
        }
        fn cas1() => {  // T.atomic.rmw.cmpxchg
            byte: u8 = @if(i.cls().is_wide(), 73, 72);
            align_hint: u8 = @if(i.cls().is_wide(), 3, 2);
            e.op(.AtomicPrefix);
            e.a&.push(byte);
            e.a&.push(align_hint);
            e.a&.push(0);  // offset  TODO: use this
        };
        fn blit1() => {
            e.a&.push_all(@const_slice(0xFC, 0x0A, 0x00, 0x00));
        }
        fn reqz() => {
            e.op(@if(i.cls().is_wide(), .I64_EqZero, .I32_EqZero));
        }
        @default => @panic("TODO: wasm encoding for % %", i.op(), { printfn(e.f, e.f.globals.debug_out); "" });
    };
}

fn push_bits(e: *EmitWasmFn, c: *Qbe.Con, k: Qbe.Cls) void = @match(k) {
    fn Ks() => {
        e.op(.F32_Const);
        e.a&.reserve_type(u32)[] = c.bits().trunc();  // f32
    }
    fn Kd() => {
        e.op(.F64_Const);
        e.a&.reserve_type(i64)[] = c.bits(); // f64
    }
    fn Kw() => {
        e.op(.I32_Const);
        i := c.bits().intcast().intcast();
        leb128_signed(e.a&, i);
    }
    fn Kl() => {
        e.op(.I64_Const);
        leb128_signed(e.a&, c.bits());
    }
    @default => unreachable();
};

fn index_patch(e: *EmitWasmFn, id: Qbe.Sym, _space: Wasm.ImportType) void = {
    use_symbol(e.m, id) { s |
        patch_at := u8.raw_from_ptr(e.a.maybe_uninit.ptr.offset(e.a.len));
        push_fixup(e.m, s, (patch_at = patch_at, type = .WasmIndex));
    };
    e.pad(WASM_INDEX_PADDING);
}

// This is the amount of space reserved when i need to output a leb index too soon so it needs a patch. 
// The placeholder is hlt instructions so hopefully you'll notice immediately if it isn't filled (because of a compiler bug).
// After the patch the extra space is filled with zero bytes with just the continuation bit set 
// (which is less annoying to look at in the disassembly than if i used nops, but it also hides the bloat which is bad). 
// So a trivial size optimization would be to go over the whole module and repack the numbers. 
// TODO: use smaller padding when you know it's a function index vs a jit data address that could be near the end of linear memory. 
WASM_INDEX_PADDING :: 5;

fn pad(e: *EmitWasmFn, count: i64) void = range(0, count) { _ |
    e.op(.Unreachable);
};

fn op(e: *EmitWasmFn, o: Wasm.Inst) void = {
    ::enum(Wasm.Inst);
    e.a&.push(o.raw());
}

fn fixup_wasm32(self: *QbeModule, symbol: *SymbolInfo, fixup: *Fixup, new_got_reloc: *?Fixup) void = {
    // TODO: this is kinda using segment==Code to mean is it a function but that will also be set for data symbol imports. 
    //       the problem is put_jit_addr won't know the type if you do it before any calls. 
    indirect_index := @if(self.goal.type == .JitOnly && symbol.segment != .Code, 
        symbol.jit_addr.int_from_rawptr(), symbol.got_lookup_offset);
    
    //@if(TODOWASM) @if(self.goal.type == .JitOnly)
    //    @println("fixup_wasm32 %; jit_addr=%; got_lookup_offset=%; shim_addr=%;", symbol.name, symbol.jit_addr, symbol.got_lookup_offset, symbol.shim_addr);
    @match(fixup.type) {
        // used when the value is an immediate 
        fn WasmIndex() => {
            @debug_assert(symbol.segment == .Code);
            @debug_assert(self.goal.type != .JitOnly || symbol.kind == .Local, "wasm jit cannot direct call an import: %", symbol.name);
            @debug_assert_ne(symbol.wasm_module_function_index, -1, "WasmIndex: missing index for $%", symbol.name);
            lst: List(u8) = fixed_list(ptr = u8.ptr_from_raw(fixup.patch_at), len = WASM_INDEX_PADDING);
            value := symbol.wasm_module_function_index.intcast();
            when_debug(self, .Patch, fn(out) => @fmt(out, "# WasmIndex(%) = %\n", symbol.name, value)); self.flush_debug();
            leb128_unsigned(lst&, value, WASM_INDEX_PADDING);
        }
        // used when producing the value on the stack
        fn WasmAddr(it) => {
            lst: List(u8) = fixed_list(ptr = u8.ptr_from_raw(fixup.patch_at), len = WASM_INDEX_PADDING);
            base := @if(self.goal.type == .JitOnly, indirect_index, symbol.offset);
            value := if(symbol.segment == .Code, => indirect_index, => base + it.increment.zext());
            @if(symbol.name != "__franca_base_address"/*HACK*/) 
                @debug_assert_gt(value, 0, "WasmAddr: missing index for $%", symbol.name);
            when_debug(self, .Patch, fn(out) => @fmt(out, "# WasmAddr(%) = %\n", symbol.name, value)); self.flush_debug();
            leb128_signed(lst&, value, WASM_INDEX_PADDING); // NOTE: immediate of T.const is always signed
        }
        fn DataAbsolute(it) => {
            value := if symbol.segment == .Code && symbol.wasm_type_index != -1 {
                symbol.got_lookup_offset
            } else {
                @debug_assert(symbol.kind == .Local);
                // the symbol's offset already includes :WasmZeroPage
                base := @if(self.goal.type == .JitOnly, indirect_index, symbol.offset);
                base + it.increment
            };
            when_debug(self, .Patch, fn(out) => @fmt(out, "# DataAbsolute(%) = %\n", symbol.name, value)); self.flush_debug();
            i64.ptr_from_raw(fixup.patch_at)[] = value;
        }
        @default => @panic("invalid fixup type for wasm %", fixup.type&.tag());
    };
}

fn push_global_index(e: *EmitWasmFn, id: Qbe.Sym) void = @if_else {
    @if(id == e.m.wasm_symbol_stackbase) => e.a&.push(0);
    @if(id == e.m.wasm_symbol_env_parameter) => {
        e.m.wasm_any_pare = true;
        e.a&.push(1);
    };
    @else => panic("unknown symbol for wasm global");  // :OnlyTwoGlobals e.index_patch(id, .Global);
};


// 
// got_lookup_offset
//   the index in the indirect function table. 
//   - AOT: first import is 0, local functions are after all imports. 
//   - JIT: no wasm imports, the first local function is at the old length of the table. 
// wasm_module_function_index
//   real wasm index in the module for direct calls. 
//   first import is 0, local functions are after all imports. 
//   JIT: no imports so it can be assigned early  
//        
//
// The order of segments matters. 
fn output_wasm_module(m: *QbeModule) [][]u8 = {
    default_import_module := "env";
    min_mem_pages := 200; // TODO: set this to something good
    stack_size := 1.shift_left(23); // 8MB
    
    memory_reserved := wasm_page_size;  // :WasmZeroPage
    data_seg := m.segments&[.MutableData]&;
    data_size := data_seg.len() - wasm_page_size;  // :WasmZeroPage
    memory_reserved += data_size;
    
    // TODO: allow these, they're just the same. zeroinit can go at the end so you don't need to include the bytes to init it. 
    @assert_eq(m.segments&[.ConstantData]&.len(), m.goal.max_got_size, "wasm constdata len");
    @assert_eq(m.segments&[.ZeroInitData]&.len(), 0, "wasm bss len");
    
    // TODO: remove
    code_count := m.segments&[.Code]&.len() - m.goal.commands_size_guess;
    @if(show_backend_stats())
    @eprintln(">>> % bytes of code, % bytes of data.", code_count, data_size);
    
    @debug_assert(m.imports.len == 0);
    
    {
        // make sure the zeroth function pointer is invalid. 
        // i could just change the mapping to table indices and put a nullref there, 
        // but also i think it's nicer if you can give your own error message instead of the engine doing it. 
        // (but since the signeture won't match you don't really get that away, so idk) 
        id := m.intern("null");
        m.imports&.push(id);
        use_symbol(m, id) { s |
            s.wasm_type_index = 0;  // don't care
        };
    };
    
    for_symbols m { id, symbol |
        @assert(symbol.got_lookup_offset == -1, "got_lookup_offset %", symbol.name);
        @assert(symbol.wasm_module_function_index == -1, "wasm_module_function_index %", symbol.name);
    };
    
    // import functions ids start at zero. 
    // we need to know how many imports there are before we can start patching anything. 
    for_symbols m { id, symbol | // :SLOW  just keep list of pending symbols instead
        is_import := @is(symbol.kind, .Pending, .DynamicPatched);
        if is_import && (symbol.fixups.len != 0 || symbol.referenced) && symbol.alias == Qbe.no_symbol_S {
            @debug_assert(symbol.segment == .Code, "% segment", symbol.name);
            symbol.got_lookup_offset = m.imports.len;
            symbol.wasm_module_function_index = m.imports.len.intcast();
            m.imports&.push(id); 
            symbol.kind = .DynamicPatched;
        };
    };
    
    // our own function ids come after all the imports, but we already started assigning numbers so apply that offset here. 
    // this is also where we do fixups (ie. to get a function pointer out of a table). 
    i := m.imports.len;
    for m.local_needs_reloc& { id |
        use_symbol(m, id) { symbol | 
            @debug_assert(symbol.segment == .Code && symbol.kind == .Local, "% nonlocal", symbol.name);
            @debug_assert(symbol.got_lookup_offset == -1 && symbol.wasm_module_function_index == -1);
            // TODO: to make got_lookup_offset != wasm_module_function_index, need to fix outputtting the table elem below. 
            symbol.got_lookup_offset = i;
            symbol.wasm_module_function_index = i.intcast();
            m.do_fixups(symbol);
            i += 1;
        };
    };
    
    for m.temporary_alias { target, alias |
        use_symbol(m, target) { t | 
            use_symbol(m, alias) { a | 
                // TODO: make sure it works if the alias is exported but not the target. 
                a.got_lookup_offset = t.got_lookup_offset;
                a.wasm_module_function_index = t.wasm_module_function_index;
                ::import("@/backend/wasm/abi.fr");
                @if(t.wasm_type_index != -1) save_signature(m, alias, t.wasm_type_index);
                @if(a.wasm_type_index != -1) save_signature(m, target, a.wasm_type_index);
                m.do_fixups(a);
            };
        };
    };
    
    // if we were just doing functions we could use `local_needs_reloc` instead of iterating 
    // every symbol but there are also globals in the data segment which need fixups as well. 
    for_symbols m { id, symbol |
        ref := symbol.referenced || symbol.fixups.len != 0;
        new := symbol.segment != .Code || symbol.kind != .Local;
        if ref && new {
            m.do_fixups(symbol);
        } else {
            @debug_assert(symbol.segment != .Code || symbol.got_lookup_offset != -1 || !symbol.referenced, "% unfilled", symbol.name);
        }
    };
    
    // TODO: better error when you run out of commands_size_guess or better just dynamically allocate this 
    //       becaause offsets don't matter (make sure im not relying on pointer stability here but i don't think so).
    //       needed because you run out fast if you want to export every function so you get symbol names. 
    code_segment := m.segments&[.Code]&;
    chunks := list([]u8, temp());
    cursor: List(u8) = fixed_list(ptr = code_segment.mmapped.ptr, len = m.goal.commands_size_guess);  // :UnacceptablePanic
    push_buf :: fn($body: @Fn(buf: *List(u8)) void) void = {
        start := cursor.len;
        @must_return body(cursor&);
        chunks&.push(cursor.items().rest(start)); 
    };
    
    push_buf { buf |
        buf.push_all(Wasm.version_magic); 
    };
    
    f_count := m.local_needs_reloc.len;
    
    {
        push_buf { buf |
            buf.push(@as(u8) Wasm.Section.Type);
            leb128_unsigned(buf, m.wasm_types&.len() + uleb_size(m.wasm_function_type_count.intcast()));
            leb128_unsigned(buf, m.wasm_function_type_count.intcast());
        };
        chunks&.push(m.wasm_types.mmapped.slice(0, m.wasm_types&.len()));
    }; 
    
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Import);
        uleb_patch_delta buf {
            leb128_unsigned(buf, m.imports.len);
            for m.imports& { id |
                use_symbol(m, id) { s |
                    mod, name := m.wasm_name(s);
                    push_qual_name(buf, mod, name);
                    // For now we only do functions
                    push(buf, @as(u8) Wasm.ImportType.Func);
                    @debug_assert_ge(s.wasm_type_index, 0, "unknown type for function $%", s.name);
                    leb128_unsigned(buf, s.wasm_type_index.intcast());
                };
            };
        };
    };
    
    push_buf { buf |
        write_function_section(m, buf);
    };
    
    // TODO: reserve index 0 as null for sanity purposes
    // :OnlyOneTable for indirect function calls
    @if(m.wasm_has_any_indirect_calls)
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Table);
        uleb_patch_delta buf {
            buf.push(1); // length of tables vec
            buf.push(@as(u8) Wasm.ValType.FuncRef); 
            number_of_functions := f_count + m.imports.len;
            // TODO: it seems polite to promise it wont grow for most progams but i can't have a max if the module jits
            //      but it might force me to if i want threads? idk if that rule is only for memories. 
            write_limits(buf, number_of_functions, .None);
        };
    };
    
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Memory);
        uleb_patch_delta buf {
            buf.push(1); // length of memories vec
            write_limits(buf, min_mem_pages, .None);
        };
    };
    
    {
        // :OnlyTwoGlobals [__stackbase, __env_parameter] so don't need patches
        // TODO: this will have to change if we want to allow frontends to create thier own wasm globals. 
    
        // Create a global for the stack pointer
        memory_reserved = align_to(memory_reserved, 16);
        memory_reserved += stack_size;
        stack_base := memory_reserved;  // :WasmZeroPage
        
        push_buf { buf |
            buf.push(@as(u8) Wasm.Section.Global);
            uleb_patch_delta buf {
                push(buf, 1 + int(m.wasm_any_pare).trunc()); // length of globals vector
                
                // __stackbase
                push(buf, @as(u8) Wasm.ValType.I32); // type k_mem
                push(buf, 1); // mutable
                
                // initial value
                push(buf, @as(u8) Wasm.Inst.I32_Const);
                leb128_signed(buf, stack_base);
                push(buf, @as(u8) Wasm.Inst.End);
                
                if m.wasm_any_pare {
                    // __env_parameter
                    push(buf, @as(u8) Wasm.ValType.I64); // type
                    push(buf, 1); // mutable
                    
                    // initial value
                    push(buf, @as(u8) Wasm.Inst.I64_Const);
                    leb128_signed(buf, 0);
                    push(buf, @as(u8) Wasm.Inst.End);
                };
            };
        };
    };
    
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Export);
        uleb_patch_delta buf {
            leb128_unsigned(buf, m.exports.len + 2 + int(m.wasm_has_any_indirect_calls) + int(m.wasm_any_pare));
            for m.exports& { id |
                use_symbol(m, id) { s |
                    @debug_assert_ne(s.wasm_module_function_index, -1, "unassigned got_lookup_offset %", s.name);
                    leb128_unsigned(buf, s.name.len);
                    push_all(buf, s.name);
                    buf.push(@as(u8) Wasm.ImportType.Func);
                    leb128_unsigned(buf, s.wasm_module_function_index.intcast());
                };
            };
            
            // TODO: let the user choose if they want to import a memory or export the memory and what the name should be. 
            {
                name := "memory";
                leb128_unsigned(buf, name.len);
                push_all(buf, name);
                buf.push(@as(u8) Wasm.ImportType.Memory);
                leb128_unsigned(buf, 0);
            };
            
            {
                name := "__stackbase";
                leb128_unsigned(buf, name.len);
                push_all(buf, name);
                buf.push(@as(u8) Wasm.ImportType.Global);
                leb128_unsigned(buf, 0);
            };
            
            if m.wasm_any_pare {
                name := "__env_parameter";
                leb128_unsigned(buf, name.len);
                push_all(buf, name);
                buf.push(@as(u8) Wasm.ImportType.Global);
                leb128_unsigned(buf, 1);
            };
            
            if m.wasm_has_any_indirect_calls {
                name := "__indirect_table";
                leb128_unsigned(buf, name.len);
                push_all(buf, name);
                buf.push(@as(u8) Wasm.ImportType.Table);
                leb128_unsigned(buf, 0);
            };
        };
    };
    
    // START goes here
    
    // :OnlyOneTable 
    // TODO: would be better to assign indexes out of order so we wouldn't need fixups 
    //       and would only need to put functions that are actually referenced indirectly in the table. 
    @if(m.wasm_has_any_indirect_calls)
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Element);
        uleb_patch_delta buf {
            leb128_unsigned(buf, 1);  // length
            
            leb128_unsigned(buf, 0);  // elem tag for active, table 0, funcref
            // offset in table
            push(buf, @as(u8) Wasm.Inst.I32_Const);
            leb128_signed(buf, 0);
            push(buf, @as(u8) Wasm.Inst.End);
            
            n := f_count+m.imports.len;
            leb128_unsigned(buf, n);  // length
            range(0, n) { i |
                leb128_unsigned(buf, i);
            };
        };
    };
    
    {
        padding := m.goal.commands_size_guess;
        total_code_bytes := code_segment.len() - padding;
        push_buf { buf |
            buf.push(@as(u8) Wasm.Section.Code);
            f_count := m.local_needs_reloc.len;
            total_section_bytes := total_code_bytes + uleb_size(f_count);
            leb128_unsigned(buf, total_section_bytes);
            leb128_unsigned(buf, f_count);
        };
        
        chunks&.push(code_segment.mmapped.slice(padding, code_segment.len()));
    };
    
    if data_size > 0 {
        push_buf { buf |
            buf.push(@as(u8) Wasm.Section.Data);
            leb128_unsigned(buf, 1 + 1 + 1 + sleb_size(wasm_page_size) + 1 + uleb_size(data_size) + data_size);
            leb128_unsigned(buf, 1); // number of datas 
            leb128_unsigned(buf, 0); // mode: active
            
            // offset expr
            buf.push(@as(u8) Wasm.Inst.I32_Const);
            leb128_signed(buf, wasm_page_size);  // :wasmzeropage
            buf.push(@as(u8) Wasm.Inst.End);
            
            leb128_unsigned(buf, data_size);
        };
        chunks&.push(data_seg.mmapped.slice(wasm_page_size, data_seg.len()));
    };
    
    total_size := 0;
    for chunks { c |
        total_size += c.len;
    };
    @if(show_backend_stats())
    @eprintln(">>> % bytes total module", total_size); 
    @assert_le(memory_reserved, min_mem_pages * wasm_page_size);
    
    chunks.items()
}

fn write_function_section(m: *Qbe.Module, buf: *List(u8)) void = {
    buf.push(@as(u8) Wasm.Section.Function);
    uleb_patch_delta buf {
        leb128_unsigned(buf, m.local_needs_reloc.len);
        for m.local_needs_reloc& { id |
            use_symbol(m, id) { s |
                @debug_assert_ge(s.wasm_type_index, 0, "unknown type for local function $%", s.name);
                leb128_unsigned(buf, s.wasm_type_index.intcast()); 
            };
        };
    };
}

fn write_limits(buf: *List(u8), min: i64, max: ?i64) void = {
    buf.push(int(max.is_some()).trunc());
    leb128_unsigned(buf, min);
    if max { max |
        leb128_unsigned(buf, max);
    };
}

fn push_qual_name(buf: *List(u8), mod: Str, name: Str) void = {
    leb128_unsigned(buf, mod.len);
    push_all(buf, mod);
    leb128_unsigned(buf, name.len);
    push_all(buf, name);
}

// TODO: this thing with the $ is a hack from before i had SymbolInfo.library. they need to be unified!
//       this is extra not ok because i have symbol names that have dollar signs in them (like readdir$INODE64)
//       ... so i changed it to ^ and it turns out nothing actually uses it. 
//       but at least it gives you the ability to do real namespacing in a hacky way. 
// TODO: use s.library as the module. but that doesn't quite work because you need to allow the same name in multiple modules. 
//       i'm not sure what to do about this. maybe pre-mangling the name is actually the easist
fn wasm_name(m: *Qbe.Module, s: *SymbolInfo) Ty(Str, Str) = {
    sep := index_of(s.name, "^".ascii());
    mod, name := ("", s.name);
    if sep { sep |
        name = s.name.slice(0, sep);
        mod = s.name.rest(sep + 1);
    } else {
        mod = "env"; // TODO: m.goal.default_wasm_import_module;
    };
    (mod, name)
}

//       :WasmJitExportAllFunctions
// note: i rely on all functions being exported. 
//       so indirect calls are going through the same table that gets filled out when calling instantiate_module_jit.
//       you need function pointers to able to move between the modules. 
// TODO: refactor this to remove duplication with the normal output_wasm_module
fn output_wasm_module_jit(m: *QbeModule, indirect_table_base: i64) [][]u8 = {
    @debug_assert(indirect_table_base >= 0, "failed to grow table");
    
    @assert_eq(m.segments&[.ConstantData]&.len(), m.goal.max_got_size, "wasm constdata len");
    @assert_eq(m.segments&[.ZeroInitData]&.len(), 0, "wasm bss len");
    // note: MutableData is fine even though i don't output a data for this module. 
    //       since it imports the whole memory, it can just use the literal pointers. 
    
    @debug_assert_eq(m.exports.len, m.local_needs_reloc.len, "missing exports");
    enumerate m.local_needs_reloc& { i, id |
        use_symbol(m, id[]) { s |
            if s.got_lookup_offset == -1 {
                // if it hasn't already needed a GOT slot, assign one now, because that's the only way i export functions on jit-wasm. 
                s.got_lookup_offset = indirect_table_base + i;
            };
            s.jit_addr = s.got_lookup_offset.rawptr_from_int();
            @debug_assert_eq(s.wasm_module_function_index, i.intcast());
        };
        @debug_assert_eq(m.exports[i].id, id[].id, "export order wrong");
    };
    
    when_debug(m, .Patch) { out |
        @fmt(out, "# Jit: [");
        enumerate m.local_needs_reloc& { i, id |
            use_symbol(m, id[]) { s |
                @fmt(out, "%: %, ", s.jit_addr, s.name);
            };
        };
        @fmt(out, "]\n");
    };
    @debug_assert_eq(m.imports.len, 0, "imports list should be empty when jitting on wasm");
    
    for_symbols m { id, symbol |
        if symbol.export || symbol.referenced || symbol.fixups.len != 0 {
            ::enum(@type symbol.kind);
            if symbol.kind == .Local && symbol.segment == .Code {
                @assert(symbol.wasm_module_function_index != -1, "missing local function $%", symbol.name);
            };
            s := symbol;
            addr := s.jit_addr.int_from_rawptr();
            if s.wasm_type_index != -1 && s.wasm_module_function_index == -1 && addr != 0 {
                if s.got_lookup_offset == -1 {
                    s.got_lookup_offset = addr;
                } else {
                    table_assign(s.got_lookup_offset.intcast(), s.jit_addr.int_from_rawptr().intcast());
                }
            };
            
            m.do_fixups(symbol);
        };
    };
    
    // TODO: better error when you run out of commands_size_guess or better just dynamically allocate this 
    //       becaause offsets don't matter (make sure im not relying on pointer stability here but i don't think so).
    //       needed because you run out fast if you want to export every function so you get symbol names. 
    code_segment := m.segments&[.Code]&;
    chunks := list([]u8, temp());
    cursor: List(u8) = fixed_list(ptr = code_segment.mmapped.ptr, len = m.goal.commands_size_guess);  // :UnacceptablePanic
    push_buf :: fn($body: @Fn(buf: *List(u8)) void) void = {
        start := cursor.len;
        @must_return body(cursor&);
        chunks&.push(cursor.items().rest(start)); 
    };
    
    push_buf { buf |
        buf.push_all(Wasm.version_magic); 
    };
    
    f_count := m.local_needs_reloc.len;
    
    {
        push_buf { buf |
            buf.push(@as(u8) Wasm.Section.Type);
            leb128_unsigned(buf, m.wasm_types&.len() + uleb_size(m.wasm_function_type_count.intcast()));
            leb128_unsigned(buf, m.wasm_function_type_count.intcast());
        };
        chunks&.push(m.wasm_types.mmapped.slice(0, m.wasm_types&.len()));
    }; 
    
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Import);
        uleb_patch_delta buf {
            leb128_unsigned(buf, 3 + int(m.wasm_any_pare));
            
            push_qual_name(buf, "main", "memory");
            push(buf, @as(u8) Wasm.ImportType.Memory);
            write_limits(buf, 0, .None);
            
            push_qual_name(buf, "main", "__indirect_table");
            push(buf, @as(u8) Wasm.ImportType.Table);
            buf.push(@as(u8) Wasm.ValType.FuncRef); 
            write_limits(buf, 0, .None);  // TODO: do these numbers need to match the other one? i think so
            
            push_qual_name(buf, "main", "__stackbase");
            push(buf, @as(u8) Wasm.ImportType.Global);
            push(buf, @as(u8) Wasm.ValType.I32); // type k_mem
            push(buf, 1); // mutable
            
            if m.wasm_any_pare {
                push_qual_name(buf, "main", "__env_parameter");
                push(buf, @as(u8) Wasm.ImportType.Global);
                push(buf, @as(u8) Wasm.ValType.I64); // type
                push(buf, 1); // mutable
            };
        };
    };
    
    push_buf { buf |
        write_function_section(m, buf);
    };
    
    push_buf { buf |
        buf.push(@as(u8) Wasm.Section.Export);
        uleb_patch_delta buf {
            leb128_unsigned(buf, m.exports.len);
            for m.exports& { id |
                use_symbol(m, id) { s |
                    @debug_assert_ne(s.wasm_module_function_index, -1, "unassigned wasm_module_function_index % (jit_addr=%)", s.name, s.jit_addr);
                    leb128_unsigned(buf, s.name.len);
                    push_all(buf, s.name);
                    buf.push(@as(u8) Wasm.ImportType.Func);
                    leb128_unsigned(buf, s.wasm_module_function_index.intcast());
                };
            };
        };
    };
    
    // START goes here
    
    {
        padding := m.goal.commands_size_guess;
        total_code_bytes := code_segment.len() - padding;
        push_buf { buf |
            buf.push(@as(u8) Wasm.Section.Code);
            f_count := m.local_needs_reloc.len;
            total_section_bytes := total_code_bytes + uleb_size(f_count);
            leb128_unsigned(buf, total_section_bytes);
            leb128_unsigned(buf, f_count);
        };
        
        chunks&.push(code_segment.mmapped.slice(padding, code_segment.len()));
    };
    
    
    chunks.items()
}

// the new module imports our [function table, memory, __stack_base, __env_parameter], 
// so function/data imports are just the literal values from the current module, same as native. 
make_exec :: fn(m: *Qbe.Module) void = {
    @if(!TODOWASM) unreachable();
    @if(m.exports.len == 0) return();

    first_export := table_grow(m.exports.len.intcast()).intcast();
    
    module := output_wasm_module_jit(m, first_export);
    module := concat(module&, temp());
    
    when_debug(m, .Disassemble) { out |
        // TODO: use `out`!
        import("@/examples/dump_wasm.fr")'dump(module);
    };
    
    // creates the module and appends the exported functions to our indirect function table. 
    jit_instantiate_module :: fn(address: *u8, length: i64, table_start: i64, table_index: u32) void #weak #import("libc");
    
    @debug_assert_gt(first_export, 0, "failed to grow table");
    jit_instantiate_module(module.ptr, module.len, first_export, 0);

    enumerate m.exports& { i, id |
        use_symbol(m, id[]) { s |
            @debug_assert_ne(s.got_lookup_offset, -1);
            if s.got_lookup_offset != -1 && s.got_lookup_offset != first_export + i {
                table_assign(s.got_lookup_offset.intcast(), intcast(first_export + i));
            };
            s.jit_addr = s.got_lookup_offset.rawptr_from_int();
            
            // forget that this was a local
            s.wasm_module_function_index = -1;
            s.kind = .DynamicPatched;
            
            // TODO: maybe keep DataAbsolute so if it was a shim you can update it, 
            //       but it's wasteful to keep spam filling the same fixup each time you make_exec. 
            s.fixups.len = 0;
        };
    };
    
    for m.temporary_alias { target, alias |
        use_symbol(m, target) { t | 
            use_symbol(m, alias) { a | 
                if a.got_lookup_offset == -1 {
                    a.got_lookup_offset = t.got_lookup_offset;
                };
                table_assign(a.got_lookup_offset.intcast(), t.got_lookup_offset.intcast());
            };
        };
    };
    
    // it's fine to call this multiple times to keep adding jitted code to the module, 
    // just don't re-emit stuff that's already done. 
    // TODO: make sure not to re-fixup these
    m.exports&.clear();
    m.local_needs_reloc&.clear();
    // TODO: could reuse the memory instead
    m.goal.commands_size_guess = m.segments&[.Code]&.len();
};

// This relies on indirect function table being index 0. 
// Grow it by delta and fill the new entries with the zeroth function. 
// Returns the old size of the table. 
table_grow :: AsmFunctionWasmOnly(fn(delta: i32) i32 = (), fn(out) = {
    out.push(0x00);  // 0 locals
    out.push(0x41);  // i32.const(0)
    out.push(0x00);  // ^
    out.push(0x25);  // table.get(0)
    out.push(0x00);  // ^
    out.push(0x20);  // local.get(0)
    out.push(0x00);  // ^
    out.push(0xFC);  // table.grow(0)
    out.push(0x0F);  // ^
    out.push(0x00);  // ^
    out.push(0x0B);  // end
});

// table[dest] = table[src]
table_assign :: AsmFunctionWasmOnly(fn(dest: i32, src: i32) void = (), fn(out) = {
    out.push(0x00);  // 0 locals
    out.push(0x20);  // local.get(0)
    out.push(0x00);  // ^
    out.push(0x20);  // local.get(1)
    out.push(0x01);  // ^
    out.push(0x25);  // table.get(0)
    out.push(0x00);  // ^
    out.push(0x26);  // table.set(0)
    out.push(0x00);  // ^
    out.push(0x0B);  // end
});

//
// After the magic + version, the thing is a list of sections. 
// Each section is [id byte, size u32, data]
// Note that many sections are a vec of something which is encoded as [count u32, data], 
// so you sometimes have two lengths, one in bytes and then one in elements. 
// Also, when the spec says sizes like u32, that doesn't mean encode using that many bytes, 
// you still use the leb128 stuff. The type is just a range restriction. 
//

#use("@/backend/lib.fr");
