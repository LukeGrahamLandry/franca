//!
//! - instructions act on a stack instead of referencing values directly
//! - addresses are 32 bit (but we allow frontends to use Cls.l for consistancy and just truncate here)
//! - phi nodes are converted into block arguments (and we need to remember the types)
//! - convert cfg to structured blocks/loops
//! - can't implicitly use the low bits of an i64 as an i32
//! - stack pointer is stored in a global
//! - cmp always produces an i32 (which we may need to extend)
//! - no instruction for ordered/unordered fcmp
//!

//
// I AM AWARE THIS GENERATES GARBAGE CODE! WIP!
//
// TODO: remove redundant push-pop pairs
//       like when you pop it off, do some stuff, then push it again: you could just use tee and leave it on the stack. 
// TODO: reorder + flip args to create redundant push-pop pairs
// TODO: address offset folding 
// TODO: more tests for dynalloc
// TODO: make it work with elide_abi_slots and simplify_jump_chains
// TODO: `%c =l ceqK %a, %b; jnz %c;` generates `K.eq i64.extend_i32_u i32.wrap_i64 br_if` instead of `K.eq br_if`,
//        probably same problem for address computation that's used immediately. 
//

fn wasm_isel(f: *Qbe.Fn) void = {
    assign_alloc_slots(f);
    
    insert_blocks_for_conditional_phis(f);
    
    // We want to help the compiler on the other end:
    // we know that we don't care about any changes to this global inside our callees (because they'll always reset it),
    // so use a local that doesn't escape so it doesn't have to reload it on every single stack access. 
    stackbase := f.symcon(f.globals.wasm_symbol_stackbase);
    did_sp_load := false; // :UGLY
    f.slot = align_to(f.slot, 16);
    fslot := f.slot;
    need_sp := f.slot > 0 || f.dynalloc || f.varcall;
    if need_sp {
        f.retr = f.newtmp("SP", .Kl);
        f.get_temporary(f.retr)[].nuse = 99; // HACK
    } else {
        f.retr = QbeNull;
    };
    saved_stack := f.retr;
    
    if f.dynalloc {
        saved_stack = f.newtmp("BP", .Kl);
    };
    
    for_blocks f { b | 
        f.reset_scratch();
        
        // Last thing before we jump to another block: push phi values to the stack. 
        @match(b.jmp.type) {
            fn ret0() => {
                if need_sp && !f.leaf {
                    f.emit(.global_set, .Kl, QbeNull, stackbase, QbeNull);
                    f.wasm_push(.Kl, saved_stack);
                };
            }
            fn hlt() => ();
            fn jmp() => {
                for_phi b.s1 { p |
                    arg := p.arg.index(index_in_phi(b, p))[];
                    f.wasm_push(p.cls, arg);
                };
            }
            fn jnz() => {
                @debug_assert(b.s1.phi.is_null() && b.s2.phi.is_null());
                f.wasm_push(.Kw, b.jmp.arg);
                b.jmp.arg = QbeNull;
            }
            @default => @panic("invalid jump type in $% @%", f.name(), b.name());
        };
        @debug_assert(b.jmp.arg == QbeNull);
        
        maybe_sp_load :: fn() => if !did_sp_load && need_sp {
            if !f.leaf {
                if saved_stack != f.retr {
                    f.emitter_for_wasm(.copy, .Kl, saved_stack, f.retr, Qbe.Null);
                };
                
                // TODO: parse "global_setK" same as "argK" so i don't need this hack with null placeholder
                @emit_instructions((f = f, emit = emitter_for_wasm), (stackbase, f.retr, f.getcon(f.slot.intcast()), Qbe.Null), """
                @start
                    %prev_stack =l global_get %0
                    %1 =l sub %prev_stack, %2
                    %3 =l global_set %0, %1
                """);
            } else {
                f.emitter_for_wasm(.global_get, .Kl, f.retr, stackbase, Qbe.Null);
            };
            did_sp_load = true;
        };
        
        for_insts_rev b { i |
            if(i[].op() == .par, => maybe_sp_load());
            sel_wasm(f, i[]);
        };
        
        if(f.start.identical(b), => maybe_sp_load());
        
        // First thing that happens: any incoming phis are on the stack (as block arguments) and we pop them into thier locals. 
        
        for_phi_rev_TODOSLOW b { p |
            f.emit(.pop, p.cls, p.to, QbeNull, QbeNull);
        };
        
        if !identical(f.start, b) {
            f.emit(.nop, .Kw, QbeNull, QbeNull, QbeNull);  // Just to be helpful, insert a slot to put a O.flow:loop without reallocating. 
        };
        f.copy_instructions_from_scratch(b);
    };
    
    // phis are now represented as block arguments, but keep them anyway so emit() can use them to know block types. 
    
    f.retr = QbeNull;
    wasm_flow(f);
    @debug_assert_eq(f.slot, fslot, "can't make new slots after incrementing SP");

    when_debug_printfn(f, .InstSelect, "\n## After instruction selection:\n");
}

// :SLOW
fn for_phi_rev_TODOSLOW(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Phi) void) void = {
    phis := list(*Qbe.Phi, temp());
    for_phi b { p |
        phis&.push(p);
    };
    for_rev phis { p |
        body(p);
    };
}

// returns the number that when encoded in sleb128 will encode this type. 
fn wasm_type_preleb(t: Wasm.ValType) i32 = {
    ::enum(@type t);
    mask :: bit_and(-1, bit_not(1.shift_left(6) - 1));
    x :i64= bit_or(t.raw().zext(), mask);
    @if(::safety_check_enabled(.DebugAssertions)) {
        buf := u8.list(temp());
        leb128_signed(buf&, x);
        @debug_assert_eq(t.raw(), buf[0]);
    };
    x.intcast()
}

fn sel_wasm(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    k := i.cls();
    o := i.op();
    no_kill := @is(o, .par, .sel1, .cas1);
    if !no_kill && try_kill_inst(f, i) {
        if o == .pop {
            // inserted after a call by abi. even tho we don't use the value, 
            // we need to do some stack book-keeping to satisfy the verifier. 
            f.emit(.pop, k, QbeNull, QbeNull, QbeNull);
        };
        return();
    };
    
    @match(o) {
        fn push() => {
            @debug_assert(i.to == QbeNull);
            f.wasm_push(i.cls(), i.arg&[0]);  // fixarg
            return();
        }
        fn call() => {
            @debug_assert(i.to == QbeNull);
            // the way i do jit you don't get any imports. TODO: maybe that's bad and slow, idk. 
            if rtype(i.arg&[0]) != .RCon || (f.globals.goal.type == .JitOnly && could_be_import(f, f.get_constant(i.arg&[0]))) {
                f.emit(.call_indirect, .Kw, QbeNull, QbeNull, i.arg&[1]);
                wasm_push_addr(f, i.arg&[0]);
                return();
            };
            f.emit(i[]);
            return();
        }
        fn loadsw() => if k == .Kw {
            o = .load;
        };
        fn loaduw() => if k == .Kw {
            o = .load;
        };
        fn cod()  => { sel_nan_cmp(f, i, .ceqd, .and, .Kd); return(); };
        fn cuod() => { sel_nan_cmp(f, i, .cned, .or, .Kd); return(); };
        fn cos()  => { sel_nan_cmp(f, i, .ceqs, .and, .Ks); return(); };
        fn cuos() => { sel_nan_cmp(f, i, .cnes, .or, .Ks); return(); };
        fn byteswap() => {
            f.sel_byteswap(i);
            return();
        }
        fn neg() => if k.is_int() {
            o = .sub;
            i.set_op(.sub);
            i.arg&[1] = Qbe.ConZero;
            i.arg&.items().swap(0, 1);
        };
        @default => {
            if is_par(o) || (@is(o, .pop, .dbgloc)) {
                f.emit(i[]);
                return();
            };
        };
    };
    
    if i.to != QbeNull {
        f.wasm_pop(i.cls(), i.to);
    };
    
    // TODO: this is kinda wrong for the ext, it's only if k=kl && k(arg)=kw, but this way works just with two casts instead of one. 
    //       (problem is argcls is Kw but wasm wants i64 input if the output is i64)
    // our cmp can return Kw or Kl, but wasm always returns I32
    needs_ext := iscmp(o) || @is(o, .extsh, .extsb);
    if needs_ext && i.cls() == .Kl {
        i.set_cls(.Kw); k = .Kw;
        f.emit(.extuw, .Kl, QbeNull, QbeNull, QbeNull);
    };
    
    // wasm doesn't have zero extend from 8/16 so it becomes a mask
    if @is(o, .extub, .extuh) {
        mask: i64 = @if(o == .extub, 0x00FF, 0xFFFF);
        f.emit(.and, k, QbeNull, QbeNull, QbeNull);
        f.emit(.push, k, QbeNull, f.getcon(mask), QbeNull);
        f.wasm_push(k, i.arg&[0]);
        return()
    };
    
    if @is(o, .global_get, .global_set) {
        f.emit(o, i.cls(), QbeNull, i.arg&[0], QbeNull);
        if o == .global_set {
            f.wasm_push(argcls(i, 1), i.arg&[1]);
        };
        return()
    };

    if is_alloc(o) {  // dynamic size alloca
        // TODO: align THIS IS NOT FINISHED
        // TODO: this feels about right but i haven't actually thought about it at all and theres only one test that uses it.
        SP := f.retr;
        @debug_assert_ne(SP, QbeNull, "ICE: alloca without loading stack");
        stackbase := f.symcon(f.globals.wasm_symbol_stackbase);
        f.emit(.global_set, .Kl, QbeNull, stackbase, QbeNull);  // save for callees
        f.emit(.push, .Kl, QbeNull, SP, QbeNull);
        f.emit(.push, .Kl, QbeNull, SP, QbeNull);  // result of this inst
        f.emit(.pop, .Kl, SP, QbeNull, QbeNull);  // save locally
        i.set_op(.sub);  // decrement sp
        o = .sub;
        i.arg&[1] = i.arg&[0];
        i.arg&[0] = SP;
    };
    
    if o == .cas0 {
        wasm_push_addr(f, i.arg&[0]);
        return();
    };
    
    // copy is just push+pop, no actual op
    if o != .copy {
        f.emit(o, i.cls(), QbeNull, QbeNull, QbeNull);
    };
    
    if is_store(o) {
        k := argcls(i, 0);
        f.wasm_push(k, i.arg&[0]);
        wasm_push_addr(f, i.arg&[1]);
        return();
    };
    
    if is_load(o) {
        wasm_push_addr(f, i.arg&[0]);
        return();
    };
    
    // flip the order. condition goes last. 
    if o == .sel1 {
        sel0 := i.offset(-1);
        @debug_assert_eq(sel0.op(), .sel0);
        f.wasm_push(.Kw, sel0.arg&[0]);
        sel0.set_nop();
        // TODO: fix if condition was an L
    };
    
    enumerate_rev i.arg&.items() { idx, a |
        if a[] != QbeNull {
            kk := argcls(i, idx);
            if idx == 1 && { (@is(o, .shl, .shr, .sar, .rotl, .rotr)) }  {
                kk = k;
            };
            f.wasm_push(kk, a[]);
        };
    };
}

fn wasm_push(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) void = {
    @match(rtype(r)) {
        fn RTmp() => {
            t := f.get_temporary(r);
            slot_offset := t.slot;
            if slot_offset != -1 {
                return(push_slot_addr(f, k, slot_offset));
            };
            
            if k != t.cls {
                @debug_assert(k.is_int() && t.cls.is_int(), "non-int push wrong cls % %", r, => printfn(f, f.globals.debug_out));
                o: Qbe.O = @if(k == .Kw, .truncl, .extuw);
                f.emit(o, k, QbeNull, QbeNull, QbeNull);
                k = t.cls;
            };
        }
        fn RSlot() => return(push_slot_addr(f, k, r.rsval()));
        @default => ();
    };
    f.emit(.push, k, QbeNull, r, QbeNull);
}

fn push_slot_addr(f: *Qbe.Fn, k: Qbe.Cls, slot_offset: i32) void = {
    // TODO: i think i want to split the memory folding out from coming out of ssa form. 
    //r = SLOT(slot_offset);
    //if k == .Kl {
    //    f.emit(.extuw, k, QbeNull, QbeNull, QbeNull);
    //    k = Qbe.Cls.Kw;
    //};
    //slot_offset = f.slot - slot_offset;
    // Stack grows downwards and __stack_base points to the bottom of our frame. 
    SP := f.retr;
    @debug_assert(SP != QbeNull, "missing SP %", { printfn(f, f.globals.debug_out); "" });
    ins := make_ins(.add, .Kl, QbeNull, SP, f.getcon(slot_offset.intcast()));
    f.sel_wasm(ins&);
}

fn need_trunc(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) bool = {
    k == .Kw && rtype(r) == .RTmp && f.get_temporary(r)[].cls == .Kl
}

fn wasm_pop(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) void = {
    i := f.emit(.pop, k, r, QbeNull, QbeNull);
    
    i0 := i.offset(1);
    if f.len_scratch() != 0 && i0.op() == .push && i0.arg&[0] == r {
        // (a = pop, push a) -> (a = tee)
        i0.set_nop();
        i.set_op(.local_tee);
        
        if f.get_temporary(r)[].nuse <= 1 {
            i.set_nop();
        };
    };
    
    return();
}

// TODO: convert addresses to .Kw earlier so don't have to insert as many .truncl
// TODO: fold imm offset / slots
fn wasm_push_addr(f: *Qbe.Fn, r: Qbe.Ref) void = {
    k := Qbe.Cls.Kw;
    if rtype(r) == .RTmp || rtype(r) == .RSlot {//&& f.get_temporary(r)[].slot == -1 {
        f.emit(.truncl, k, QbeNull, QbeNull, QbeNull);
        k = .Kl;
    };
    f.wasm_push(k, r);
}

// J.jmp: phi nodes can trivially be converted to block arguments. 
// J.jnz: we need to insert extra blocks to express `if cond then goto s1(x, y) else goto s2(z);`
fn insert_blocks_for_conditional_phis(f: *Qbe.Fn) void = {
    new_blocks_list := Qbe.Blk.ptr_from_int(0);
    last_block := f.start;
    for_blocks f { b | 
        continue :: local_return;
        last_block = b;
        if(b.jmp.type != .jnz, => continue());
        
        // TODO: would it be better to use select() instead of inserting extra blocks when the targets have matching signetures? 
        for_jump_targets_mut b { s |
            continue :: local_return;
            if(s.phi.is_null(), => continue());
            
            b1 := newblk();
            name_phi_block(f, b, s[], b1);
            b1.jmp.type = .jmp;
            b1.s1 = s[];
            for_phi s[] { p | 
                n := index_in_phi(b, p);
                p.blk[n] = b1;
            };
            s[] = b1;
            
            // build up a chain to add at the end
            b1.link = new_blocks_list;
            new_blocks_list = b1;
            f.nblk += 1;
        };
    };
    @debug_assert(last_block.link.is_null());
    last_block.link = new_blocks_list;
}

// 
// https://medium.com/leaningtech/solving-the-structured-control-flow-problem-once-and-for-all-5123117b1ee2
//
// for reducible control flow,
// forward edges are a DAG. use that to sort the blocks in topological order. 
// all blocks in a loop must be dominated by the loop header. 
// each loop goes inside a loop-end. backedges become Br. 
// for forward edges that are consecutive topologicallly, do nothing. 
// otherwise insert block-end so the destination is just after the end (so you can Br forward to it). 
// starting point of those blocks doesn't matter but nicer if 
// "just outside the outermost scope that closes between the break and the end of the scope." 
// 
// for more readable output, 
// when sorting, after adding a block to the ordering, visit successors with that block as only forward predecessor first.  
// if a block has only once forward predecessor it can go directly inside a branching statement in that predecessor. 
//
// TODO: deal with irreducible control flow. I think franca code can't produce that but the c compiler will. 
//

// TODO: either actually use if/else or remove them
Flow :: @enum(i64) (end, loop, block, if, else, br, br_if);

// this is the only architecture of which the news has come to harvard.
// there may be many others but they havent been discovered.
fn wasm_flow(f: *Qbe.Fn) void = {
    fillpreds(f);
    fillrpo(f);
    fillloop(f);
    fillrpo(f);
    set_link_from_rpo(f);
    
    // dot a.dot -Tsvg > a.svg && open a.svg
    //@println("digraph{");
    //for_blocks f { b |
    //    for_jump_targets b { s |
    //        @println("\"%  #%  L%\" -> \"%  #%  L%\"", b.name, b.id, b.loop, s.name, s.id, s.loop);
    //    };
    //}
    //@println("}");
    
    ::enum(Flow);
    
    // TODO: push to the previous block instead so you don't have to shift all the instructions 
    fn prepend(b: *Qbe.Blk, i: Qbe.Ins) void = {  // :SLOW
        b.ins&.reserve(1, temp());
        j := 0;
        cond :: fn(i1) => is_par(i1.op()) || (i1.op() == .flow && {
            if i1.arg&[0] == INT(Flow.loop.raw()) {
                true
            } else {
                i1.arg&[1].rsval() > i.arg&[1].rsval()
            }
        });
        while => j < b.ins.len && cond(b.ins.index(j)) {
            j += 1;
        };
        if b.ins.index(j).op() == .nop {
            b.ins[j] = i;
            return();
        };
        b.ins.len += 1;
        b.ins.items().copy_overlapping(j + 1, j, b.ins.len - 1 - j);
        b.ins[j] = i;
    }
    
    fn INT(i: *Qbe.Blk) Qbe.Ref #inline = INT(i.id.intcast());
    fn INT(i: Flow) Qbe.Ref #inline     = INT(i.raw());

    // find the last block that branches backwards to `b`
    end_of_loop :: fn(b: *Qbe.Blk) i32 = {
        eol: i32 = -1;
        for_pred b { p |
            eol = max(eol, p.id);
        };
        eol
    };
    
    Loop :: @struct(entry: i32, end: i32);
    loop_stack := list(Loop, temp());
    range(0, f.nblk.zext()) { i |
        b := f.rpo[i];
        eol := b.end_of_loop();
        if eol >= b.id {
            // This block is the start of a loop 
            prepend(b, make_ins(.flow, .Kw, QbeNull, INT(Flow.loop), INT(b)));
            loop_stack&.push(entry = b.id, end = eol);
        };
        
        invert_cond := b.jmp.type == .jnz && b.s1.id == b.id + 1;
        if invert_cond {
            // we want to fall through as the true branch so will br_if to jump away when the condition is false
            push(b, make_ins(.push, .Kw, QbeNull, QbeConZero, QbeNull));
            push(b, make_ins(.ceqw, .Kw, QbeNull, QbeNull, QbeNull));
            s := b.s1; b.s1 = b.s2; b.s2 = s;
        };
        
        x := -1;
        for_jump_targets b { s |
            continue :: local_return;
            x += 1;
            br := @if(b.jmp.type == .jnz && x == 0, Flow.br_if, Flow.br);
            
            dest := INT(s.id.intcast());
            if s.id <= b.id {
                // back edge / continue
                push(b, make_ins(.flow, .Kw, QbeNull, INT(br), dest));
                continue();
            };
            
            // forward edge / break
            
            if b.id + 1 == s.id {
                @debug_assert_ne(br, .br_if, "% %", b.name(), { printfn(f, f.globals.debug_out); "" });
                continue();
            };
            
            start := (=> {
                break :: local_return;
                // TODO: keep track of blocks as well so we can pull it closer
                for_rev loop_stack&.items() { l |
                    if l.end >= s.id {
                        // src and dest are both inside the loop `l`
                        break f.rpo[l.entry.intcast()];
                    };
                };
                f.start
            })();
            
            block := INT(Flow.block);
            exists := false;
            for_insts_forward start { i |  // TODO: don't iterate the whole thing since we know its at the beginning or not at all :SLOW
                exists = exists || (i.op() == .flow && i.arg&[0] == block && i.arg&[1] == dest);
            };
            if !exists {
                // this is the first time we've branched to dest in this scope
                prepend(start, make_ins(.flow, .Kw, QbeNull, block, dest));
                prepend(s, make_ins(.flow, .Kw, QbeNull, INT(Flow.end.raw()), dest));
            };
            push(b, make_ins(.flow, .Kw, QbeNull, INT(br), dest));
        };
        
        // - only take from the end of the stack so nesting works out. 
        // - check id so only end a loop once there are no more jumps to it. 
        while => loop_stack.len > 0 && loop_stack[loop_stack.len - 1].end <= b.id {
            // insert the end of a loop
            l := loop_stack&.pop().unwrap();
            push(b, make_ins(.flow, .Kw, QbeNull, INT(Flow.end), INT(l.entry.intcast()))); 
        };
    };
    @debug_assert_eq(loop_stack.len, 0, "unfinished loops!");
    
    when_debug(f, .InstSelect, fn(out) => dump_flow_cfg(f, out));
}

// several operations that all interesting cpus have but we have to pretend don't exist 
// and just hope the other side notices the pattern. waste of everyones time. 
// TODO: i wonder if there's a blessed pattern you're supposed to use to make sure they fix it. 
//       "idiom recognition" is the magic phrase. 

// use rotations to swap groups of 4 and then swap groups of 2.
// same constants work for both widths because i truncate on emit.
fn sel_byteswap(f: *Qbe.Fn, i: *Qbe.Ins) void = @emit_instructions((f = f, emit = emitter_for_wasm), (i.cls(), i.arg&[0], i.to), """
@start
    %mask     =0 copy 0xFFFF0000FFFF0000
    %mask_odd =0 xor %mask, -1
    %even     =0 and %1, %mask
    %odd      =0 and %1, %mask_odd
    %even_l   =0 rotl %even, 16
    %odd_r    =0 rotr %odd, 16
    %x        =0 or %even_l, %odd_r
    
    %Mask     =0 copy 0x00FF00FF00FF00FF
    %Mask_odd =0 xor %Mask, -1
    %Even     =0 and %x, %Mask
    %Odd      =0 and %x, %Mask_odd
    %Even_l   =0 rotl %Even, 8
    %Odd_r    =0 rotr %Odd, 8
    %2        =0 or %Even_l, %Odd_r
""");

// o: !(nan|nan), uo: (nan|nan)
fn sel_nan_cmp(f: *Qbe.Fn, i: *Qbe.Ins, cmp: Qbe.O, join: Qbe.O, k: Qbe.Cls) void = {
    f.emit(.pop, i.cls(), i.to, QbeNull, QbeNull);
    if i.cls() == .Kl {
        f.emit(.extuw, .Kl, QbeNull, QbeNull, QbeNull);
    };
    
    f.emit(join, .Kw, QbeNull, QbeNull, QbeNull);
    f.emit(cmp, .Kw, QbeNull, QbeNull, QbeNull);
    f.wasm_push(k, i.arg&[1]);
    f.wasm_push(k, i.arg&[1]);
    f.emit(cmp, .Kw, QbeNull, QbeNull, QbeNull);
    f.wasm_push(k, i.arg&[0]);
    f.wasm_push(k, i.arg&[0]);
}

emitter_for_wasm :: fn(f: *Qbe.Fn, o: Qbe.O, k: Qbe.Cls, r: Qbe.Ref, a0: Qbe.Ref, a1: Qbe.Ref) void = {
    i := make_ins(o, k, r, a0, a1);
    chuse(a0, 1, f);
    chuse(a1, 1, f);
    sel_wasm(f, i&);
};

#use("@/backend/lib.fr");
