//!
//! - instructions act on a stack instead of referencing values directly
//! - addresses are 32 bit (but we allow frontends to use Cls.l for consistancy and just truncate here)
//! - phi nodes are converted into block arguments (and we need to remember the types)
//! - can't implicitly use the low bits of an i64 as an i32
//! - stack pointer is stored in a global
//! - cmp always produces an i32 (which we may need to extend)
//! - no instruction for ordered/unordered fcmp
//!

// TODO: remove redundant push-pop pairs
// TODO: reorder + flip args to create redundant push-pop pairs
// TODO: if a local is used again but also by the next instruction, use local.tee

fn wasm_isel(f: *Qbe.Fn) void = {
    assign_alloc_slots(f);
    
    insert_blocks_for_conditional_phis(f);
    
    // We want to help the compiler on the other end:
    // we know that we don't care about any changes to this global inside our callees (because they'll always reset it),
    // so use a local that doesn't escape so it doesn't have to reload it on every single stack access. 
    stackbase := f.symcon(f.globals.wasm_symbol_stackbase);
    did_sp_load := false; // :UGLY
    if f.slot > 0 {
        f.retr = f.newtmp("SP", .Kw);
        f.get_temporary(f.retr)[].nuse = 99; // HACK
    } else {
        f.retr = QbeNull;
    };
    
    for_blocks f { b | 
        f.reset_scratch();
        
        // Last thing before we jump to another block: push phi values to the stack. 
        @match(b.jmp.type) {
            fn ret0() => {
                if f.slot > 0 && !f.leaf {
                    f.emit(.global_set, .Kw, QbeNull, stackbase, QbeNull);
                    f.wasm_push(.Kw, f.retr);
                };
            }
            fn hlt()  => ();
            fn jmp() => {
                for_phi b.s1 { p |
                    arg := p.arg.index(index_in_phi(b, p))[];
                    f.emit(.push, p.cls, QbeNull, arg, QbeNull);
                };
            }
            fn jnz() => {
                @debug_assert(b.s1.phi.is_null() && b.s2.phi.is_null());
                f.emit(.push, .Kw, QbeNull, b.jmp.arg, QbeNull);
                b.jmp.arg = QbeNull;
            }
            @default => @panic("invalid jump type in $% @%", f.name(), b.name());
        };
        @debug_assert(b.jmp.arg == QbeNull);
        
        do_sp_load :: fn() => if f.slot > 0 {
            f.emit(.pop, .Kw, f.retr, QbeNull, QbeNull);
            f.emit(.global_get, .Kw, QbeNull, stackbase, QbeNull);
            did_sp_load = true;
        };
        
        for_insts_rev b { i |
            if !did_sp_load && i[].op() == .par {
                do_sp_load();
            };
            sel_wasm(f, i[]);
        };
        
        if !did_sp_load && f.start.identical(b) {
            do_sp_load();
        };
        
        // First thing that happens: any incoming phis are on the stack (as block arguments) and we pop them into thier locals. 
        
        for_phi_rev_TODOSLOW b { p |
            f.emit(.pop, p.cls, p.to, QbeNull, QbeNull);
        };
        
        if !identical(f.start, b) {
            f.emit(.nop, .Kw, QbeNull, QbeNull, QbeNull);  // Just to be helpful, insert a slot to put a O.flow:loop without reallocating. 
        };
        f.copy_instructions_from_scratch(b);
    };
    
    // phis are now represented as block arguments, but keep them anyway so emit() can use them to know block types. 
    
    f.retr = QbeNull;
    wasm_flow(f);

    when_debug(f, .InstSelect) { out | 
        write(out, "\n> After instruction selection:\n");
        printfn(f, out);
    }
}

// :SLOW
fn for_phi_rev_TODOSLOW(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Phi) void) void = {
    phis := list(*Qbe.Phi, temp());
    for_phi b { p |
        phis&.push(p);
    };
    for_rev phis { p |
        body(p);
    };
}

// returns the number that when encoded in sleb128 will encode this type. 
fn wasm_type_preleb(t: Wasm.ValType) i32 = {
    ::enum(@type t);
    mask :: bit_and(-1, bit_not(1.shift_left(6) - 1));
    x :i64= bit_or(t.raw().zext(), mask);
    { // :SLOW. TODO: can be removed when i trust. 
        buf := u8.list(temp());
        leb128_signed(buf&, x);
        @debug_assert_eq(t.raw(), buf[0]);
    };
    x.intcast()
}

fn sel_wasm(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    k := i.cls();
    is_pop := i.op() == .pop;
    if try_kill_inst(f, i) {
        if is_pop {
            // inserted after a call by abi. even tho we don't use the value, 
            // we need to do some stack book-keeping to satisfy the verifier. 
            f.emit(.pop, k, QbeNull, QbeNull, QbeNull);
        };
        return();
    };
    
    @match(i.op()) {
        fn push() => {
            @debug_assert(i.to == QbeNull);
            f.wasm_push(i.cls(), i.arg&[0]);  // fixarg
            return();
        }
        fn call() => {
            @debug_assert(i.to == QbeNull);
            if rtype(i.arg&[0]) != .RCon {
                f.emit(.call_indirect, .Kw, QbeNull, QbeNull, i.arg&[1]);
                wasm_push_addr(f, i.arg&[0]);
                return();
            };
            f.emit(i[]);
            return();
        }
        fn loadsw() => if i.cls() == .Kw {
            i.set_op(.load);
        };
        fn cod()  => panic("TODO: wasm fcmp (un?)ordered");
        fn cuod() => panic("TODO: wasm fcmp (un?)ordered");
        fn cos()  => panic("TODO: wasm fcmp (un?)ordered");
        fn cuos() => panic("TODO: wasm fcmp (un?)ordered");
        fn byteswap() => {
            f.sel_byteswap(i);
            return();
        }
        @default => {
            if is_par(i.op()) || (@is(i.op(), .pop)) {  // TODO: call_indirect
                f.emit(i[]);
                return();
            };
        };
    };
    
    if i.to != QbeNull {
        f.wasm_pop(i.cls(), i.to);
    };

    // our cmp can return Kw or Kl, but wasm always returns I32
    if iscmp(i.op()) && i.cls() == .Kl {
        i.set_cls(.Kw);
        f.emit(.extuw, .Kl, QbeNull, QbeNull, QbeNull);
    };
    
    g := @is(i.op(), .global_get, .global_set);
    if g {
        f.emit(i.op(), i.cls(), QbeNull, i.arg&[0], QbeNull);
    } else {
        // copy is just push+pop, no actual op
        if i.op() != .copy {
            f.emit(i.op(), i.cls(), QbeNull, QbeNull, QbeNull);
        };
    };
    
    if is_store(i.op()) {
        k := argcls(i, 0);
        f.wasm_push(k, i.arg&[0]);
        wasm_push_addr(f, i.arg&[1]);
        return();
    };
    
    if is_load(i.op()) {
        wasm_push_addr(f, i.arg&[0]);
        return();
    };
    
    enumerate_rev i.arg&.items() { idx, a |
        if a[] != QbeNull && !(g && idx == 0) {
            k := argcls(i, idx);
            f.wasm_push(k, a[]);
        };
    };
}

fn wasm_push(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) void = {
    if rtype(r) == .RTmp {
        slot_offset := f.get_temporary(r)[].slot;
        if slot_offset != -1 {
            // TODO: i think i want to split the memory folding out from coming out of ssa form. 
            //r = SLOT(slot_offset);
            if k == .Kl {
                f.emit(.extuw, k, QbeNull, QbeNull, QbeNull);
                k = Qbe.Cls.Kw;
            };
            slot_offset = f.slot - slot_offset;
            // Stack grows downwards and __stack_base points to the bottom of our frame. 
            SP := f.retr;
            @debug_assert(SP != QbeNull);
            ins := make_ins(.add, k, QbeNull, SP, f.getcon(slot_offset.intcast()));
            f.sel_wasm(ins&);
            return();
        };
    };
    f.emit(.push, k, QbeNull, r, QbeNull);
}

fn wasm_pop(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) void = {
    f.emit(.pop, k, r, QbeNull, QbeNull);
}

// TODO: convert addresses to .Kw earlier so don't have to insert as many .truncl
// TODO: fold imm offset / slots
fn wasm_push_addr(f: *Qbe.Fn, r: Qbe.Ref) void = {
    k := Qbe.Cls.Kw;
    if rtype(r) == .RTmp && f.get_temporary(r)[].slot == -1 {
        f.emit(.truncl, k, QbeNull, QbeNull, QbeNull);
        k = .Kl;
    };
    f.wasm_push(k, r);
}

// J.jmp: phi nodes can trivially be converted to block arguments. 
// J.jnz: we need to insert extra blocks to express `if cond then goto s1(x, y) else goto s2(z);`
fn insert_blocks_for_conditional_phis(f: *Qbe.Fn) void = {
    new_blocks_list := Qbe.Blk.ptr_from_int(0);
    last_block := f.start;
    for_blocks f { b | 
        continue :: local_return;
        last_block = b;
        if(b.jmp.type != .jnz, => continue());
        
        // TODO: would it be better to use select() instead of inserting extra blocks when the targets have matching signetures? 
        for_jump_targets_mut b { s |
            continue :: local_return;
            if(s.phi.is_null(), => continue());
            
            b1 := newblk();
            name_phi_block(f, b, s[], b1);
            b1.jmp.type = .jmp;
            b1.s1 = s[];
            for_phi s[] { p | 
                n := index_in_phi(b, p);
                p.blk[n] = b1;
            };
            s[] = b1;
            
            // build up a chain to add at the end
            b1.link = new_blocks_list;
            new_blocks_list = b1;
            f.nblk += 1;
        };
    };
    @debug_assert(last_block.link.is_null());
    last_block.link = new_blocks_list;
}

// 
// https://medium.com/leaningtech/solving-the-structured-control-flow-problem-once-and-for-all-5123117b1ee2
//
// for reducible control flow,
// forward edges are a DAG. use that to sort the blocks in topological order. 
// all blocks in a loop must be dominated by the loop header. 
// each loop goes inside a loop-end. backedges become Br. 
// for forward edges that are consecutive topologicallly, do nothing. 
// otherwise insert block-end so the destination is just after the end (so you can Br forward to it). 
// starting point of those blocks doesn't matter but nicer if 
// "just outside the outermost scope that closes between the break and the end of the scope." 
// 
// for more readable output, 
// when sorting, after adding a block to the ordering, visit successors with that block as only forward predecessor first.  
// if a block has only once forward predecessor it can go directly inside a branching statement in that predecessor. 
//
// TODO: deal with irreducible control flow. I think franca code can't produce that but the c compiler will. 
//

// TODO: either actually use if/else or remove them
Flow :: @enum(i64) (end, loop, block, if, else, br, br_if);

fn wasm_flow(f: *Qbe.Fn) void = {
    fillpreds(f);
    fillrpo(f);
    set_link_from_rpo(f);
    
    ::enum(Flow);
    
    // TODO: push to the previous block instead so you don't have to shift all the instructions 
    fn prepend(b: *Qbe.Blk, i: Qbe.Ins) void = {  // :SLOW
        b.ins&.grow(b.nins.zext() + 1);
        j := 0;
        cond :: fn() => (b.ins.index(j).op() == .flow && (b.ins[j].arg&[0] == INT(Flow.loop.raw()) || b.ins[j].arg&[1].rsval() > i.arg&[1].rsval()));
        while => j < b.nins.zext() && (is_par(b.ins.index(j).op()) || cond()) {
            j += 1;
        };
        if b.ins.index(j).op() == .nop {
            b.ins[j] = i;
            return();
        };
        b.ins.slice(0, b.nins.zext() + 1).copy_overlapping(j + 1, j, b.nins.zext() - j);
        b.ins[j] = i;
        b.nins += 1;
    }
    
    for_blocks f { b |
        b.visit = -1;
    };
    
    fn INT(i: *Qbe.Blk) Qbe.Ref #inline = INT(i.id.intcast());
    fn INT(i: Flow) Qbe.Ref #inline     = INT(i.raw());

    end_of_loop :: fn(b: *Qbe.Blk) i32 = {
        eol: i32 = -1;
        for_pred b { p |
            eol = max(eol, p.id);
        };
        eol
    };
    
    Loop :: @struct(entry: i32, end: i32);
    loop_stack := list(Loop, temp());
    range(0, f.nblk.zext()) { i |
        b := f.rpo[i];
        eol := b.end_of_loop();
        if eol >= b.id {
            // This block is the start of a loop 
            prepend(b, make_ins(.flow, .Kw, QbeNull, INT(Flow.loop), INT(b)));
            loop_stack&.push(entry = b.id, end = eol);
            s := f.rpo[eol.intcast()]&;
            @debug_assert_eq(s.visit, @as(i32) -1, "ICE: hoping that you're only ever the end of one loop");
            s.visit = b.id;
        };
        
        invert_cond := b.jmp.type == .jnz && b.s1.id == b.id + 1;
        if invert_cond {
            // we want to fall through as the true branch so will br_if to jump away when the condition is false
            push(b, make_ins(.push, .Kw, QbeNull, QbeConZero, QbeNull));
            push(b, make_ins(.ceqw, .Kw, QbeNull, QbeNull, QbeNull));
            s := b.s1; b.s1 = b.s2; b.s2 = s;
        };
        
        x := -1;
        for_jump_targets b { s |
            continue :: local_return;
            x += 1;
            br := @if(b.jmp.type == .jnz && x == 0, Flow.br_if, Flow.br);
            
            dest := INT(s.id.intcast());
            if s.id <= b.id {
                // back edge / continue
                push(b, make_ins(.flow, .Kw, QbeNull, INT(br), dest));
                continue();
            };
            
            // forward edge / break
            
            if b.id + 1 == s.id {
                @debug_assert_ne(br, .br_if, "% %", b.name(), { printfn(f, f.globals.debug_out); "" });
                continue();
            };
            
            start := (=> {
                break :: local_return;
                // TODO: keep track of blocks as well so we can pull it closer
                for_rev loop_stack&.items() { l |
                    if l.end >= s.id {
                        // src and dest are both inside the loop `l`
                        break f.rpo[l.entry.intcast()];
                    };
                };
                f.start
            })();
            
            block := INT(Flow.block);
            exists := false;
            for_insts_forward start { i |  // TODO: don't iterate the whole thing since we know its at the beginning or not at all :SLOW
                exists = exists || (i.op() == .flow && i.arg&[0] == block && i.arg&[1] == dest);
            };
            if !exists {
                // this is the first time we've branched to dest in this scope
                prepend(start, make_ins(.flow, .Kw, QbeNull, block, dest));
                prepend(s, make_ins(.flow, .Kw, QbeNull, INT(Flow.end.raw()), dest));
            };
            push(b, make_ins(.flow, .Kw, QbeNull, INT(br), dest));
        };
        
        if b.visit != -1 {
            // insert the end of a loop
            push(b, make_ins(.flow, .Kw, QbeNull, INT(Flow.end), INT(b.visit.intcast())));
            l := loop_stack&.pop().unwrap();
            @debug_assert_eq(l.end, b.id, "ICE: bad loop nesting");
            @debug_assert_eq(l.entry, b.visit, "ICE: bad loop nesting");
        };
    };
}

emitter_for_wasm :: fn(f: *Qbe.Fn, o: Qbe.O, k: Qbe.Cls, r: Qbe.Ref, a0: Qbe.Ref, a1: Qbe.Ref) void = {
    i := make_ins(o, k, r, a0, a1);
    sel_wasm(f, i&);
};

// i hate this so much
fn sel_byteswap(f: *Qbe.Fn, i: *Qbe.Ins) void = @emit_instructions((f = f, emit = emitter_for_wasm), (i.cls(), i.arg&[0], i.to), """
@start
    %mask     =0 copy 18446462603027742720 # 0xFFFF0000FFFF0000
    %mask_odd =0 xor %mask, -1
    %even     =0 and %1, %mask
    %odd      =0 and %1, %mask_odd
    %even_l   =0 rotl %even, 16
    %odd_r    =0 rotr %odd, 16
    %x        =0 or %even_l, %odd_r
    
    %Mask     =0 copy 71777214294589695 # 0x00FF00FF00FF00FF
    %Mask_odd =0 xor %Mask, -1
    %Even     =0 and %x, %Mask
    %Odd      =0 and %x, %Mask_odd
    %Even_l   =0 rotl %Even, 8
    %Odd_r    =0 rotr %Odd, 8
    %2        =0 or %Even_l, %Odd_r
""");
