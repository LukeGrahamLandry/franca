//!
//! - instructions act on a stack instead of referencing values directly
//! - addresses are 32 bit (but we allow frontends to use Cls.l for consistancy and just truncate here)
//! - phi nodes are converted into block arguments (and we need to remember the types)
//! - can't implicitly use the low bits of an i64 as an i32
//! - stack pointer is stored in a global
//! - cmp always produces an i32 (which we may need to extend)
//! - no instruction for ordered/unordered fcmp
//!

// TODO: remove redundant push-pop pairs
// TODO: reorder + flip args to create redundant push-pop pairs
// TODO: if a local is used again but also by the next instruction, use local.tee

fn wasm_isel(f: *Qbe.Fn) void = {
    assign_alloc_slots(f);
    
    insert_blocks_for_conditional_phis(f);
    
    // We want to help the compiler on the other end:
    // we know that we don't care about any changes to this global inside our callees (because they'll always reset it),
    // so use a local that doesn't escape so it doesn't have to reload it on every single stack access. 
    stackbase := f.symcon(f.globals.wasm_symbol_stackbase);
    did_sp_load := false; // :UGLY
    if f.slot > 0 {
        f.retr = f.newtmp("SP", .Kw);
        f.get_temporary(f.retr)[].nuse = 99; // HACK
    } else {
        f.retr = QbeNull;
    };
    
    for_blocks f { b | 
        f.reset_scratch();
        
        b.wasm_type = wasm_block_type_from_phis(f.globals, b);
        
        // Last thing before we jump to another block: push phi values to the stack. 
        @match(b.jmp.type) {
            fn ret0() => {
                if f.slot > 0 && !f.leaf {
                    f.emit(.global_set, .Kw, QbeNull, stackbase, QbeNull);
                    f.wasm_push(.Kw, f.retr);
                };
            }
            fn hlt()  => ();
            fn jmp() => {
                // TODO: backwards?
                for_phi b.s1 { p |
                    arg := p.arg.index(index_in_phi(b, p))[];
                    f.emit(.push, p.cls, QbeNull, arg, QbeNull);
                };
            }
            fn jnz() => {
                @debug_assert(b.s1.phi.is_null() && b.s2.phi.is_null());
                f.emit(.push, .Kw, QbeNull, b.jmp.arg, QbeNull);
                b.jmp.arg = QbeNull;
            }
            @default => @panic("invalid jump type in $% @%", f.name(), b.name());
        };
        @debug_assert(b.jmp.arg == QbeNull);
        
        do_sp_load :: fn() => if f.slot > 0 {
            f.emit(.pop, .Kw, f.retr, QbeNull, QbeNull);
            f.emit(.global_get, .Kw, QbeNull, stackbase, QbeNull);
            did_sp_load = true;
        };
        
        for_insts_rev b { i |
            if !did_sp_load && i[].op() == .par {
                do_sp_load();
            };
            sel_wasm(f, i[]);
        };
        
        if !did_sp_load && f.start.identical(b) {
            do_sp_load();
        };
        
        // First thing that happens: any incoming phis are on the stack (as block arguments) and we pop them into thier locals. 
        
        for_phi_rev_TODOSLOW b { p |
            f.emit(.pop, p.cls, p.to, QbeNull, QbeNull);
        };
        
        f.copy_instructions_from_scratch(b);
    };
    
    // emit() looks at the first `b.loop = block_arg_count` pops to know block type.  
    for_blocks f { b | 
        b.phi = zeroed(@type b.phi);
    };
    f.retr = QbeNull;

    when_debug(f, .InstSelect) { out | 
        write(out, "\n> After instruction selection:\n");
        printfn(f, out);
    }
}

// :SLOW
fn for_phi_rev_TODOSLOW(b: *Qbe.Blk, $body: @Fn(p: *Qbe.Phi) void) void = {
    phis := list(*Qbe.Phi, temp());
    for_phi b { p |
        phis&.push(p);
    };
    for_rev phis { p |
        body(p);
    };
}

// We need a type annotation for the block's affect on the stack: (incoming phis) -> (outgoing phis)
fn wasm_block_type_from_phis(m: *QbeModule, b: *Qbe.Blk) i32 = {
    has_inputs  := !b.phi.is_null();
    has_outputs := b.jmp.type == .jmp && !b.s1.phi.is_null();
    has_multiple_outputs := has_outputs && !b.s1.phi.link.is_null();
    
    if !has_inputs {
        //if(!has_outputs,          => return(wasm_type_preleb(.EmptyBlock)));                 // () -> ()
        //if(!has_multiple_outputs, => return(wasm_type_preleb(to_wasm_type(b.s1.phi.cls))));  // () -> (k)
        return(wasm_type_preleb(.EmptyBlock))
    };
    // Otherwise it's an index into the type table. 
    
    pack_wasm_result_type m { $yield_arg #duplicated |
        for_phi_rev_TODOSLOW b { p |
            yield_arg(p.cls);
        };
    } and { $yield_ret |
        //for_phi b.s1 { p |
        //    yield_ret(p.cls);
        //};
    }
}

// returns the number that when encoded in sleb128 will encode this type. 
fn wasm_type_preleb(t: Wasm.ValType) i32 = {
    ::enum(@type t);
    mask :: bit_and(-1, bit_not(1.shift_left(6) - 1));
    x :i64= bit_or(t.raw().zext(), mask);
    { // :SLOW. TODO: can be removed when i trust. 
        buf := u8.list(temp());
        leb128_signed(buf&, x);
        @debug_assert_eq(t.raw(), buf[0]);
    };
    x.intcast()
}

fn sel_wasm(f: *Qbe.Fn, i: *Qbe.Ins) void = {
    if(try_kill_inst(f, i), => return());
    
    @match(i.op()) {
        fn push() => {
            @debug_assert(i.to == QbeNull);
            f.wasm_push(i.cls(), i.arg&[0]);  // fixarg
            return();
        }
        fn call() => {
            @debug_assert(i.to == QbeNull);
            if rtype(i.arg&[0]) != .RCon {
                f.emit(.call_indirect, .Kw, QbeNull, QbeNull, i.arg&[1]);
                wasm_push_addr(f, i.arg&[0]);
                return();
            };
            f.emit(i[]);
            return();
        }
        fn loadsw() => if i.cls() == .Kw {
            i.set_op(.load);
        };
        fn cod()  => panic("TODO: wasm fcmp (un?)ordered");
        fn cuod() => panic("TODO: wasm fcmp (un?)ordered");
        fn cos()  => panic("TODO: wasm fcmp (un?)ordered");
        fn cuos() => panic("TODO: wasm fcmp (un?)ordered");
        @default => {
            if is_par(i.op()) || (@is(i.op(), .pop)) {  // TODO: call_indirect
                f.emit(i[]);
                return();
            };
        };
    };
    
    if i.to != QbeNull {
        f.wasm_pop(i.cls(), i.to);
    };

    // our cmp can return Kw or Kl, but wasm always returns I32
    if iscmp(i.op()) && i.cls() == .Kl {
        i.set_cls(.Kw);
        f.emit(.extuw, .Kl, QbeNull, QbeNull, QbeNull);
    };
    
    g := @is(i.op(), .global_get, .global_set);
    if g {
        f.emit(i.op(), i.cls(), QbeNull, i.arg&[0], QbeNull);
    } else {
        // copy is just push+pop, no actual op
        if i.op() != .copy {
            f.emit(i.op(), i.cls(), QbeNull, QbeNull, QbeNull);
        };
    };
    
    if is_store(i.op()) {
        k := argcls(i, 0);
        f.wasm_push(k, i.arg&[0]);
        wasm_push_addr(f, i.arg&[1]);
        return();
    };
    
    if is_load(i.op()) {
        wasm_push_addr(f, i.arg&[0]);
        return();
    };
    
    enumerate_rev i.arg&.items() { idx, a |
        if a[] != QbeNull && !(g && idx == 0) {
            k := argcls(i, idx);
            f.wasm_push(k, a[]);
        };
    };
}

fn wasm_push(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) void = {
    if rtype(r) == .RTmp {
        slot_offset := f.get_temporary(r)[].slot;
        if slot_offset != -1 {
            // TODO: i think i want to split the memory folding out from coming out of ssa form. 
            //r = SLOT(slot_offset);
            if k == .Kl {
                f.emit(.extuw, k, QbeNull, QbeNull, QbeNull);
                k = Qbe.Cls.Kw;
            };
            slot_offset = f.slot - slot_offset;
            // Stack grows downwards and __stack_base points to the bottom of our frame. 
            SP := f.retr;
            @debug_assert(SP != QbeNull);
            ins := make_ins(.add, k, QbeNull, SP, f.getcon(slot_offset.intcast()));
            f.sel_wasm(ins&);
            return();
        };
    };
    f.emit(.push, k, QbeNull, r, QbeNull);
}

fn wasm_pop(f: *Qbe.Fn, k: Qbe.Cls, r: Qbe.Ref) void = {
    f.emit(.pop, k, r, QbeNull, QbeNull);
}

// TODO: convert addresses to .Kw earlier so don't have to insert as many .truncl
// TODO: fold imm offset / slots
fn wasm_push_addr(f: *Qbe.Fn, r: Qbe.Ref) void = {
    k := Qbe.Cls.Kw;
    if rtype(r) == .RTmp && f.get_temporary(r)[].slot == -1 {
        f.emit(.truncl, k, QbeNull, QbeNull, QbeNull);
        k = .Kl;
    };
    f.wasm_push(k, r);
}

// J.jmp: phi nodes can trivially be converted to block arguments. 
// J.jnz: we need to insert extra blocks to express `if cond then goto s1(x, y) else goto s2(z);`
fn insert_blocks_for_conditional_phis(f: *Qbe.Fn) void = {
    new_blocks_list := Qbe.Blk.ptr_from_int(0);
    last_block := f.start;
    for_blocks f { b | 
        continue :: local_return;
        last_block = b;
        if(b.jmp.type != .jnz, => continue());
        
        // TODO: would it be better to use select() instead of inserting extra blocks when the targets have matching signetures? 
        for_jump_targets_mut b { s |
            continue :: local_return;
            if(s.phi.is_null(), => continue());
            
            b1 := newblk();
            name_phi_block(b, s[], b1);
            b1.jmp.type = .jmp;
            b1.s1 = s[];
            for_phi s[] { p | 
                n := index_in_phi(b, p);
                p.blk[n] = b1;
            };
            s[] = b1;
            
            // build up a chain to add at the end
            b1.link = new_blocks_list;
            new_blocks_list = b1;
            f.nblk += 1;
        };
    };
    @debug_assert(last_block.link.is_null());
    last_block.link = new_blocks_list;
}
