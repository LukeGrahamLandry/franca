//! 
//! Mach-O is the object file format used by the XNU kernal (so macOS and iOS). 
//! There are 3 types of Mach-O files we care about. 
//! Executable: a program the operating system can run
//!     - dynamic imports and pointers in data are in ChainedFixups
//!     - empty __PAGEZERO, __TEXT contains the Mach-O header and load commands, __LINKEDIT must have a load command
//!     - must have a CodeSigneture (sha256 of each appended to the file)
//!     - segments must be page aligned 
//! Relocatable Object: input for static linking with another program
//!     - relocations listed for each section (instead of ChainedFixups)
//! Dynamic Library: 
//!     - exports trie lists your functions that people can call
//!     - needs a LC_ID_DYLIB
//!     - otherwise same as Exe
//! 

// TODO: run non-driver tests with no-op relocatable as well as direct to exe
// TODO: generate some tests with big numbers and ranges of relocations, etc to make sure we don't overflow the fields (or at least give an error). 
// TODO: add to isel to only produce addresses one in a block to have fewer relocations at the cost of register pressure. 
// TODO: make this less ugly. !! please
// TODO: fix `ld: warning: no platform load command found in ` for relocatable.  
// TODO: emit the local symbols even for exe so you can see them in a debugger. 
//       do other debug info stuff. 
// TODO: i haven't checked if .Dynamic needs zero_page

#include_std("backend/macho/exports.fr");
#include_std("backend/macho/sha256.fr");

WipSegment :: @struct(
    name: Str,
    prot: u32,
    virtual_size := 0,
    file_size := 0,
    sections: []WipSection = empty(),
    extra_file_offset := 0, // HACK __TEXT doesn't include the headers for a relocatable objdect but it does for an exe. 
);

WipSection :: @struct(
    name: Str,
    flag: u32,
    offset_in_segment: i64,
    size: i64,
    reserved2: u32 = 0,
);

text_padding :: SIZE_OF_STUBS + COMMANDS_SIZE_GUESS;

// I guess this is to make sure you segfault on low addresses. 
// But it can't guarentee my base address is zero right? so what's the point? clearly im missing something. 
// TODO: probably don't do this if we're making a linkable .o file. 
zero_page_size :: 0x100000000;

// results are in temp()
fn output_macho(m: *QbeModule) List([]u8) = {
    // TODO: remove
    code_count := m.segments&[.Code]&.len() - COMMANDS_SIZE_GUESS - SIZE_OF_STUBS;
    data_count := m.segments&[.MutableData]&.len();
    @eprintln(">>> % bytes of code, % bytes of data.", code_count, data_count);

    m.fill_from_libc(); // TODO: don't waste time doing this just to bump reloc state. 
    alignment := if(m.goal.type == .Relocatable, => 8, => macos_page_size);
    for_enum SegmentType { s | 
        //m.align_to(s, 8);  // forgeting this manifested as getting fucked on __LINKEDIT (when reading not writing because you're writting into a new allocation)
        // but do we need to do this? it makes it look more like clang does. 
        m.align_to(s, alignment);
    };
    //m.align_to(.Code, macos_page_size);
    m.fixups_locked = true; // we're going to stomp the data section with relocation info so you can run it jitted anymore after this. 
    chunks := list([]u8, 10, temp());
    chunks&.push(empty()); // this used to be for commands but now thats part of .Code which is stupid. :commandsispartofcodenow
    
    e: EmitMacho = (
        m = m, 
        commands = (maybe_uninit = (ptr = m.segment_address(.Code, 0), len = COMMANDS_SIZE_GUESS), len = 0, gpa = page_allocator), // :cantreallocatebutgivegooderrormessage
    );

    cpu_type, cpu_subtype := @if(m.goal.arch == .aarch64, (0x0100000C, 0x00000000), (0x01000007, 0x00000003));
    macho_header := e.commands&.reserve_type(MachoHeader);
    macho_header[] = (
        cpu_type = cpu_type,
        cpu_subtype = cpu_subtype,
        file_type = @match(m.goal.type) {
            fn Exe() => .Executable;
            fn Relocatable() => .Object;
            fn Dynamic() => .Dynamic;
            @default => unreachable();
        },
        commands_count = 0, 
        commands_size = 0, 
        flags = 0b00000000001000000000000010000101, // ¯\_(ツ)_/¯ 
    );
    if m.goal.type == .Relocatable {
        macho_header.flags = 0;  // dont claim no_undefs
    };
    
    ::enum_basic(LoadCommand);
    
    segments := WipSegment.list(6, temp());
    if m.goal.type == .Exe || m.goal.type == .Dynamic {
        // TODO: waste. dead zone Code[stubs_cursor..SIZE_OF_STUBS]
        chunks&.push(live_segment_part(m, .Code));  // :MySegmentsStartAt1 :CodeIsFirst
        chunks&.push(live_segment_part(m, .ConstantData));  // TODO: waste. dead zone ConstantData[got_cursor..MAX_GOT_SIZE]
        chunks&.push(live_segment_part(m, .MutableData));
        
        segments&.push(prot = 5, name = "__TEXT");
        segments&.push(prot = 3, name = "__DATA_CONST");
        segments&.push(prot = 3, name = "__DATA");
        enumerate segments& { i, s |
            s.virtual_size = m.segments&[@as(SegmentType) i].mmapped.len;
            s.file_size = chunks[i + 1].len;
        };
        segments&.insert(0, (prot = 0, name = "__PAGEZERO", virtual_size = zero_page_size)); 
        
        code_flag := ATTR_PURE_INSTRUCTIONS.bit_or(ATTR_SOME_INSTRUCTIONS);
        xx: WipSection = (name = "__text", flag = code_flag, offset_in_segment = COMMANDS_SIZE_GUESS + SIZE_OF_STUBS, size = chunks[1].len - SIZE_OF_STUBS - COMMANDS_SIZE_GUESS);
        // apparently you're supposed to do reserved2 = 12 is the size of each stub (adrp, ldr, br). 
        // the os doesn't care but you need it if you want to do `objdump --macho --indirect-symbols`
        // size_of_one_stub: u32 = @if(m.goal.arch, 12, 6); //TODO: this needs to be a compile error :FUCKED
        // ... but this still doesn't work. maybe i need to put shit in the other symbol table (that i only do for relocatable rn)? just the thing for imports is not enough.
        size_of_one_stub: u32 = @if(m.goal.arch == .aarch64, 12, 6); 
        yy: WipSection = (name = "__stubs", size = SIZE_OF_STUBS, flag = code_flag.bit_or(S_SYMBOL_STUBS), offset_in_segment = COMMANDS_SIZE_GUESS, reserved2 = size_of_one_stub);
        segments[1].sections = @slice(xx, yy);
        // TODO: do i need reserved1 set to something? maybe thats an offset into LC_SYMTAB? or maybe undefined_symbol_count, that seems more consistant in the binraries i checked. 
        xx: WipSection = (name = "__got", size = chunks[2].len, flag = S_NON_LAZY_SYMBOL_POINTERS, offset_in_segment = 0);
        segments[2].sections = @slice(xx);
        xx: WipSection = (name = "__data", size = chunks[3].len, flag = 0, offset_in_segment = 0);
        segments[3].sections = @slice(xx);

        // TODO: S_THREAD_LOCAL_VARIABLES
    };
    if m.goal.type == .Relocatable {
        s := m.segments&[.ConstantData]&;
        align_to :: fn(offset: i64, align: i64) i64 = {
            extra := offset.mod(align);
            if extra == 0 {
                offset
            } else {
                offset + align - extra
            }
        };

        @assert(ptr_diff(s.mmapped.ptr, s.next) == MAX_GOT_SIZE, "TODO: const data segment");
        
        chunks&.push(live_segment_part(m, .Code));
        segments&.push(prot = 7, name = "__TEXT", extra_file_offset = text_padding);
        segments[0].virtual_size = m.segments&[.Code].mmapped.len;
        segments[0].file_size = chunks[1].len - text_padding;
        code_flag := ATTR_PURE_INSTRUCTIONS.bit_or(ATTR_SOME_INSTRUCTIONS);
        xx: WipSection = (name = "__text", flag = code_flag, offset_in_segment = 0, size = chunks[1].len - text_padding);
        segments[0].sections = @slice(xx);
        s := m.segments&[.MutableData]&;
        has_mutable_data := ptr_diff(s.mmapped.ptr, s.next) != 0;
        if has_mutable_data {
            chunks&.push(live_segment_part(m, .MutableData));
            segments&.push(prot = 7, name = "__DATA");
            segments[1].virtual_size = m.segments&[.MutableData].mmapped.len;
            segments[1].file_size = chunks[2].len;
            xx: WipSection = (name = "__data", flag = 0, offset_in_segment = 0, size = chunks[2].len);
            segments[1].sections = @slice(xx);
        };
    };
    
    base_file_offset := @slice(0, 0, 0, 0); 
    e.virtual_segment_offset = @slice(0, 0, 0, 0);
    e.relocations = temp().alloc_zeroed(List(u8), SegmentType.enum_count() + 4);
    e.load_commands = temp().alloc(*SegmentSection, SegmentType.enum_count() + 4);
    n_sections := 0;
    {
        enumerate segments& { segment_index, s | 
            start := e.commands.len;
            h := e.commands&.reserve_type(LoadCommandHeader);
            h.type = LoadCommand.SegmentLoad.raw();
            e.command_count += 1;
            cmd := e.commands&.reserve_type(SegmentLoad);
            cmd[] = SegmentLoad.zeroed();
            // you do need this tho or you crash in dyld`dyld4::fixupPage64(void*, mwl_info_hdr const*, dyld_chained_starts_in_segment const*, unsigned int, bool) 
            if s.name == "__DATA_CONST" {
                cmd.flag = 16; // TODO: why :HACK
            };
            cmd.max_prot = s.prot;
            cmd.init_prot = s.prot;
            cmd.name = str_to_16_byte_value(s.name);
            cmd.address = e.virtual_offset.bitcast();
            cmd.address_size = if(s.file_size != 0, => s.file_size.bitcast(), => s.virtual_size.bitcast());
            e.virtual_segment_offset[segment_index] = e.virtual_offset;
            e.file_offset += s.extra_file_offset;
            cmd.file_offset = if(s.file_size != 0, => e.file_offset.bitcast(), => 0);
            base_file_offset[segment_index] = cmd.file_offset.bitcast();
            cmd.size = s.file_size.bitcast(); 
            cmd.section_count = s.sections.len.trunc();
            each s.sections { sec | 
                c  := e.commands&.reserve_type(SegmentSection);
                c[] = SegmentSection.zeroed();
                c.segment_name = cmd.name;
                c.section_name = str_to_16_byte_value(sec.name);
                c.address      = cmd.address + sec.offset_in_segment.bitcast();
                c.file_offset  = cmd.file_offset.trunc() + sec.offset_in_segment.trunc(); // :overflow
                c.alignment    = 3; // TODO
                c.flag         = sec.flag;
                c.size         = sec.size.bitcast();
                c._reserved2   = sec.reserved2;
                @assert(n_sections < e.load_commands.len, "TODO: too many sections");
                e.load_commands[n_sections] = c;
                e.relocations[n_sections] = list(temp());
                n_sections += 1;
            };
            h.size = trunc(e.commands.len - start);
            e.virtual_offset += s.virtual_size;
            e.file_offset += s.file_size;
        };
    };
    
    if e.m.goal.type == .Exe {
        macho_load_dylinker(e.commands&, e.command_count&, "/usr/lib/dyld");
        
        // Ask it to call us!
        cmd := load_command(e&, .MainEntryPoint, MainEntryPoint);
        symbol := m.get_symbol_info(m.intern("main"));
        assert(symbol.kind == .Local, "no exported main function?");
        // offset is from the start of interesting virtual memory? 
        cmd[] = (entry_offset = symbol.offset.bitcast(), stack_size = 0);  // :CodeIsFirst its fine to change this, you just have to do the math here. 
        @debug_assert_ge(symbol.offset, COMMANDS_SIZE_GUESS + SIZE_OF_STUBS);
    };
    if e.m.goal.type == .Dynamic {
        // LC_ID_DYLIB, it seems the name doesn't matter? 
        macho_load_dylib(e.commands&, e.command_count&, .LinkLibrarySelf, "foo");
    };
    chained := u8.list(CHAINED_SIZE_GUESS, temp());
    
    // layout is [commands, waste, sections, chained, relocations?]
    // only relinkable needs relocations and only loadable needs a signeture at the end of chained. 
    
    m := e.m;
    // note: virtual_segment_offset is only good when MachoExe because we skip segments for MachoRelocatable. 
    if e.m.goal.type == .Exe || m.goal.type == .Dynamic {
        // TODO: don't assume you want libc. but on macos you always do. 
        macho_load_dylib(e.commands&, e.command_count&, .LinkLibrary, "/usr/lib/libSystem.B.dylib");
        
        emit_exe_fixups(e&, chained&);
        zero_pad_len_to_align(chained&, 16);
        
        // this is a bit painful because the thing you're signing includes the offset + size of the signeture itself. 
        // so we need to reserve this space up front and fill it in later. 
        
        // We want this to be the last thing in the file so the signeture can go at the end? is this the rules? 
        
        load_codesign := load_command(e&, .CodeSigneture, LinkEditBlob);
        
        codesign_ident := "";  // It seems this can be whatever you want since we're not doing a real signeture. I've tried "foo"
        //
        // WARNING WARNING WARNING
        // if you forget to align_to here so you're missing the last hash, it fully crashes the operating system (macos 14.6.1)
        //
        page_count := ((e.file_offset + chained.len).align_to(codesign_page_size) / codesign_page_size);
        signeture_size := SuperBlob.size_of() + CodeDirectory.size_of() + (32 * page_count) + codesign_ident.len + 1;
        chained&.reserve(signeture_size);
        codesign_output_bytes := chained.maybe_uninit.slice(chained.len, chained.len + signeture_size);
        load_codesign.offset = trunc(e.file_offset + chained.len);
        load_codesign.size = codesign_output_bytes.len.trunc();
        
        chunks&.push(chained.items());
        
        chained.len += codesign_output_bytes.len;
        
        // You are required to tell it to load __LINKEDIT even tho we don't use it for anything directly. 
        load_command(e&, .SegmentLoad, SegmentLoad)[] = (
            name = str_to_16_byte_value("__LINKEDIT"),
            address_size = chained.len.bitcast(),
            size = chained.len.bitcast(),
            max_prot = 1,
            init_prot = 1,
            file_offset = e.file_offset.bitcast(),
            address = e.virtual_offset.bitcast(),
            section_count = 0,
            flag = 0,
        );
        
        e.file_offset += chained.len;
        seal_load_commands(e&, macho_header);
        
        write_adhoc_signature(
            exec_seg_base = base_file_offset[1],
            exec_seg_limit = segments[0].file_size,
            file_size = e.file_offset - signeture_size, 
            input_chunks = chunks.items(),
            dylib = m.goal.type == .Dynamic,
            ident = codesign_ident,
            output_buf = codesign_output_bytes,
        );
        
        @println(">>> repro: %", sha256_hex(codesign_output_bytes));
        
        chunks&.push(codesign_output_bytes);
    } else {
        @debug_assert(e.m.goal.type == .Relocatable);
        emit_reloc_symbols(e&, chained&);
        
        seal_load_commands(e&, macho_header);
            
        e.file_offset += chained.len;
        chunks&.push(chained.items());
        
        enumerate e.relocations { i, r | 
            if r.len != 0 {
                e.load_commands[i].relocations_count = trunc(r.len / size_of_one_relocation);
                e.load_commands[i].relocations_file_offset = e.file_offset.trunc();
                chunks&.push(r.items());
                e.file_offset += r.len;
            }
        };
    };
    
    @assert_le(chained.len, CHAINED_SIZE_GUESS, "resized and now the header pointer is junk"); 
    @debug_assert_eq(e.commands.len, COMMANDS_SIZE_GUESS);
    chunks
}

fn seal_load_commands(e: *EmitMacho, macho_header: *MachoHeader) void = {
    macho_header.commands_size = (e.commands.len - MachoHeader.size_of()).trunc();
    macho_header.commands_count = e.command_count.trunc();
    e.commands&.zero_pad_to_align(macos_page_size);
    // :cantreallocatebutgivegooderrormessage
    // now we want exact match because we put it on the front of the text section for some reason who knows idk man. 
    @assert_eq(e.commands.len, COMMANDS_SIZE_GUESS, "we guessed wrong so the array resized and all the pointers are junk");
}

fn load_command(e: *EmitMacho, cmd: LoadCommand, $T: Type) *T #generic = {
    start := e.commands.len;
    h := e.commands&.reserve_type(LoadCommandHeader);
    h.type = cmd.raw();
    e.command_count += 1;
    cmd := e.commands&.reserve_type(T);
    h.size = trunc(e.commands.len - start);
    cmd
}

EmitMacho :: @struct(
    m: *QbeModule, 
    commands: List(u8), 
    command_count := 0, 
    file_offset := 0, 
    relocations: []List(u8) = empty(), 
    load_commands: []*SegmentSection = empty(), 
    virtual_segment_offset: []i64 = empty(), 
    virtual_offset := 0,
    offset_to_csis: []u32 = empty(),
    csis_by_segment: []*ChainedStartsInSegment = empty(),
    starts_offset := 0,
);

CHAINED_SIZE_GUESS :: 2000000;  // TODO: prescan to caluclate?

fn emit_exe_fixups(e: *EmitMacho, chained: *List(u8)) void #inline = {
    m := e.m;
    chain_header := chained.reserve_type(ChainedFixupsHeader);
    
    fixups, symbols := collect_aot_fixups(m);
    @debug_assert_ge(fixups.len, e.m.got&.len() / 8, "at least each __got entry needs a fixup");
    sort :: quicksort(FixP, fn(a, b) => ptr_diff(a.fix.patch_at, b.fix.patch_at) >= 0);
    sort(fixups);
    // at this point, `fixups` is in the order of the patch targets in virtual memory which is the order we need to emit the fixup chains. 
    import_count := 0;
    for symbols { s |
        if s.kind != .Local {
            s.got_lookup_offset = import_count;
            import_count += 1;
        };
    };
    // imports are fixed size. so we can reserve that early
    imports_len_b := import_count * 4;
    imports_offset := chained.len; // TODO: is this right?
    chained.reserve(imports_len_b);
    chained.len += imports_len_b;
    symbols_offset := chained.len; // TODO: is this right?
    imports_bytes := chained.items().slice(chained.len - imports_len_b, chained.len);
    imports_bytes: []u32 = (ptr = ptr_cast_unchecked(u8, u32, imports_bytes.ptr), len = import_count);
    for symbols { s |
        if s.kind != .Local {
            n: u32 = trunc(chained.len - symbols_offset);
            imports_bytes[s.got_lookup_offset] = encode_import(lib_ordinal = 1, weak = false, name_offset = n); // TODO: don't assume libc
            mangled := items(@format("_%", s.name) temp()); // :slow don't do an extra copy
            append_padded_cstr(chained, mangled, 1);
        };
    };
    zero_pad_to_align(chained, 4);
    
    //
    // Now we have to do the fixup chain page starts. 
    // This is how we tell it where it has to do relocations. 
    //
    e.starts_offset = chained.len;
    // TODO: uncomment the real number when ready
    seg_count :: 2 + enum_count(SegmentType);  // __zeropage, <the rest>, __linkedit
    chained.reserve_type(u32)[] = seg_count.trunc();
    // indexed by segment index
    e.offset_to_csis = (ptr = ptr_cast_unchecked(u8, u32, chained.maybe_uninit.ptr.offset(chained.len)), len = seg_count); // from chained[starts_offset]
    e.csis_by_segment = temp().alloc_zeroed(*ChainedStartsInSegment, seg_count); // :last2bytesalias
    range(0, seg_count) { i |
        o := chained.reserve_type(u32);
        o[] = unprocessed_sentinal;
    };
    
    chained.zero_pad_to_align(8);
    
    prev_fixup: ?WipFixup = .None;
    each fixups { fix |
        emit_one_exe_fixup(e, fix, prev_fixup&, import_count, chained);
    };
    
    // Flush the final entry of the final segment. 
    if prev_fixup& { prev | 
        emplace_fixup(e, prev.fix, 0, import_count);
        prev_fixup = .None;
    };

    enumerate e.offset_to_csis { i, it |
        @debug_assert(u32.int_from_ptr(it) == u8.int_from_ptr(chained.maybe_uninit.ptr) + e.starts_offset + 4 + i * 4, "miscounted offset_to_csis");
    };
    
    @debug_assert(e.offset_to_csis[0] == 0 && e.offset_to_csis[e.offset_to_csis.len - 1] == 0, "__PAGEZERO and __LINKEDIT shouldn't have fixups.");
    
    chained.zero_pad_to_align(8);
    @assert_le(chained.len, CHAINED_SIZE_GUESS, "resized and now the header pointer is junk");
    chain_header[] = (
        imports_count = import_count.trunc(), 
        imports_format = 1, 
        symbols_format = 0, 
        version = 0, 
        starts_offset = e.starts_offset.trunc(), 
        imports_offset = imports_offset.trunc(), 
        symbols_offset = symbols_offset.trunc(),
    );
    chained_size := chained.len;
    
    // this must include the part before symbol table or llvm-objcopy complains bout them overlapping
    cmd := load_command(e, .ChainedFixups, LinkEditBlob);
    // TODO: using file_offset way down here feels a bit fragile. 
    cmd[] = (offset = e.file_offset.trunc(), size = chained_size.trunc());  // :CodeIsFirst its fine to change this, you just have to do the math here. 

    if e.m.goal.type == .Dynamic {
        new_file_offset := e.file_offset + chained_size;
        // Now we need to tell it about our exports
        // TODO: do i need to give it linkeditsymboltable? rn i only do that for relocatable objects. 
        
        trie: ExportsTrie.T = init();
        for e.m.exports { export_id | 
            s := e.m.get_symbol_info(export_id);
            @debug_assert(s.kind == .Local, "can only export local symbols");
            trie&.put(@tfmt("_%", s.name), s.offset);
        };
        trie&.write(chained);
        
        cmd := load_command(e, .ExportsTrie, LinkEditBlob);
        cmd[] = (offset = new_file_offset.trunc(), size = (chained.len - chained_size).trunc());  // :CodeIsFirst its fine to change this, you just have to do the math here. 
    };
}

WipFixup :: @struct(
    page_index: i64,
    offset_in_page: i64,
    fix: *FixP,
);

unprocessed_sentinal :: 0; // TODO: scope this

fn emit_one_exe_fixup(e: *EmitMacho, fix: *FixP, prev_fixup: *?WipFixup, import_count: i64, chained: *List(u8)) void #inline = {
    m := e.m;
    @debug_assert(fix.fix.type&.is(.DataAbsolute), "expected fixup for .Exe to be .DataAbsolute");
    seg, off_in_segment := compiler_address_to_segment_offset(m, fix.fix.patch_at);
    segment_index := seg.raw() + 1;  // :MySegmentsStartAt1
    page_index, offset_in_page := off_in_segment.div_mod(macos_page_size);
    if e.offset_to_csis[segment_index] == unprocessed_sentinal.trunc() {
        // This is our first time seeing this segment. 
        
        // Flush the last entry of the previous segment. 
        if prev_fixup { prev | 
            emplace_fixup(e, prev.fix, 0, import_count);
            prev_fixup[] = .None;
        };
        
        start_of_csis := chained.len;
        csis_off := chained.len - e.starts_offset;
        @debug_assert_ne(csis_off, 0);
        csis := chained.reserve_type(ChainedStartsInSegment);  // one of these per segment.  
        chained.len -= 2; // take it back now yall :last2bytesalias
        csis[] = ChainedStartsInSegment.zeroed();
        e.csis_by_segment[segment_index] = csis;
    
        // offset to the first page in this segment. 
        // not a file offset! its virtual offset from first interesting segment. 
        // coincententially those are the same number for binaries made by clang...
        csis.segment_offset = e.virtual_segment_offset[segment_index].bitcast() - zero_page_size;
        
        e.offset_to_csis[segment_index] = csis_off.trunc();
        
        csis.page_count = 1;
        range(0, page_index) { _ |
            // TODO: a test that gets here
            csis.page_count += 1;  // :calcsizeatend
            chained.reserve_type(u16)[] = page_start_no_fixups;
        };
        chained.reserve_type(u16)[] = offset_in_page.trunc(); // :calcsizeatend
        
        csis.pointer_format = 6; // who knows man, i just work here. TODO
        csis.page_size = macos_page_size;
        csis.max_valid_pointer = 0; // TODO: what does this mean?
        csis.size = trunc(chained.len - start_of_csis); // :calcsizeatend
    };
    if prev_fixup { prev | 
        // will always be the same segment as before or we would have flushed the fixup above. 
        next_offset := if(prev.page_index == page_index, => (offset_in_page - prev.offset_in_page) / 4) {
            // we've moved on to a different page in the same section. 
            // so add another entry to the csis.page_start array.
            while => prev.page_index + 1 != page_index {    
                prev.page_index += 1;
                e.csis_by_segment[segment_index].page_count += 1;
                e.csis_by_segment[segment_index].size += 2;
                chained.reserve_type(u16)[] = page_start_no_fixups;
            };
            e.csis_by_segment[segment_index].page_count += 1;
            e.csis_by_segment[segment_index].size += 2;
            chained.reserve_type(u16)[] = offset_in_page.trunc();
            0
        };
        emplace_fixup(e, prev.fix, next_offset, import_count);
        prev_fixup[] = .None;
    };
    prev_fixup[] = (Some = (page_index = page_index, offset_in_page = offset_in_page, fix = fix));
}

fn emplace_fixup(e: *EmitMacho, fix: *FixP, next_encoded: i64, import_count: i64) void #inline = {
    @debug_assert_ge(next_encoded, 0, "fixups cant go backwards. we sorted them so this can't happen.");
    encoded := encode_chained(
        next = next_encoded.trunc(),
        reserved = 0,
        payload = @if(fix.symbol.kind == .Local, {
            dest_segment := fix.symbol.segment.raw() + 1; // :MySegmentsStartAt1
            target := e.virtual_segment_offset[dest_segment] - zero_page_size + fix.symbol.offset;
            @assert_lt(target, 1.shift_left(36), "rebase > 36 bits");
            (Rebase = (high8 = 0, target = target))
        }, {
            // got_lookup_offset has been converted to an ordinal already. 
            o := fix.symbol.got_lookup_offset;
            @debug_assert(o >= 0 && o < import_count, "bad ordinal %", o);
            (Bind = (addend = 0, ordinal = o.trunc()))
        })
    );
    ref := ptr_cast_unchecked(u8, i64, fix.fix.patch_at);
    ref[] = encoded;
};
    
fn emit_reloc_symbols(e: *EmitMacho, chained: *List(u8)) void #inline = {
    m := e.m;
    cmd := load_command(e, .LinkEditSymbolInfo, LinkEditSymbolInfo);
    cmd[] = LinkEditSymbolInfo.zeroed();
    // TODO: output local symbols for debugging (in exes as well). 
    cmd.local_symbol_count = trunc(1 + e.m.local_needs_reloc.len); // I think you always need index 0 to be junk.
    cmd.external_symbol_index = cmd.local_symbol_index + cmd.local_symbol_count;  // for consistancy
    cmd.external_symbol_count = e.m.exports.len.trunc();
    cmd.undefined_symbol_index = cmd.external_symbol_index + cmd.external_symbol_count; // must be set even if count is zero or lld complains 
    cmd.undefined_symbol_count = e.m.imports.len.trunc();
    // TODO: we are going to need undefined symbols for imports but haven't got that far yet. 
    
    syms_cmd := load_command(e, .LinkEditSymbolTable, LinkEditSymbolTable);
    syms_cmd[] = LinkEditSymbolTable.zeroed();
    syms_cmd.symbol_count = cmd.external_symbol_count + cmd.local_symbol_count + cmd.undefined_symbol_count;
    // the data for those goes in __LINKEDIT
    // TODO: using file_offset way down here feels a bit fragile. 
    syms_cmd.symbols_offset = trunc(e.file_offset + chained.len);
    size_of_symbols: i64 = syms_cmd.symbol_count.zext() * size_of(SymbolEntry);
    syms_cmd.strings_offset = syms_cmd.symbols_offset + trunc(size_of_symbols);
    
    symbols_bytes: []SymbolEntry = (ptr = ptr_cast_unchecked(u8, SymbolEntry, chained.maybe_uninit.ptr.offset(chained.len)), len = syms_cmd.symbol_count.zext());
    chained.len += size_of_symbols; @debug_assert(chained.len <= chained.maybe_uninit.len);
    strings_start := chained.len;
    symbols_bytes.set_zeroed();
    symbols_bytes[0].section_number = 1; // junk
    symbols_bytes[0].symbol_type    = 14; // junk
    symbols_bytes[0].name_offset = trunc(chained.len - strings_start);
    append_padded_cstr(chained, "_junk", 1);
    
    // TODO: deal with things being both local_needs_reloc and export
    
    base := 1;
    enumerate e.m.local_needs_reloc { i, id | 
        s := e.m.get_symbol_info(id[]);
        entry := symbols_bytes.index(i + base);
        entry.symbol_type = 14;  // 14 means defined in section.
        // note: 1 indexed. 
        entry.section_number = which_section(s.segment);
        entry.desc = 0; // i think exports don't use this.     for imports, 256 ordinal 1.
        entry.name_offset = trunc(chained.len - strings_start);
        segment_padding := if(s.segment == .Code, => text_padding, => 0); // :HACK
        @debug_assert(s.offset >= segment_padding, "what are you smoking?");
        // note: symbol_address is the address in virtual memory. not the section offset. 
        // this math relies on there being one section per segment
        start: i64 = bitcast(e.load_commands[entry.section_number.zext() - 1].address);
        entry.symbol_address = bitcast(s.offset - segment_padding + start);
        ugh := items(@format("_%", s.name) temp());
        append_padded_cstr(chained, ugh, 1);
        append_relocations(i + base, id[], m, e.relocations, e.load_commands);
        // TODO: assert they're all DataAbsolute becuse functions are in the same segment and you could just call them. 
    };
    base += e.m.local_needs_reloc.len;
    enumerate e.m.exports { i, id |
        s := e.m.get_symbol_info(id[]);
        entry := symbols_bytes.index(i + base);
        // this only works for exports!
        entry.symbol_type = 15;  // bottom bit means external. 14 means defined in section.
        // note: 1 indexed. 
        entry.section_number = 1;   @assert(s.segment == .Code, "TODO: export data");
        entry.desc = 0; // i think exports don't use this.     for imports, 256 ordinal 1.
        entry.name_offset = trunc(chained.len - strings_start);
        @debug_assert(s.offset >= text_padding, "what are you smoking");
        entry.symbol_address = bitcast(s.offset - text_padding);      @assert(s.segment == .Code, "TODO: export data");
        ugh := items(@format("_%", s.name) temp());
        append_padded_cstr(chained, ugh, 1);
        // TODO: we probably don't want it to appear twice in the symbol table if it's a data export and also has local relocations
    };
    base += e.m.exports.len;
    enumerate e.m.imports { i, id |
        s := e.m.get_symbol_info(id[]);
        entry := symbols_bytes.index(i + base);
        entry.symbol_type = 1; // undef ext
        entry.section_number = symbol_no_section; 
        entry.desc = 0; // TODO: for dynamic imports, 256 is ordinal 1.
        entry.name_offset = trunc(chained.len - strings_start);
        ugh := items(@format("_%", s.name) temp());
        append_padded_cstr(chained, ugh, 1);
        append_relocations(i + base, id[], e.m, e.relocations, e.load_commands);
    };
    base += e.m.imports.len;
    syms_cmd.strings_size = trunc(chained.len - strings_start);
}

fn which_section(s: SegmentType) u8 = @match(s) {
    fn Code() => 1;
    fn MutableData() => 2;
    fn ConstantData() => panic("TODO: we don't use constant data yet");
};

fn append_relocations(symbol_table_index: i64, id: u32, m: *QbeModule, relocations: []List(u8), load_commands: []*SegmentSection) void = {
    s := m.get_symbol_info(id);
    each s.fixups { fix | 
        continue :: local_return;
        patch_segment, patch_segment_offset := compiler_address_to_segment_offset(m, fix.patch_at);
        if patch_segment == .ConstantData && patch_segment_offset < m.got&.len() {
            // :track_got_reloc
            // We bound this symbol for jitting and reserved a __got slot for it.
            // However, we want to let static linker move stuff around and pack its own __got.
            // So we skip this relocation and instead emit one for each place we accessed that __got slot. 
            //
            @debug_assert(fix.type&.tag() == .DataAbsolute && s.kind != .Local, "__got reloc must be data");
            continue();
        };
        sec: i64 = which_section(patch_segment).zext();
        rs := relocations[sec - 1]&;
        segment_padding := if(patch_segment == .Code, => text_padding, => 0); // :HACK
        r: RelocationFields = (
            offset = patch_segment_offset - segment_padding, 
            symbol = trunc(symbol_table_index),
            length = 2,   // one instruction
            type = 0,  // patch
            extern = true, //s.kind != .Local,
            relative = true,  // patch
        );
        
        @match(fix.type) {
            fn InReg(it) => {
                @debug_assert(it.increment == 0, "TODO: offset symbols"); // :ArmAddendReloc
                
                // in the instruction we tell it: which register to use
                inst: []u32 = (ptr = ptr_cast_unchecked(u8, u32, fix.patch_at), len = 2);
                inst[0] = adrp(0, 0, @as(u5) it.r);
                inst[1] = add_im(Bits.X64, @as(u5) it.r, @as(u5) it.r, 0, 0);
                
                r.relative = true;
                r.type = 3;  // ARM64_RELOC_PAGE21
                rs.reserve_type(i64)[] = encode_reloc(r);
                r.offset += 4;
                r.type = 4; // ARM64_RELOC_PAGEOFF12
                r.relative = false;
                rs.reserve_type(i64)[] = encode_reloc(r);
            }
            fn Call() => {
                // in the instruction we tell it: set link bit
                inst: []u32 = (ptr = ptr_cast_unchecked(u8, u32, fix.patch_at), len = 1);
                inst[0] = b(0, 1);
                
                r.relative = true;
                r.type = 2;  // ARM64_RELOC_BRANCH26
                rs.reserve_type(i64)[] = encode_reloc(r);
            }
            fn DataAbsolute() => {
                r.relative = false;
                r.type = 0;   // ARM64_RELOC_UNSIGNED
                r.length = 3; // one pointer
                rs.reserve_type(i64)[] = encode_reloc(r);
                ptr_cast_unchecked(u8, i64, fix.patch_at)[] = 0;
            }
            fn RipDisp32(it) => {
                r.relative = true;
                // I guess thats fair, it wants to know if its legal to point you at a stub or if you're expecting to access data. 
                X86_64_RELOC_BRANCH :: 2;
                X86_64_RELOC_SIGNED :: 1;
                r.type = if(it.call, => X86_64_RELOC_BRANCH, => X86_64_RELOC_SIGNED);
                rs.reserve_type(i64)[] = encode_reloc(r);
                ptr_cast_unchecked(u8, u32, fix.patch_at)[] = it.increment;
            }
        };
    };
};
    
fn macho_load_dylib(commands: *List(u8), command_count: *i64, cmd: LoadCommand, name: Str) void = {
    start := commands.len;
    h := commands.reserve_type(LoadCommandHeader);
    h.type = cmd.raw();
    command_count[] += 1;
    cmd := commands.reserve_type(LinkLibrary);
    // TODO: where does one get these numbers? i used to just do paste clang used but 0s seem fine
    cmd[] = (
        time_date_stamp = 0,
        current_version = 0,
        compatible_version = 0,
    );
    append_padded_cstr(commands, name, 8);
    h.size = trunc(commands.len - start);
}

fn macho_load_dylinker(commands: *List(u8), command_count: *i64, name: Str) void #once = {
    start := commands.len;
    h := commands.reserve_type(LoadCommandHeader);
    h.type = LoadCommand.LoadDynamicLinker.raw();
    command_count[] += 1;
    commands.reserve_type(u32)[] = 12;
    append_padded_cstr(commands, name, 8);
    h.size = trunc(commands.len - start);
}

fn compiler_address_to_segment_offset(m: *QbeModule, address: *u8) Ty(SegmentType, i64) = {
    low  := MAX_i64;
    high := 0;
    for_enum SegmentType { type | 
        seg := m.segments&[type]&;
        dist_from_start := ptr_diff(seg.mmapped.ptr, address);
        if dist_from_start >= 0 && dist_from_start < seg.mmapped.len {
            @assert(ptr_diff(seg.next, address) < 0, "address was inside segment %'s allocation but after next[]", type);
            return(type, dist_from_start);
        };
        // TODO: waste of code. they're always ordered. 
        high = high.max(u8.int_from_ptr(seg.mmapped.ptr) + seg.mmapped.len);
        low = low.min(u8.int_from_ptr(seg.mmapped.ptr));
    };
    @panic("% was not found in module.segments (low = %, high = %)", u8.int_from_ptr(address), low, high)
}

// TODO: collect_aot_fixups compiles if you typo to *Symbol, :FUCKED
FixP :: @struct(symbol: *SymbolInfo, fix: *Fixup);
fn collect_aot_fixups(m: *QbeModule) Ty([]FixP, []*SymbolInfo) = {
    // :SLOW keep a list because most will probably be local with no relocations. 
    fixups := list(FixP, temp());
    symbols := list(*SymbolInfo, temp());
    
    symbol_count := 0;
    for_symbols m { _, s | 
        first := true;
        each s.fixups& { f | 
            if f.type&.is(.DataAbsolute) {
                fixups&.push(symbol = s, fix = f);
                if first {
                    first = false;
                    symbols&.push(s);
                }
            };
        };
    };
    
    (fixups.items(), symbols.items())
}

fn live_segment_part(m: *QbeModule, s: SegmentType) []u8 = {
    s := m.segments&[s]&;
    s.mmapped.slice(0, ptr_diff(s.mmapped.ptr, s.next))
}

fn append_padded_cstr(bytes: *List(u8), s: Str, align: i64) i64 = {
    start := bytes.len;
    bytes.push_all(s);
    bytes.push(0);
    bytes.zero_pad_to_align(align);
    bytes.len - start
}

// TODO: do we want to put these helpers in a different file so other people can use them less painfully? 

fn reserve_type(bytes: *List(u8), $T: Type) *T #generic = {
    bytes.reserve(T.size_of());
    ptr := bytes.maybe_uninit.ptr.offset(bytes.len);
    // TODO: do we really care?
    //@debug_assert(u8.int_from_ptr(ptr).mod(T.align_of()) == 0, "unaligned reserve");
    ptr := ptr_cast_unchecked(u8, T, ptr);
    bytes.len += T.size_of();
    ptr
}

fn zero_pad_to_align(bytes: *List(u8), align: i64) void = {
    ptr := bytes.maybe_uninit.ptr.offset(bytes.len);
    extra := u8.int_from_ptr(ptr).mod(align);
    if extra != 0 {
        extra := align - extra;
        bytes.reserve(extra);
        range(0, extra) { _ |
            bytes.push(0);
        };
    };
    // TODO: this seems to make it fault all the time :FUCKED -- Nov 28
    //@debug_assert_eq(bytes.len.mod(align), 0, "alignment confusion. allocation aligned differently from our length.");
}

fn zero_pad_len_to_align(bytes: *List(u8), align: i64) void = {
    extra := bytes.len.mod(align);
    if extra != 0 {
        extra := align - extra;
        bytes.reserve(extra);
        range(0, extra) { _ |
            bytes.push(0);
        };
    };
}


fn str_to_16_byte_value(s: Str) u128 = {
    assert(s.len.le(16), "A segment name cannot be larger than 16 text characters in bytes");
    low: u64 = 0;
    high: u64 = 0;
    enumerate s { i, c |
        c: u64 = c[].zext();
        if i < 8 {
            low = low.bit_or(c.shift_left(i * 8));
        } else {
            high = high.bit_or(c.shift_left((i - 8) * 8));
        };
    };
    (low = low, high = high)
}

// this also works if you use 4096 but that seems strange to me. 
codesign_page_size :: macos_page_size;

//
// Loosly based on ziglang/zig/src/link/MachO/CodeSignature.zig. MIT License. Copyright (c) Zig contributors.
// This replaces running `codesign -s - a.out` so you can cross compile targetting macos. 
// If you don't do this (on recent-ish versions of macos) your program will just be `killed` immediately. 
//
// Writes a header and then the sha256 hash of each page in `input_chunks` to `output_buf`.
// Each chunk must be a multiple of the page size except for the last one (because I don't want to deal with it). 
// Caller must precalulate the correct size of everything
// because we need to hash all the load commands as well so you can't change them later. 
//
// TODO: do we need this on x64? if so is page size is 4k instead of 16k?
fn write_adhoc_signature(
    exec_seg_base: i64,
    exec_seg_limit: i64,
    file_size: i64,
    input_chunks: [][]u8,
    dylib: bool,
    ident: []u8,
    output_buf: []u8,
) void = {
    //
    // WARNING WARNING WARNING
    // if you forget to align_to here so you're missing the last hash, it fully crashes the operating system (macos 14.6.1)
    //
    total_pages := file_size.align_to(codesign_page_size) / codesign_page_size;
    hash_size   := 32;
    all_hashes_size := hash_size * total_pages;
    code_dir_size := CodeDirectory.size_of() + all_hashes_size + ident.len + 1;
    
    out: List(u8) = (maybe_uninit = output_buf, len = 0, gpa = panicking_allocator); out := out&;
    
    header := out.reserve_type(SuperBlob);
    header[] = (
        magic = CSMAGIC_EMBEDDED_SIGNATURE,
        total_length = trunc(SuperBlob.size_of() + code_dir_size), 
        blob_count = 1,
        type = CSSLOT_CODEDIRECTORY,
        offset = SuperBlob.size_of(),
    );
    SuperBlob.byte_swap_fields(header);
    
    c := out.reserve_type(CodeDirectory);
    c[] = CodeDirectory.zeroed();
    c.execSegBase = exec_seg_base.bitcast();
    c.execSegLimit = exec_seg_limit.bitcast();
    c.execSegFlags = int(!dylib).bitcast();
    c.codeLimit = file_size.trunc();
    c.magic = CSMAGIC_CODEDIRECTORY;
    c.length = code_dir_size.trunc();
    c.version = CS_SUPPORTSEXECSEG;
    c.flags = CS_ADHOC.bit_or(CS_LINKER_SIGNED);
    c.identOffset = size_of(CodeDirectory);
    c.hashSize = hash_size.trunc();
    c.hashType = CS_HASHTYPE_SHA256;
    c.pageSize = 14;
    @debug_assert_eq(1.shift_left(@as(i64) c.pageSize.zext()), codesign_page_size);
    c.nCodeSlots = total_pages.trunc();
    c.hashOffset = trunc(code_dir_size - all_hashes_size);
    CodeDirectory.byte_swap_fields(c);
    
    out.push_all(ident);  // useless
    out.push(0);
    
    // Now spam out all the bytes of hashes.
    i := 0;
    last_page := false;
    for input_chunks { chunk | 
        while => chunk.len > 0 {
            @assert(!last_page, "TODO: deal with page spread across chunks");
            if chunk.len < codesign_page_size {
                last_page = true;
                @debug_assert(output_buf.ptr.in_memory_after(chunk.ptr.offset(chunk.len-1)), "aliased output and input");
            };
            page := chunk.slice(0, codesign_page_size.min(chunk.len));
            hash := page.sha256();
            out.push_all(hash&.items().interpret_as_bytes());
            i += 1;
            chunk.ptr = chunk.ptr.offset(codesign_page_size);
            chunk.len -= codesign_page_size;
        };
    };
    diff := i * codesign_page_size - file_size;
    @assert(diff <= codesign_page_size && diff >= 0, "codesigned chunks amount did not match file_size");
    
    @assert_eq(out.len, out.maybe_uninit.len, "wrong signeture size");
}

fn byte_swap_fields($T: Type, self: *T) void #generic = {
    inline_for T.get_fields() { $f | 
        @if(:: (f[].ty == u32 || f[].ty == u64)) {
            inner := T.get_field_ptr(self, f);
            inner[] = byte_swap(inner[]);
        };
    };
}
