//! 
//! Mach-O is the object file format used by the XNU kernel (so macOS and iOS). 
//! There are 3 types of Mach-O files we care about. 
//! Executable: a program the operating system can run
//!     - dynamic imports and pointers in data are in ChainedFixups
//!     - empty __PAGEZERO, __TEXT contains the Mach-O header and load commands, __LINKEDIT must have a load command
//!     - must have a CodeSigneture (sha256 of each appended to the file)
//!     - segments must be page aligned 
//! Relocatable Object: input for static linking with another program
//!     - relocations listed for each section (instead of ChainedFixups)
//! Dynamic Library: 
//!     - exports trie lists your functions that people can call
//!     - needs a LC_ID_DYLIB
//!     - otherwise same as Exe
//! 

// TODO: run non-driver tests with no-op relocatable as well as direct to exe
// TODO: generate some tests with big numbers and ranges of relocations, etc to make sure we don't overflow the fields (or at least give an error). 
// TODO: add to isel to only produce addresses one in a block to have fewer relocations at the cost of register pressure. 
// TODO: make this less ugly. !! please
// TODO: fix `ld: warning: no platform load command found in ` for relocatable.  
// TODO: emit the local symbols even for exe so you can see them in a debugger. 
//       do other debug info stuff. 
// TODO: i haven't checked if .Dynamic needs zero_page

#use("@/backend/macho/exports.fr");
#use("@/backend/macho/bits.fr");

WipSegment :: @struct(
    name: Str,
    prot: u32,
    virtual_size := 0,
    file_size := 0,
    sections: []WipSection = empty(),
    extra_file_offset := 0, // HACK __TEXT doesn't include the headers for a relocatable objdect but it does for an exe. 
);

WipSection :: @struct(
    name: Str,
    flag: u32,
    offset_in_segment: i64,
    size: i64,
    reserved2: u32 = 0,
);


// I guess this is to make sure you segfault on low addresses. 
// But it can't guarentee my base address is zero right? so what's the point? clearly im missing something. 
// TODO: probably don't do this if we're making a linkable .o file. 
zero_page_size :: 0x100000000;

// results are in temp()
fn output_macho(m: *QbeModule) [][]u8 = {
    debug_log_byte_size(m);
    patch_pending_symbols(m);
    
    alignment := if(m.goal.type == .Relocatable, => 8, => macos_page_size);
    for_enum SegmentType { s | 
        //m.align_to(s, 8);  // forgeting this manifested as getting fucked on __LINKEDIT (when reading not writing because you're writting into a new allocation)
        // but do we need to do this? it makes it look more like clang does. 
        m.align_to(s, alignment);
    };
    //m.align_to(.Code, macos_page_size);
    m.fixups_locked = true; // we're going to stomp the data section with relocation info so you can run it jitted anymore after this. 
    chunks := list([]u8, 10, temp());
    
    // TODO: take this out and fix indexes +1 everywhere. 
    chunks&.push(empty()); // this used to be for commands but now thats part of .Code which is stupid. :commandsispartofcodenow
    
    segment_base := u8.ptr_from_raw(m.segment_address(.Code, 0));
    e: EmitMacho = (
        m = m, 
        commands = fixed_list(ptr = segment_base, len = m.goal.commands_size_guess), // :cantreallocatebutgivegooderrormessage
    );

    cpu_type, cpu_subtype := @if(m.goal.arch == .aarch64, (0x0100000C, 0x00000000), (0x01000007, 0x00000003));
    macho_header := e.commands&.reserve_type(MachoHeader);
    macho_header[] = (
        cpu_type = cpu_type,
        cpu_subtype = cpu_subtype,
        file_type = @match(m.goal.type) {
            fn Exe() => .Executable;
            fn Relocatable() => .Object;
            fn Dynamic() => .Dynamic;
            @default => unreachable();
        },
        commands_count = 0, 
        commands_size = 0, 
        flags = 0b00000000001000000000000010000101, // ¯\_(ツ)_/¯ 
    );
    if m.goal.type == .Relocatable {
        macho_header.flags = 0;  // dont claim no_undefs
    };
    
    // There's also MH_BINDS_TO_WEAK :: 0x10000, to bit_or into macho_header.flags, but it doesn't seem to do anything. 
    
    ::enum_basic(LoadCommand);
    
    segments := WipSegment.list(6, temp());
    if m.goal.type == .Exe || m.goal.type == .Dynamic {
        chunks&.push(live_segment_part(m, .Code));  // :MySegmentsStartAt1 :CodeIsFirst
        chunks&.push(live_segment_part(m, .ConstantData));  // TODO: waste. dead zone ConstantData[got_cursor..MAX_GOT_SIZE]
        chunks&.push(live_segment_part(m, .MutableData));
        
        segments&.push(prot = 5, name = "__TEXT");
        segments&.push(prot = 3, name = "__DATA_CONST");
        segments&.push(prot = 3, name = "__DATA");
        enumerate segments& { i, s |
            s.virtual_size = m.segments&[@as(SegmentType) i].mmapped.len;
            s.file_size = chunks[i + 1].len;
        };
        s    := m.segments&.index(.ZeroInitData);
        size := ptr_diff(s.mmapped.ptr, s.next);
        segments&.push(prot = 3, name = "__BSS", virtual_size = size);
        segments&.insert(0, (prot = 0, name = "__PAGEZERO", virtual_size = zero_page_size)); 
        
        code_flag := ATTR_PURE_INSTRUCTIONS.bit_or(ATTR_SOME_INSTRUCTIONS);
        xx: WipSection = (
            name = "__text", 
            flag = code_flag, 
            offset_in_segment = e.m.goal.commands_size_guess, 
            size = chunks[1].len - e.m.goal.commands_size_guess,
        );
        segments[1].sections = @slice(xx);
        // TODO: do i need reserved1 set to something? maybe thats an offset into LC_SYMTAB? or maybe undefined_symbol_count, that seems more consistant in the binraries i checked. 
        xx: WipSection = (name = "__got", size = chunks[2].len, flag = 0, offset_in_segment = 0);
        segments[2].sections = @slice(xx);
        xx: WipSection = (name = "__data", size = chunks[3].len, flag = 0, offset_in_segment = 0);
        segments[3].sections = @slice(xx);
        xx: WipSection = (name = "__bss", size = 0, flag = 0, offset_in_segment = 0);
        segments[4].sections = @slice(xx);
    };
    if m.goal.type == .Relocatable {
        align_to :: fn(offset: i64, align: i64) i64 = {
            extra := offset.mod(align);
            if extra == 0 {
                offset
            } else {
                offset + align - extra
            }
        };
        
        chunks&.push(live_segment_part(m, .Code));
        segments&.push(prot = 7, name = "__TEXT", extra_file_offset = m.text_padding());
        segments[0].virtual_size = m.segments&[.Code].mmapped.len;
        segments[0].file_size = chunks[1].len - m.text_padding();
        code_flag := ATTR_PURE_INSTRUCTIONS.bit_or(ATTR_SOME_INSTRUCTIONS);
        xx: WipSection = (name = "__text", flag = code_flag, offset_in_segment = 0, size = chunks[1].len - m.text_padding());
        segments[0].sections = @slice(xx);
        
        // Even if some sections are empty, you need them to make the indexes in append_relocations/which_section work out. 
        
        chunks&.push(live_segment_part(m, .ConstantData));
        segments&.push(prot = 7, name = "__DATA_CONST");
        segments[1].virtual_size = m.segments&[.ConstantData].mmapped.len;
        segments[1].file_size = chunks[2].len;
        xx: WipSection = (name = "__data_const", flag = 0, offset_in_segment = 0, size = chunks[2].len);
        segments[1].sections = @slice(xx);
        
        chunks&.push(live_segment_part(m, .MutableData));
        segments&.push(prot = 7, name = "__DATA");
        segments[2].virtual_size = m.segments&[.MutableData].mmapped.len;
        segments[2].file_size = chunks[3].len;
        xx: WipSection = (name = "__data", flag = 0, offset_in_segment = 0, size = chunks[3].len);
        segments[2].sections = @slice(xx);
        
        s    := m.segments&.index(.ZeroInitData);
        size := ptr_diff(s.mmapped.ptr, s.next);
        segments&.push(prot = 3, name = "__BSS", virtual_size = size);
        xx: WipSection = (name = "__bss", flag = S_ZEROFILL, offset_in_segment = 0, size = size);
        segments[3].sections = @slice(xx);
    };
    
    base_file_offset := @slice(0, 0, 0, 0, 0); 
    e.virtual_segment_offset = @slice(0, 0, 0, 0, 0);
    e.relocations = temp().alloc_zeroed(List(u8), SegmentType.enum_count() + 5);
    e.load_commands = temp().alloc(*SegmentSection, SegmentType.enum_count() + 5);
    n_sections := 0;
    {
        enumerate segments& { segment_index, s | 
            start := e.commands.len;
            h := e.commands&.reserve_type(LoadCommandHeader);
            h.type = LoadCommand.SegmentLoad.raw();
            e.command_count += 1;
            cmd := e.commands&.reserve_type(SegmentLoad);
            cmd[] = SegmentLoad.zeroed();
            // you do need this tho or you crash in dyld`dyld4::fixupPage64(void*, mwl_info_hdr const*, dyld_chained_starts_in_segment const*, unsigned int, bool) 
            // "This segment is made read-only after relocations are applied if needed."
            if s.name == "__DATA_CONST" {
                cmd.flag = 16;
            };
            cmd.max_prot = s.prot;
            cmd.init_prot = s.prot;
            cmd.name = str_to_16_byte_value(s.name);
            cmd.address = e.virtual_offset.bitcast();
            cmd.address_size = if(s.file_size != 0, => s.file_size.bitcast(), => s.virtual_size.bitcast());
            e.virtual_segment_offset[segment_index] = e.virtual_offset;
            e.file_offset += s.extra_file_offset;
            cmd.file_offset = if(s.file_size != 0, => e.file_offset.bitcast(), => 0);
            base_file_offset[segment_index] = cmd.file_offset.bitcast();
            cmd.size = s.file_size.bitcast(); 
            cmd.section_count = s.sections.len.trunc();
            each s.sections { sec | 
                c  := e.commands&.reserve_type(SegmentSection);
                c[] = SegmentSection.zeroed();
                c.segment_name = cmd.name;
                c.section_name = str_to_16_byte_value(sec.name);
                c.address      = cmd.address + sec.offset_in_segment.bitcast();
                c.file_offset  = cmd.file_offset.trunc() + sec.offset_in_segment.trunc(); // :overflow
                c.alignment    = 3; // TODO
                c.flag         = sec.flag;
                c.size         = sec.size.bitcast();
                c._reserved2   = sec.reserved2;
                @assert(n_sections < e.load_commands.len, "TODO: too many sections");
                e.load_commands[n_sections] = c;
                e.relocations[n_sections] = list(temp());
                n_sections += 1;
            };
            h.size = trunc(e.commands.len - start);
            e.virtual_offset += s.virtual_size;
            e.file_offset += s.file_size;
        };
    };
    if e.m.goal.type == .Exe {
        macho_load_dylinker(e.commands&, e.command_count&, "/usr/lib/dyld");
        
        // Ask it to call us!
        cmd := load_command(e&, .MainEntryPoint, MainEntryPoint);
        // offset is from the start of interesting virtual memory? 
        off := main_entry_point_vaddr(m);
        cmd[] = (entry_offset = off.bitcast(), stack_size = 0);  // :CodeIsFirst its fine to change this, you just have to do the math here. 
    };
    if e.m.goal.type == .Dynamic {
        // LC_ID_DYLIB, it seems the name doesn't matter? 
        macho_load_dylib(e.commands&, e.command_count&, .LinkLibrarySelf, "foo");
    };
    chained := u8.list(m.goal.chained_size_guess, temp());
    
    // layout is [commands, waste, sections, chained, relocations?]
    // only relinkable needs relocations and only loadable needs a signeture at the end of chained. 
    
    m := e.m;
    // note: virtual_segment_offset is only good when MachoExe because we skip segments for MachoRelocatable. 
    if e.m.goal.type == .Exe || m.goal.type == .Dynamic {
        // Need to always ask for every library the frontend added even if no symbols from it 
        // were actually imported because they might be using objc_getClass at runtime. 
        for e.m.libraries.items().rest(1) { name | 
            name := guess_path(name);
            macho_load_dylib(e.commands&, e.command_count&, .LinkLibrary, name);
        };
        
        emit_exe_fixups(e&, chained&);
        zero_pad_len_to_align(chained&, 16);
        
        if m.goal.exe_debug_symbol_table {
            // This won't include any relocation entries, but the symbol table format for a static linker 
            // is the same as what lldb will read for setting break points, showing backtrace, etc.
            emit_reloc_symbols(e&, chained&);
            zero_pad_len_to_align(chained&, 16);
            
            // This tells lldb where to stop when you `dis` (so it just disassembles the one function you're in). 
            // - if you have a symbol table but not a function-starts, 
            //   it barrels past the end of the function and keeps disassembling garbage
            // - if you have neither, it just stops after a few instructions
            cmd := load_command(e&, LoadCommand.FunctionStarts, LinkEditBlob);
            data := m.debug_info.macho_function_starts.items();
            //cmd[] = (offset = e.file_offset.trunc(), size = data.len.trunc());
            //e.file_offset += data.len;
            //chunks&.push(data);
            // TODO: can't do the easy thing ^ because my codesign doesn't like that it's not page aligned 
            // and it's wasteful to do an extra align_to just for this
            cmd[] = (offset = (e.file_offset+chained.len).trunc(), size = data.len.trunc());
            chained&.push_all(data);
        };
        
        //
        // WARNING WARNING WARNING
        // if you forget to align_to here so you're missing the last hash, it fully crashes the operating system (macos 14.6.1)
        //
        codesign_ident := "";  // It seems this can be whatever you want since we're not doing a real signeture. I've tried "foo"
        page_count := ((e.file_offset + chained.len).align_to(codesign_page_size) / codesign_page_size);
        signeture_size := SuperBlob.size_of() + CodeDirectory.size_of() + (32 * page_count) + codesign_ident.len + 1;
        chained&.reserve(signeture_size);
        codesign_output_bytes := chained.maybe_uninit.slice(chained.len, chained.len + signeture_size);
        chunks&.push(chained.items());  // doesn't include codesign_output_bytes yet, that's added later
        
        // arm: the os won't run unsigned programs. 
        // amd: it doesn't care so don't waste time computing the hash. 
        need_signature := m.goal.arch == .aarch64;
        
        // this is a bit painful because the thing you're signing includes the offset + size of the signeture itself. 
        // so we need to reserve this space up front and fill it in later. 
        if need_signature {
            load_codesign := load_command(e&, .CodeSigneture, LinkEditBlob);
            load_codesign.offset = trunc(e.file_offset + chained.len);
            load_codesign.size = codesign_output_bytes.len.trunc();
        
            chained.len += codesign_output_bytes.len;
        };
        
        // You are required to tell it to load __LINKEDIT even tho we don't use it for anything directly. 
        load_command(e&, .SegmentLoad, SegmentLoad)[] = (
            name = str_to_16_byte_value("__LINKEDIT"),
            address_size = chained.len.bitcast(),
            size = chained.len.bitcast(),
            max_prot = 1,
            init_prot = 1,
            file_offset = e.file_offset.bitcast(),
            address = e.virtual_offset.bitcast(),
            section_count = 0,
            flag = 0,
        );
        
        e.file_offset += chained.len;
        seal_load_commands(e&, macho_header);
        
        if need_signature {
            // This has to be the very last thing you do, 
            // after all the load commands are done, 
            // because the hash includes them as well. 
            write_adhoc_signature(
                exec_seg_base = base_file_offset[1],
                exec_seg_limit = segments[0].file_size,
                file_size = e.file_offset - signeture_size, 
                input_chunks = chunks.items(),
                dylib = m.goal.type == .Dynamic,
                ident = codesign_ident,
                output_buf = codesign_output_bytes,
            );
            
            @if(show_backend_stats())
            @eprintln(">>> repro: %", Sha256'hex(codesign_output_bytes)); 
            
            chunks&.push(codesign_output_bytes);
        };
    } else {
        @debug_assert(e.m.goal.type == .Relocatable);
        emit_reloc_symbols(e&, chained&);
        
        seal_load_commands(e&, macho_header);
            
        e.file_offset += chained.len;
        chunks&.push(chained.items());
        
        enumerate e.relocations { i, r | 
            if r.len != 0 {
                e.load_commands[i].relocations_count = trunc(r.len / size_of(RelocationFields));
                e.load_commands[i].relocations_file_offset = e.file_offset.trunc();
                chunks&.push(r.items());
                e.file_offset += r.len;
            }
        };
    };
    
    @assert_le(chained.len, m.goal.chained_size_guess, "resized and now the header pointer is junk"); 
    @debug_assert_eq(e.commands.len, e.m.goal.commands_size_guess);
    chunks.items()
}

fn seal_load_commands(e: *EmitMacho, macho_header: *MachoHeader) void = {
    macho_header.commands_size = (e.commands.len - MachoHeader.size_of()).trunc();
    macho_header.commands_count = e.command_count.trunc();
    end := e.commands.len;
    // :cantreallocatebutgivegooderrormessage
    // now we want exact match because we put it on the front of the text section for some reason who knows idk man. 
    @assert_le(end, e.m.goal.commands_size_guess, "we guessed wrong so the array resized and all the pointers are junk");
    e.commands.len = e.m.goal.commands_size_guess;
    e.commands.items().rest(end).set_zeroed();
}

fn load_command(e: *EmitMacho, cmd: LoadCommand, $T: Type) *T #generic = {
    start := e.commands.len;
    h := e.commands&.reserve_type(LoadCommandHeader);
    h.type = cmd.raw();
    e.command_count += 1;
    cmd := e.commands&.reserve_type(T);
    h.size = trunc(e.commands.len - start);
    cmd
}

EmitMacho :: @struct(
    m: *QbeModule, 
    commands: List(u8), 
    command_count := 0, 
    file_offset := 0, 
    relocations: []List(u8) = empty(),  // these bytes are RelocationFields 
    load_commands: []*SegmentSection = empty(), 
    virtual_segment_offset: []i64 = empty(), 
    virtual_offset := 0,
    offset_to_csis: []u32 = empty(),
    csis_by_segment: []*ChainedStartsInSegment = empty(),
    starts_offset := 0,
);

fn emit_exe_fixups(e: *EmitMacho, chained: *List(u8)) void #inline = {
    m := e.m;
    chain_header := chained.reserve_type(ChainedFixupsHeader);
    
    fixups, symbols := collect_aot_fixups(m);
    @debug_assert_ge(fixups.len, e.m.got&.len() / 8, "at least each __got entry needs a fixup");
    sort :: quicksort(FixP, fn(a, b) => ptr_diff(a.fix.patch_at, b.fix.patch_at) >= 0);
    sort(fixups);
    // at this point, `fixups` is in the order of the patch targets in virtual memory which is the order we need to emit the fixup chains. 
    import_count := 0;
    for symbols { s |
        if s.kind != .Local {
            s.got_lookup_offset = import_count;
            import_count += 1;
        };
    };
    // imports are fixed size. so we can reserve that early
    imports_len_b := import_count * 4;
    imports_offset := chained.len; // TODO: is this right?
    chained.reserve(imports_len_b);
    chained.len += imports_len_b;
    symbols_offset := chained.len; // TODO: is this right?
    imports_bytes := chained.items().slice(chained.len - imports_len_b, chained.len);
    imports_bytes: []ChainedImport = (ptr = ptr_cast_unchecked(u8, ChainedImport, imports_bytes.ptr), len = import_count);
    for symbols { s |
        if s.kind != .Local {
            n := chained.len - symbols_offset;
            // :DefaultToLibc
            // don't forget :force_dep_on_memmove_hack when you remove this
            if s.library == 0 {
                ::?u32;
                s.library = e.m.find_library("libc") || {
                    @panic("import % from unknown library", s.name)
                };
            };
            imports_bytes[s.got_lookup_offset] = construct(
                lib_ordinal = s.library.zext(), 
                weak = int(!s.strong), 
                name_offset = n,
            );
            
            append_sym_name(chained, s.name);
        };
    };
    zero_pad_to_align(chained, 4);
    
    //
    // Now we have to do the fixup chain page starts. 
    // This is how we tell it where it has to do relocations. 
    //
    e.starts_offset = chained.len;
    // TODO: uncomment the real number when ready
    seg_count :: 2 + enum_count(SegmentType);  // __zeropage, <the rest>, __linkedit
    chained.reserve_type(u32)[] = seg_count.trunc();
    // indexed by segment index
    e.offset_to_csis = (ptr = ptr_cast_unchecked(u8, u32, chained.maybe_uninit.ptr.offset(chained.len)), len = seg_count); // from chained[starts_offset]
    e.csis_by_segment = temp().alloc_zeroed(*ChainedStartsInSegment, seg_count); // :last2bytesalias
    range(0, seg_count) { i |
        o := chained.reserve_type(u32);
        o[] = unprocessed_sentinal;
    };
    
    chained.zero_pad_to_align(8);
    
    prev_fixup: ?WipFixup = .None;
    each fixups { fix |
        emit_one_exe_fixup(e, fix, prev_fixup&, import_count, chained);
    };
    
    // Flush the final entry of the final segment. 
    if prev_fixup& { prev | 
        emplace_fixup(e, prev.fix, 0, import_count);
        prev_fixup = .None;
    };

    enumerate e.offset_to_csis { i, it |
        @debug_assert(u32.int_from_ptr(it) == u8.int_from_ptr(chained.maybe_uninit.ptr) + e.starts_offset + 4 + i * 4, "miscounted offset_to_csis");
    };
    
    @debug_assert(e.offset_to_csis[0] == 0 && e.offset_to_csis[e.offset_to_csis.len - 1] == 0, "__PAGEZERO and __LINKEDIT shouldn't have fixups.");
    
    chained.zero_pad_to_align(8);
    @assert_le(chained.len, m.goal.chained_size_guess, "resized and now the header pointer is junk");
    chain_header[] = (
        imports_count = import_count.trunc(), 
        imports_format = 1, 
        symbols_format = 0, 
        version = 0, 
        starts_offset = e.starts_offset.trunc(), 
        imports_offset = imports_offset.trunc(), 
        symbols_offset = symbols_offset.trunc(),
    );
    chained_size := chained.len;
    
    // this must include the part before symbol table or llvm-objcopy complains bout them overlapping
    cmd := load_command(e, .ChainedFixups, LinkEditBlob);
    // TODO: using file_offset way down here feels a bit fragile. 
    cmd[] = (offset = e.file_offset.trunc(), size = chained_size.trunc());  // :CodeIsFirst its fine to change this, you just have to do the math here. 

    if e.m.goal.type == .Dynamic {
        new_file_offset := e.file_offset + chained_size;
        
        T :: import("@/backend/macho/exports.fr").ExportsTrie;
        trie: T = init();
        for e.m.exports { export_id | 
            s := e.m.get_symbol_info(export_id);
            @debug_assert(s.kind == .Local, "can only export local symbols");
            trie&.put(@tfmt("_%", s.name), s.offset);
        };
        trie&.write(chained);
        
        cmd := load_command(e, .ExportsTrie, LinkEditBlob);
        cmd[] = (offset = new_file_offset.trunc(), size = (chained.len - chained_size).trunc());  // :CodeIsFirst its fine to change this, you just have to do the math here. 
    };
}

// :HACK. the user should provide these paths but for now it just needs to work so i can stop using the linker for graphics examples. 
// -- May 1, 2025
guess_path :: fn(name: Str) Str = @if_else {
    // :MultiDylibLibc if compiling from linux, libm will be libc*** or whatever, 
    // so just ignore that part by not using == 
    @if(name.starts_with("libc")) => "/usr/lib/libSystem.dylib";
    @if(name == "objc") => "/usr/lib/libobjc.dylib";
    @else => @tfmt("/System/Library/Frameworks/%.framework/%", name, name);
};

WipFixup :: @struct(
    page_index: i64,
    offset_in_page: i64,
    fix: *FixP,
);

unprocessed_sentinal :: 0; // TODO: scope this

fn emit_one_exe_fixup(e: *EmitMacho, fix: *FixP, prev_fixup: *?WipFixup, import_count: i64, chained: *List(u8)) void #inline = {
    m := e.m;
    @debug_assert(fix.fix.type&.is(.DataAbsolute), "expected fixup for .Exe to be .DataAbsolute");
    seg, off_in_segment := compiler_address_to_segment_offset(m, fix.fix.patch_at);
    segment_index := seg.raw() + 1;  // :MySegmentsStartAt1
    page_index, offset_in_page := off_in_segment.div_mod(macos_page_size);
    if e.offset_to_csis[segment_index] == unprocessed_sentinal.trunc() {
        // This is our first time seeing this segment. 
        
        // Flush the last entry of the previous segment. 
        if prev_fixup { prev | 
            emplace_fixup(e, prev.fix, 0, import_count);
            prev_fixup[] = .None;
        };
        
        start_of_csis := chained.len;
        csis_off := chained.len - e.starts_offset;
        @debug_assert_ne(csis_off, 0);
        csis := chained.reserve_type(ChainedStartsInSegment);  // one of these per segment.  
        chained.len -= 2; // take it back now yall :last2bytesalias
        csis[] = ChainedStartsInSegment.zeroed();
        e.csis_by_segment[segment_index] = csis;
    
        // offset to the first page in this segment. 
        // not a file offset! its virtual offset from first interesting segment. 
        // coincententially those are the same number for binaries made by clang...
        csis.segment_offset = e.virtual_segment_offset[segment_index].bitcast() - zero_page_size;
        
        e.offset_to_csis[segment_index] = csis_off.trunc();
        
        csis.page_count = 1;
        range(0, page_index) { _ |
            // TODO: a test that gets here
            csis.page_count += 1;  // :calcsizeatend
            chained.reserve_type(u16)[] = page_start_no_fixups;
        };
        chained.reserve_type(u16)[] = offset_in_page.trunc(); // :calcsizeatend
        
        csis.pointer_format = 6; // who knows man, i just work here. TODO
        csis.page_size = macos_page_size;
        csis.max_valid_pointer = 0; // TODO: what does this mean?
        csis.size = trunc(chained.len - start_of_csis); // :calcsizeatend
    };
    if prev_fixup { prev | 
        // will always be the same segment as before or we would have flushed the fixup above. 
        next_offset := if(prev.page_index == page_index, => (offset_in_page - prev.offset_in_page) / 4) {
            // we've moved on to a different page in the same section. 
            // so add another entry to the csis.page_start array.
            while => prev.page_index + 1 != page_index {    
                prev.page_index += 1;
                e.csis_by_segment[segment_index].page_count += 1;
                e.csis_by_segment[segment_index].size += 2;
                chained.reserve_type(u16)[] = page_start_no_fixups;
            };
            e.csis_by_segment[segment_index].page_count += 1;
            e.csis_by_segment[segment_index].size += 2;
            chained.reserve_type(u16)[] = offset_in_page.trunc();
            0
        };
        emplace_fixup(e, prev.fix, next_offset, import_count);
        prev_fixup[] = .None;
    };
    prev_fixup[] = (Some = (page_index = page_index, offset_in_page = offset_in_page, fix = fix));
}

fn emplace_fixup(e: *EmitMacho, fix: *FixP, next_encoded: i64, import_count: i64) void #inline = {
    @debug_assert(fix.fix.type&.is(.DataAbsolute), "emplace_fixup only DataAbsolute");
    addend := fix.fix.type.DataAbsolute.increment; // other addends are handled elsewhere? which is kinda confusing
    @debug_assert_ge(next_encoded, 0, "fixups cant go backwards. we sorted them so this can't happen.");
    is_bind := fix.symbol.kind != .Local;
    ::assert_eq(size_of(ChainedReloc), size_of(i64));
    r := ChainedReloc.ptr_from_raw(fix.fix.patch_at);
    r[] = zeroed @type r[];
    set(r.bind&, .next, next_encoded);
    set(r.bind&, .is_bind, int(is_bind));
        
    if is_bind {
        // got_lookup_offset has been converted to an ordinal already. 
        o := fix.symbol.got_lookup_offset;
        @debug_assert(o >= 0 && o < import_count, "bad ordinal %", o);
        // TODO: test that gets here with non-zero
        @assert_ge(addend, 0, "TODO: does macho chained fixup allow negative addend to import?");
        set(r.bind&, .addend, addend);
        set(r.bind&, .ordinal, o);
    } else {
        // need a pointer in memory to a local thing, so we just tell it the offset from the memory location to the target.
        // local stuff doesn't need the same strictness about which part is the "symbol" so addend just gets lumped in to the one offset. 
        dest_segment := fix.symbol.segment.raw() + 1; // :MySegmentsStartAt1
        target := e.virtual_segment_offset[dest_segment] - zero_page_size + fix.symbol.offset + addend;
        @assert_lt(target, 1.shift_left(36), "[macho/emit.fr] can't encode rebase > 36 bits to %+%", fix.symbol.name, addend);
        set(r.rebase&, .target, target);
    };
};
    
fn emit_reloc_symbols(e: *EmitMacho, chained: *List(u8)) void #inline = {
    m := e.m;
    cmd := load_command(e, .LinkEditSymbolInfo, LinkEditSymbolInfo);
    cmd[] = LinkEditSymbolInfo.zeroed();
    cmd.local_symbol_count = trunc(e.m.local_needs_reloc.len);
    cmd.external_symbol_index = cmd.local_symbol_index + cmd.local_symbol_count;  // for consistancy
    cmd.external_symbol_count = e.m.exports.len.trunc();
    cmd.undefined_symbol_index = cmd.external_symbol_index + cmd.external_symbol_count; // must be set even if count is zero or lld complains 
    cmd.undefined_symbol_count = e.m.imports.len.trunc();
    
    syms_cmd := load_command(e, .LinkEditSymbolTable, LinkEditSymbolTable);
    syms_cmd[] = LinkEditSymbolTable.zeroed();
    syms_cmd.symbol_count = cmd.external_symbol_count + cmd.local_symbol_count + cmd.undefined_symbol_count;
    // the data for those goes in __LINKEDIT
    // TODO: using file_offset way down here feels a bit fragile. 
    syms_cmd.symbols_offset = trunc(e.file_offset + chained.len);
    size_of_symbols: i64 = syms_cmd.symbol_count.zext() * size_of(SymbolEntry);
    syms_cmd.strings_offset = syms_cmd.symbols_offset + trunc(size_of_symbols);
    
    symbols_bytes: []SymbolEntry = (
        ptr = ptr_cast_unchecked(u8, SymbolEntry, chained.maybe_uninit.ptr.offset(chained.len)), 
        len = syms_cmd.symbol_count.zext(),
    );
    chained.len += size_of_symbols; @debug_assert(chained.len <= chained.maybe_uninit.len);
    strings_start := chained.len;
    symbols_bytes.set_zeroed();
    
    // see devlog.md/Jun27,2025;
    // clang always names your first symbol <ltmp>, but you just need to waste a byte of string, 
    // or objdump won't show the first symbol name. 
    chained.push(0);
    
    // when something is both local_needs_reloc and export it will have two entries in the symbol table, 
    // which is stupid but seems to work (test/fixup4.ssa)
    base := 0;
    enumerate e.m.local_needs_reloc { i, id | 
        s := e.m.get_symbol_info(id[]);
        entry := symbols_bytes.index(i + base);
        entry.symbol_type = 14;  // 14 means defined in section.
        // note: 1 indexed. 
        entry.section_number = which_section(s.segment);
        entry.desc = 0; // i think exports don't use this.     for imports, 256 ordinal 1.
        entry.name_offset = trunc(chained.len - strings_start);
        segment_padding := if(s.segment == .Code, => m.text_padding(), => 0); // :HACK
 
        @debug_assert(s.offset >= segment_padding, "what are you smoking?");
        // note: symbol_address is the address in virtual memory. not the section offset. 
        start: i64 = bitcast(e.load_commands[entry.section_number.zext() - 1].address);
        entry.symbol_address = bitcast(s.offset - segment_padding + start);
        append_sym_name(chained, s.name);
        append_relocations(i + base, id[], m, e.relocations);
        // TODO: assert they're all DataAbsolute becuse functions are in the same segment and you could just call them. 
    };
    base += e.m.local_needs_reloc.len;
    enumerate e.m.exports { i, id |
        s := e.m.get_symbol_info(id[]);
        entry := symbols_bytes.index(i + base);
        // this only works for exports!
        entry.symbol_type = 15;  // bottom bit means external. 14 means defined in section.
        // note: 1 indexed. 
        entry.section_number = which_section(s.segment);
        entry.desc = 0; // i think exports don't use this.     for imports, 256 ordinal 1.
        entry.name_offset = trunc(chained.len - strings_start);
        // copy-paste
        segment_padding := if(s.segment == .Code, => m.text_padding(), => 0); // :HACK
        @debug_assert(s.offset >= segment_padding, "what are you smoking?");
        start: i64 = bitcast(e.load_commands[entry.section_number.zext() - 1].address);
        entry.symbol_address = bitcast(s.offset - segment_padding + start);
        append_sym_name(chained, s.name);
        // TODO: we probably don't want it to appear twice in the symbol table if it's a data export and also has local relocations
    };
    base += e.m.exports.len;
    enumerate e.m.imports { i, id |
        s := e.m.get_symbol_info(id[]);
        entry := symbols_bytes.index(i + base);
        entry.symbol_type = 1; // undef ext
        entry.section_number = symbol_no_section; 
        entry.desc = 0; // TODO: for dynamic imports, 256 is ordinal 1.
        
        // if s.weak, you might want to bit_or N_WEAK_REF :: 0x0040 into entry.desc, but it doesn't do anything. 
        // Either way you have to pass `-Wl,-undefined,dynamic_lookup` to clang or it errors. 
        // Same behaviour using clang to compile a program with `__attribute__((weak))` so it's not just me. 
        // !! that's only true on arm. with `-target x86_64-apple-darwin`, you need both the flag and the linker option. 
        N_WEAK_REF :: 0x0040;
        if !s.strong {
            entry.desc = bit_or(entry.desc, N_WEAK_REF);  // clang only cares on amd
        };
        
        entry.name_offset = trunc(chained.len - strings_start);
        append_sym_name(chained, s.name);
        append_relocations(i + base, id[], e.m, e.relocations);
    };
    base += e.m.imports.len;
    syms_cmd.strings_size = trunc(chained.len - strings_start);
}

fn which_section(s: SegmentType) u8 = 1 + trunc @as(i64) s;

fn append_relocations(symbol_table_index: i64, id: Qbe.Sym, m: *QbeModule, relocations: []List(u8)) void = {
    // this gets called when making a symbol table for an exe but don't want to stomp the existing local offsets in that case. 
    if(m.goal.type != .Relocatable, => return());
    s := m.get_symbol_info(id);
    each s.fixups { fix | 
        continue :: local_return;
        patch_segment, patch_segment_offset := compiler_address_to_segment_offset(m, fix.patch_at);
        if patch_segment == .ConstantData && patch_segment_offset < m.got&.len() {
            // :track_got_reloc
            // We bound this symbol for jitting and reserved a __got slot for it.
            // However, we want to let static linker move stuff around and pack its own __got.
            // So we skip this relocation and instead emit one for each place we accessed that __got slot. 
            //
            // previously this asserted `s.kind != .Local` but now that we support importing data as well, 
            // we need to conservitively emit more got references if we address a var before knowing that 
            // it's a function and we might guess wrong. i think this is just wasteful, not a correctness issue -- Feb 14, 2025
            // TODO: but we still incorrectly export those below
            @debug_assert(fix.type&.tag() == .DataAbsolute, "__got reloc must be data. %");
            continue();
        };
        sec: i64 = which_section(patch_segment).zext();
        rs := relocations[sec - 1]&;
        segment_padding := if(patch_segment == .Code, => m.text_padding(), => 0); // :HACK
        r := RelocationFields.zeroed();
        p := r.packed&;
        r.offset = trunc(patch_segment_offset - segment_padding);
        set(p, .symbol, symbol_table_index);
        set(p, .length, 2);  // one instruction
        set(p, .extern, 1);  // you'd think `s.kind != .Local` but no
        
        @match(fix.type) {
            fn InReg(it) #use(Arm) => {
                @assert(it.increment == 0, "TODO: offset symbols"); // :ArmAddendReloc
                it.got_load = it.got_load && s.kind != .Local;
                
                // in the instruction we tell it: which register to use
                // 
                // also the linker bitches about it if you use the wrong instruction (add vs ldr). 
                // even tho it's redundant because it knows from the relocation below. 
                // but thankfully the error message is helpful. 
                reg := @as(u5) it.r;
                inst: []u32 = (ptr = u32.ptr_from_raw(fix.patch_at), len = 2);
                inst[0] = adrp(0, 0, reg);
                inst[1] = if(it.got_load, => ldr_uo(Bits.X64, reg, reg, 0), => add_im(Bits.X64, reg, reg, 0, 0));  // TODO: does it need to go in both slots? 
                
                // You are required to swap the order of these relocations. 
                // Otherwise you fail an assertion in whatever version of lld github actions uses on Feb 15, 2025. 
                // Who knows man. Very grateful for the cranelift people rn tho. That would have taken me so long. 
                // - https://github.com/bytecodealliance/wasmtime/issues/8730
                // - https://github.com/gimli-rs/object/pull/702
                
                r.offset += 4;
                set(p, .type, if(it.got_load, => ARM64_RELOC_GOT_LOAD_PAGEOFF12, => ARM64_RELOC_PAGEOFF12));
                rs.reserve_type(RelocationFields)[] = r;  // first of two!
                
                set(p, .relative, 1);
                r.offset -= 4;
                set(p, .type, if(it.got_load, => ARM64_RELOC_GOT_LOAD_PAGE21, => ARM64_RELOC_PAGE21));
            }
            fn Call(it) => {
                // in the instruction we tell it: set link bit
                inst: []u32 = (ptr = u32.ptr_from_raw(fix.patch_at), len = 1);
                inst[0] = b(0, @if(it.set_link, 1, 0));
                
                set(p, .relative, 1);
                set(p, .type, ARM64_RELOC_BRANCH26);
            }
            fn DataAbsolute(it) => {
                set(p, .type, ARM64_RELOC_UNSIGNED);
                set(p, .length, 3);  // one pointer
                i64.ptr_from_raw(fix.patch_at)[] = it.increment;  // TODO: a test that gets here with non-zero
            }
            fn RipDisp32(it) => {
                set(p, .relative, 1);
                // I guess thats fair, it wants to know if its legal to point you at a stub or if you're expecting to access data. 
                set(p, .type, @if_else {
                    @if(it.call)     => X86_64_RELOC_BRANCH;
                    @if(it.got_load) => X86_64_RELOC_GOT_LOAD;
                    @else            => X86_64_RELOC_SIGNED;
                });
                // mach-o does :IncOffByFour for us without asking for it explicitly. 
                i32.ptr_from_raw(fix.patch_at)[] = it.increment.intcast();
            }
            @default => panic("invalid fixup type for mach-o");
        };
        rs.reserve_type(RelocationFields)[] = r;
    };
};

// These are for lib_ordinal
fn macho_load_dylib(commands: *List(u8), command_count: *i64, cmd: LoadCommand, name: Str) void = {
    start := commands.len;
    h := commands.reserve_type(LoadCommandHeader);
    h.type = cmd.raw();
    command_count[] += 1;
    cmd := commands.reserve_type(LinkLibrary);
    // TODO: where does one get these numbers? i used to just do paste clang used but 0s seem fine
    cmd[] = (
        time_date_stamp = 0,
        current_version = 0,
        compatible_version = 0,
    );
    append_padded_cstr(commands, name, 8);
    h.size = trunc(commands.len - start);
}

fn macho_load_dylinker(commands: *List(u8), command_count: *i64, name: Str) void #once = {
    start := commands.len;
    h := commands.reserve_type(LoadCommandHeader);
    h.type = LoadCommand.LoadDynamicLinker.raw();
    command_count[] += 1;
    commands.reserve_type(u32)[] = 12;
    append_padded_cstr(commands, name, 8);
    h.size = trunc(commands.len - start);
}

fn compiler_address_to_segment_offset(m: *QbeModule, address: rawptr) Ty(SegmentType, i64) = {
    first := m.segments&[.Code]&;  // :CodeIsFirst
    dist_from_start := ptr_diff(first.mmapped.ptr, address);
    @debug_assert_ge(dist_from_start, 0, "% is outside segments", address);
    seg_index, off := dist_from_start.div_mod(first.mmapped.len);  // :SegmentsAreTheSameSize
    @debug_assert_ult(seg_index, 3, "% is outside segments. for now we don't allow relocations in BSS", address);
    (@as(SegmentType) seg_index, off)
}

fn append_padded_cstr(bytes: *List(u8), s: Str, align: i64) i64 = {
    start := bytes.len;
    bytes.push_all(s);
    bytes.push(0);
    bytes.zero_pad_to_align(align);
    bytes.len - start
}

fn str_to_16_byte_value(s: Str) u128 = {
    assert(s.len.le(16), "A segment name cannot be larger than 16 text characters in bytes");
    low: u64 = 0;
    high: u64 = 0;
    enumerate s { i, c |
        c: u64 = c[].zext();
        if i < 8 {
            low = low.bit_or(c.shift_left(i * 8));
        } else {
            high = high.bit_or(c.shift_left((i - 8) * 8));
        };
    };
    (low = low, high = high)
}

fn append_sym_name(bytes: *List(u8), s: Str) void = {
    bytes.push("_".ascii());
    bytes.push_all(s);
    bytes.push(0);
}

// this also works if you use 4096 but that seems strange to me. 
codesign_page_size :: macos_page_size;

//
// Loosly based on ziglang/zig/src/link/MachO/CodeSignature.zig. MIT License. Copyright (c) Zig contributors.
// (but somehow mine is 75 lines instead of 400 so clearly not that similar)
// 
// This replaces running `codesign -s - a.out` so you can cross compile targetting macos. 
// If you don't do this (on recent-ish versions of macos) your program will just be `killed` immediately. 
//
// Writes a header and then the sha256 hash of each page in `input_chunks` to `output_buf`.
// Each chunk must be a multiple of the page size except for the last one (because I don't want to deal with it). 
// Caller must precalulate the correct size of everything
// because we need to hash all the load commands as well so you can't change them later. 
//
fn write_adhoc_signature(
    exec_seg_base: i64,
    exec_seg_limit: i64,
    file_size: i64,
    input_chunks: [][]u8,
    dylib: bool,
    ident: []u8,
    output_buf: []u8,
) void = {
    //
    // WARNING WARNING WARNING
    // if you forget to align_to here so you're missing the last hash, it fully crashes the operating system (macos 14.6.1)
    //
    total_pages := file_size.align_to(codesign_page_size) / codesign_page_size;
    hash_size   := 32;
    all_hashes_size := hash_size * total_pages;
    code_dir_size := CodeDirectory.size_of() + all_hashes_size + ident.len + 1;
    
    out: List(u8) = fixed_list(output_buf); out := out&;
    
    header := out.reserve_type(SuperBlob);
    header[] = (
        magic = CSMAGIC_EMBEDDED_SIGNATURE,
        total_length = trunc(SuperBlob.size_of() + code_dir_size), 
        blob_count = 1,
        type = CSSLOT_CODEDIRECTORY,
        offset = SuperBlob.size_of(),
    );
    SuperBlob.byte_swap_fields(header);
    
    c := out.reserve_type(CodeDirectory);
    c[] = CodeDirectory.zeroed();
    c.execSegBase = exec_seg_base.bitcast();
    c.execSegLimit = exec_seg_limit.bitcast();
    c.execSegFlags = int(!dylib).bitcast();
    c.codeLimit = file_size.trunc();
    c.magic = CSMAGIC_CODEDIRECTORY;
    c.length = code_dir_size.trunc();
    c.version = CS_SUPPORTSEXECSEG;
    c.flags = CS_ADHOC.bit_or(CS_LINKER_SIGNED);
    c.identOffset = size_of(CodeDirectory);
    c.hashSize = hash_size.trunc();
    c.hashType = CS_HASHTYPE_SHA256;
    c.pageSize = 14;
    @debug_assert_eq(1.shift_left(@as(i64) c.pageSize.zext()), codesign_page_size);
    c.nCodeSlots = total_pages.trunc();
    c.hashOffset = trunc(code_dir_size - all_hashes_size);
    CodeDirectory.byte_swap_fields(c);
    
    out.push_all(ident);  // useless
    out.push(0);
    
    // Now spam out all the bytes of hashes.
    i := 0;
    last_page := false;
    for input_chunks { chunk | 
        while => chunk.len > 0 {
            @assert(!last_page, "TODO: deal with page spread across chunks");
            if chunk.len < codesign_page_size {
                last_page = true;
                @debug_assert(output_buf.ptr.in_memory_after(chunk.ptr.offset(chunk.len-1)), "aliased output and input");
            };
            page := chunk.slice(0, codesign_page_size.min(chunk.len));
            hash := page.Sha256'hash();
            out.push_all(hash&.items().interpret_as_bytes());
            i += 1;
            chunk.ptr = chunk.ptr.offset(codesign_page_size);
            chunk.len -= codesign_page_size;
        };
    };
    diff := i * codesign_page_size - file_size;
    @assert(diff <= codesign_page_size && diff >= 0, "codesigned chunks amount did not match file_size");
    
    @assert_eq(out.len, out.maybe_uninit.len, "wrong signeture size");
}

fn byte_swap_fields($T: Type, self: *T) void #generic = {
    inline_for T.get_fields() { $f | 
        @if(:: (f[].ty == u32 || f[].ty == u64)) {
            inner := T.get_field_ptr(self, f);
            inner[] = byte_swap(inner[]);
        };
    };
}

#use("@/backend/lib.fr");
