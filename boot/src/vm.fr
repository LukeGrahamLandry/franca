
// TODO: the combinatorics of (shim, dyncall) x (native, easy_abi) are getting annoying. 
//       but the plan is to convert the boot part to another language so adding abstraction now isn't super useful.  

// caller: easy_abi, compiler: easy_abi, callee: native
easy_to_native_dyncall_shim :: fn(self: *SelfHosted, f_ty: FnType) DynCallShim = {
    @debug_assert(abi_shift_easy_to_native);
    shared := self.comptime_codegen;
    ins: RawList(Qbe.Ins) = empty();
    id := Qbe.no_symbol_S; 
    enter_task shared { entry |
        f := temp().box_uninit(Qbe.Fn);
        f.default_init(shared.m);
        f.leaf = false;
        f.lnk.no_inline = true;
        id = shared.m.intern(@tfmt("dyncallEN%_%_%", f_ty.arg.as_index(), f_ty.ret.as_index(), int(f_ty.unary)));
        f.lnk.id = id;
        entry.task = (Func = f);
        
        input, output := (f.newtmp("input", .Kl), f.newtmp("output", .Kl));
        addins(ins&, make_ins(.par, .Kl, output, QbeNull, QbeNull));
        addins(ins&, make_ins(.par, .Kl, input, QbeNull, QbeNull));
        
        arg_p  := f.newtmp("arg", .Kl);
        ret_p  := f.newtmp("ret", .Kl);
        callee := f.newtmp("call", .Kl);
        
        a, r, c := (input, f.newtmp("r", .Kl), f.newtmp("c", .Kl));
        addins(ins&, make_ins(.add, .Kl, r, input, f.getcon(8)));
        addins(ins&, make_ins(.add, .Kl, c, input, f.getcon(16)));
        addins(ins&, make_ins(.load, .Kl, arg_p, a, QbeNull));
        addins(ins&, make_ins(.load, .Kl, ret_p, r, QbeNull));
        addins(ins&, make_ins(.load, .Kl, callee, c, QbeNull));
        
        ins = emit_dyncall_shim_epilogue(self, shared, f, f_ty, arg_p, ret_p, callee, Qbe.Null, ins);
    };
    (Pending = id)
};

// caller: easy_abi, compiler: native, callee: easy_abi
easy_abi_emit_shim :: fn(c: *SelfHosted, fid: FuncId) void = {
    @debug_assert(abi_shift_native_to_easy);
    shared := c.comptime_codegen;
    handler: rawptr = @run wrap_with_easy_abi(report_called_uncompiled_or_just_fix_the_problem);
    id := Qbe.no_symbol_S;
    enter_task shared { entry |
        id = shared.m.intern(@tfmt("%__Eshim", fid));
        
        f := temp().box_uninit(Qbe.Fn);
        f.default_init(shared.m);
        f.lnk.no_inline = true;
        f.lnk.id = id;
        f.start = newblk(); 
        f.nblk = 1;
        target_id := c.comptime_jit_symbol_id(fid);
        entry.task = (Shim = (f = f, shim_for = target_id));
        
        @emit_instructions((f = f), (SelfHosted.int_from_ptr(c), fid.as_index(), handler.int_from_rawptr()), """
        @start
            %output =l par
            %input =l par
            %arg =l alloc8 16
            %ret =l alloc8 8
            %arg2 =l add %arg, 8
            storel %0, %arg
            storel %1, %arg2
            call %2(l %ret, l %arg)
            %callee =l load %ret
            call %callee(l %output, l %input)
        """);
        f.start.ins&.push_all(f.slice_pending_scratch(), temp());
        f.reset_scratch();
        
        f.ret_cls = .Kx;
        f.start.jmp.type = .ret0;
    };
    c.created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id));
};

// caller: native, compiler: easy_abi, callee: native
native_abi_emit_shim :: fn(c: *SelfHosted, fid: FuncId) void = {
    @debug_assert(abi_shift_easy_to_native);
    shared := c.comptime_codegen;
    handler: rawptr = report_called_uncompiled_or_just_fix_the_problem;
    m := shared.m; 
    func := c.get_function(fid);
    id := Qbe.no_symbol_S;
    enter_task shared { entry |
        id = m.intern(@tfmt("%__Nshim", fid));
        
        f := temp().box_uninit(Qbe.Fn);
        f.default_init(m);
        f.lnk.no_inline = true;
        f.lnk.id = id;
        f.start = newblk(); 
        f.nblk = 1;
        target_id := c.comptime_jit_symbol_id(fid);
        entry.task = (Shim = (f = f, shim_for = target_id));
        
        arg_ty := func.finished_arg.expect("arg infered");
        ret_ty := func.finished_ret.expect("ret infered");
        pars := emit_par_instructions(c.comp(), shared, func.arg&, arg_ty, f, .None);
        callee := f.newtmp("callee", .Kl);
        
        // running on the vm but jitting to real code, handler is in the compiler 
        // so can't direct call it, need to escape to the vm. 
        @emit_instructions((f = f), (SelfHosted.int_from_ptr(c), fid.as_index(), handler.int_from_rawptr(), FR_vm_escape_call.int_from_rawptr(), callee), """
        @start
            %arg =l alloc8 16
            %ret =l alloc8 8
            %arg2 =l add %arg, 8
            storel %0, %arg
            storel %1, %arg2
            %r0 =l alloc8 24
            %r1 =l add %r0, 8
            %r2 =l add %r0, 16
            storel %ret, %r0
            storel %arg, %r1
            storel %2, %r2
            call %3(l 0, l %r0)
            %4 =l load %ret
        """);
        f.start.ins&.push_all(f.slice_pending_scratch(), temp());
        f.reset_scratch();
        
        emit_native_shim_epilogue(c.comp(), shared, f, ret_ty, f.start, pars, callee);
    };
    c.created_jit_fn_ptr_value(fid, shared.wait_for_symbol(id));
};

FR_vm_escape_call :: fn() void #weak #libc;

// this is the inverse of create_dyncall_shim. 
// caller: native, compiler: easy_abi, callee: easy_abi
wrap_with_native_abi :: fn(self: *SelfHosted, func: *Func, callee: i64) rawptr = {
    @if(callee == 0) return(zeroed(rawptr));
    
    @debug_assert(abi_shift_easy_to_native);
    shared := self.comptime_codegen;
    m := shared.m; 
    f_ty := func.finished_ty().unwrap();
    
    name := @tfmt("dyncallNE_%_%_%_%_%", 
        callee, f_ty.arg, f_ty.ret, f_ty.unary.int(), self.pool.get(func.name));
    id := m.intern(name);
    if m.get_addr(id) { p, _ |
        return p;
    };
    enter_task shared { entry |
        f := temp().box_uninit(Qbe.Fn);
        f.default_init(m);
        f.lnk.no_inline = true;
        f.lnk.id = id;
        f.start = newblk(); 
        f.nblk = 1;
        entry.task = (Func = f);
        
        arg_ty := func.finished_arg.expect("arg infered");
        ret_ty := func.finished_ret.expect("ret infered");
        pars := emit_par_instructions(self.comp(), shared, func.arg&, arg_ty, f, .None);
        
        input, output := (f.newtmp("input", .Kl), f.newtmp("output", .Kl));
        b := f.start;
        
        push(b, .alloc8, .Kl, output, f.getcon(self.get_info(f_ty.ret)[].stride_bytes.zext()), Qbe.Null);
        push(b, .alloc8, .Kl, input, f.getcon(self.get_info(f_ty.arg)[].stride_bytes.zext()), Qbe.Null);
        
        off := 0;
        enumerate pars { i, it |
            dest := f.newtmp("in", .Kl);  
            info := self.get_info(it.type);
            size: i64 = info.stride_bytes.zext();
            off = off.ualign_to(info.align_bytes.zext());
            push(b, .add, .Kl, dest, input, f.getcon(off));
            off = off.add(size);
            if it.type_index != Qbe.Null {
                push(b, .blit0, .Kw, Qbe.Null, it.r, dest);
                push(b, .blit1, .Kw, Qbe.Null, INT(size), Qbe.Null);
            } else {
                if size != 0 {
                    o := store_op(self.comp(), it.type);
                    push(b, o, .Kw, Qbe.Null, it.r, dest);
                };
            };
        };
        
        if callee < 0 {
            // its not as simple as just calling vm.eval because if you get here 
            // it means you're running in the vm right now, you need to call into 
            // the parent's vm not a new one. 
            r0, r1, r2 := (f.newtmp("stash", .Kl), f.newtmp("stash", .Kl), f.newtmp("stash", .Kl));
            push(b, .alloc8, .Kl, r0, f.getcon(24), Qbe.Null);
            push(b, .add, .Kl, r1, r0, f.getcon(8));
            push(b, .add, .Kl, r2, r0, f.getcon(16));
            push(b, .storel, .Kw, Qbe.Null, output, r0);
            push(b, .storel, .Kw, Qbe.Null, input, r1);
            push(b, .storel, .Kw, Qbe.Null, f.getcon(callee), r2);
            input = r0;
            callee = int_from_rawptr(@as(rawptr) FR_vm_escape_call);
        };
        
        push(b, .arg, .Kl, Qbe.Null, output, Qbe.Null);
        push(b, .arg, .Kl, Qbe.Null, input, Qbe.Null);
        push(b, .call, .Kw, Qbe.Null, f.getcon(callee), Qbe.Null);
        
        o, k := load_op(self.comp(), f_ty.ret);
        if k == .Ke {
            if self.get_info(f_ty.ret)[].stride_bytes != 0 {
                f.retty = get_aggregate(self.comp(), shared, f_ty.ret);
                f.ret_cls = .Ke;
                b.jmp = (type = .retc, arg = output);
            } else {
                b.jmp.type = .ret0;
            };
        } else {
            r := f.newtmp("r", k);
            push(b, .load, k, r, output, Qbe.Null);
            b.jmp = (type = retk(k), arg = r);
            f.ret_cls = k;
        };
    };
    shared.wait_for_symbol(id)
};

vm_escape_call :: fn(output: *u8, input: *u8, callee: i64) void = {
    @if(USE_VM) @debug_assert_lt(callee, 0, "vm_escape_call");
    self := current_comptime();
    // TODO: pass in the right sizes
    self.vm.eval(callee, output.slice(4096), input.slice(4096));
};

hack_trace_module :: fn(m: *Qbe.Module) void = {
    _ := push_resolver(QbeModule, m, find_ip_in_module);  // :LEAK
}

make_fake_func :: fn(self: *SelfHosted, name: Symbol, f_ty: FnType) Func = {
    args := self.arg_types(f_ty.arg);
    func := zeroed Func;
    func.name = name;
    enumerate args { i, a |
        func.arg.bindings&.push((
            name = (Ident = self.comp().get_tuple_field_name(i)),
            ty = (Finished = a[]),
            nullable_tag = zeroed(*Annotations),
            default = binding_missing_default(zeroed(Span)),
            kind = .Var,
        ), temp());
    };
    func.finished_arg = (Some = f_ty.arg);
    func.finished_ret = (Some = f_ty.ret);
    func
};

// :SLOW memo this
wrap_alloc_with_native :: fn(self: *SelfHosted, a: *Alloc) void = {
    name := self.pool.insert_owned("anon_alloc");
    func := make_fake_func(self, name, (  // :get_or_create_type
        arg = self.tuple_of(@slice(i64, i64, i64, i64, i64)),
        ret = self.tuple_of(@slice(i64, i64)),
        unary = false,
    ));
    callee := bit_cast_unchecked(@type a.vptr, i64, a.vptr);
    callee := wrap_with_native_abi(self, func&, callee);
    a.vptr = bit_cast_unchecked(rawptr, @type a.vptr, callee);
};

// :SLOW memo this
wrap_vtable_with_native :: fn(self: *SelfHosted, vtable: *ImportVTable) void = {
    var := find_var_in_scope(self, self.pool.insert_owned("ImportVTable"), TOP_LEVEL_SCOPE).unwrap();
    res := self.poll_in_place(Ty(Values, Type), => self.find_const(var, (Specific = Type)));
    value, type := self.unwrap_report_error(Ty(Values, Type), res);
    @assert_eq(type, Type);  // :get_or_create_type
    T := Type.assume_cast(value&)[];
    T := self.raw_type(T);
    info := self.get_type(T);
    @assert(info.is(.Struct));
    for info.Struct.fields { f |
        info := self.get_type(f.ty);
        @if_let(info) fn FnPtr(it) => {
            func := Vm'make_fake_func(self, f.name, it.ty);
            off := f.byte_offset;
            p := bit_cast_unchecked(*ImportVTable, *rawptr, vtable).offset_bytes(off);
            p[] = Vm'wrap_with_native_abi(self, func&, p[].int_from_rawptr());
        };
    };
};

init_vm_and_tls :: fn(self: *SelfHosted) void = {
    n := franca_required_stack_bits - 1;
    opts := BuildOptions.ptr_from_raw(self.env.build_options);
    a := self.get_alloc();
    opts.tls_stack_bits = n;
    if USE_VM {
        stack := import("@/lib/sys/threads.fr")'allocate_stack_raw(prefer_syscalls(), current_os(), n);
        self.vm = self.get_alloc().boxed(Vm.Vm, (
            stack = stack.usable,
            frame = self.get_alloc().box_zeroed(Vm.Frame),
            fns = list(self.get_alloc()),
        ));
        self.vm.frame.stack_top = self.vm.stack.len;
        self.vm.fns&.push(zeroed(Vm.FnPair));
        t := bit_cast_unchecked(*u8, *StaticTls, stack.usable.ptr);
        Vm'init_child_tls(a, t);
        tls(.panic_hook)[] = vm_backtracing_panic_handler;
    } else {
        t := get_tls_from_sp(n);
        Vm'init_child_tls(a, t);
    };
};

get_tls_from_sp :: fn(n: i64) *StaticTls = {
    aa := @uninitialized i64;
    base := i64.int_from_ptr(aa&).bit_and(bit_not(1.shift_left(n) - 1));
    StaticTls.ptr_from_int(base)
};

// TODO: since `t` is on the vm's stack, i don't have to change franca_required_stack_bits.
//       so could get rid of the extra code i added for getting it out of BuildOptions. 
init_child_tls :: fn(a: Alloc, t: *StaticTls) void = {
    @if(@run(__driver_abi_version < 1512)) unreachable();
    @debug_assert(abi_shift_native_to_easy);
    
    real_tls := StaticTls.ptr_from_int(get_stack_base_for_tls());
    ::ptr_utils(StaticTls);
    @assert(!identical(t, real_tls));
    t[] = real_tls[];
    // the jitted code gets it from `__baked_compctx` instead. 
    // this needs to be null at the end when calling into the driver 
    // since it wants to run the fully jitted compiler. might as well do it now. 
    t.comptime = zeroed(rawptr);
    
    Crash :: import("@/lib/crash_report.fr");
    it :: vm_backtracing_panic_handler;
    hook := bit_cast_unchecked(rawptr, @type tls(.panic_hook)[], @run wrap_with_easy_abi(it));
    t.panic_hook = hook;
    A :: import("@/lib/alloc/fixed_block.fr").BlockAlloc;
    gen := a.boxed(A, init(page_allocator));
    gen := gen.borrow();
    gen.vptr = bit_cast_unchecked(rawptr, @type gen.vptr, @run wrap_with_easy_abi(A.A));
    t.general_allocator = gen;
    tmp := a.boxed(Arena.Allocator, init(page_allocator, 1.shift_left(20)));
    tmp := tmp.borrow();
    tmp.vptr = bit_cast_unchecked(rawptr, @type tmp.vptr, @run wrap_with_easy_abi(Arena.arena_allocator_fn));
    t.temporary_allocator_i = tmp;
};

enqueue :: fn(shared: *CodegenShared, entry: *CodegenEntry) void = {
    @if(@run(__driver_abi_version < 1512)) unreachable();
    @debug_assert(!shared.threaded, "threaded");
    vm, m := (current_comptime()[].vm, shared.m);
    
    log := entry.logging.len != 0;
    @match(entry.task) {
        fn Func(f) => {
            if log {
                f.set_block_id();
                printfn(f, m.debug_out);
                m.flush_debug();
            };
        
            use_symbol(m, f.lnk.id) { s |
                s.jit_addr = vm.push_fn(f);
                s.kind = .Local;
                @debug_assert_le(s.fixups.len, 1);  // no emitdat, just the one for Shim
                m.do_fixups(s);
            };
        }
        fn JitImport(it) => put_jit_addr(m, it.lnk.id, it.addr);
        fn Bounce(it) => {
            use_symbol(m, it.target) { target_s |
                use_symbol(m, it.lnk.id) { alias_s |
                    @assert(!target_s.jit_addr.is_null(), "TODO: bounce");
                    alias_s.jit_addr = target_s.jit_addr;
                    alias_s.kind = target_s.kind;
                    @debug_assert_le(alias_s.fixups.len, 1);  // no emitdat, just the one for Shim
                    m.do_fixups(alias_s);
                };
            };
        }
        fn Shim(it) => {
            if log {
                f := it.f;
                f.set_block_id();
                printfn(f, m.debug_out);
                m.flush_debug();
            };
            use_symbol(m, it.shim_for) { s |
                s.jit_addr = vm.push_fn(it.f);
                s.shim_addr = s.jit_addr;
                use_symbol(m, it.f.lnk.id) { shim_s |
                    shim_s.jit_addr = s.jit_addr;
                    @debug_assert_eq(shim_s.fixups.len, 0);
                };
                @debug_assert_eq(s.fixups.len, 0);
                if ensure_got_slot(m, s, s.jit_addr) { fix |
                    push_fixup(m, s, fix);
                };
            };
        }
        @default => @panic("boot/enqueue: %", entry.task&.tag());     
    };
    
    // TODO: annoying that you have to remember to do this
    shared.pipe.done&.push(entry);
};

fn push_fn(self: *Vm, f: *Qbe.Fn) rawptr = {
    m := f.globals;
    if f.name().starts_with("replace_switches") {
        // HACK: todo: how can this possibly be the only one it doesn't work for, thats a crazy coincidence
        replace_switches(f);
    } else {
        run_qbe_passes_common(f);
        //Simplify'simplify(f);
        f.assign_alloc_slots();
        f.collapse_addr();
        f.assign_rslot();
        //import("@/backend/opt/slots.fr")'elide_abi_slots(f); 
    };
    slots := f.slot.intcast();
    f := clone(f);
    i := self.fns.len;
    self.fns&.push(f = f, m = m, slots = slots.ualign_to(8));
    rawptr_from_int(-i)
}

assign_rslot :: fn(f: *Qbe.Fn) void = {
    for_blocks f { b | 
        fix :: fn(r: *Qbe.Ref) => if f.get_tmp(r[]) { t |
            if t.slot != -1 {
                r[] = SLOT(t.slot);
            };
        };
        for_phi b { p |
            each(p.arg.slice(0, p.narg.zext()), fix);
        };
        each b.ins { i |
            each(i.arg&, fix);
        };
        fix(b.jmp.arg&);
    };
};

// tag is the high bit of the function pointer. 
// real function pointers are just their value so just wrap_with_easy_abi is enough and it's easy to toggle USE_VM. 
fn unpack_callable(self: *Vm, f: i64) Callable = {
    @if(f < 0) return(Fn = self.fns[-f]);
    (EasyAbi = bit_cast_unchecked(i64, EasyAbiFn, f))
}

Vm :: @struct {
    // must be AlignedStack because the child program will needs to have its own tls. 
    stack: []u8;  
    frame: *Frame;
    fns: List(FnPair);
};
Frame :: @rec @struct {
    prev: *Frame;
    f: *Qbe.InlineFn;
    m: *Qbe.Module;
    // index into .stack of next unused byte. starts at vm.stack.len and grows down. 
    stack_top: i64;  
    stack_base: i64;  // stack[RSlot(0)]
    refs: []rawptr;  // [RTmp]
    b: *Qbe.Blk;
    i: i64;          // b.ins[i]
    par: Array(rawptr, 2);
    dbgloc := zeroed(*Qbe.Ins);
    is_entry := false;
};
FnPair :: @struct(f: *Qbe.InlineFn, m: *Qbe.Module, slots := 0);
EasyAbiFn :: @FnPtr(r: rawptr, a: rawptr) void;
Callable :: @tagged(
    Fn: FnPair,
    EasyAbi: EasyAbiFn,
);

fn eval(self: *Vm, f: i64, r: []u8, a: []u8) void = {
    rr, aa := (u8.raw_from_ptr(r.ptr), u8.raw_from_ptr(a.ptr));
    @match(self.unpack_callable(f)) {
        fn Fn(f) => self.eval(f, rr, aa);
        fn EasyAbi(f) => f(rr, aa);
    };
}

fn eval(self: *Vm, f: FnPair, output: rawptr, input: rawptr) void = {
    end_frame := self.frame;
    self.push_frame(f, output, input);
    self.frame.is_entry = true;
    self.spin(end_frame);
}

// :SLOW
// this could be made much faster if i ran the opt passes on the ir first. 
// especially converting to an ir designed for interpreting not compiling (ie. not ssa). 
// but the point is to eventually rewrite it in c (where it can't depend my opt stuff)
// and only use it once to bootstrap so maybe there's no point. 
fn spin(self: *Vm, end_frame: *Frame) void = {
    operator_index :: index_unchecked;
    loop {
        break :: local_return;
        n_par := 0;
        frame := self.frame;
        b := frame.b;
        idx := frame.i;
        while => idx < b.ins.len {
            i := b.ins[idx]&; 
            idx += 1;
            @match(i.op()) {
                fn par() => {
                    frame.set_ref(i.to, frame.par&[n_par]);
                    n_par += 1;
                }
                fn arg() => {
                    a0 := self.get_ref(i.arg&[0]);
                    i := b.ins[idx]&; idx += 1;
                    @debug_assert_eq(i.op(), .arg);
                    a1 := self.get_ref(i.arg&[0]);
                    i := b.ins[idx]&; idx += 1;
                    @debug_assert_eq(i.op(), .call);
                    frame.b = b;
                    frame.i = idx;
                    
                    f := self.get_ref(i.arg&[0]).int_from_rawptr();
                    @match(self.unpack_callable(f)) {
                        fn Fn(f) => self.push_frame(f, a0, a1);
                        // it might call self.eval again so must be re-entrant. 
                        // (and end_frame isn't always the one with stack_top=stack.len)
                        fn EasyAbi(f) => f(a0, a1);
                    };
                    break();  // tail self.spin();
                }
                fn cas0() => {
                    a0 := self.get_ref(i.arg&[0]);
                    i := b.ins[idx]&; idx += 1;
                    @debug_assert_eq(i.op(), .cas1);
                    p, old, new := (a0, self.get_ref(i.arg&[0]), self.get_ref(i.arg&[1]));
                    result := @if(i.cls() == .Kw, 
                        do_cas(.Kw, p, old, new), 
                        do_cas(.Kl, p, old, new));
                    frame.set_ref(i.to, result);
                }
                fn blit0() => {
                    a0 := self.get_ref(i.arg&[0]);
                    a1 := self.get_ref(i.arg&[1]);
                    i := b.ins[idx]&; idx += 1;
                    @debug_assert_eq(i.op(), .blit1);
                    src, dest, size := (a0, a1, i.arg&[0].rsval().intcast().abs());
                    copy_bytes(dest, src, size);
                }
                fn nop() => ();
                fn dbgloc() => {
                    frame.dbgloc = i;
                }
                fn copy() => {
                    frame.set_ref(i.to, self.get_ref(i.arg&[0]));
                }
                // only if doing opt passes
                fn sel0() => {
                    c := i.arg&[0];
                    i := b.ins[idx]&; idx += 1;
                    @debug_assert_eq(i.op(), .sel1);
                    r := int(!self.eval_cond(c));
                    r := self.get_ref(i.arg&[r]);
                    frame.set_ref(i.to, r);
                }
                @default => self.simple_op(frame, i);
            };
        };
        
        frame.i = 0;
        @match(b.jmp.type) {
            fn ret0() => {
                self.frame = frame.prev;
                ::ptr_utils(Frame);
                if identical(self.frame, end_frame) {
                    return();
                };
            }
            fn jmp() => {
                self.jump_to(frame, b.s1);
            }
            fn jnz() => {
                c := self.eval_cond(b.jmp.arg);
                self.jump_to(frame, @if(c, b.s1, b.s2));
            }
            fn hlt() => panic("vm halted");
            @default => @panic("vm bad jmp: %", b.jmp.type);
        };
        // tail self.spin();
    };
}

fn eval_cond(self: *Vm, r: Qbe.Ref) bool #inline = {
    c := self.get_ref(r);
    c: u32 = c.int_from_rawptr().trunc();  // :jnz_is_Kw
    c != 0
}

fn jump_to(self: *Vm, frame: *Frame, b: *Qbe.Blk) void #inline = {
    // only if doing opt passes
    for_phi b { p |
        a := index_in_phi(frame.b, p);
        frame.set_ref(p.to, self.get_ref(p.arg[a]));
    };
    
    frame.b = b;
}

fn simple_op(self: *Vm, frame: *Frame, i: *Qbe.Ins) void = {
    operator_index :: index_unchecked;
    
    // only if not doing opt passes
    if @is(i.op(), .alloc4, .alloc8) {
        size := frame.f.con[i.arg&[0].val()]&;
        @debug_assert(rtype(i.arg&[0]) == .RCon && size.type() == .CBits);
        size := size.bits();
        frame.stack_top -= size.ualign_to(8);
        result := bit_cast_unchecked(*u8, rawptr, self.stack.index(frame.stack_top));
        frame.set_ref(i.to, result);
        return();
    };
    
    a0, a1 := (
        self.get_ref(i.arg&[0]).int_from_rawptr(),
        self.get_ref(i.arg&[1]).int_from_rawptr(),
    );
    
    #use("@/backend/opt/fold.fr");
    is_memory :: fn(o: Qbe.O) = @is(o, 
        .storeb, .storeh, .storew, .storel, .stores, .stored,
        .loadsb, .loadub, .loadsh, .loaduh, .loadsw, .loaduw, .load,
    );
    result := if is_memory(i.op()) {
        impl :: gen_do_fold_impl(is_memory);
        do_fold(i.op(), i.cls(), a0, a1, impl)
    } else {
        @debug_assert(OpTab'get(i.op(), .can_fold), "bad vm op %", i.op());
        do_fold(i.op(), i.cls(), a0, a1)
    };
    frame.set_ref(i.to, result.rawptr_from_int());
};

fn do_cas($k: Qbe.Cls, p: rawptr, old: rawptr, new: rawptr) rawptr = {
    @run enable_franca_ir_types(Ty(Qbe.O, Qbe.Cls));
    T :: @if(k == .Kl, rawptr, u32);
    part0 :: fn(p: rawptr) void #ir(.cas0, .Kw);
    part1 :: fn(old: rawptr, new: rawptr) T #ir({ (.cas1, k) });
    part0(p);
    r := part1(old, new);
    @if(k == .Kl, r, r.zext().rawptr_from_int())
};

fn set_ref(frame: *Frame, r: Qbe.Ref, result: rawptr) void = {
    operator_index :: index_unchecked;
    @debug_assert(@is(rtype(r), .RTmp, .RNull), "bad set_ref");
    frame.refs[r.val()] = result;
}

fn get_ref(self: *Vm, r: Qbe.Ref) rawptr = {
    operator_index :: index_unchecked;
    @match(rtype(r)) {
        fn RTmp() => self.frame.refs[r.val()];
        fn RCon() => {
            c := self.frame.f.con[r.val()]&;
            base := @match(c.type()) {
                fn CBits() => zeroed(rawptr);
                fn CAddr() => {
                    // a, _ := get_addr(self.frame.m, c.sym).unwrap();
                    // use_threads=false
                    m, id := (self.frame.m, c.sym);
                    bid: i64 = id.id.zext().bit_and(IMask);
                    bucket := m.symbols[bid]&;
                    idx := id.id.shift_right_logical(IBits);
                    symbol := bucket.data.index(idx.zext());
                    a := symbol.jit_addr;
                    @debug_assert(!a.is_null());
                    a
                };
            };
            base.offset(c.bits())
        }
        fn RSlot() => {
            p := self.stack.index(self.frame.stack_base).offset(r.val());
            u8.raw_from_ptr(p)
        }
        fn RNull() => zeroed(rawptr);  // avoids special casing unary ops
        @default => @panic("get_ref %", r);
    }
}

fn push_frame(self: *Vm, f: FnPair, output: rawptr, input: rawptr) void = {
    frame_start := self.frame.stack_top - size_of(Frame);
    frame := bit_cast_unchecked(*u8, *Frame, self.stack.index(frame_start));
    n := f.f.ntmp.zext();
    refs_start := frame_start - (n * size_of(rawptr));
    refs := bit_cast_unchecked(*u8, *rawptr, self.stack.index(refs_start));
    stack_base := refs_start - f.slots;
    frame[] = (
        stack_base = stack_base,
        stack_top = stack_base,
        refs = refs.slice(n),
        f = f.f,
        m = f.m,
        prev = self.frame,
        b = f.f.blocks[0]&,
        i = 0,
        par = (output, input),
    );
    self.frame = frame;
}

fn log_backtrace(self: *Vm) void = {
    @if(!USE_VM) return();
    frame := self.frame;
    @if(frame.stack_top == self.stack.len) return();
    out := @ref u8.list(temp());
    @fmt(out, "\n--- vm backtrace ---\n");
    while => frame.stack_top != self.stack.len {
        @fmt(out, "- $% @%[%] ", frame.m.str(frame.f.lnk.id), frame.b.name(), frame.i);
        @if(!frame.dbgloc.is_null()) if frame.m.codemap { codemap |
            // TODO: instead of tracking frame.dbgloc, just walk backwards from frame.i
            //       then can also skip DebugTag.HIDE
            dump_dbgloc(codemap, frame.dbgloc, out);
        };
        @fmt(out, "\n");
        if frame.is_entry {
            @fmt(out, "---\n");
        }
        frame = frame.prev;
    };
    print(out.items());
}
