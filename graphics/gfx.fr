// simple 3D API wrapper
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.
// commit: 41247ec237c1eebab46630caaadaf080ba1914a4
// 
// This is independent of how you open a window and get the 3D context,
// app.fr has sglue_*** functions that provide the information we need.   
// 
// Changes from Sokol
// - generics are a surprise tool that can help us later
// - just use the field for: sg_isvalid, sg_query_backend, sg_query_features, sg_query_limits
// - removed the query_*_(desc/info/____) functions
// - no limit for commit listeners. removing doesn't make holes in the array 
//   so the order will be different. 
// - _start_canary is not super exciting to me
//
// UNFINISHED
// - logging and trace hooks don't work
// - init(Attachments) 
// - compute shaders
// - validate Shader.Desc _sg_validate_slot_bits
// - shutdown (dealloc/deinit/fail resources)
// - move structs from bindings/sokol to here
// - clean up defaults (compiler: implement partial array literals that respect default fields)
// 
// TODO: meta programming thing to replace _SG_TRACE_ARGS with #trace
//       call sg_trace_hooks with args and return value. 
//       (.tail) there are early returns but only hook the fallthough at the end
//       (.before) do it before the implementation 
//
// TODO: go through an replace my @eprintln with a proper logging system
// 

SOKOL_DEBUG :: true;  // TODO: use .DebugAssertions
fn ASSERT_METAL() void = @debug_assert(SOKOL_BACKEND.is_metal());


SlotHeader :: @struct {
    id: u32;
    state: SgResourceState;
};

// resource tracking (for keeping track of gpu-written storage resources
_sg_tracker_t :: RawList(Buffer);

_SG_STRING_SIZE :: 32;
_SG_SLOT_SHIFT :: 16;
_SG_SLOT_MASK :: _SG_SLOT_SHIFT-1;
_SG_MAX_POOL_SIZE :: 1.shift_left(_SG_SLOT_SHIFT);
_SG_DEFAULT_BUFFER_POOL_SIZE :: 128;
_SG_DEFAULT_IMAGE_POOL_SIZE :: 128;
_SG_DEFAULT_SAMPLER_POOL_SIZE :: 64;
_SG_DEFAULT_SHADER_POOL_SIZE :: 32;
_SG_DEFAULT_PIPELINE_POOL_SIZE :: 64;
_SG_DEFAULT_ATTACHMENTS_POOL_SIZE :: 16;
_SG_DEFAULT_UB_SIZE :: 4 * 1024 * 1024;
_SG_DEFAULT_MAX_DISPATCH_CALLS_PER_PASS :: 1024;
_SG_DEFAULT_MAX_COMMIT_LISTENERS :: 1024;
_SG_DEFAULT_WGPU_BINDGROUP_CACHE_SIZE :: 1024;
SG_INVALID_ID :: 0;
SG_NUM_INFLIGHT_FRAMES :: 2;
SG_MAX_COLOR_ATTACHMENTS :: 4;
SG_MAX_UNIFORMBLOCK_MEMBERS :: 16;
SG_MAX_VERTEX_ATTRIBUTES :: 16;
SG_MAX_MIPMAPS :: 16;
SG_MAX_TEXTUREARRAY_LAYERS :: 128;
SG_MAX_UNIFORMBLOCK_BINDSLOTS :: 8;
SG_MAX_VERTEXBUFFER_BINDSLOTS :: 8;
SG_MAX_IMAGE_BINDSLOTS :: 16;
SG_MAX_SAMPLER_BINDSLOTS :: 16;
SG_MAX_STORAGEBUFFER_BINDSLOTS :: 8;
SG_MAX_IMAGE_SAMPLER_PAIRS :: 16;

fn _sg_def(val: ~T, def: T) T #where = 
    @if(bit_cast_unchecked(T, u32, val) == 0, def, val); 

BufferCommon :: @struct {
    size: i32;
    append_pos: i32;
    append_overflow: bool;
    update_frame_index: u32;
    append_frame_index: u32;
    num_slots: i32;
    active_slot: i32;
    type: SgBufferType;
    usage: SgUsage;
};

fn common_init(cmn: *BufferCommon, desc: *SgBufferDesc) void = {
    cmn.size = intcast desc.size;
    cmn.append_pos = 0;
    cmn.append_overflow = false;
    cmn.update_frame_index = 0;
    cmn.append_frame_index = 0;
    cmn.num_slots = @if(desc.usage == .IMMUTABLE, 1, SG_NUM_INFLIGHT_FRAMES);
    cmn.active_slot = 0;
    cmn.type = desc.type;
    cmn.usage = desc.usage;
}

ImageCommon :: @struct {
    upd_frame_index: u32;
    num_slots: i32;
    active_slot: i32;
    type: SgImageType;
    render_target: bool;
    width: i32;
    height: i32;
    num_slices: i32;
    num_mipmaps: i32;
    usage: SgUsage;
    pixel_format: SgPixelFormat;
    sample_count: i32;
};

fn common_init(cmn: *ImageCommon, desc: *Image.Desc) void = {
    cmn.upd_frame_index = 0;
    cmn.num_slots = @if(desc.usage == .IMMUTABLE, 1, Sg.SG_NUM_INFLIGHT_FRAMES);
    cmn.active_slot = 0;
    cmn.type = desc.type;
    cmn.render_target = desc.render_target;
    cmn.width = desc.width;
    cmn.height = desc.height;
    cmn.num_slices = desc.num_slices;
    cmn.num_mipmaps = desc.num_mipmaps;
    cmn.usage = desc.usage;
    cmn.pixel_format = desc.pixel_format;
    cmn.sample_count = desc.sample_count;
}

fn common_init(cmn: *SamplerCommon, desc: *Sampler.Desc) void = {
    cmn[] = desc.common;
}

ShaderAttrBaseType :: @enum(u32) (Undefined, Float, SInt, UInt);

_sg_shader_attr_t :: @struct {
    base_type: ShaderAttrBaseType;
};

_sg_shader_uniform_block_t ::  @struct {
    stage: SgShaderStage;
    size: u32;
};

_sg_shader_storage_buffer_t :: @struct {
    stage: SgShaderStage;
    readonly: bool;
};

_sg_shader_image_t :: @struct {
    stage: SgShaderStage;
    image_type: SgImageType;
    sample_type: SgImageSampleType;
    multisampled: bool;
};

_sg_shader_sampler_t :: @struct {
    stage: SgShaderStage;
    sampler_type: SgSamplerType;
};

// combined image sampler mappings, only needed on GL
_sg_shader_image_sampler_t :: @struct {
    stage: SgShaderStage;
    image_slot: u8;
    sampler_slot: u8;
};

ShaderCommon :: @struct {
    required_bindings_and_uniforms: u32;
    is_compute: bool;
    attrs: Array(_sg_shader_attr_t, SG_MAX_VERTEX_ATTRIBUTES);
    uniform_blocks: Array(_sg_shader_uniform_block_t, SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    storage_buffers: Array(_sg_shader_storage_buffer_t, SG_MAX_STORAGEBUFFER_BINDSLOTS);
    images: Array(_sg_shader_image_t, SG_MAX_IMAGE_BINDSLOTS);
    samplers: Array(_sg_shader_sampler_t, SG_MAX_SAMPLER_BINDSLOTS);
    image_samplers: Array(_sg_shader_image_sampler_t, SG_MAX_IMAGE_SAMPLER_PAIRS);
};

fn common_init(cmn: *ShaderCommon, desc: *Shader.Desc) void = {
    cmn.is_compute = !desc.compute_func.source.is_null() || !desc.compute_func.bytecode.ptr.is_null();
    range(0, SG_MAX_VERTEX_ATTRIBUTES) { i |
        cmn.attrs&[i].base_type = desc.attrs&[i].base_type;
    }
    range(0, SG_MAX_UNIFORMBLOCK_BINDSLOTS) { i |
        src := desc.uniform_blocks&[i]&;
        dst := cmn.uniform_blocks&[i]&;
        if src.stage != .NONE {
            it := cmn.required_bindings_and_uniforms&;
            it[] = bit_or(it[], 1.shift_left(i));
            dst.stage = src.stage;
            dst.size = src.size;
        }
    }
    required_bindings_flag: u32 = 1.shift_left(SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    range(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
        src := desc.storage_buffers&[i]&;
        dst := cmn.storage_buffers&[i]&;
        if src.stage != .NONE {
            it := cmn.required_bindings_and_uniforms&;
            it[] = bit_or(it[], required_bindings_flag);
            dst.stage = src.stage;
            dst.readonly = src.readonly;
        }
    }
    range(0, SG_MAX_IMAGE_BINDSLOTS) { i |
        src := desc.images&[i]&;
        dst := cmn.images&[i]&;
        if src.stage != .NONE {
            it := cmn.required_bindings_and_uniforms&;
            it[] = bit_or(it[], required_bindings_flag);
            dst.stage = src.stage;
            dst.image_type = src.image_type;
            dst.sample_type = src.sample_type;
            dst.multisampled = src.multisampled;
        }
    }
    range(0, SG_MAX_SAMPLER_BINDSLOTS) { i |
        src := desc.samplers&[i]&;
        dst := cmn.samplers&[i]&;
        if src.stage != .NONE {
            it := cmn.required_bindings_and_uniforms&;
            it[] = bit_or(it[], required_bindings_flag);
            dst.stage = src.stage;
            dst.sampler_type = src.sampler_type;
        }
    }
    range(0, SG_MAX_IMAGE_SAMPLER_PAIRS) { i |
        src := desc.image_sampler_pairs&[i]&;
        dst := cmn.image_samplers&[i]&;
        if src.stage != .NONE {
            dst.stage = src.stage;
            @debug_assert((src.image_slot >= 0) && (src.image_slot < SG_MAX_IMAGE_BINDSLOTS));
            @debug_assert(desc.images&[zext src.image_slot].stage == src.stage);
            dst.image_slot = src.image_slot;
            @debug_assert((src.sampler_slot >= 0) && (src.sampler_slot < SG_MAX_SAMPLER_BINDSLOTS));
            @debug_assert(desc.samplers&[zext src.sampler_slot].stage == src.stage);
            dst.sampler_slot = src.sampler_slot;
        }
    }
}

PipelineCommon :: @struct {
    vertex_buffer_layout_active: Array(bool, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    use_instanced_draw: bool;
    required_bindings_and_uniforms: u32;
    common: SgPipelineShared #use;
    shader_t: *Shader.T;
};

fn common_init(cmn: *Pipeline.Common, desc: *Pipeline.Desc) void = {
    @debug_assert((desc.color_count >= 0) && (desc.color_count <= intcast SG_MAX_COLOR_ATTACHMENTS));

    // FIXME: most of this isn't needed for compute pipelines

    required_bindings_flag: u32 = 1.shift_left(SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    rbau := cmn.required_bindings_and_uniforms&;
    range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
        a_state := desc.layout.attrs&.index(i);
        if (a_state.format != .INVALID) {
            @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
            cmn.vertex_buffer_layout_active&[intcast a_state.buffer_index] = true;
            rbau[] = bit_or(rbau[], required_bindings_flag);
        }
    }
    cmn.use_instanced_draw = false;
    cmn.common = desc.common;
    if cmn.index_type != .NONE {
        rbau[] = bit_or(rbau[], required_bindings_flag);
    }
}

AttachmentsCommon :: @struct {
    width: i32;
    height: i32;
    num_colors: i32;
    colors: Array(SgAttachmentDesc, SG_MAX_COLOR_ATTACHMENTS);
    resolves: Array(SgAttachmentDesc, SG_MAX_COLOR_ATTACHMENTS);
    depth_stencil: SgAttachmentDesc;
};

fn common_init(cmn: *Attachments.Common, desc: *Attachments.Desc, width: i32, height: i32) void = {
    @debug_assert((width > 0) && (height > 0));
    cmn.width = width;
    cmn.height = height;
    range(0, SG_MAX_COLOR_ATTACHMENTS) { i |
        if (desc.colors[i].image.id != SG_INVALID_ID) {
            cmn.num_colors += 1;
            cmn.colors&[i] = desc.colors&[i];
            cmn.resolves&[i] = desc.resolves&[i];
        }
    }
    if (desc.depth_stencil.image.id != SG_INVALID_ID) {
        cmn.depth_stencil = desc.depth_stencil;
    }
}

_SG_INVALID_SLOT_INDEX :: 0;  // NOT a (random number chosen by fair die roll)

_sg_commit_listeners_t :: RawList(SgCommitListener);

ResolvedBindings :: @struct {
    pip: *Pipeline.T;
    vb_offsets: Array(i32, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    ib_offset: i32;
    vbs: Array(*Buffer.T, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    ib: *Buffer.T;
    imgs: Array(*Image.T, SG_MAX_IMAGE_BINDSLOTS);
    smps: Array(*Sampler.T, SG_MAX_SAMPLER_BINDSLOTS);
    sbufs: Array(*Buffer.T, SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Self :: @rec @struct {
    mtl: Metal.Impl;
    valid: bool;
    desc: SgDesc;    // original desc with default values patched in
    frame_index: u32;
    cur_pass: @struct {
        valid: bool;
        in_pass: bool;
        is_compute: bool;
        atts_id: Attachments;     // SG_INVALID_ID in a swapchain pass
        atts: *Attachments.T;    // 0 in a swapchain pass
        width: i32;
        height: i32;
        swapchain: @struct {
            color_fmt: SgPixelFormat;
            depth_fmt: SgPixelFormat;
            sample_count: i32;
        };
    };
    cur_pipeline: Pipeline;
    next_draw_valid: bool;
    required_bindings_and_uniforms: u32;    // used to check that bindings and uniforms are applied after applying pipeline
    applied_bindings_and_uniforms: u32;     // bits 0..7: uniform blocks, bit 8: bindings
    validate_error: Str;
    compute: @struct {
        readwrite_sbufs: _sg_tracker_t;  // tracks read/write storage buffers used in compute pass
    };
    pools: EnumMap(PoolType, DynPool);
    backend: SgBackend;
    features: SgFeatures;
    limits: SgLimits;
    formats: EnumMap(SgPixelFormat, _sg_pixelformat_info_t);
    stats_enabled: bool;
    stats: FrameStat;
    prev_stats: FrameStat;
    hooks: sg_trace_hooks;
    commit_listeners: _sg_commit_listeners_t;
};

sg_trace_hooks :: void; // TODO

FrameStat :: EnumMap(Stat, u32);
Stat :: @enum(
    frame_index, num_passes, num_apply_viewport, num_apply_scissor_rect, 
    num_apply_pipeline, num_apply_bindings, num_apply_uniforms, num_draw, 
    num_dispatch, num_update_buffer, num_append_buffer, num_update_image,
    size_apply_uniforms, size_update_buffer, size_append_buffer, size_update_image,
    mtl_num_garbage_collected, mtl_idpool_num_released, mtl_idpool_num_added,
    mtl_bindings_num_set_vertex_buffer, mtl_bindings_num_set_compute_texture,
    mtl_bindings_num_set_fragment_texture, mtl_bindings_num_set_vertex_texture, 
    mtl_bindings_num_set_compute_sampler_state, mtl_bindings_num_set_fragmnet_sampler_state,
    mtl_bindings_num_set_vertex_sampler_state, mtl_bindings_num_set_compute_buffer, 
    mtl_bindings_num_set_fragment_buffer, mtl_pipeline_num_set_blend_color, 
    mtl_pipeline_num_set_cull_mode, mtl_pipeline_num_set_front_facing_winding, 
    mtl_pipeline_num_set_stencil_reference_value, mtl_pipeline_num_set_depth_bias,
    mtl_pipeline_num_set_render_pipeline_state, mtl_pipeline_num_set_depth_stencil_state, 
    mtl_uniforms_num_set_compute_buffer_offset, mtl_uniforms_num_set_fragment_buffer_offset,
    mtl_uniforms_num_set_vertex_buffer_offset, 
);

fn stat(sg: *Self, stat: Stat, n: u32) void = {
    sg.stats&[stat] += n;
}

fn impl(sg: *Self) *Metal.Impl = {
    ASSERT_METAL();
    sg.mtl&
}

fn _sg_log(log_item: sg_log_item, log_level: u32, msg: CStr, line_nr: u32) void = {
    if (sg.desc.logger.func) {
        const char* filename = 0;
        if SOKOL_DEBUG {
            filename = __FILE__;
            if (0 == msg) {
                msg = _sg_log_messages[log_item];
            }
        }
        sg.desc.logger.func("sg", log_level, (uint32_t)log_item, msg, line_nr, filename, sg.desc.logger.user_data);
    } else {
        // for log level PANIC it would be 'undefined behaviour' to continue
        if (log_level == 0) {
            abort();
        }
    }
}

fn _sg_align_u32(val: u32, align: u32) u32 = {
    @debug_assert((align > 0) && ((align & (align - 1)) == 0));
    (val + (align - 1)) & ~(align - 1)
}

_sg_recti_t :: @struct(x: i32, y: i32, w: i32, h: i32);

fn clipi(x: i32, y: i32, w: i32, h: i32, clip_width: i32, clip_height: i32) _sg_recti_t = {
    x = min(max(0, x), clip_width-1);
    y = min(max(0, y), clip_height-1);
    if ((x + w) > clip_width) {
        w = clip_width - x;
    }
    if ((y + h) > clip_height) {
        h = clip_height - y;
    };
    (x = x, y = y, w = max(w, 1), h = max(h, 1))
}

fn vertexformat_bytesize(it: SgVertexFormat) i32 = @map_enum(it) (
    FLOAT  = 4, FLOAT2  = 8, FLOAT3  = 12, FLOAT4  = 16, 
    INT    = 4, INT2    = 8, INT3    = 12, INT4    = 16, 
    UINT   = 4, UINT2   = 8, UINT3   = 12, UINT4   = 16,
    BYTE4  = 4, BYTE4N  = 4, UBYTE4  = 4, UBYTE4N  = 4, 
    SHORT2 = 4, SHORT2N = 4, USHORT2 = 4, USHORT2N = 4,
    SHORT4 = 8, SHORT4N = 8, USHORT4 = 8, USHORT4N = 8, 
    UINT10_N2 = 4, HALF2 = 4, HALF4 = 8, INVALID = 0,
);

fn vertexformat_basetype(fmt: SgVertexFormat) ShaderAttrBaseType = {
    if @is(fmt, .FLOAT, .FLOAT2, .FLOAT3, .FLOAT4, .HALF2, .HALF4, .BYTE4N, .UBYTE4N, .SHORT2N, .USHORT2N, .USHORT4N, .UINT10_N2) {
        return(.Float);
    };
    if @is(fmt, .INT, .INT2, .INT3, .BYTE4, .SHORT2, .SHORT4) {
        return(.SInt);
    };
    if @is(fmt, .UINT, .UINT2, .UINT3, .UBYTE4, .USHORT2, .USHORT4) {
        return(.UInt);
    };
    unreachable()
}

fn uniform_alignment(type: sg_uniform_type, array_count: i32, ub_layout: sg_uniform_layout) u32 = {
    if(ub_layout == .NATIVE, => return(1));
    @debug_assert(array_count > 0);
    if(array_count != 1, => return(16));
    @map_enum(type) (FLOAT = 4, INT = 4, FLOAT2 = 8, INT2 = 8, FLOAT3 = 16, FLOAT4 = 16, INT3 = 16, INT4 = 16, MAT4 = 16)
}

fn uniform_size(type: sg_uniform_type, array_count: u32, ub_layout: sg_uniform_layout) u32 = {
    @debug_assert(array_count > 0);
    single := @map_enum(type) (FLOAT = 4, INT = 4, FLOAT2 = 8, INT2 = 8, FLOAT3 = 12, INT3 = 12, FLOAT4 = 16, INT4 = 16, MAT4 = 64);
    if array_count == 1 || ub_layout == .NATIVE || type == .MAT4 {
        return single * array_count;
    } 
    16 * array_count
}

fn is_compressed(fmt: SgPixelFormat) bool = @is(fmt, 
    .BC1_RGBA, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC4_R, .BC4_RSN, 
    .BC5_RG, .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA, 
    .ETC2_RGB8, .ETC2_SRGB8, .ETC2_RGB8A1, .ETC2_RGBA8, .ETC2_SRGB8A8, 
    .EAC_R11, .EAC_R11SN, .EAC_RG11, .EAC_RG11SN, .ASTC_4x4_RGBA, .ASTC_4x4_SRGBA,
);

fn is_valid_rendertarget_color_format(fmt: sg_pixel_format) bool = {
    const int fmt_index = (int) fmt;
    @debug_assert((fmt_index >= 0) && (fmt_index < _SG_PIXELFORMAT_NUM));
    return sg.formats[fmt_index].render && !sg.formats[fmt_index].depth;
}

fn is_valid_rendertarget_depth_format(sg: *Self, fmt: SgPixelFormat) bool = {
    sg.formats&[fmt]&[.render] && sg.formats&[fmt]&[.depth]
}

fn is_depth_or_depth_stencil_format(fmt: SgPixelFormat) bool = 
    @is(fmt, .DEPTH, .DEPTH_STENCIL);

fn is_depth_stencil_format(fmt: SgPixelFormat) bool = 
    fmt == .DEPTH_STENCIL;

fn pixelformat_bytesize(it: SgPixelFormat) i32 = @if_else {
    @if(@is(it, .R8, .R8SN, .R8UI, .R8SI)) => 1;
    @if(@is(it, .R16, .R16SN, .R16UI, .R16SI, .R16F, .RG8, .RG8SN, .RG8UI, .RG8SI)) => 2;
    @if(@is(it, .R32UI, .R32SI, .R32F, .RG16, .RG16SN, .RG16UI, .RG16SI, .RG16F, .RGBA8)) => 4;
    @if(@is(it, .SRGB8A8, .RGBA8SN, .RGBA8UI, .RGBA8SI, .BGRA8, .RGB10A2, .RG11B10F, .RGB9E5)) => 4;
    @if(@is(it, .RG32UI, .RG32SI, .RG32F, .RGBA16, .RGBA16SN, .RGBA16UI, .RGBA16SI, .RGBA16F)) => 8;
    @if(@is(it, .RGBA32UI, .RGBA32SI, .RGBA32F)) => 16;
    @if(@is(it, .DEPTH, .DEPTH_STENCIL)) => 4;
    @else => unreachable();
};

fn _sg_roundup(val: i32, round_to: i32) i32 = {
    (val+(round_to-1)).bit_and(bit_not(round_to-1))
}

fn _sg_roundup_u32(val: u32, round_to: u32) u32 = {
    return (val+(round_to-1)) & ~(round_to-1);
}

fn _sg_roundup_u64(val: u64, round_to: u64) u64 = {
    return (val+(round_to-1)) & ~(round_to-1);
}

fn _sg_multiple_u64(val: i64, of: i64) bool = 
    bit_and(val, (of-1)) == 0;

/* return row pitch for an image

    see ComputePitch in https://github.com/microsoft/DirectXTex/blob/master/DirectXTex/DirectXTexUtil.cpp
*/
fn row_pitch(it: SgPixelFormat, width: i32, row_align_bytes: i32) i32 = {
    @debug_assert(width > 0 && (row_align_bytes > 0) && ispow2(row_align_bytes));
    pitch := @if_else {
        @if(@is(it, 
            .BC1_RGBA, .BC4_R, .BC4_RSN, .ETC2_RGB8, .ETC2_SRGB8, 
            .ETC2_RGB8A1, .EAC_R11, .EAC_R11SN,
        )) => intcast max(8, ((intcast width + 3) / 4) * 8); 
        @if(@is(it, 
            .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC5_RG, .BC5_RGSN, 
            .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA, .ETC2_RGBA8, 
            .ETC2_SRGB8A8, .EAC_RG11, .EAC_RG11SN, .ASTC_4x4_RGBA, .ASTC_4x4_SRGBA,
        )) => intcast max(16, ((intcast width + 3) / 4) * 16);
        @else => width * pixelformat_bytesize(it); 
    };
    _sg_roundup(pitch, row_align_bytes)
}

// compute the number of rows in a surface depending on pixel format
fn num_rows(fmt: SgPixelFormat, height: i32) i32 = {
    adjust := @is(fmt, 
        .BC1_RGBA, .BC4_R, .BC4_RSN, .ETC2_RGB8, .ETC2_SRGB8, .ETC2_RGB8A1, .ETC2_RGBA8, .ETC2_SRGB8A8,
        .EAC_R11, .EAC_R11SN, .EAC_RG11, .EAC_RG11SN, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC5_RG, 
        .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA, .ASTC_4x4_RGBA, .ASTC_4x4_SRGBA,
    );
    max(@if(adjust, ((height + 3) / 4), height), 1)
}

// return size of a mipmap level
miplevel_dim :: fn(base_dim: i32, mip_level: i32) i32 = {
    max(base_dim.shift_right_arithmetic(intcast mip_level), 1)
}

ispow2 :: fn(val: i32) bool = val.bit_and(val-1) == 0;

/* return pitch of a 2D subimage / texture slice
    see ComputePitch in https://github.com/microsoft/DirectXTex/blob/master/DirectXTex/DirectXTexUtil.cpp
*/
fn surface_pitch(fmt: SgPixelFormat, width: i32, height: i32, row_align_bytes: i32) i32 = {
    @debug_assert((width > 0) && (height > 0) && (row_align_bytes > 0) && ispow2(row_align_bytes));
    num_rows := num_rows(fmt, height);
    num_rows * row_pitch(fmt, width, row_align_bytes)
}

fn _sg_pass_action_defaults(action: *SgPassAction) SgPassAction = {
    res := action[];
    range(0, SG_MAX_COLOR_ATTACHMENTS) { i |
        it := res.colors&[i]&;
        ::enum(@type it.load_action);
        if (it.load_action == .DEFAULT) {
            it.load_action = .CLEAR;
            // random colour chosen by fair die roll
            it.clear_value = (r = 0.5, g = 0.5, b = 0.5, a = 1.0);
        }
        if (it.store_action == .DEFAULT) {
            it.store_action = .STORE;
        }
    }
    
    if (res.depth.load_action == .DEFAULT) {
        res.depth.load_action = .CLEAR;
        res.depth.clear_value = 1.0;
    }
    if (res.depth.store_action == .DEFAULT) {
        res.depth.store_action = .DONTCARE;
    }
    if (res.stencil.load_action == .DEFAULT) {
        res.stencil.load_action = .CLEAR;
        res.stencil.clear_value = 0;
    }
    if (res.stencil.store_action == .DEFAULT) {
        res.stencil.store_action = .DONTCARE;
    }
    res
}

fn init_pool(pool: *DynPool, a: Alloc, num: i64, sizeof_t: i64) void #once = {
    @debug_assert(num >= 1 && num < _SG_MAX_POOL_SIZE);
    pool[] = (
        sizeof_t = sizeof_t,
        // slot 0 is reserved for the 'invalid id', so bump the pool size by 1
        size = num.intcast() + 1,
        data = a.alloc_raw((num + 1) * sizeof_t, 8),
        // generation counters indexable by pool slot index, slot 0 is reserved
        gen_ctrs = a.alloc_zeroed(u32, num + 1),
        // it's not a bug to only reserve 'num' here
        // never allocate the zero-th pool item since the invalid id is 0
        free_queue = a.alloc_init(i32, num, fn(i) => intcast num - i),
        queue_top = intcast num,
    );
    pool.data.set_zeroed();
}

fn pool_discard(pool: *DynPool, a: Alloc) void = {
    a.dealloc_raw(p.data, p.sizeof_t * p.size, 8);
    a.dealloc(i32, pool.free_queue);
    a.dealloc(u32, pool.gen_ctrs);
    pool[] = zeroed DynPool;
}

fn alloc_index(pool: *DynPool) i32 = {
    if(pool.queue_top <= 0, => return(_SG_INVALID_SLOT_INDEX));  // pool exhausted
    pool.queue_top -= 1;
    slot_index := pool.free_queue[intcast pool.queue_top];
    @debug_assert((slot_index > 0) && (slot_index < pool.size), "bad slot_index %", slot_index);
    slot_index
}

fn free_index(pool: *DynPool, slot_index: i32) void = {
    @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < pool.size));
    @debug_assert(pool.queue_top < pool.size);
    if SOKOL_DEBUG {
        // debug check against double-free
        for (int i = 0; i < pool.queue_top; i++) {
            @debug_assert(pool.free_queue[i] != slot_index);
        }
    }
    pool.free_queue[pool.queue_top] = slot_index;
    pool.queue_top += 1;
    @debug_assert(pool.queue_top <= (pool.size-1));
}

fn setup_pools(sg: *Self, desc: *SgDesc) void = {
    // note: the pools here will have an additional item, since slot 0 is reserved
    X :: fn($R, count) => init_pool(sg.pools&.index(R.Tag), desc.allocator, count.intcast(), size_of(R.T));
    X(Buffer, desc.buffer_pool_size);
    X(Image, desc.image_pool_size);
    X(Sampler, desc.sampler_pool_size);
    X(Shader, desc.shader_pool_size);
    X(Pipeline, desc.pipeline_pool_size);
    X(Attachments, desc.attachments_pool_size);
}

fn discard_pools(sg: *Self) void = {
    for_enum PoolType { it |
        pool_discard(sg.pools&.index(it), sg.allocator);
    };
}

/* allocate the slot at slot_index:
    - bump the slot's generation counter
    - create a resource id from the generation counter and slot index
    - set the slot's id to this id
    - set the slot's state to ALLOC
    - return the resource id
*/
fn slot_alloc(pool: *DynPool, slot_index: i32) u32 = {
    /* FIXME: add handling for an overflowing generation counter,
       for now, just overflow (another option is to disable
       the slot)
    */
    slot := pool.get_slot(slot_index);
    @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < pool.size), "bad pool.size %", pool.size);
    @debug_assert_eq(slot.id, SG_INVALID_ID, "bad id");
    @debug_assert_eq(slot.state, .INITIAL);
    pool.gen_ctrs[intcast slot_index] += 1;
    ctr := pool.gen_ctrs[intcast slot_index];
    slot.id = bit_or(ctr.shift_left(_SG_SLOT_SHIFT), bitcast slot_index.bit_and(_SG_SLOT_MASK));
    slot.state = .ALLOC;
    slot.id
}

fn get_slot(pool: *DynPool, slot_index: i32) *SlotHeader = 
    SlotHeader.ptr_from_raw(pool.data.ptr.offset(pool.sizeof_t * intcast slot_index));

// extract slot index from id
fn _sg_slot_index(id: u32) i32 = {
    slot_index := bit_and(id, _SG_SLOT_MASK);
    @debug_assert(slot_index != _SG_INVALID_SLOT_INDEX);
    slot_index.bitcast()
}

fn discard_all_resources(sg: *Self) void = {
    /*  this is a bit dumb since it loops over all pool slots to
        find the occupied slots, on the other hand it is only ever
        executed at shutdown
        NOTE: ONLY EXECUTE THIS AT SHUTDOWN
              ...because the free queues will not be reset
              and the resource slots not be cleared!
    */
    
    inline_for_enum PoolType { $it |
        p := sg.pools&.index(it[]);
        range(1, p.size) { i |
            sg_resource_state state = p.buffers[i].slot.state;
            if ((state == .VALID) || (state == SG_RESOURCESTATE_FAILED)) {
                sg.discard(p.get_typed(it[]));
            }
        }
    };
}

fn _sg_validate_begin(sg: *Self) void = if SOKOL_DEBUG {
    sg.validate_error = "";
};

fn _sg_validate_end(sg: *Self) bool = {
    @if(!SOKOL_DEBUG) return(true);
    if(sg.validate_error.len == 0, => return(true));
    panic(sg.validate_error);
    false
}

fn validate(sg: *Self, desc: *Buffer.Desc) bool = @validate(sg) {
    @V(desc.size > 0, VALIDATE_BUFFERDESC_EXPECT_NONZERO_SIZE);
    injected := desc.gl_buffers&[0] != 0 ||
                    !desc.mtl_buffers&[0].is_null() ||
                    !desc.d3d11_buffer.is_null() ||
                    !desc.wgpu_buffer.is_null();
    if (!injected && (desc.usage == .IMMUTABLE)) {
        if !desc.data.ptr.is_null() {
            @V(desc.size == desc.data.len, VALIDATE_BUFFERDESC_EXPECT_MATCHING_DATA_SIZE);
        } else {
            @V(desc.data.len == 0, VALIDATE_BUFFERDESC_EXPECT_ZERO_DATA_SIZE);
        }
    } else {
        @V(desc.data.ptr.is_null(), VALIDATE_BUFFERDESC_EXPECT_NO_DATA);
        @V(desc.data.len == 0, VALIDATE_BUFFERDESC_EXPECT_ZERO_DATA_SIZE);
    }
    if (desc.type == .STORAGEBUFFER) {
        @V(sg.features.compute, VALIDATE_BUFFERDESC_STORAGEBUFFER_SUPPORTED);
        @V(_sg_multiple_u64(desc.size, 4), VALIDATE_BUFFERDESC_STORAGEBUFFER_SIZE_MULTIPLE_4);
    }
};

fn validate(data: *SgImageData, fmt: SgPixelFormat, width: i32, height: i32, num_faces: i32, num_mips: i32, num_slices: i32) void = {
    range(0, intcast num_faces) { face_index |
        range(0, intcast num_mips) { mip_index |
            @V( data.subimage&[face_index]&[mip_index].has_data(), VALIDATE_IMAGEDATA_NODATA);
            mip_width := miplevel_dim(width, intcast mip_index);
            mip_height := miplevel_dim(height, intcast mip_index);
            bytes_per_slice := surface_pitch(fmt, mip_width, mip_height, 1);
            expected_size := bytes_per_slice * num_slices;
            @V(expected_size == intcast data.subimage&[face_index]&[mip_index].len, VALIDATE_IMAGEDATA_DATA_SIZE);
        }
    }
}

fn has_data(self: []u8) bool = 
    !self.ptr.is_null() && self.len > 0;

valid_enum :: fn(self: ~T) bool #where = 
    (@as(u32) self) >= 0 && (@as(u32) self) < T.enum_count();

fn validate(sg: *Self, desc: *Image.Desc) bool = @validate(sg) {
    @V(desc.width > 0, VALIDATE_IMAGEDESC_WIDTH);
    @V(desc.height > 0, VALIDATE_IMAGEDESC_HEIGHT);
    fmt := desc.pixel_format;
    usage := desc.usage;
    injected := desc.gl_textures&[0] != 0 ||
                            !desc.mtl_textures&[0].is_null() ||
                            !desc.d3d11_texture.is_null() ||
                            !desc.wgpu_texture.is_null();
    if (is_depth_or_depth_stencil_format(fmt)) {
        @V(desc.type != ._3D, VALIDATE_IMAGEDESC_DEPTH_3D_IMAGE);
    }
    if (desc.render_target) {
        @debug_assert(fmt.valid_enum());
        @V(sg.formats&[fmt]&[.render], VALIDATE_IMAGEDESC_RT_PIXELFORMAT);
        @V(usage == .IMMUTABLE, VALIDATE_IMAGEDESC_RT_IMMUTABLE);
        @V(desc.data.subimage&[0]&[0].ptr.is_null(), VALIDATE_IMAGEDESC_RT_NO_DATA);
        if (desc.sample_count > 1) {
            @V(sg.formats&[fmt]&[.msaa], VALIDATE_IMAGEDESC_NO_MSAA_RT_SUPPORT);
            @V(desc.num_mipmaps == 1, VALIDATE_IMAGEDESC_MSAA_NUM_MIPMAPS);
            @V(desc.type != ._3D, VALIDATE_IMAGEDESC_MSAA_3D_IMAGE);
            @V(desc.type != .CUBE, VALIDATE_IMAGEDESC_MSAA_CUBE_IMAGE);
        }
    } else {
        @V(desc.sample_count == 1, VALIDATE_IMAGEDESC_MSAA_BUT_NO_RT);
        valid_nonrt_fmt := !sg.is_valid_rendertarget_depth_format(fmt);
        @V(valid_nonrt_fmt, VALIDATE_IMAGEDESC_NONRT_PIXELFORMAT);
        is_immutable := usage == .IMMUTABLE;
        if is_compressed(desc.pixel_format) {
            @V(is_immutable, VALIDATE_IMAGEDESC_COMPRESSED_IMMUTABLE);
        }
        if (!injected && is_immutable) {
            // image desc must have valid data
            validate(desc.data&,
                desc.pixel_format,
                desc.width,
                desc.height,
                @if(desc.type == .CUBE, 6, 1),
                desc.num_mipmaps,
                desc.num_slices);
        } else {
            // image desc must not have data
            range(0, SgCubeFace.enum_count()) { face_index |
                range(0, SG_MAX_MIPMAPS) { mip_index |
                    no_data := desc.data.subimage&[face_index]&[mip_index].ptr.is_null();
                    no_size := desc.data.subimage&[face_index]&[mip_index].len == 0;
                    if (injected) {
                        @V(no_data && no_size, VALIDATE_IMAGEDESC_INJECTED_NO_DATA);
                    }
                    if (!is_immutable) {
                        @V(no_data && no_size, VALIDATE_IMAGEDESC_DYNAMIC_NO_DATA);
                    }
                }
            }
        }
    }
};

fn validate(sg: *Self, desc: *Sampler.Desc) bool = @validate(sg) {
    // restriction from WebGPU: when anisotropy > 1, all filters must be linear
    if (desc.max_anisotropy > 1) {
        @V((desc.min_filter == .LINEAR)
                    && (desc.mag_filter == .LINEAR)
                    && (desc.mipmap_filter == .LINEAR),
                    VALIDATE_SAMPLERDESC_ANISTROPIC_REQUIRES_LINEAR_FILTERING);
    }
};

_sg_u128_t :: @struct {
    lo: i64;
    hi: i64;
};

// TODO
fn _sg_validate_set_slot_bit(bits: _sg_u128_t, stage: SgShaderStage, slot: u8) _sg_u128_t = {
    switch (stage) {
        case .NONE:
            @debug_assert(slot < 128);
            if (slot < 64) {
                bits.lo |= 1ULL << slot;
            } else {
                bits.hi |= 1ULL << (slot - 64);
            }
            break();
        case SG_SHADERSTAGE_VERTEX:
            @debug_assert(slot < 64);
            bits.lo |= 1ULL << slot;
            break();
        case SG_SHADERSTAGE_FRAGMENT:
            @debug_assert(slot < 64);
            bits.hi |= 1ULL << slot;
            break();
        case SG_SHADERSTAGE_COMPUTE:
            @debug_assert(slot < 64);
            bits.lo |= 1ULL << slot;
            break();
        default:
            SOKOL_UNREACHABLE;
            break();
    }
    return bits;
}

// TODO
fn _sg_validate_slot_bits(bits: _sg_u128_t, stage: SgShaderStage, slot: u8) bool = {
    _sg_u128_t mask = zeroed(_sg_u128_t);
    switch (stage) {
        case .NONE:
            @debug_assert(slot < 128);
            if (slot < 64) {
                mask.lo = 1ULL << slot;
            } else {
                mask.hi = 1ULL << (slot - 64);
            }
            break();
        case SG_SHADERSTAGE_VERTEX:
            @debug_assert(slot < 64);
            mask.lo = 1ULL << slot;
            break();
        case SG_SHADERSTAGE_FRAGMENT:
            @debug_assert(slot < 64);
            mask.hi = 1ULL << slot;
            break();
        case SG_SHADERSTAGE_COMPUTE:
            @debug_assert(slot < 64);
            mask.lo = 1ULL << slot;
            break();
        default:
            SOKOL_UNREACHABLE;
            break();
    }
    return ((bits.lo & mask.lo) == 0) && ((bits.hi & mask.hi) == 0);
}

fn validate(sg: FatExpr, body: FatExpr) FatExpr #macro #outputs(bool) = @{
    @if(!SOKOL_DEBUG, true, {
        @[sg].desc.disable_validation || {
            _sg_validate_begin(@[sg]);
            @[body];
            _sg_validate_end(@[sg])
        }
    })
}

V :: fn(a: FatExpr) FatExpr #macro = @{ 
    if !@[a&.items()[0]] {
        panic(@[@literal a&.items().index(1).ident().expect("@V(cond, MSG)").str()]);
    };
};

fn has_source(self: SgShaderFunction) bool = 
    !self.source.is_null();

fn has_bytecode(self: SgShaderFunction) bool = 
    !self.bytecode.ptr.is_null();

fn is_set(self: SgShaderFunction) bool = 
    self.has_source() || self.has_bytecode();

// TODO: much of this would disappear by using @tagged
fn validate(sg: *Self, desc: *Shader.Desc) bool = @validate(sg) {
    is_compute_shader := desc.compute_func.has_source() || desc.compute_func.has_bytecode();
    ASSERT_METAL();
    // on Metal or D3D11, must provide shader source code or byte code
    if (is_compute_shader) {
        @V(desc.compute_func.is_set(), VALIDATE_SHADERDESC_COMPUTE_SOURCE_OR_BYTECODE);
    } else {
        @V(desc.vertex_func.is_set(), VALIDATE_SHADERDESC_VERTEX_SOURCE_OR_BYTECODE);
        @V(desc.fragment_func.is_set(), VALIDATE_SHADERDESC_FRAGMENT_SOURCE_OR_BYTECODE);
    }
    if (is_compute_shader) {
        @V(!desc.vertex_func.is_set(), VALIDATE_SHADERDESC_INVALID_SHADER_COMBO);
        @V(!desc.fragment_func.is_set(), VALIDATE_SHADERDESC_INVALID_SHADER_COMBO);
    } else {
        @V(!desc.compute_func.is_set(), VALIDATE_SHADERDESC_INVALID_SHADER_COMBO);
    }
    ASSERT_METAL();
    if (is_compute_shader) {
        @V(desc.mtl_threads_per_threadgroup.x > 0, VALIDATE_SHADERDESC_METAL_THREADS_PER_THREADGROUP);
        @V(desc.mtl_threads_per_threadgroup.y > 0, VALIDATE_SHADERDESC_METAL_THREADS_PER_THREADGROUP);
        @V(desc.mtl_threads_per_threadgroup.z > 0, VALIDATE_SHADERDESC_METAL_THREADS_PER_THREADGROUP);
    }
    range(0, SG_MAX_VERTEX_ATTRIBUTES) { i |
        ASSERT_METAL();
    }
    // TODO
    // if shader byte code, the size must also be provided
    /*if (0 != desc.vertex_func.bytecode.ptr) {
        @V(desc.vertex_func.bytecode.size > 0, VALIDATE_SHADERDESC_NO_BYTECODE_SIZE);
    }
    if (0 != desc.fragment_func.bytecode.ptr) {
        @V(desc.fragment_func.bytecode.size > 0, VALIDATE_SHADERDESC_NO_BYTECODE_SIZE);
    }
    if (0 != desc.compute_func.bytecode.ptr) {
        @V(desc.compute_func.bytecode.size > 0, VALIDATE_SHADERDESC_NO_BYTECODE_SIZE);
    }

    _sg_u128_t msl_buf_bits = zeroed(_sg_u128_t);
    _sg_u128_t msl_tex_bits = zeroed(_sg_u128_t);
    _sg_u128_t msl_smp_bits = zeroed(_sg_u128_t);
    ASSERT_METAL();
    range(0, SG_MAX_UNIFORMBLOCK_BINDSLOTS) { ub_idx |
        ub_desc := desc.uniform_blocks&.index(ub_idx);
        if (ub_desc.stage == .NONE) {
            continue();
        }
        @V(ub_desc.size > 0, VALIDATE_SHADERDESC_UNIFORMBLOCK_SIZE_IS_ZERO);
        @V(ub_desc.msl_buffer_n < _SG_MTL_MAX_STAGE_UB_BINDINGS, VALIDATE_SHADERDESC_UNIFORMBLOCK_METAL_BUFFER_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_buf_bits, ub_desc.stage, ub_desc.msl_buffer_n), VALIDATE_SHADERDESC_UNIFORMBLOCK_METAL_BUFFER_SLOT_COLLISION);
        msl_buf_bits = _sg_validate_set_slot_bit(msl_buf_bits, ub_desc.stage, ub_desc.msl_buffer_n);
        ASSERT_METAL();
        ASSERT_METAL();
    }

    range(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { sbuf_idx |
        continue :: local_return;
        sbuf_desc := desc.storage_buffers&[sbuf_idx]&;
        if (sbuf_desc.stage == .NONE) {
            continue();
        }
        @V((sbuf_desc.msl_buffer_n >= _SG_MTL_MAX_STAGE_UB_BINDINGS) && (sbuf_desc.msl_buffer_n < _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS), VALIDATE_SHADERDESC_STORAGEBUFFER_METAL_BUFFER_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_buf_bits, sbuf_desc.stage, sbuf_desc.msl_buffer_n), VALIDATE_SHADERDESC_STORAGEBUFFER_METAL_BUFFER_SLOT_COLLISION);
        msl_buf_bits = _sg_validate_set_slot_bit(msl_buf_bits, sbuf_desc.stage, sbuf_desc.msl_buffer_n);
        ASSERT_METAL();
    }

    uint32_t img_slot_mask = 0;
    range(0, SG_MAX_IMAGE_BINDSLOTS) { img_idx |
        continue :: local_return;
        img_desc := desc.images&[img_idx]&;
        if (img_desc.stage == .NONE) {
            continue();
        }
        img_slot_mask = bit_or(img_slot_mask, 1.shift_left(img_idx));
        @V(img_desc.msl_texture_n < _SG_MTL_MAX_STAGE_IMAGE_BINDINGS, VALIDATE_SHADERDESC_IMAGE_METAL_TEXTURE_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_tex_bits, img_desc.stage, img_desc.msl_texture_n), VALIDATE_SHADERDESC_IMAGE_METAL_TEXTURE_SLOT_COLLISION);
        msl_tex_bits = _sg_validate_set_slot_bit(msl_tex_bits, img_desc.stage, img_desc.msl_texture_n);
        ASSERT_METAL();
    }

    uint32_t smp_slot_mask = 0;
    range(0, SG_MAX_SAMPLER_BINDSLOTS) { smp_idx |
        continue :: local_return;
        smp_desc = desc.samplers&[smp_idx]&;
        if (smp_desc.stage == .NONE) {
            continue();
        }
        smp_slot_mask = bit_or(smp_slot_mask, 1.shift_left(smp_idx));
        @V(smp_desc.msl_sampler_n < _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS, VALIDATE_SHADERDESC_SAMPLER_METAL_SAMPLER_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_smp_bits, smp_desc.stage, smp_desc.msl_sampler_n), VALIDATE_SHADERDESC_SAMPLER_METAL_SAMPLER_SLOT_COLLISION);
        msl_smp_bits = _sg_validate_set_slot_bit(msl_smp_bits, smp_desc.stage, smp_desc.msl_sampler_n);
        ASSERT_METAL();
    }

    uint32_t ref_img_slot_mask = 0;
    uint32_t ref_smp_slot_mask = 0;
    range(0, SG_MAX_IMAGE_SAMPLER_PAIRS) { img_smp_idx |
        continue :: local_return;
        img_smp_desc := desc.image_sampler_pairs&[img_smp_idx]&;
        if (img_smp_desc.stage == .NONE) {
            continue();
        }
        ASSERT_METAL();
        img_slot_in_range := img_smp_desc.image_slot < SG_MAX_IMAGE_BINDSLOTS;
        smp_slot_in_range := img_smp_desc.sampler_slot < SG_MAX_SAMPLER_BINDSLOTS;
        @V(img_slot_in_range, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_IMAGE_SLOT_OUT_OF_RANGE);
        @V(smp_slot_in_range, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_SAMPLER_SLOT_OUT_OF_RANGE);
        if (img_slot_in_range && smp_slot_in_range) {
            ref_img_slot_mask = bit_or(ref_img_slot_mask, 1.shift_left(img_smp_desc.image_slot));
            ref_smp_slot_mask = bit_or(ref_smp_slot_mask, 1.shift_left(img_smp_desc.sampler_slot));
            img_desc := desc.images&[img_smp_desc.image_slot]&;
            smp_desc := desc.samplers&[img_smp_desc.sampler_slot]&;
            @V(img_desc.stage == img_smp_desc.stage, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_IMAGE_STAGE_MISMATCH);
            @V(smp_desc.stage == img_smp_desc.stage, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_SAMPLER_STAGE_MISMATCH);
            needs_nonfiltering := (img_desc.sample_type == SG_IMAGESAMPLETYPE_UINT)
                                            || (img_desc.sample_type == SG_IMAGESAMPLETYPE_SINT)
                                            || (img_desc.sample_type == SG_IMAGESAMPLETYPE_UNFILTERABLE_FLOAT);
            needs_comparison := img_desc.sample_type == SG_IMAGESAMPLETYPE_DEPTH;
            if (needs_nonfiltering) {
                @V(needs_nonfiltering && (smp_desc.sampler_type == SG_SAMPLERTYPE_NONFILTERING), VALIDATE_SHADERDESC_NONFILTERING_SAMPLER_REQUIRED);
            }
            if (needs_comparison) {
                @V(needs_comparison && (smp_desc.sampler_type == SG_SAMPLERTYPE_COMPARISON), VALIDATE_SHADERDESC_COMPARISON_SAMPLER_REQUIRED);
            }
        }
    }
    // each image and sampler must be referenced by an image sampler
    @V(img_slot_mask == ref_img_slot_mask, VALIDATE_SHADERDESC_IMAGE_NOT_REFERENCED_BY_IMAGE_SAMPLER_PAIRS);
    @V(smp_slot_mask == ref_smp_slot_mask, VALIDATE_SHADERDESC_SAMPLER_NOT_REFERENCED_BY_IMAGE_SAMPLER_PAIRS);
    */
}

fn validate(sg: *Self, desc: *Pipeline.Desc) bool = @validate(sg) {
    @V(desc.shader.id != 0, VALIDATE_PIPELINEDESC_SHADER);
    if sg.lookup(desc.shader) { shd |
        @V(shd.slot.state == .VALID, VALIDATE_PIPELINEDESC_SHADER);
        if (desc.compute) {
            @V(shd.cmn.is_compute, VALIDATE_PIPELINEDESC_COMPUTE_SHADER_EXPECTED);
        } else {
            @V(!shd.cmn.is_compute, VALIDATE_PIPELINEDESC_NO_COMPUTE_SHADER_EXPECTED);
            attrs_cont := true;
            rangec(0, SG_MAX_VERTEX_ATTRIBUTES) { attr_index, $continue |
                a_state := desc.layout.attrs&[attr_index]&;
                if (a_state.format == .INVALID) {
                    attrs_cont = false;
                    continue()
                }
                @V(attrs_cont, VALIDATE_PIPELINEDESC_NO_CONT_ATTRS);
                @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
                ::enum(ShaderAttrBaseType);
                // vertex format must match expected shader attribute base type (if provided)
                if (shd.cmn.attrs&[attr_index].base_type != .Undefined) {
                    if (vertexformat_basetype(a_state.format) != shd.cmn.attrs&[attr_index].base_type) {
                        @V(false, VALIDATE_PIPELINEDESC_ATTR_BASETYPE_MISMATCH);
                        @eprintln("attr format: % shader attr base type: %", a_state.format, shd.cmn.attrs&[attr_index].base_type);
                    }
                }
                ASSERT_METAL();
            }
            // must only use readonly storage buffer bindings in render pipelines
            each shd.cmn.storage_buffers& { it |
                @V(it.stage == .NONE || it.readonly, VALIDATE_PIPELINEDESC_SHADER_READONLY_STORAGEBUFFERS);
            }
            range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { buf_index |
                continue :: local_return;
                l_state := desc.layout.buffers&[buf_index]&;
                if (l_state.stride == 0) {
                    continue();
                }
                @V(_sg_multiple_u64(intcast l_state.stride, 4), VALIDATE_PIPELINEDESC_LAYOUT_STRIDE4);
            }
        }
    } else {
        @V(false, VALIDATE_PIPELINEDESC_SHADER);
    }
    range(0, intcast desc.color_count) { color_index |
        bs := desc.colors&[color_index].blend;
        ::enum(@type bs.op_rgb);
        ::enum(@type bs.src_factor_rgb);
        if ((bs.op_rgb == .MIN) || (bs.op_rgb == .MAX)) {
            @V((bs.src_factor_rgb == .ONE) && (bs.dst_factor_rgb == .ONE), VALIDATE_PIPELINEDESC_BLENDOP_MINMAX_REQUIRES_BLENDFACTOR_ONE);
        }
        if ((bs.op_alpha == .MIN) || (bs.op_alpha == .MAX)) {
            @V((bs.src_factor_alpha == .ONE) && (bs.dst_factor_alpha == .ONE), VALIDATE_PIPELINEDESC_BLENDOP_MINMAX_REQUIRES_BLENDFACTOR_ONE);
        }
    }
};

fn validate(sg: *Self, desc: *Attachments.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    @V(desc._start_canary == 0, VALIDATE_ATTACHMENTSDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_ATTACHMENTSDESC_CANARY);
    atts_cont := true;
    color_width := -1; color_height := -1; color_sample_count := -1;
    has_color_atts := false;
    range(0, SG_MAX_COLOR_ATTACHMENTS) { att_index |
        continue :: local_return;
        att := desc.colors&[att_index];
        if (att.image.id == SG_INVALID_ID) {
            atts_cont = false;
            continue();
        }
        @V(atts_cont, VALIDATE_ATTACHMENTSDESC_NO_CONT_COLOR_ATTS);
        has_color_atts = true;
        img := sg.lookup(att.image);
        @V(img, VALIDATE_ATTACHMENTSDESC_IMAGE);
        if (0 != img) {
            @V(img.slot.state == .VALID, VALIDATE_ATTACHMENTSDESC_IMAGE);
            @V(img.cmn.render_target, VALIDATE_ATTACHMENTSDESC_IMAGE_NO_RT);
            @V(att.mip_level < img.cmn.num_mipmaps, VALIDATE_ATTACHMENTSDESC_MIPLEVEL);
            @match(img.cmn.type) {
                fn CUBE() => @V(att.slice < 6, VALIDATE_ATTACHMENTSDESC_FACE);
                fn ARRAY() => @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_LAYER);
                fn _3D() => @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_SLICE);
                @default => ();
            }
            if (att_index == 0) {
                color_width = miplevel_dim(img.cmn.width, att.mip_level);
                color_height = miplevel_dim(img.cmn.height, att.mip_level);
                color_sample_count = img.cmn.sample_count;
            } else {
                @V(color_width == miplevel_dim(img.cmn.width, att.mip_level), VALIDATE_ATTACHMENTSDESC_IMAGE_SIZES);
                @V(color_height == miplevel_dim(img.cmn.height, att.mip_level), VALIDATE_ATTACHMENTSDESC_IMAGE_SIZES);
                @V(color_sample_count == img.cmn.sample_count, VALIDATE_ATTACHMENTSDESC_IMAGE_SAMPLE_COUNTS);
            }
            @V(is_valid_rendertarget_color_format(img.cmn.pixel_format), VALIDATE_ATTACHMENTSDESC_COLOR_INV_PIXELFORMAT);

            // check resolve attachment
            res_att := desc.resolves&[att_index]&;
            if (res_att.image.id != SG_INVALID_ID) {
                // associated color attachment must be MSAA
                @V(img.cmn.sample_count > 1, VALIDATE_ATTACHMENTSDESC_RESOLVE_COLOR_IMAGE_MSAA);
                res_img := sg.lookup(res_att.image);
                @V(res_img, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE);
                if (res_img != 0) {
                    @V(res_img.slot.state == .VALID, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE);
                    @V(res_img.cmn.render_target, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_NO_RT);
                    @V(res_img.cmn.sample_count == 1, VALIDATE_ATTACHMENTSDESC_RESOLVE_SAMPLE_COUNT);
                    @V(res_att.mip_level < res_img.cmn.num_mipmaps, VALIDATE_ATTACHMENTSDESC_RESOLVE_MIPLEVEL);
                    @match(res_img.cmn.type) {
                        fn CUBE() => @V(res_att.slice < 6, VALIDATE_ATTACHMENTSDESC_RESOLVE_FACE);
                        fn ARRAY() => @V(res_att.slice < res_img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_RESOLVE_LAYER);
                        fn _3D() => @V(res_att.slice < res_img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_RESOLVE_SLICE);
                        @default => ();
                    }
                    @V(img.cmn.pixel_format == res_img.cmn.pixel_format, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_FORMAT);
                    @V(color_width == miplevel_dim(res_img.cmn.width, res_att.mip_level), VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_SIZES);
                    @V(color_height == miplevel_dim(res_img.cmn.height, res_att.mip_level), VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_SIZES);
                }
            }
        }
    }
    bool has_depth_stencil_att = false;
    if (desc.depth_stencil.image.id != SG_INVALID_ID) {
        att := desc.depth_stencil&;
        img := sg.lookup(att.image);
        @V(img, VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE);
        has_depth_stencil_att = true;
        if (img) {
            @V(img.slot.state == .VALID, VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE);
            @V(att.mip_level < img.cmn.num_mipmaps, VALIDATE_ATTACHMENTSDESC_DEPTH_MIPLEVEL);
            @match(img.cmn.type) {
                fn CUBE() => @V(img.slice < 6, VALIDATE_ATTACHMENTSDESC_DEPTH_FACE);
                fn ARRAY() => @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_DEPTH_LAYER);
                // NOTE: this can't actually happen because of VALIDATE_IMAGEDESC_DEPTH_3D_IMAGE
                fn _3D() => @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_DEPTH_SLICE);
                @default => ();
            }
            @V(img.cmn.render_target, VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_NO_RT);
            @V((color_width == -1) || (color_width == miplevel_dim(img.cmn.width, att.mip_level)), VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_SIZES);
            @V((color_height == -1) || (color_height == miplevel_dim(img.cmn.height, att.mip_level)), VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_SIZES);
            @V((color_sample_count == -1) || (color_sample_count == img.cmn.sample_count), VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_SAMPLE_COUNT);
            @V(sg.is_valid_rendertarget_depth_format(img.cmn.pixel_format), VALIDATE_ATTACHMENTSDESC_DEPTH_INV_PIXELFORMAT);
        }
    }
    @V(has_color_atts || has_depth_stencil_att, VALIDATE_ATTACHMENTSDESC_NO_ATTACHMENTS);
};

fn validate_begin_pass(sg: *Self, pass: *SgPass) bool = @validate(sg) {
    is_compute_pass := pass.compute;
    is_swapchain_pass := !is_compute_pass && (pass.attachments.id == SG_INVALID_ID);
    is_offscreen_pass := !(is_compute_pass || is_swapchain_pass);
    @V(pass._start_canary == 0, VALIDATE_BEGINPASS_CANARY);
    @V(pass._end_canary == 0, VALIDATE_BEGINPASS_CANARY);
    @if_else {
    @if(is_compute_pass) => {
        // this is a compute pass
        @V(pass.attachments.id == SG_INVALID_ID, VALIDATE_BEGINPASS_EXPECT_NO_ATTACHMENTS);
    };
    @if(is_swapchain_pass) => {
        @V(pass.swapchain.width > 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_WIDTH);
        @V(pass.swapchain.height > 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_HEIGHT);
        @V(pass.swapchain.sample_count > 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_SAMPLECOUNT);
        @V(!@is(pass.swapchain.color_format, .NONE, .DEFAULT), VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_COLORFORMAT);
        // NOTE: depth buffer is optional, so depth_format is allowed to be invalid
        // NOTE: the GL framebuffer handle may actually be 0
        @V(!pass.swapchain.metal.current_drawable.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_CURRENTDRAWABLE);
        if (pass.swapchain.depth_format == .NONE) {
            @V(pass.swapchain.metal.depth_stencil_texture.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_DEPTHSTENCILTEXTURE_NOTSET);
        } else {
            @V(!pass.swapchain.metal.depth_stencil_texture.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_DEPTHSTENCILTEXTURE);
        }
        if (pass.swapchain.sample_count > 1) {
            @V(!pass.swapchain.metal.msaa_color_texture.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_MSAACOLORTEXTURE);
        } else {
            @V(pass.swapchain.metal.msaa_color_texture.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_MSAACOLORTEXTURE_NOTSET);
        }
        ASSERT_METAL();
    };
    @else => {
        // this is an 'offscreen pass'
        if sg.lookup(pass.attachments) { atts |
            @V(atts.slot.state == .VALID, VALIDATE_BEGINPASS_ATTACHMENTS_VALID);
            range(0, SG_MAX_COLOR_ATTACHMENTS) { i |
                color_att := atts.cmn.colors&[i]&;
                if sg.impl().attachments_color_image(atts, i) { color_img |
                    @V(color_img.slot.state == .VALID, VALIDATE_BEGINPASS_COLOR_ATTACHMENT_IMAGE);
                    @V(color_img.slot.id == color_att.image.id, VALIDATE_BEGINPASS_COLOR_ATTACHMENT_IMAGE);
                }
                resolve_att := atts.cmn.resolves&[i]&;
                if sg.impl().attachments_resolve_image(atts, i) { resolve_img |
                    @V(resolve_img.slot.state == .VALID, VALIDATE_BEGINPASS_RESOLVE_ATTACHMENT_IMAGE);
                    @V(resolve_img.slot.id == resolve_att.image.id, VALIDATE_BEGINPASS_RESOLVE_ATTACHMENT_IMAGE);
                }
            }
            if sg.impl().attachments_ds_image(atts) { ds_img |
                att := atts.cmn.depth_stencil&;
                @V(ds_img.slot.state == .VALID, VALIDATE_BEGINPASS_DEPTHSTENCIL_ATTACHMENT_IMAGE);
                @V(ds_img.slot.id == att.image.id, VALIDATE_BEGINPASS_DEPTHSTENCIL_ATTACHMENT_IMAGE);
            }
        } else {
            @V(false, VALIDATE_BEGINPASS_ATTACHMENTS_EXISTS);
        }
    };
    };
    if (is_compute_pass || is_offscreen_pass) {
        it := pass.swapchain&;
        @V(it.width == 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_WIDTH_NOTSET);
        @V(it.height == 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_HEIGHT_NOTSET);
        @V(it.sample_count == 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_SAMPLECOUNT_NOTSET);
        @V(it.color_format == .DEFAULT, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_COLORFORMAT_NOTSET);
        @V(it.depth_format == .DEFAULT, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_DEPTHFORMAT_NOTSET);
        @V(it.metal.current_drawable.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_CURRENTDRAWABLE_NOTSET);
        @V(it.metal.depth_stencil_texture.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_DEPTHSTENCILTEXTURE_NOTSET);
        @V(it.metal.msaa_color_texture.is_null(), VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_MSAACOLORTEXTURE_NOTSET);
        ASSERT_METAL();
    }
};

fn validate_apply_viewport(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && !sg.cur_pass.is_compute, VALIDATE_AVP_RENDERPASS_EXPECTED);
};

fn validate_apply_scissor_rect(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && !sg.cur_pass.is_compute, VALIDATE_ASR_RENDERPASS_EXPECTED);
};

fn validate_apply_pipeline(sg: *Self, pip_id: Pipeline) bool = @validate(sg) {
    // the pipeline object must be alive and valid
    @V(pip_id.id != SG_INVALID_ID, VALIDATE_APIP_PIPELINE_VALID_ID);
    pip := sg.lookup(pip_id) || {
        @V(false, VALIDATE_APIP_PIPELINE_EXISTS);
        return(_sg_validate_end(sg))
    };
    @V(pip.slot.state == .VALID, VALIDATE_APIP_PIPELINE_VALID);
    // the pipeline's shader must be alive and valid
    @V(sg.cur_pass.in_pass, VALIDATE_APIP_PASS_EXPECTED);
    @V(pip.cmn.shader_t.slot.id == pip.cmn.shader.id, VALIDATE_APIP_SHADER_EXISTS);
    @V(pip.cmn.shader_t.slot.state == .VALID, VALIDATE_APIP_SHADER_VALID);
    if (pip.cmn.compute) {
        @V(sg.cur_pass.is_compute, VALIDATE_APIP_COMPUTEPASS_EXPECTED);
    } else {
        @V(!sg.cur_pass.is_compute, VALIDATE_APIP_RENDERPASS_EXPECTED);
        // check that pipeline attributes match current pass attributes
        if (sg.cur_pass.atts_id.id != SG_INVALID_ID) {
            // an offscreen pass
            atts := sg.cur_pass.atts;
            @V(atts.slot.id == sg.cur_pass.atts_id.id, VALIDATE_APIP_CURPASS_ATTACHMENTS_EXISTS);
            @V(atts.slot.state == .VALID, VALIDATE_APIP_CURPASS_ATTACHMENTS_VALID);

            @V(pip.cmn.color_count == atts.cmn.num_colors, VALIDATE_APIP_ATT_COUNT);
            range(0, intcast pip.cmn.color_count) { i |
                att_img := sg.impl().attachments_color_image(atts, i).unwrap();
                @V(pip.cmn.colors&[i].pixel_format == att_img.cmn.pixel_format, VALIDATE_APIP_COLOR_FORMAT);
                @V(pip.cmn.sample_count == att_img.cmn.sample_count, VALIDATE_APIP_SAMPLE_COUNT);
            }
            if sg.impl().attachments_ds_image(atts) { att_dsimg |
                @V(pip.cmn.depth.pixel_format == att_dsimg.cmn.pixel_format, VALIDATE_APIP_DEPTH_FORMAT);
            } else {
                @V(pip.cmn.depth.pixel_format == .NONE, VALIDATE_APIP_DEPTH_FORMAT);
            }
        } else {
            // default pass
            @V(pip.cmn.color_count == 1, VALIDATE_APIP_ATT_COUNT);
            @V(pip.cmn.colors&[0].pixel_format == sg.cur_pass.swapchain.color_fmt, VALIDATE_APIP_COLOR_FORMAT);
            @V(pip.cmn.depth.pixel_format == sg.cur_pass.swapchain.depth_fmt, VALIDATE_APIP_DEPTH_FORMAT);
            @V(pip.cmn.sample_count == sg.cur_pass.swapchain.sample_count, VALIDATE_APIP_SAMPLE_COUNT);
        }
    }
}

fn validate_apply_bindings(sg: *Self, bindings: *SgBindings) bool = @validate(sg) {
    // must be called in a pass
    @V(sg.cur_pass.in_pass, VALIDATE_ABND_PASS_EXPECTED);

    // bindings must not be empty
    has_any_bindings := bindings.index_buffer.id != SG_INVALID_ID
        || !bindings.vertex_buffers&.items().interpret_as_bytes().is_all_zeroes()
        || !bindings.images&.items().interpret_as_bytes().is_all_zeroes()
        || !bindings.samplers&.items().interpret_as_bytes().is_all_zeroes()
        || !bindings.storage_buffers&.items().interpret_as_bytes().is_all_zeroes()
    ;
    @V(has_any_bindings, VALIDATE_ABND_EMPTY_BINDINGS);

    // a pipeline object must have been applied
    @V(sg.cur_pipeline.id != SG_INVALID_ID, VALIDATE_ABND_PIPELINE);
    pip := sg.lookup(sg.cur_pipeline) || {
        @V(false, VALIDATE_ABND_PIPELINE_EXISTS);
        return(_sg_validate_end(sg))
    }
    @V(pip.slot.state == .VALID, VALIDATE_ABND_PIPELINE_VALID);
    shd := pip.cmn.shader_t;
    @debug_assert(pip.cmn.shader.id == shd.slot.id);

    if (sg.cur_pass.is_compute) {
        @V(bindings.vertex_buffers&.items().interpret_as_bytes().is_all_zeroes(), VALIDATE_ABND_COMPUTE_EXPECTED_NO_VBS);
    } else {
        // has expected vertex buffers, and vertex buffers still exist
        range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            if (pip.cmn.vertex_buffer_layout_active&[i]) {
                v := bindings.vertex_buffers&[i];
                @V(v.id != SG_INVALID_ID, VALIDATE_ABND_EXPECTED_VB);
                // buffers in vertex-buffer-slots must be of type SG_BUFFERTYPE_VERTEXBUFFER
                if sg.lookup(bindings.vertex_buffers&[i]) { buf |
                    if buf.slot.state == .VALID {
                        ::enum(@type buf.cmn.type);
                        @V(buf.cmn.type == .VERTEXBUFFER, VALIDATE_ABND_VB_TYPE);
                        @V(!buf.cmn.append_overflow, VALIDATE_ABND_VB_OVERFLOW);
                    }
                } else {
                    @V(false, VALIDATE_ABND_VB_EXISTS);
                }
            }
        }
    }

    if (sg.cur_pass.is_compute) {
        @V(bindings.index_buffer.id == SG_INVALID_ID, VALIDATE_ABND_COMPUTE_EXPECTED_NO_IB);
    } else {
        // index buffer expected or not, and index buffer still exists
        if (pip.cmn.index_type == .NONE) {
            // pipeline defines non-indexed rendering, but index buffer provided
            @V(bindings.index_buffer.id == SG_INVALID_ID, VALIDATE_ABND_IB);
        } else {
            // pipeline defines indexed rendering, but no index buffer provided
            @V(bindings.index_buffer.id != SG_INVALID_ID, VALIDATE_ABND_NO_IB);
        }
        if (bindings.index_buffer.id != SG_INVALID_ID) {
            // buffer in index-buffer-slot must be of type SG_BUFFERTYPE_INDEXBUFFER
            if sg.lookup(bindings.index_buffer) { buf |
                if buf.slot.state == .VALID {
                    @V(buf.cmn.type == .INDEXBUFFER, VALIDATE_ABND_IB_TYPE);
                    @V(!buf.cmn.append_overflow, VALIDATE_ABND_IB_OVERFLOW);
                }
            } else {
                @V(false, VALIDATE_ABND_IB_EXISTS);
            }
        }
    }

    // has expected images
    rangec(0, SG_MAX_IMAGE_BINDSLOTS) { i, $continue |
        if(shd.cmn.images&[i].stage == .NONE, => continue());
        @V(bindings.images&[i].id != 0, VALIDATE_ABND_EXPECTED_IMAGE_BINDING);
        if (bindings.images&[i].id != 0) {
            img := sg.lookup(bindings.images&[i]) || {
                @V(false, VALIDATE_ABND_IMG_EXISTS);
                continue()
            };
            if(img.slot.state != .VALID, => continue());
            ::enum(@type img.cmn.type);
            @V(img.cmn.type == shd.cmn.images&[i].image_type, VALIDATE_ABND_IMAGE_TYPE_MISMATCH);
            if (!sg.features.msaa_image_bindings) {
                @V(img.cmn.sample_count == 1, VALIDATE_ABND_IMAGE_MSAA);
            }
            if (shd.cmn.images&[i].multisampled) {
                @V(img.cmn.sample_count > 1, VALIDATE_ABND_EXPECTED_MULTISAMPLED_IMAGE);
            }
            info := sg.formats&[img.cmn.pixel_format]&;
            ::enum(SgImageSampleType);
            @match(shd.cmn.images&[i].sample_type) {
                fn FLOAT() => @V(info[.filter], VALIDATE_ABND_EXPECTED_FILTERABLE_IMAGE);
                fn DEPTH() => @V(info[.depth], VALIDATE_ABND_EXPECTED_DEPTH_IMAGE);
                @default   => ();
            };
        }
    }

    // has expected samplers
    rangec(0, SG_MAX_SAMPLER_BINDSLOTS) { i, $continue |
        if(shd.cmn.samplers&[i].stage == .NONE, => continue());
        @V(bindings.samplers&[i].id != 0, VALIDATE_ABND_EXPECTED_SAMPLER_BINDING);
        if (bindings.samplers&[i].id != 0) {
            smp := sg.lookup(bindings.samplers&[i]) || {
                @V(false, VALIDATE_ABND_SMP_EXISTS);
                continue()
            };
            ::enum(SgSamplerType);
            ::enum(SgCompareFunc);
            ::enum(SgFilter);
            if (shd.cmn.samplers&[i].sampler_type == .COMPARISON) {
                @V(smp.cmn.compare != .NEVER, VALIDATE_ABND_UNEXPECTED_SAMPLER_COMPARE_NEVER);
            } else {
                @V(smp.cmn.compare == .NEVER, VALIDATE_ABND_EXPECTED_SAMPLER_COMPARE_NEVER);
            }
            if (shd.cmn.samplers&[i].sampler_type == .NONFILTERING) {
                nonfiltering := (smp.cmn.min_filter != .LINEAR)
                                        && (smp.cmn.mag_filter != .LINEAR)
                                        && (smp.cmn.mipmap_filter != .LINEAR);
                @V(nonfiltering, VALIDATE_ABND_EXPECTED_NONFILTERING_SAMPLER);
            }
        }
    }

    // has expected storage buffers
    rangec(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { i, $continue |
        if (shd.cmn.storage_buffers&[i].stage != .NONE) {
            sbuf := sg.lookup(bindings.storage_buffers&[i]) || {
                @V(false, VALIDATE_ABND_STORAGEBUFFER_EXISTS);
                continue()
            };
            @V(sbuf.cmn.type == .STORAGEBUFFER, VALIDATE_ABND_STORAGEBUFFER_BINDING_BUFFERTYPE);
            // read/write bindings are only allowed for immutable buffers
            if (!shd.cmn.storage_buffers&[i].readonly) {
                ::enum(SgUsage);
                @V(sbuf.cmn.usage == .IMMUTABLE, VALIDATE_ABND_STORAGEBUFFER_READWRITE_IMMUTABLE);
            }
        }
    }
};

fn validate_apply_uniforms(sg: *Self, ub_slot: i32, data: []u8) bool = @validate(sg) {
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @V(sg.cur_pass.in_pass, VALIDATE_AU_PASS_EXPECTED);
    @V(sg.cur_pipeline.id != SG_INVALID_ID, VALIDATE_AU_NO_PIPELINE);
    pip := sg.lookup(sg.cur_pipeline).expect("cur_pipeline");
    @debug_assert(pip.slot.id == sg.cur_pipeline.id);
    shd := pip.cmn.shader_t;
    ::ptr_utils(@type shd[]);
    @debug_assert(!shd.is_null() && (shd.slot.id == pip.cmn.shader.id));

    it := shd.cmn.uniform_blocks&[intcast ub_slot]&;
    @V(it.stage != .NONE, VALIDATE_AU_NO_UNIFORMBLOCK_AT_SLOT);
    @V(data.len == zext it.size, VALIDATE_AU_SIZE);
};

fn validate_draw(sg: *Self, base_element: i32, num_elements: i32, num_instances: i32) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && !sg.cur_pass.is_compute, VALIDATE_DRAW_RENDERPASS_EXPECTED);
    @V(base_element >= 0, VALIDATE_DRAW_BASEELEMENT);
    @V(num_elements >= 0, VALIDATE_DRAW_NUMELEMENTS);
    @V(num_instances >= 0, VALIDATE_DRAW_NUMINSTANCES);
    @V(sg.required_bindings_and_uniforms == sg.applied_bindings_and_uniforms, VALIDATE_DRAW_REQUIRED_BINDINGS_OR_UNIFORMS_MISSING);
};

fn _sg_validate_dispatch(sg: *Self, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && sg.cur_pass.is_compute, VALIDATE_DISPATCH_COMPUTEPASS_EXPECTED);
    @V((num_groups_x >= 0) && (num_groups_x < 1.shift_left(16)), VALIDATE_DISPATCH_NUMGROUPSX);
    @V((num_groups_y >= 0) && (num_groups_y < 1.shift_left(16)), VALIDATE_DISPATCH_NUMGROUPSY);
    @V((num_groups_z >= 0) && (num_groups_z < 1.shift_left(16)), VALIDATE_DISPATCH_NUMGROUPSZ);
    @V(sg.required_bindings_and_uniforms == sg.applied_bindings_and_uniforms, VALIDATE_DRAW_REQUIRED_BINDINGS_OR_UNIFORMS_MISSING);
};

fn validate_update_buffer(sg: *Self, buf: *Buffer.T, data: []u8) bool = @validate(sg) {
    @debug_assert(!data.ptr.is_null());
    @V(buf.cmn.usage != .IMMUTABLE, VALIDATE_UPDATEBUF_USAGE);
    @V(buf.cmn.size.intcast() >= data.len, VALIDATE_UPDATEBUF_SIZE);
    @V(buf.cmn.update_frame_index != sg.frame_index, VALIDATE_UPDATEBUF_ONCE);
    @V(buf.cmn.append_frame_index != sg.frame_index, VALIDATE_UPDATEBUF_APPEND);
}

fn _sg_validate_append_buffer(sg: *Self, buf: *Buffer.T, data: []u8) bool = @validate(sg) {
    @debug_assert(buf && data && data.ptr);
    @V(buf.cmn.usage != SG_USAGE_IMMUTABLE, VALIDATE_APPENDBUF_USAGE);
    @V(buf.cmn.size >= (buf.cmn.append_pos + data.size), VALIDATE_APPENDBUF_SIZE);
    @V(buf.cmn.update_frame_index != sg.frame_index, VALIDATE_APPENDBUF_UPDATE);
}

fn validate(sg: *Self, img: *Image.T, data: *sg_image_data) bool = @validate(sg) {
    @debug_assert(img && data);
    @V(img.cmn.usage != SG_USAGE_IMMUTABLE, VALIDATE_UPDIMG_USAGE);
    @V(img.cmn.upd_frame_index != sg.frame_index, VALIDATE_UPDIMG_ONCE);
    _sg_validate_image_data(data,
        img.cmn.pixel_format,
        img.cmn.width,
        img.cmn.height,
        @if(img.cmn.type == SG_IMAGETYPE_CUBE, 6, 1),
        img.cmn.num_mipmaps,
        img.cmn.num_slices);
}

fn desc_defaults(_: *Self, desc: *Buffer.Desc) Buffer.Desc = {
    def := desc[];
    if (def.size == 0) {
        def.size = def.data.len;
    }
    def
}

fn desc_defaults(sg: *Self, desc: *Image.Desc) Image.Desc = {
    def := desc[];
    def.type = _sg_def(def.type, ._2D);
    def.num_slices = _sg_def(def.num_slices, 1);
    def.num_mipmaps = _sg_def(def.num_mipmaps, 1);
    def.usage = _sg_def(def.usage, .IMMUTABLE);
    if (desc.render_target) {
        def.pixel_format = _sg_def(def.pixel_format, sg.desc.environment.defaults.color_format);
        def.sample_count = _sg_def(def.sample_count, sg.desc.environment.defaults.sample_count);
    } else {
        def.pixel_format = _sg_def(def.pixel_format, .RGBA8);
        def.sample_count = _sg_def(def.sample_count, 1);
    }
    return def;
}

fn _SG_ERROR(s: Str) void = @eprintln("%", s);
fn _SG_LOGMSG(s: Str, c: CStr) void = @eprintln("%: %", s, c);

fn desc_defaults(_: *Self, desc: *Sampler.Desc) Sampler.Desc = {
    desc[]
}

fn desc_defaults(_: *Self, desc: *Shader.Desc) Shader.Desc = {
    def := desc[];
    def.vertex_func.entry = _sg_def(def.vertex_func.entry, "_main");
    def.fragment_func.entry = _sg_def(def.fragment_func.entry, "_main");
    def.compute_func.entry = _sg_def(def.compute_func.entry, "_main");
    ASSERT_METAL();
    def.mtl_threads_per_threadgroup.y = _sg_def(desc.mtl_threads_per_threadgroup.y, 1);
    def.mtl_threads_per_threadgroup.z = _sg_def(desc.mtl_threads_per_threadgroup.z, 1);
    range(0, SG_MAX_UNIFORMBLOCK_BINDSLOTS) { ub_index |
        ub_desc := def.uniform_blocks&[ub_index]&;
        if ub_desc.stage != .NONE {
            ub_desc.layout = _sg_def(ub_desc.layout, .NATIVE);
            rangeb(0, SG_MAX_UNIFORMBLOCK_MEMBERS) { u_index, $break |
                u_desc := ub_desc.glsl_uniforms&[u_index]&;
                ::enum(@type u_desc.type);
                if (u_desc.type == .INVALID) {
                    break();
                }
                u_desc.array_count = _sg_def(u_desc.array_count, 1);
            }
        }
    }
    range(0, SG_MAX_IMAGE_BINDSLOTS) { img_index |
        img_desc := def.images&[img_index]&;
        if img_desc.stage != .NONE {
            img_desc.image_type = _sg_def(img_desc.image_type, ._2D);
            img_desc.sample_type = _sg_def(img_desc.sample_type, .FLOAT);
        }
    }
    range(0, SG_MAX_SAMPLER_BINDSLOTS) { smp_index |
        smp_desc := def.samplers&[smp_index]&;
        if smp_desc.stage != .NONE {
            smp_desc.sampler_type = _sg_def(smp_desc.sampler_type, .FILTERING);
        }
    }
    def
}

fn desc_defaults(sg: *Self, desc: *Pipeline.Desc) Pipeline.Desc = {
    def := desc[];

    // FIXME: should we actually do all this stuff for a compute pipeline?

    // TODO: #use doesn't work with #where
    def.primitive_type = _sg_def(def.common.primitive_type, .TRIANGLES);
    def.index_type = _sg_def(def.common.index_type, .NONE);
    def.cull_mode = _sg_def(def.common.cull_mode, .NONE);
    def.face_winding = _sg_def(def.common.face_winding, .CW);
    def.sample_count = _sg_def(def.common.sample_count, sg.desc.environment.defaults.sample_count);

    def.stencil.front.compare = _sg_def(def.common.stencil.front.compare, .ALWAYS);
    def.stencil.front.fail_op = _sg_def(def.common.stencil.front.fail_op, .KEEP);
    def.stencil.front.depth_fail_op = _sg_def(def.common.stencil.front.depth_fail_op, .KEEP);
    def.stencil.front.pass_op = _sg_def(def.common.stencil.front.pass_op, .KEEP);
    def.stencil.back.compare = _sg_def(def.common.stencil.back.compare, .ALWAYS);
    def.stencil.back.fail_op = _sg_def(def.common.stencil.back.fail_op, .KEEP);
    def.stencil.back.depth_fail_op = _sg_def(def.common.stencil.back.depth_fail_op, .KEEP);
    def.stencil.back.pass_op = _sg_def(def.common.stencil.back.pass_op, .KEEP);

    ::enum(@type def.common.depth.compare);
    def.depth.compare = _sg_def(def.common.depth.compare, .ALWAYS);
    def.depth.pixel_format = _sg_def(def.common.depth.pixel_format, sg.desc.environment.defaults.depth_format);
    if (def.colors&[0].pixel_format == .NONE) {
        // special case depth-only rendering, enforce a color count of 0
        def.color_count = 0;
    } else {
        def.color_count = _sg_def(def.common.color_count, 1);
    }
    if (def.color_count > intcast SG_MAX_COLOR_ATTACHMENTS) {
        def.color_count = SG_MAX_COLOR_ATTACHMENTS;
    }
    range(0, intcast def.color_count) { i |
        cs := def.colors&[i]&;
        cs.pixel_format = _sg_def(cs.pixel_format, sg.desc.environment.defaults.color_format);
        cs.write_mask = _sg_def(cs.write_mask, .RGBA);
        bs := def.colors&[i].blend&;
        bs.op_rgb = _sg_def(bs.op_rgb, .ADD);
        bs.src_factor_rgb = _sg_def(bs.src_factor_rgb, .ONE);
        ::enum(@type bs.op_rgb);
        if ((bs.op_rgb == .MIN) || (bs.op_rgb == .MAX)) {
            bs.dst_factor_rgb = _sg_def(bs.dst_factor_rgb, .ONE);
        } else {
            bs.dst_factor_rgb = _sg_def(bs.dst_factor_rgb, .ZERO);
        }
        bs.op_alpha = _sg_def(bs.op_alpha, .ADD);
        bs.src_factor_alpha = _sg_def(bs.src_factor_alpha, .ONE);
        if ((bs.op_alpha == .MIN) || (bs.op_alpha == .MAX)) {
            bs.dst_factor_alpha = _sg_def(bs.dst_factor_alpha, .ONE);
        } else {
            bs.dst_factor_alpha = _sg_def(bs.dst_factor_alpha, .ZERO);
        }
    }

    rangeb(0, SG_MAX_VERTEX_ATTRIBUTES) { attr_index, $break |
        a_state := def.layout.attrs&[attr_index]&;
        if (a_state.format == .INVALID) {
            break();
        }
        @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
        l_state := def.layout.buffers&[intcast a_state.buffer_index]&;
        l_state.step_func = _sg_def(l_state.step_func, .PER_VERTEX);
        l_state.step_rate = _sg_def(l_state.step_rate, 1);
    };

    // resolve vertex layout strides and offsets
    auto_offset := zeroed Array(i32, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    use_auto_offset := true;
    range(0, SG_MAX_VERTEX_ATTRIBUTES) { attr_index |
        // to use computed offsets, *all* attr offsets must be 0
        if def.layout.attrs&[attr_index].offset != 0 {
            use_auto_offset = false;
        }
    }
    rangeb(0, SG_MAX_VERTEX_ATTRIBUTES) { attr_index, $break |
        a_state := def.layout.attrs&[attr_index]&;
        if (a_state.format == .INVALID) {
            break();
        }
        @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
        if (use_auto_offset) {
            a_state.offset = auto_offset&[intcast a_state.buffer_index];
        }
        auto_offset&[intcast a_state.buffer_index] += vertexformat_bytesize(a_state.format);
    }
    // compute vertex strides if needed
    range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { buf_index |
        l_state := def.layout.buffers&[buf_index]&;
        if (l_state.stride == 0) {
            l_state.stride = auto_offset&[buf_index];
        }
    }

    return def;
}

// TODO: make my langauge less trash
fn rangeb(lo: i64, hi: i64, $body: @Fn(i: i64, break: LabelId) void) void = {
    break :: return;
    range(lo, hi, fn(i) => body(i, break));
}
fn rangec(lo: i64, hi: i64, $body: @Fn(i: i64, continue: LabelId) void) void = {
    range(lo, hi) { i |
        continue :: local_return;
        body(i, continue);
    };
}

fn desc_defaults(_: *Self, desc: *Attachments.Desc) Attachments.Desc = {
    desc[]
}

Metal :: import("@/graphics/macos/gfx.fr");

// https://floooh.github.io/2018/06/17/handles-vs-pointers.html
/*
    The 32-bit resource id is split into a 16-bit pool index in the lower bits,
    and a 16-bit 'generation counter' in the upper bits. The index allows fast
    pool lookups, and combined with the generation-counter it allows to detect
    'dangling accesses' (trying to use an object which no longer exists, and
    its pool slot has been reused for a new object)
*/

Buffer :: Resource(.Buffer, SgBufferDesc, BufferCommon, Metal.Buffer);  // vertex- and index-buffers
Image :: Resource(.Image, SgImageDesc, ImageCommon, Metal.Image);  // images used as textures and render-pass attachments
Sampler :: Resource(.Sampler, SgSamplerDesc, SamplerCommon, Metal.Sampler);  // sampler objects describing how a texture is sampled in a shader
Shader :: Resource(.Shader, SgShaderDesc, ShaderCommon, Metal.Shader);  // vertex- and fragment-shaders and shader interface information
Pipeline :: Resource(.Pipeline, SgPipelineDesc, PipelineCommon, Metal.Pipeline);  // associated shader and vertex-layouts, and render states
Attachments :: Resource(.Attachments, SgAttachmentsDesc, AttachmentsCommon, Metal.Attachments);  // a baked collection of render pass attachment images

PoolType :: { X :: @enum(u8) (Buffer, Image, Sampler, Shader, Pipeline, Attachments); enum(X); X };

DynPool :: @struct {
    size: i32;
    queue_top: i32;
    gen_ctrs: []u32;
    free_queue: []i32;
    sizeof_t: i64;
    data: []u8;  // []T
};

fn Resource($_Tag: PoolType, $_Desc: Type, $_Common: Type, $_Impl: Type) Type = {
    I :: @struct {
        id: u32;
        Tag :: _Tag;
        // The internal data we store in the pool
        T :: @struct {
            slot: SlotHeader;
            cmn: Common;
            mtl: Impl;
        };     
        Desc :: _Desc;  // Info the user gives us to create one 
        Common :: _Common;
        Impl :: _Impl;
    };
    
    fn alloc(sg: *Self) I #trace_args = {
        p := sg.pools&.index(I.Tag);
        slot_index := p.alloc_index();
        id := if slot_index != _SG_INVALID_SLOT_INDEX {
            p.slot_alloc(slot_index)
        } else {
            _SG_ERROR("SHADER_POOL_EXHAUSTED");
            0
        };
        (id = id)
    }
    
    fn make(sg: *Self, desc: I.Desc) I #trace_args = {
        @debug_assert(sg.valid);
        desc_def: I.Desc = sg.desc_defaults(desc&);
        id: I = sg.alloc();
        if id.id != SG_INVALID_ID {
            it := sg.at(id);
            @debug_assert(it.slot.state == .ALLOC);
            it.slot.state = sg.init(it, desc_def&);
            @debug_assert(@is(it.slot.state, .VALID, .FAILED));
        };
        id
    }
    
    // returns pointer to resource by id without matching id check
    fn at(sg: *Self, id: I) *I.T = {
        p := sg.pools&.index(I.Tag);
        @debug_assert(id.id != SG_INVALID_ID);
        slot_index := _sg_slot_index(id.id);
        @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < p.size));
        ptr_cast_unchecked(u8, I.T, p.data.ptr.offset(slot_index.intcast() * size_of(I.T)))
    }
    
    // returns pointer to resource with matching id check, may return 0
    fn lookup(sg: *Self, id: I) ?*I.T = {
        if(id.id == SG_INVALID_ID, => return(.None));
        it: *I.T = sg.at(id);
        if(it.slot.id != id.id, => return(.None));
        (Some = it)
    }
    
    // TODO
    fn dealloc(sg: *Self, id: I) void #trace_args = {
        it := sg.lookup(id) || return();
        if (it.slot.state == SG_RESOURCESTATE_ALLOC) {
            sg.dealloc(it);
        } else {
            _SG_ERROR(DEALLOC_SHADER_INVALID_STATE);
        }
    }
    
    // TODO
    fn dealloc(sg: *Self, it: *I.T) void = {
        p := sg.pools&.index(I.Tag);
        SOKOL_ASSERT((it->slot.state == SG_RESOURCESTATE_ALLOC) && (it->slot.id != SG_INVALID_ID));
        p.free_index(_sg_slot_index(it.slot.id));
        p.slot = zeroed SlotHeader;
    }
    
    fn init(sg: *Self, id: I, desc: *I.Desc) void #trace_args = {
        @debug_assert(sg.valid);
        desc_def := sg.desc_defaults(desc);
        it := sg.lookup(id) || return();
        if it.slot.state == .ALLOC {
            it.slot.state = init(it, &desc_def);
            @debug_assert(@is(shd.slot.state, .VALID, .FAILED));
        } else {
            _SG_ERROR(INIT_SHADER_INVALID_STATE);
        }
    }
    
    // TODO
    fn uninit(sg: *Self, id: I) void #trace_args = {
        @debug_assert(sg.valid);
        it := sg.lookup(id) || return();
        if ((it.slot.state == .VALID) || (it.slot.state == SG_RESOURCESTATE_FAILED)) {
            sg.impl().discard(it);
            reset_to_alloc_state(shd);
            @debug_assert(shd.slot.state == SG_RESOURCESTATE_ALLOC);
        } else {
            _SG_ERROR(UNINIT_SHADER_INVALID_STATE);
        }
    }
    
    // TODO
    fn fail(sg: *Self, id: I) void #trace_args = {
        @debug_assert(sg.valid);
        it := sg.lookup(id) || return();
        if (it.slot.state == SG_RESOURCESTATE_ALLOC) {
            it.slot.state = SG_RESOURCESTATE_FAILED;
        } else {
            _SG_ERROR(FAIL_SHADER_INVALID_STATE);  // TODO
        }
    }
    
    ::@if(::!@is(I.Tag, .Pipeline, .Attachments)) {
        fn init(sg: *Self, it: *I.T, desc: *I.Desc) SgResourceState = {
            @debug_assert(it.slot.state == .ALLOC);
            if(!sg.validate(desc), => return(.FAILED));
            common_init(it.cmn&, desc);
            sg.impl().create(it, desc)
        }
    };
    
    // TODO
    fn destroy(sg: *Self, id: I) void #trace_args(.before) = {
        @debug_assert(sg.valid);
        it := sg.lookup(id) || return();
        if ((it.slot.state == .VALID) || (it.slot.state == SG_RESOURCESTATE_FAILED)) {
            sg.uninit(it);
            @debug_assert(it.slot.state == SG_RESOURCESTATE_ALLOC);
        }
        if (it.slot.state == SG_RESOURCESTATE_ALLOC) {
            sg.dealloc(it);
            @debug_assert(it.slot.state == SG_RESOURCESTATE_INITIAL);
        }
    }
    
    fn reset_to_alloc_state(it: *I.T) void = {
        id := it.slot.id;
        it[] = zeroed I.T;
        it.slot = (id = id, state = .ALLOC);
    }
    
    I
}

fn init(sg: *Self, pip: *Pipeline.T, desc: *Pipeline.Desc) SgResourceState = {
    @debug_assert(pip.slot.state == .ALLOC);
    if(!sg.validate(desc), => return(.FAILED));
    shd := sg.lookup(desc.shader) || return(.FAILED);
    if(shd.slot.state != .VALID, => return(.FAILED));
    common_init(pip.cmn&, desc);
    s := sg.impl().create(pip, shd, desc);
    @debug_assert(@is(s, .VALID, .FAILED));
    s
}

// TODO
fn init(sg: *Self, atts: *Attachments.T, desc: *Attachments.Desc) SgResourceState = {
    @debug_assert(atts && atts.slot.state == SG_RESOURCESTATE_ALLOC);
    @debug_assert(desc);
    if !_sg_validate_attachments_desc(desc)) {
        atts.slot.state = SG_RESOURCESTATE_FAILED;
        return();
    };
    // lookup pass attachment image pointers
    color_images := zeroed Array(*Image.T, SG_MAX_COLOR_ATTACHMENTS);
    resolve_images := zeroed Array(*Image.T, SG_MAX_COLOR_ATTACHMENTS);
    ds_image := zeroed(*Image.T);
    // NOTE: validation already checked that all surfaces are same width/height
    width, height := (0, 0);
    range(0, SG_MAX_COLOR_ATTACHMENTS) { i |
        if (desc.colors[i].image.id) {
            color_images[i] = _sg_lookup_image(&sg.pools, desc.colors[i].image.id);
            if (!(color_images[i] && color_images[i].slot.state == .VALID)) {
                atts.slot.state = SG_RESOURCESTATE_FAILED;
                return();
            }
            mip_level := desc.colors[i].mip_level;
            width = miplevel_dim(color_images[i].cmn.width, mip_level);
            height = miplevel_dim(color_images[i].cmn.height, mip_level);
        }
        if (desc.resolves[i].image.id) {
            resolve_images[i] = _sg_lookup_image(&sg.pools, desc.resolves[i].image.id);
            if (!(resolve_images[i] && resolve_images[i].slot.state == .VALID)) {
                atts.slot.state = SG_RESOURCESTATE_FAILED;
                return();
            }
        }
    }
    if (desc.depth_stencil.image.id) {
        ds_image = _sg_lookup_image(&sg.pools, desc.depth_stencil.image.id);
        if (!(ds_image && ds_image.slot.state == .VALID)) {
            atts.slot.state = SG_RESOURCESTATE_FAILED;
            return();
        }
        mip_level := desc.depth_stencil.mip_level;
        width = miplevel_dim(ds_image.cmn.width, mip_level);
        height = miplevel_dim(ds_image.cmn.height, mip_level);
    }
    common_init(atts.cmn&, desc, width, height);
    atts.slot.state = sg.impl().create(atts, color_images, resolve_images, ds_image, desc);
    @debug_assert((atts.slot.state == .VALID)||(atts.slot.state == SG_RESOURCESTATE_FAILED));
}

// - trying to add_commit_listener() twice needs to stay an "error" 
//   because i don't guarentee fnptr stability (ie. with jit shims)
fn eq(a: *SgCommitListener, b: *SgCommitListener) bool = {
    identical(a.user_data, b.user_data) && bit_cast_unchecked(@type a.func, i64, a.func) == bit_cast_unchecked(@type b.func, i64, b.func)
}

fn notify_commit_listeners(sg: *Self) void = 
    for sg.commit_listeners { listener |
        listener'func(listener.user_data);
    };

fn add_commit_listener(sg: *Self, new_listener: SgCommitListener) bool = {
    added := add_unique(sg.commit_listeners&, new_listener, sg.desc.allocator);
    if !added {
        _SG_ERROR("IDENTICAL_COMMIT_LISTENER"); // TODO
    }
    added
}

fn remove_commit_listener(sg: *Self, listener: sg_commit_listener) bool = {
    i := index_of(sg.commit_listeners&, listener&) || return(false);
    unordered_remove(sg.commit_listeners&, i);
    true
}

fn setup_compute(sg: *Self, desc: *SgDesc) void = {
    @debug_assert((desc.max_dispatch_calls_per_pass > 0));
    max_tracked_sbufs := desc.max_dispatch_calls_per_pass * SG_MAX_STORAGEBUFFER_BINDSLOTS;
    sg.compute.readwrite_sbufs = init(sg.desc.allocator, max_tracked_sbufs.intcast());
}

fn discard_compute(sg: *Self) void = {
    sg.compute.readwrite_sbufs&.drop(sg.allocator);
}

fn compute_pass_track_storage_buffer(sg: *Self, sbuf: *Buffer.T, readonly: bool) void = {
    if !readonly {
        sbuf: Buffer = (id = sbuf.slot.id);
        try_push(sg.compute.readwrite_sbufs&, sbuf);
    }
}

fn compute_on_endpass(sg: *Self) void = {
    @debug_assert(sg.cur_pass.in_pass && sg.cur_pass.is_compute);
    sg.compute.readwrite_sbufs.len = 0;
}

fn pass_defaults(sg: *Self, pass: *SgPass) SgPass = {
    res := pass[];
    // TODO
    // ... this sure isn't a program?
    if (!res.compute) {
        if (res.compute && res.attachments.id == SG_INVALID_ID) {
            // this is a swapchain-pass
            res.swapchain.sample_count = _sg_def(res.swapchain.sample_count, sg.desc.environment.defaults.sample_count);
            res.swapchain.color_format = _sg_def(res.swapchain.color_format, sg.desc.environment.defaults.color_format);
            res.swapchain.depth_format = _sg_def(res.swapchain.depth_format, sg.desc.environment.defaults.depth_format);
        }
        res.action = _sg_pass_action_defaults(res.action&);
    }
    res
}

fn setup(sg: *Self, desc: SgDesc) void = {
    @debug_assert((desc._start_canary == 0) && (desc._end_canary == 0));
    sg[] = zeroed @type sg[];
    sg.impl()[].sg = sg;
    sg.desc = desc;
    sg.setup_pools(sg.desc&);
    sg.setup_compute(sg.desc&);
    @debug_assert(sg.desc.max_commit_listeners > 0);
    sg.commit_listeners = init(sg.desc.allocator, sg.desc.max_commit_listeners.intcast());
    sg.frame_index = 1;
    sg.stats_enabled = true;
    sg.impl().setup_backend(sg.desc&);
    sg.valid = true;
}

fn shutdown(sg: *Self) void = {
    sg.discard_all_resources();
    sg.impl().discard_backend();
    sg.commit_listeners&.drop(sg.allocator);
    sg.discard_compute();
    sg.discard_pools();
    sg[] = zereod Self;
}

fn query_pixelformat(sg: *Self, fmt: SgPixelFormat) SgPixelFormatInfo = {
    @debug_assert(sg.valid);
    c := is_compressed(fmt);
    (
        flags = sg.formats[@as(i64) fmt], 
        compressed = c, 
        bytes_per_pixel = @if(c, 0, pixelformat_bytesize(fmt)),
    )
}

fn begin_pass(sg: *Self, pass: *SgPass) void #trace_args(.tail) = {
    @debug_assert(sg.valid);
    cur := sg.cur_pass&;
    @debug_assert(!cur.valid);
    @debug_assert(!cur.in_pass);
    @debug_assert((pass._start_canary == 0) && (pass._end_canary == 0));
    pass_def := sg.pass_defaults(pass);
    if (!sg.validate_begin_pass(pass_def&)) {
        return();
    }
    if (!pass_def.compute) {
        if (pass_def.attachments.id != SG_INVALID_ID) {
            // an offscreen pass
            ::ptr_utils(@type cur.atts[]);
            @debug_assert(cur.atts.is_null());
            ::?*Attachments.T;
            cur.atts = sg.lookup(pass_def.attachments) || {
                @panic("BEGINPASS_ATTACHMENT_INVALID");
                return()
            }
            cur.atts_id = pass_def.attachments;
            cur.width = cur.atts.cmn.width;
            cur.height = cur.atts.cmn.height;
        } else {
            // a swapchain pass
            sc := pass_def.swapchain&;
            @debug_assert(sc.width > 0);
            @debug_assert(sc.height > 0);
            @debug_assert(valid_enum(sc.color_format));
            cur.width = sc.width;
            cur.height = sc.height;
            cur.swapchain.color_fmt = sc.color_format;
            cur.swapchain.depth_fmt = sc.depth_format;
            cur.swapchain.sample_count = sc.sample_count;
        }
    }
    cur.valid = true;  // may be overruled by backend begin-pass functions
    cur.in_pass = true;
    cur.is_compute = pass_def.compute;
    sg.impl().begin_pass(pass_def&);
}

fn apply_viewport(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) void #trace_args = {
    @debug_assert(sg.valid);
    if(!sg.validate_apply_viewport(x, y, width, height, origin_top_left), => return());
    sg.stat(.num_apply_viewport, 1);
    if(!sg.cur_pass.valid, => return());
    sg.impl().apply_viewport(x, y, width, height, origin_top_left);
}

fn apply_viewportf(sg: *Self, x: f32, y: f32, width: f32, height: f32, origin_top_left: bool) void = {
    sg.apply_viewport((int)x, (int)y, (int)width, (int)height, origin_top_left);
}

fn apply_scissor_rect(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) void #trace_args(.tail) = {
    @debug_assert(sg.valid);
    if(!sg.validate_apply_scissor_rect(x, y, width, height, origin_top_left), => return());
    sg.stat(.num_apply_scissor_rect, 1);
    if(!sg.cur_pass.valid, => return());
    sg.impl().apply_scissor_rect(x, y, width, height, origin_top_left);
}

fn apply_scissor_rectf(sg: *Self, x: f32, y: f32, width: f32, height: f32, origin_top_left: bool) void = {
    sg.apply_scissor_rect((int)x, (int)y, (int)width, (int)height, origin_top_left);
}

fn apply_pipeline(sg: *Self, pip_id: Pipeline) void #trace(.tail) = {
    @debug_assert(sg.valid);
    sg.stat(.num_apply_pipeline, 1);
    if (!sg.validate_apply_pipeline(pip_id)) {
        sg.next_draw_valid = false;
        return();
    }
    if(!sg.cur_pass.valid, => return());
    sg.cur_pipeline = pip_id;
    pip := sg.lookup(pip_id) || @panic("tried to apply invalid pipeline");

    sg.next_draw_valid = pip.slot.state == .VALID;
    if(!sg.next_draw_valid, => return());

    @debug_assert(pip.cmn.shader_t.slot.id == pip.cmn.shader.id);
    sg.impl().apply_pipeline(pip);

    // set the expected bindings and uniform block flags
    sg.required_bindings_and_uniforms = bit_or(pip.cmn.required_bindings_and_uniforms, pip.cmn.shader_t.cmn.required_bindings_and_uniforms);
    sg.applied_bindings_and_uniforms = 0;
}

// this whole thing is just looking up each `I` in SgBindings 
// and putting the `*I.T` in a ResolvedBindings to pass to the backend. 
// so like if we just had pointers this would be free, so i guess the only 
// value is that it lets you gracefully skip invalid draws and just log something. 
// see inner comment.
fn apply_bindings(sg: *Self, bindings: *SgBindings) void = {
    @debug_assert(sg.valid);
    @debug_assert((bindings._start_canary == 0) && (bindings._end_canary==0));
    sg.stat(.num_apply_bindings, 1);
    abu := sg.applied_bindings_and_uniforms&;
    abu[] = bit_or(abu[], 1.shift_left(Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    if (!sg.validate_apply_bindings(bindings)) {
        sg.next_draw_valid = false;
        return();
    }
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());

    bnd := zeroed ResolvedBindings;
    ::?*Pipeline.T;
    bnd.pip = sg.lookup(sg.cur_pipeline) || {
        sg.next_draw_valid = false;
        return()
    };
    @debug_assert(bnd.pip.cmn.shader.id == bnd.pip.cmn.shader_t.slot.id);
    shd := bnd.pip.cmn.shader_t;
    
    check :: fn(b: bool) => { sg.next_draw_valid = sg.next_draw_valid && b; };
    
    // TODO: sokol allows you to have ids that fail the slot check and just sets next_draw_valid=false instead of panicking. 
    //       and in the stage != .NONE cases ASSERTS it's not 0 but just records the fail if its dealloced. 
    //       do i really want to be making the code more convoluted to jsut ignore using a resource 
    //       after freeing it instead of crashing? but also it's a very desirable property to 
    //       say your core graphics layer shall not crash, and will just log errors for you to 
    //       handle how you want. like you might want to do interactive demo thing and handle 
    //       mistakes gracefully, so maybe i need to change this back. 
    
    if !sg.cur_pass.is_compute {
        range(0, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            if bnd.pip.cmn.vertex_buffer_layout_active&[i] {
                ::?*Buffer.T;
                bnd.vbs&[i] = sg.lookup(bindings.vertex_buffers&[i]) || panic("invalid vertex bind");
                bnd.vb_offsets&[i] = bindings.vertex_buffer_offsets&[i];
                ::enum(SgResourceState);
                check(bnd.vbs&[i].slot.state == .VALID);
                check(!bnd.vbs&[i].cmn.append_overflow);
            };
        }
        if bindings.index_buffer.id != 0 { 
            bnd.ib = sg.lookup(bindings.index_buffer) || panic("invalid index bind");
            bnd.ib_offset = bindings.index_buffer_offset;
            check(bnd.ib.slot.state == .VALID);
            check(!bnd.ib.cmn.append_overflow);
        }
    }

    range(0, SG_MAX_IMAGE_BINDSLOTS) { i |
        if shd.cmn.images&[i].stage != .NONE {
            bnd.imgs&[i] = sg.lookup(bindings.images&[i]) || panic("invalid image bind");
            check(bnd.imgs&[i].slot.state == .VALID);
        }
    }

    range(0, SG_MAX_SAMPLER_BINDSLOTS) { i |
        if shd.cmn.samplers&[i].stage != .NONE {
            bnd.smps&[i] = sg.lookup(bindings.samplers&[i]) || panic("invalid sampler bind");
            check(bnd.smps&[i].slot.state == .VALID);
        }
    }

    range(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
        if shd.cmn.storage_buffers&[i].stage != .NONE {
            bnd.sbufs&[i] = sg.lookup(bindings.storage_buffers&[i]) || panic("invalid storage bind");
            check(bnd.sbufs&[i].slot.state == .VALID);
            if (sg.cur_pass.is_compute) {
                sg.compute_pass_track_storage_buffer(bnd.sbufs&[i], shd.cmn.storage_buffers&[i].readonly);
            }
        }
    }

    if (sg.next_draw_valid) {
        check(sg.impl().apply_bindings(bnd&));
        //_SG_TRACE_ARGS(apply_bindings, bindings);  // TODO
    }
}

fn apply_uniforms(sg: *Self, ub_slot: i32, data: []u8) void #trace(.tail) = {
    @debug_assert(sg.valid);
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert(!data.ptr.is_null() && data.len > 0);
    sg.stat(.num_apply_uniforms, 1);
    sg.stat(.size_apply_uniforms, trunc data.len);
    abu := sg.applied_bindings_and_uniforms&;
    abu[] = bit_or(abu[], 1.shift_left(intcast ub_slot));
    if !sg.validate_apply_uniforms(ub_slot, data) {
        sg.next_draw_valid = false;
        return();
    }
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());
    sg.impl().apply_uniforms(ub_slot, data);
}

fn draw(sg: *Self, base_element: i32, num_elements: i32, num_instances: i32) void #trace(.tail) = {
    @debug_assert(sg.valid);
    if(!sg.validate_draw(base_element, num_elements, num_instances), => return());
    sg.stat(.num_draw, 1);
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());
    // skip no-op draws
    if(num_elements == 0|| num_instances == 0, => return());
    sg.impl().draw(base_element, num_elements, num_instances);
}

// TODO
fn dispatch(sg: *Self, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void #trace(.tail) = {
    @debug_assert(sg.valid);
    if(!_sg_validate_dispatch(num_groups_x, num_groups_y, num_groups_z), => return());
    _sg_stats_add(num_dispatch, 1);
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());
    // skip no-op dispatches
    if((0 == num_groups_x) || (0 == num_groups_y) || (0 == num_groups_z), => return());
    sg.impl().dispatch(num_groups_x, num_groups_y, num_groups_z);
}

fn end_pass(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    @debug_assert(sg.cur_pass.in_pass);
    sg.stat(.num_passes, 1);
    // NOTE: don't exit early if !sg.cur_pass.valid
    sg.impl().end_pass();
    sg.cur_pipeline.id = SG_INVALID_ID;
    if (sg.cur_pass.is_compute) {
        sg.compute_on_endpass();
    }
    sg.cur_pass = zeroed @type sg.cur_pass;
}

fn commit(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    @debug_assert(!sg.cur_pass.valid);
    @debug_assert(!sg.cur_pass.in_pass);
    sg.impl().commit();
    sg.stats&[.frame_index] = sg.frame_index;
    sg.prev_stats = sg.stats;
    sg.stats = zeroed @type sg.stats;
    sg.notify_commit_listeners();
    sg.frame_index += 1;  // TODO: trace frame_index will be off by one
}

fn reset_state_cache(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    sg.impl().reset_state_cache();
}

fn update_buffer(sg: *Self, buf_id: Buffer, data: []u8) void #trace = {
    @debug_assert(sg.valid && data.len > 0 && !data.ptr.is_null());
    sg.stat(.num_update_buffer, 1);
    sg.stat(.size_update_buffer, trunc data.len);
    buf := sg.lookup(buf_id) || return();
    if data.len > 0 && buf.slot.state == .VALID {
        if sg.validate_update_buffer(buf, data) {
            @debug_assert(data.len <= intcast buf.cmn.size);
            // only one update allowed per buffer and frame
            @debug_assert(buf.cmn.update_frame_index != sg.frame_index);
            // update and append on same buffer in same frame not allowed
            @debug_assert(buf.cmn.append_frame_index != sg.frame_index);
            sg.impl().update_buffer(buf, data);
            buf.cmn.update_frame_index = sg.frame_index;
        }
    }
}

// TODO
fn append_buffer(sg: *Self, buf_id: Buffer, data: []u8) i64 #trace = {
    @debug_assert(sg.valid);
    @debug_assert(data && data.ptr);
    _sg_stats_add(num_append_buffer, 1);
    _sg_stats_add(size_append_buffer, (uint32_t)data.size);
    buf := _sg_lookup_buffer(&sg.pools, buf_id.id) || return(0);  // FIXME: should we return -1 here?
    // rewind append cursor in a new frame
    if (buf.cmn.append_frame_index != sg.frame_index) {
        buf.cmn.append_pos = 0;
        buf.cmn.append_overflow = false;
    }
    if (((size_t)buf.cmn.append_pos + data.size) > (size_t)buf.cmn.size) {
        buf.cmn.append_overflow = true;
    }
    const int start_pos = buf.cmn.append_pos;
    // NOTE: the multiple-of-4 requirement for the buffer offset is coming
    // from WebGPU, but we want identical behaviour between backends
    @debug_assert(_sg_multiple_u64(zext start_pos, 4));
    if (buf.slot.state == .VALID) {
        if (_sg_validate_append_buffer(buf, data)) {
            if (!buf.cmn.append_overflow && (data.size > 0)) {
                // update and append on same buffer in same frame not allowed
                @debug_assert(buf.cmn.update_frame_index != sg.frame_index);
                sg.impl().append_buffer(buf, data, buf.cmn.append_frame_index != sg.frame_index);
                buf.cmn.append_pos += (int) _sg_roundup_u64(data.size, 4);
                buf.cmn.append_frame_index = sg.frame_index;
            }
        }
    }
    start_pos
}

// TODO
fn update_image(sg: *Self, img_id: Image, data: *sg_image_data) void #trace = {
    @debug_assert(sg.valid);
    _sg_stats_add(num_update_image, 1);
    range(0, SG_CUBEFACE_NUM) { face_index |
        range(0, SG_MAX_MIPMAPS) { mip_index |
            if (data.subimage[face_index][mip_index].size == 0) {
                break();
            }
            _sg_stats_add(size_update_image, (uint32_t)data.subimage[face_index][mip_index].size);
        }
    }
    img := _sg_lookup_image(&sg.pools, img_id.id) || return();
    if img.slot.state == .VALID {
        if (validate(img, data)) {
            @debug_assert(img.cmn.upd_frame_index != sg.frame_index);
            sg.impl().update_image(img, data);
            img.cmn.upd_frame_index = sg.frame_index;
        }
    }
}

fn push_debug_group(sg: *Self, name: CStr) void #trace = {
    @debug_assert(sg.valid);
    sg.impl().push_debug_group(name);
}

fn pop_debug_group(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    sg.impl().pop_debug_group();
}

// @map_enum(e) (name = val, ..); where e: @enum(name); -> @type val
fn map_enum(key: FatExpr, cases: FatExpr) FatExpr #macro = {
    key := compile_ast(key);
    info := get_type_info_ref(key.ty);
    @debug_assert(info.is(.Enum) && info.Enum.sequential, "map_enum");
    @debug_assert(cases.expr&.is(.StructLiteralP), "map_enum");
    fields := info.Enum.fields&;
    branches := list(Ty(i64, FatExpr), fields.len, ast_alloc());
    each cases.expr.StructLiteralP.bindings { b |
        continue :: local_return;
        name := b.ident().unwrap();
        for fields { it |
            if it._0 == name {
                ::tagged(@type it._1); 
                @debug_assert(it._1&.is(.Small));
                push(branches&, (it._1.Small._0, b.default));
                continue();
            };
        };
        @ct_assert(false, cases.loc, "% is not a member of the enum", name.str());
    };
    // TODO: detect duplicates
    make_switch(key, @{ panic("invalid value in map_enum") }, branches)
}

// @fill_formats(fmts&) (sfbrm = (.R8, ..), ..); where fmts: EnumMap(SgPixelFormat, EnumMap(PixFmtInfo, bool)); -> void
fn fill_formats(dest: FatExpr, pat: FatExpr) FatExpr #macro = {
    @debug_assert(pat.expr&.is(.StructLiteralP));
    body := @{};
    each pat.expr.StructLiteralP.bindings& { case |
        key := case.ident().unwrap().str();
        for key { c |
            for_enum PixFmtInfo { ty |
                if ty.name_str()[0] == c {
                    for case.default&.items() { value |
                        body = @{ 
                            @[body];
                            @[dest].index(@[value]).index(@[@literal ty])[] = true;
                        };
                    };
                }
            };
        };
    };
    body
}
