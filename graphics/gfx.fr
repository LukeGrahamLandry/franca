// simple 3D API wrapper
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.
// commit: 41247ec237c1eebab46630caaadaf080ba1914a4
// 
// This is independent of how you open a window and get the 3D context,
// app.fr has sglue_*** functions that provide the information we need.   
// 
// Changes from Sokol
// - generics are a surprise tool that can help us later
// 

fn ASSERT_METAL() void = @debug_assert(SOKOL_BACKEND.is_metal());

// TODO: meta programming thing to replace _SG_TRACE_ARGS with #trace
//       call sg_trace_hooks with args and return value. 
//       (.tail) there are early returns but only hook the fallthough at the end
//       (.before) do it before the implementation 
//

// resource pool slots

SlotHeader :: @struct {
    id: u32;
    state: SgResourceState;
};

// resource tracking (for keeping track of gpu-written storage resources
_sg_tracker_t :: RawList(u32);

_SG_STRING_SIZE :: 32;
_SG_SLOT_SHIFT :: 16;
_SG_SLOT_MASK :: (1<<_SG_SLOT_SHIFT)-1;
_SG_MAX_POOL_SIZE :: (1<<_SG_SLOT_SHIFT);
_SG_DEFAULT_BUFFER_POOL_SIZE :: 128;
_SG_DEFAULT_IMAGE_POOL_SIZE :: 128;
_SG_DEFAULT_SAMPLER_POOL_SIZE :: 64;
_SG_DEFAULT_SHADER_POOL_SIZE :: 32;
_SG_DEFAULT_PIPELINE_POOL_SIZE :: 64;
_SG_DEFAULT_ATTACHMENTS_POOL_SIZE :: 16;
_SG_DEFAULT_UB_SIZE :: 4 * 1024 * 1024;
_SG_DEFAULT_MAX_DISPATCH_CALLS_PER_PASS :: 1024;
_SG_DEFAULT_MAX_COMMIT_LISTENERS :: 1024;
_SG_DEFAULT_WGPU_BINDGROUP_CACHE_SIZE :: 1024;

BufferCommon :: @struct {
    size: i32;
    append_pos: i32;
    append_overflow bool;
    update_frame_index: u32;
    append_frame_index: u32;
    num_slots: i32;
    active_slot: i32;
    type: SgBufferType;
    usage: SgUsage;
};

fn _sg_buffer_common_init(cmn: *BufferCommon, desc: *SgBufferDesc) void = {
    cmn.size = (int)desc.size;
    cmn.append_pos = 0;
    cmn.append_overflow = false;
    cmn.update_frame_index = 0;
    cmn.append_frame_index = 0;
    cmn.num_slots = (desc.usage == SG_USAGE_IMMUTABLE) ? 1 : SG_NUM_INFLIGHT_FRAMES;
    cmn.active_slot = 0;
    cmn.type = desc.type;
    cmn.usage = desc.usage;
}

ImageCommon :: @struct {
    upd_frame_index: u32;
    num_slots: i32;
    active_slot: i32;
    type: SgImageType;
    render_target: bool;
    width: i32;
    height: i32;
    num_slices: i32;
    num_mipmaps: i32;
    usage: SgUsage;
    pixel_format: SgPixelFormat;
    sample_count: i32;
};

fn _sg_image_common_init(cmn: *ImageCommon, desc: *Image.Desc) {
    cmn.upd_frame_index = 0;
    cmn.num_slots = (desc.usage == SG_USAGE_IMMUTABLE) ? 1 : SG_NUM_INFLIGHT_FRAMES;
    cmn.active_slot = 0;
    cmn.type = desc.type;
    cmn.render_target = desc.render_target;
    cmn.width = desc.width;
    cmn.height = desc.height;
    cmn.num_slices = desc.num_slices;
    cmn.num_mipmaps = desc.num_mipmaps;
    cmn.usage = desc.usage;
    cmn.pixel_format = desc.pixel_format;
    cmn.sample_count = desc.sample_count;
}

fn _sg_sampler_common_init(cmn: *SamplerCommon, desc: *Sampler.Desc) {
    cmn[] = desc.common;
}

_sg_shader_attr_t :: @struct {
    base_type: sg_shader_attr_base_type;
};

_sg_shader_uniform_block_t ::  @struct {
    stage: sg_shader_stage;
    uint32_t size: u32;
};

_sg_shader_storage_buffer_t :: @struct {
    stage: sg_shader_stage;
    readonly: bool;
};

_sg_shader_image_t :: @struct {
    stage: sg_shader_stage;
    image_type: sg_image_type;
    sample_type: sg_image_sample_type;
    multisampled: bool;
};

_sg_shader_sampler_t :: @struct {
    stage: sg_shader_stage;
    sampler_type: sg_sampler_type;
};

// combined image sampler mappings, only needed on GL
_sg_shader_image_sampler_t :: @struct {
    stage: sg_shader_stage;
    image_slot: u8;
    sampler_slot: u8;
};

ShaderCommon :: @struct {
    required_bindings_and_uniforms: u32;
    is_compute: bool;
    attrs: Array(_sg_shader_attr_t, SG_MAX_VERTEX_ATTRIBUTES);
    uniform_blocks: Array(_sg_shader_uniform_block_t, SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    storage_buffers: Array(_sg_shader_storage_buffer_t, SG_MAX_STORAGEBUFFER_BINDSLOTS);
    images: Array(_sg_shader_image_t, SG_MAX_IMAGE_BINDSLOTS);
    samplers: Array(_sg_shader_sampler_t, SG_MAX_SAMPLER_BINDSLOTS);
    image_samplers: Array(_sg_shader_image_sampler_t, SG_MAX_IMAGE_SAMPLER_PAIRS);
};

fn _sg_shader_common_init(cmn: *ShaderCommon, const sg_shader_desc* desc: *Shader.Desc) void = {
    cmn.is_compute = desc.compute_func.source || desc.compute_func.bytecode.ptr;
    for (size_t i = 0; i < SG_MAX_VERTEX_ATTRIBUTES; i++) {
        cmn.attrs[i].base_type = desc.attrs[i].base_type;
    }
    for (size_t i = 0; i < SG_MAX_UNIFORMBLOCK_BINDSLOTS; i++) {
        const sg_shader_uniform_block* src = &desc.uniform_blocks[i];
        _sg_shader_uniform_block_t* dst = &cmn.uniform_blocks[i];
        if (src.stage != SG_SHADERSTAGE_NONE) {
            cmn.required_bindings_and_uniforms |= (1 << i);
            dst.stage = src.stage;
            dst.size = src.size;
        }
    }
    const uint32_t required_bindings_flag = (1 << SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        const sg_shader_storage_buffer* src = &desc.storage_buffers[i];
        _sg_shader_storage_buffer_t* dst = &cmn.storage_buffers[i];
        if (src.stage != SG_SHADERSTAGE_NONE) {
            cmn.required_bindings_and_uniforms |= required_bindings_flag;
            dst.stage = src.stage;
            dst.readonly = src.readonly;
        }
    }
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        const sg_shader_image* src = &desc.images[i];
        _sg_shader_image_t* dst = &cmn.images[i];
        if (src.stage != SG_SHADERSTAGE_NONE) {
            cmn.required_bindings_and_uniforms |= required_bindings_flag;
            dst.stage = src.stage;
            dst.image_type = src.image_type;
            dst.sample_type = src.sample_type;
            dst.multisampled = src.multisampled;
        }
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        const sg_shader_sampler* src = &desc.samplers[i];
        _sg_shader_sampler_t* dst = &cmn.samplers[i];
        if (src.stage != SG_SHADERSTAGE_NONE) {
            cmn.required_bindings_and_uniforms |= required_bindings_flag;
            dst.stage = src.stage;
            dst.sampler_type = src.sampler_type;
        }
    }
    for (size_t i = 0; i < SG_MAX_IMAGE_SAMPLER_PAIRS; i++) {
        const sg_shader_image_sampler_pair* src = &desc.image_sampler_pairs[i];
        _sg_shader_image_sampler_t* dst = &cmn.image_samplers[i];
        if (src.stage != SG_SHADERSTAGE_NONE) {
            dst.stage = src.stage;
            @debug_assert((src.image_slot >= 0) && (src.image_slot < SG_MAX_IMAGE_BINDSLOTS));
            @debug_assert(desc.images[src.image_slot].stage == src.stage);
            dst.image_slot = src.image_slot;
            @debug_assert((src.sampler_slot >= 0) && (src.sampler_slot < SG_MAX_SAMPLER_BINDSLOTS));
            @debug_assert(desc.samplers[src.sampler_slot].stage == src.stage);
            dst.sampler_slot = src.sampler_slot;
        }
    }
}

PipelineCommon :: @struct {
    vertex_buffer_layout_active: Array(bool, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    use_instanced_draw: bool;
    is_compute: bool;
    required_bindings_and_uniforms: u32;
    common: SgPipelineShared #use;
    shader: *Shader.T;
};

fn _sg_pipeline_common_init(cmn: *Pipeline.Common, desc: *Pipeline.Desc) {
    @debug_assert((desc.color_count >= 0) && (desc.color_count <= SG_MAX_COLOR_ATTACHMENTS));

    // FIXME: most of this isn't needed for compute pipelines

    const uint32_t required_bindings_flag = (1 << SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    for (int i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
        const sg_vertex_attr_state* a_state = &desc.layout.attrs[i];
        if (a_state.format != SG_VERTEXFORMAT_INVALID) {
            @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
            cmn.vertex_buffer_layout_active[a_state.buffer_index] = true;
            cmn.required_bindings_and_uniforms |= required_bindings_flag;
        }
    }
    cmn.is_compute = desc.compute;  // TODO: updated since the version im using
    cmn.use_instanced_draw = false;
    cmd.common = desc[];
    if (cmn.index_type != SG_INDEXTYPE_NONE) {
        cmn.required_bindings_and_uniforms |= required_bindings_flag;
    }
}

AttachmentsCommon :: @struct {
    width: i32;
    height: i32;
    num_colors: i32;
    colors: Array(SgAttachmentDesc, SG_MAX_COLOR_ATTACHMENTS);
    resolves: Array(SgAttachmentDesc, SG_MAX_COLOR_ATTACHMENTS);
    depth_stencil: SgAttachmentDesc;
};

fn attachments_common_init(cmn: *Attachments.Common, desc: *Attachments.Desc, width: i32, height: i32) void = {
    @debug_assert((width > 0) && (height > 0));
    cmn.width = width;
    cmn.height = height;
    range(0, SG_MAX_COLOR_ATTACHMENTS) { i |
        if (desc.colors[i].image.id != SG_INVALID_ID) {
            cmn.num_colors += 1;
            cmn.colors&[i] = desc.colors&[i];
            cmn.resolves&[i] = desc.resolves&[i];
        }
    }
    if (desc.depth_stencil.image.id != SG_INVALID_ID) {
        cmn.depth_stencil = desc.depth_stencil;
    }
}

_SG_INVALID_SLOT_INDEX :: 0;  // NOT a (random number chosen by fair die roll)

_sg_commit_listeners_t :: RawList(sg_commit_listener);

// resolved resource bindings struct
_sg_bindings_t :: @struct {
    pip: *Pipeline.T;
    vb_offsets: Array(i32, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    ib_offset: i32;
    vbs: Array(*Buffer.T, SG_MAX_VERTEXBUFFER_BINDSLOTS);
    ib: *Buffer.T;
    imgs: Array(*Image.T, SG_MAX_IMAGE_BINDSLOTS);
    smps: Array(*Sampler.T, SG_MAX_SAMPLER_BINDSLOTS);
    sbufs: Array(*Buffer.T, SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Self :: @struct {
    mtl: Metal.Impl;
    valid: bool;
    desc: SgDesc;    // original desc with default values patched in
    frame_index: i32;
    cur_pass: @struct {
        valid: bool;
        in_pass: bool;
        is_compute: bool;
        atts_id: Attachments;     // SG_INVALID_ID in a swapchain pass
        atts: *Attachments.T;    // 0 in a swapchain pass
        width: i32;
        height: i32;
        swapchain: @struct {
            color_fmt: SgPixelFormat;
            depth_fmt: SgPixelFormat;
            sample_count: i32;
        };
    };
    cur_pipeline: Pipeline;
    next_draw_valid: bool;
    required_bindings_and_uniforms: u32;    // used to check that bindings and uniforms are applied after applying pipeline
    applied_bindings_and_uniforms: u32;     // bits 0..7: uniform blocks, bit 8: bindings
    validate_error: SgLogItem;
    compute: @struct {
        readwrite_sbufs: _sg_tracker_t;  // tracks read/write storage buffers used in compute pass
    };
    pools: EnumMap(PoolType, DynPool),
    backend: SgBackend;
    features: SgFeatures;
    limits: SgLimits;
    formats: EnumMap(SgPixelFormat, _sg_pixelformat_info_t);
    stats_enabled: bool;
    stats: sg_frame_stats;
    prev_stats: sg_frame_stats;
    hooks: sg_trace_hooks;
    commit_listeners: _sg_commit_listeners_t;
};

fn impl(sg: *Self) *Metal.Impl = {
    ASSERT_METAL();
    self.mtl&
}

fn _sg_log(log_item: sg_log_item, log_level: u32, msg: CStr, line_nr: u32) void = {
    if (sg.desc.logger.func) {
        const char* filename = 0;
        if SOKOL_DEBUG {
            filename = __FILE__;
            if (0 == msg) {
                msg = _sg_log_messages[log_item];
            }
        }
        sg.desc.logger.func("sg", log_level, (uint32_t)log_item, msg, line_nr, filename, sg.desc.logger.user_data);
    } else {
        // for log level PANIC it would be 'undefined behaviour' to continue
        if (log_level == 0) {
            abort();
        }
    }
}

fn _sg_align_u32(val: u32, align: u32) u32 = {
    @debug_assert((align > 0) && ((align & (align - 1)) == 0));
    (val + (align - 1)) & ~(align - 1)
}

_sg_recti_t :: @struct(x: i32, y: i32, w: i32, h: i32);

fn clipi(x: i32, y: i32, w: i32, h: i32, clip_width: i32, clip_height: i32) _sg_recti_t = {
    x = min(max(0, x), clip_width-1);
    y = min(max(0, y), clip_height-1);
    if ((x + w) > clip_width) {
        w = clip_width - x;
    }
    if ((y + h) > clip_height) {
        h = clip_height - y;
    }
    (x = x, y = y, w = max(w, 1), h = max(h, 1))
}

fn vertexformat_bytesize(it: SgVertexFormat) i32 = @map_enum(it) (
    FLOAT  = 4, FLOAT2  = 8, FLOAT3  = 12, FLOAT4  = 16, 
    INT    = 4, INT2    = 8, INT3    = 12, INT4    = 16, 
    UINT   = 4, UINT2   = 8, UINT3   = 12, UINT4   = 16,
    BYTE4  = 4, BYTE4N  = 4, UBYTE4  = 4, UBYTE4N  = 4, 
    SHORT2 = 4, SHORT2N = 4, USHORT2 = 4, USHORT2N = 4,
    SHORT4 = 8, SHORT4N = 8, USHORT4 = 8, USHORT4N = 8, 
    UINT10_N2 = 4, HALF2 = 4, HALF4 = 8, INVALID = 0,
);

fn vertexformat_basetype(fmt: SgVertexFormat) sg_shader_attr_base_type = {
    if @is(fmt, .FLOAT, .FLOAT2, .FLOAT3, .FLOAT4, .HALF2, .HALF4, .BYTE4N, .UBYTE3N, .SHORT2N, .USHORT2N, .USHORT4N, .UINT10_N2) {
        return(.FLOAT);
    };
    if @is(fmt, .INT, .INT2, .INT3, .BYTE4, .SHORT2, .SHORT4) {
        return(.SINT);
    };
    if @is(fmt, .UINT, .UINT2, .UINT3, .UBYTE4, .USHORT2, .USHORT4) {
        return(.UINT);
    };
    unreachable()
}

fn uniform_alignment(type: sg_uniform_type, array_count: i32, ub_layout: sg_uniform_layout) u32 = {
    if(ub_layout == .NATIVE, => return(1));
    @debug_assert(array_count > 0);
    if(array_count != 1, => return(16));
    @map_enum(type) (FLOAT = 4, INT = 4, FLOAT2 = 8, INT2 = 8, FLOAT3 = 16, FLOAT4 = 16, INT3 = 16, INT4 = 16, MAT4 = 16)
}

fn uniform_size(type: sg_uniform_type, array_count: u32, ub_layout: sg_uniform_layout) u32 = {
    @debug_assert(array_count > 0);
    single := @map_enum(type) (FLOAT = 4, INT = 4, FLOAT2 = 8, INT2 = 8, FLOAT3 = 12, INT3 = 12, FLOAT4 = 16, INT4 = 16, MAT4 = 64);
    if array_count == 1 || ub_layout == .NATIVE || type == .MAT4 {
        return single * array_count;
    } 
    16 * array_count
}

fn is_compressed(fmt: SgPixelFormat) bool = @is(fmt, 
    .BC1_RGBA, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC4_R, .BC4_RSN, 
    .BC5_RG, .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA, 
    .ETC2_RGB8, .ETC2_SRGB8, .ETC2_RGB8A1, .ETC2_RGBA8, .ETC2_SRGB8A8, 
    .EAC_R11, .EAC_R11SN, .EAC_RG11, .EAC_RG11SN, .ASTC_4x4_RGBA, .ASTC_4x4_SRGBA,
);

fn is_valid_rendertarget_color_format(fmt: sg_pixel_format) bool = {
    const int fmt_index = (int) fmt;
    @debug_assert((fmt_index >= 0) && (fmt_index < _SG_PIXELFORMAT_NUM));
    return sg.formats[fmt_index].render && !sg.formats[fmt_index].depth;
}

fn is_valid_rendertarget_depth_format(sg: *Self, fmt: sg_pixel_format) bool = {
    const int fmt_index = (int) fmt;
    @debug_assert((fmt_index >= 0) && (fmt_index < _SG_PIXELFORMAT_NUM));
    sg.formats[fmt_index].render && sg.formats[fmt_index].depth
}

fn is_depth_or_depth_stencil_format(fmt: sg_pixel_format) bool = 
    @is(fmt, .DEPTH, .DEPTH_STENCIL);

fn is_depth_stencil_format(fmt: sg_pixel_format) bool = 
    fmt == .DEPTH_STENCIL;

fn pixelformat_bytesize(it: sg_pixel_format) i32 = @if_else {
    @if(@is(it, .R8, .R8SN, .R8UI, .R8SI)) => 1;
    @if(@is(it, .R16, .R16SN, .R16UI, .R16SI, .R16F, .RG8, .RG8SN, .RG8UI, .RG8SI)) => 2;
    @if(@is(it, .R32UI, .R32SI, .R32F, .RG16, .RG16SN, .RG16UI, .RG16SI, .RG16F, .RGBA8)) => 4;
    @if(@is(it, .SRGB8A8, .RGBA8SN, .RGBA8UI, .RGBA8SI, .BGRA8, .RGB10A2, .RG11B10F, .RGB9E5)) => 4;
    @if(@is(it, .RG32UI, .RG32SI, .RG32F, .RGBA16, .RGBA16SN, .RGBA16UI, .RGBA16SI, .RGBA16F)) => 8;
    @if(@is(it, .RGBA32UI, .RGBA32SI, .RGBA32F)) => 16;
    @if(@is(it, .DEPTH, .DEPTH_STENCIL)) => 4;
    @else => unreachable();
};

fn _sg_roundup(val: i32, round_to: i32) i32 = {
    return (val+(round_to-1)) & ~(round_to-1);
}

fn _sg_roundup_u32(val: u32, round_to: u32) u32 = {
    return (val+(round_to-1)) & ~(round_to-1);
}

fn _sg_roundup_u64(val: u64, round_to: u64) u64 = {
    return (val+(round_to-1)) & ~(round_to-1);
}

fn _sg_multiple_u64(val: u64, of: u64) bool = {
    return (val & (of-1)) == 0;
}

/* return row pitch for an image

    see ComputePitch in https://github.com/microsoft/DirectXTex/blob/master/DirectXTex/DirectXTexUtil.cpp
*/
fn row_pitch(it: sg_pixel_format, width: i32, row_align: i32) i32 = {
    pitch := @if_else {
        @if(@is(it, 
            .BC1_RGBA, .BC4_R, .BC4_RSN, .ETC2_RGB8, .ETC2_SRGB8, 
            .ETC2_RGB8A1, .EAC_R11, .EAC_R11SN,
        )) => max(8, ((width + 3) / 4) * 8); 
        @if(@is(it, 
            .BC2_RGBA, .RGBA, .BC3_SRGBA, .BC5_RG, .BC5_RGSN, 
            .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA, .ETC2_RGBA8, 
            .ETC2_SRGB8A8, .EAC_RG11, .EAC_RG11SN, .ASTC_4x4_RGBA, .ASTC_4x4_SRGBA,
        )) => max(16, ((width + 3) / 4) * 16);
        @else => width * pixelformat_bytesize(it); 
    };
    _sg_roundup(pitch, row_align)
}

// compute the number of rows in a surface depending on pixel format
fn _sg_num_rows(fmt: sg_pixel_format, height: i32) i32 = {
    adjust := @is(fmt, 
        .BC1_RGBA, BC4_R, .BC4_RSN, .ETC2_RGB8, .ETC2_SRGB8, .ETC2_RGB8A1, .ETC2_RGBA8, .ETC2_SRGB8A8,
        .EAC_R11, .EAC_R11SN, .EAC_RG11, .EAC_RG11SN, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC5_RG, 
        .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA, .ASTC_4x4_RGBA, .ASTC_4x4_SRGBA,
    );
    max(@if(adjust, ((height + 3) / 4), height), 1)
}

// return size of a mipmap level
fn _sg_miplevel_dim(base_dim: i32, mip_level: i32) i32 = {
    return max(base_dim >> mip_level, 1);
}

/* return pitch of a 2D subimage / texture slice
    see ComputePitch in https://github.com/microsoft/DirectXTex/blob/master/DirectXTex/DirectXTexUtil.cpp
*/
fn surface_pitch(fmt: sg_pixel_format, width: i32, height: i32, row_align: i32) i32 = {
    int num_rows = _sg_num_rows(fmt, height);
    return num_rows * row_pitch(fmt, width, row_align);
}

// TODO: damn this needs to be less painful 
fn set_fields(a: FatExpr) FatExpr #macro = @{
    it := @[a&.items()[0]];
    @[{
        body := @{};
        for a&.items() { e | 
            body = @{ 
                @[body];
                get_field(@type it[], it, @[e])[] = true;
            };
        };
        body
    }]
};

// capability table pixel format helper functions
// TODO: do a comptime thing to make these more succinct (maybe just pass in a $Str with the letters?)
fn _sg_pixelformat_all(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .filter, .blend, .render, .msaa);

fn _sg_pixelformat_s(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample);

fn _sg_pixelformat_sf(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .filter);

fn _sg_pixelformat_sr(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .render);

fn _sg_pixelformat_sfr(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .filter, .render);

fn _sg_pixelformat_srmd(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .render, .mssa, .depth);

fn _sg_pixelformat_srm(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .render, .mssa);

fn _sg_pixelformat_sfrm(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .filter, .render, .mssa);
    
fn _sg_pixelformat_sbrm(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .blend, .render, .mssa);

fn _sg_pixelformat_sbr(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .blend, .render);

fn _sg_pixelformat_sfbr(pfi: *_sg_pixelformat_info_t) void = 
    @set_fields(pfi, .sample, .filter, .blend, .render);

fn _sg_pass_action_defaults(action: *SgPassAction) SgPassAction = {
    @debug_assert(action);
    sg_pass_action res = *action;
    for (int i = 0; i < SG_MAX_COLOR_ATTACHMENTS; i++) {
        it := res.colors[i]&;
        if (it.load_action == .DEFAULT) {
            it.load_action = .CLEAR;
            // random colour chosen by fair die roll
            it.clear_value = (r = 0.5, g = 0.5, b = 0.5, a = 1.0);
        }
        if (it.store_action == .DEFAULT) {
            it.store_action = .STORE;
        }
    }
    
    if (res.depth.load_action == .DEFAULT) {
        res.depth.load_action = .CLEAR;
        res.depth.clear_value = SG_DEFAULT_CLEAR_DEPTH;
    }
    if (res.depth.store_action == .DEFAULT) {
        res.depth.store_action = .DONTCARE;
    }
    if (res.stencil.load_action == .DEFAULT) {
        res.stencil.load_action = .CLEAR;
        res.stencil.clear_value = SG_DEFAULT_CLEAR_STENCIL;
    }
    if (res.stencil.store_action == .DEFAULT) {
        res.stencil.store_action = .DONTCARE;
    }
    res
}

fn pool_init(pool: *DynPool, a: Alloc, num: i64, sizeof_t: i64) void = {
    @debug_assert(num >= 1 && num < _SG_MAX_POOL_SIZE);
    pool[] = (
        sizeof_t = sizeof_t,
        // slot 0 is reserved for the 'invalid id', so bump the pool size by 1
        size = num + 1,
        data = a.alloc_raw((num + 1) * sizeof_t, 8),
        // generation counters indexable by pool slot index, slot 0 is reserved
        gen_ctrs = a.alloc_zeroed(u32, num + 1),
        // it's not a bug to only reserve 'num' here
        // never allocate the zero-th pool item since the invalid id is 0
        free_queue = a.alloc_init(i32, num, fn(i) => num - i),
        queue_top = num + 1,
    );
}

fn pool_discard(pool: *DynPool, a: Alloc) void = {
    a.dealloc_raw(p.data, p.sizeof_t * p.size, 8);
    a.dealloc(i32, pool.free_queue);
    a.dealloc(u32, pool.gen_ctrs);
    pool[] = zeroed DynPool;
}

fn alloc_index(pool: *DynPool) i32 = {
    if(pool.queue_top <= 0, => return(_SG_INVALID_SLOT_INDEX));  // pool exhausted
    pool.queue_top -= 1;
    slot_index := pool.free_queue[pool.queue_top];
    @debug_assert((slot_index > 0) && (slot_index < pool.size));
    slot_index
}

fn free_index(pool: *DynPool, slot_index: i32) void = {
    @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < pool.size));
    @debug_assert(pool);
    @debug_assert(pool.free_queue);
    @debug_assert(pool.queue_top < pool.size);
    if SOKOL_DEBUG {
        // debug check against double-free
        for (int i = 0; i < pool.queue_top; i++) {
            @debug_assert(pool.free_queue[i] != slot_index);
        }
    }
    pool.free_queue[pool.queue_top] = slot_index;
    pool.queue_top += 1;
    @debug_assert(pool.queue_top <= (pool.size-1));
}

fn setup_pools(sg: *Self, desc: *SgDesc) void = {
    // note: the pools here will have an additional item, since slot 0 is reserved
    X :: fn($R, count) => pool_init(sg.pools.index(R.Tag), sg.allocator, count, size_of(R.T));
    X(Buffer, desc.buffer_pool_size);
    X(Image, desc.image_pool_size);
    X(Sampler, desc.sampler_pool_size);
    X(Shader, desc.shader_pool_size);
    X(Pipeline, desc.pipeline_pool_size);
    X(Attachment, desc.attachments_pool_size);
}

fn discard_pools(sg: *Self) void = {
    for_enum PoolType { it |
        pool_discard(sg.pools&.index(it), sg.allocator);
    };
}

/* allocate the slot at slot_index:
    - bump the slot's generation counter
    - create a resource id from the generation counter and slot index
    - set the slot's id to this id
    - set the slot's state to ALLOC
    - return the resource id
*/
fn slot_alloc(pool: *DynPool, slot_index: i32) u32 = {
    /* FIXME: add handling for an overflowing generation counter,
       for now, just overflow (another option is to disable
       the slot)
    */
    slot := pool.get_slot(slot_index);
    @debug_assert(pool && pool.gen_ctrs);
    @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < pool.size));
    @debug_assert(slot.id == SG_INVALID_ID);
    @debug_assert(slot.state == SG_RESOURCESTATE_INITIAL);
    pool.gen_ctrs[slot_index] += 1;
    ctr := pool.gen_ctrs[slot_index];
    slot.id = bit_or(ctr.shift_left(_SG_SLOT_SHIFT), slot_index.bit_and(_SG_SLOT_MASK));
    slot.state = .ALLOC;
    slot.id
}

fn get_slot(pool: *DynPool, slot_index: i32) *SlotHeader = 
    SlotHeader.ptr_from_raw(pool.data.offset(pool.sizeof_t));

// extract slot index from id
fn _sg_slot_index(id: u32) i32 = {
    int slot_index = (int) (id & _SG_SLOT_MASK);
    @debug_assert(_SG_INVALID_SLOT_INDEX != slot_index);
    return slot_index;
}

fn discard_all_resources(sg: *Self) void = {
    /*  this is a bit dumb since it loops over all pool slots to
        find the occupied slots, on the other hand it is only ever
        executed at shutdown
        NOTE: ONLY EXECUTE THIS AT SHUTDOWN
              ...because the free queues will not be reset
              and the resource slots not be cleared!
    */
    
    inline_for_enum PoolType { $it |
        p := sg.pools&.index(it[]);
        range(1, p.size) { i |
            sg_resource_state state = p.buffers[i].slot.state;
            if ((state == SG_RESOURCESTATE_VALID) || (state == SG_RESOURCESTATE_FAILED)) {
                sg.discard(p.get_typed(it[]));
            }
        }
    };
}

fn _sg_validate_begin(sg: *Self) void = if SOKOL_DEBUG {
    sg.validate_error = SG_LOGITEM_OK;
};

fn _sg_validate_end(sg: *Self) bool = {
    @if(!SOKOL_DEBUG) return(true);
    if(sg.validate_error == SG_LOGITEM_OK, => return(true));
    //#if !defined(SOKOL_VALIDATE_NON_FATAL)
    //    _SG_PANIC(VALIDATION_FAILED);
    //#endif
    false
}

fn validate(sg: *Self, desc: *Buffer.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    @V(desc._start_canary == 0, VALIDATE_BUFFERDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_BUFFERDESC_CANARY);
    @V(desc.size > 0, VALIDATE_BUFFERDESC_EXPECT_NONZERO_SIZE);
    bool injected = (0 != desc.gl_buffers[0]) ||
                    (0 != desc.mtl_buffers[0]) ||
                    (0 != desc.d3d11_buffer) ||
                    (0 != desc.wgpu_buffer);
    if (!injected && (desc.usage == SG_USAGE_IMMUTABLE)) {
        if (desc.data.ptr) {
            @V(desc.size == desc.data.size, VALIDATE_BUFFERDESC_EXPECT_MATCHING_DATA_SIZE);
        } else {
            @V(desc.data.size == 0, VALIDATE_BUFFERDESC_EXPECT_ZERO_DATA_SIZE);
        }
    } else {
        @V(0 == desc.data.ptr, VALIDATE_BUFFERDESC_EXPECT_NO_DATA);
        @V(desc.data.size == 0, VALIDATE_BUFFERDESC_EXPECT_ZERO_DATA_SIZE);
    }
    if (desc.type == SG_BUFFERTYPE_STORAGEBUFFER) {
        @V(sg.features.compute, VALIDATE_BUFFERDESC_STORAGEBUFFER_SUPPORTED);
        @V(_sg_multiple_u64(desc.size, 4), VALIDATE_BUFFERDESC_STORAGEBUFFER_SIZE_MULTIPLE_4);
    }
};

fn validate(data: *SgImageData, fmt: SgPixelFormat, width: i32, height: i32, num_faces: i32, num_mips: i32, num_slices: i32) void = {
    range(0, num_faces) { face_index |
        range(0, num_mips) { mip_index |
            has_data := data.subimage[face_index][mip_index].ptr != 0;
            has_size := data.subimage[face_index][mip_index].size > 0;
            @V(has_data && has_size, VALIDATE_IMAGEDATA_NODATA);
            mip_width := _sg_miplevel_dim(width, mip_index);
            mip_height := _sg_miplevel_dim(height, mip_index);
            bytes_per_slice := _sg_surface_pitch(fmt, mip_width, mip_height, 1);
            expected_size := bytes_per_slice * num_slices;
            @V(expected_size == (int)data.subimage[face_index][mip_index].size, VALIDATE_IMAGEDATA_DATA_SIZE);
        }
    }
}

fn validate(sg: *Self, desc: *Image.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    @V(desc._start_canary == 0, VALIDATE_IMAGEDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_IMAGEDESC_CANARY);
    @V(desc.width > 0, VALIDATE_IMAGEDESC_WIDTH);
    @V(desc.height > 0, VALIDATE_IMAGEDESC_HEIGHT);
    const sg_pixel_format fmt = desc.pixel_format;
    const sg_usage usage = desc.usage;
    const bool injected = (0 != desc.gl_textures[0]) ||
                            (0 != desc.mtl_textures[0]) ||
                            (0 != desc.d3d11_texture) ||
                            (0 != desc.wgpu_texture);
    if (is_depth_or_depth_stencil_format(fmt)) {
        @V(desc.type != SG_IMAGETYPE_3D, VALIDATE_IMAGEDESC_DEPTH_3D_IMAGE);
    }
    if (desc.render_target) {
        @debug_assert(((int)fmt >= 0) && ((int)fmt < _SG_PIXELFORMAT_NUM));
        @V(sg.formats[fmt].render, VALIDATE_IMAGEDESC_RT_PIXELFORMAT);
        @V(usage == SG_USAGE_IMMUTABLE, VALIDATE_IMAGEDESC_RT_IMMUTABLE);
        @V(desc.data.subimage[0][0].ptr==0, VALIDATE_IMAGEDESC_RT_NO_DATA);
        if (desc.sample_count > 1) {
            @V(sg.formats[fmt].msaa, VALIDATE_IMAGEDESC_NO_MSAA_RT_SUPPORT);
            @V(desc.num_mipmaps == 1, VALIDATE_IMAGEDESC_MSAA_NUM_MIPMAPS);
            @V(desc.type != SG_IMAGETYPE_3D, VALIDATE_IMAGEDESC_MSAA_3D_IMAGE);
            @V(desc.type != SG_IMAGETYPE_CUBE, VALIDATE_IMAGEDESC_MSAA_CUBE_IMAGE);
        }
    } else {
        @V(desc.sample_count == 1, VALIDATE_IMAGEDESC_MSAA_BUT_NO_RT);
        valid_nonrt_fmt := !sg.is_valid_rendertarget_depth_format(fmt);
        @V(valid_nonrt_fmt, VALIDATE_IMAGEDESC_NONRT_PIXELFORMAT);
        is_immutable := (usage == SG_USAGE_IMMUTABLE);
        if is_compressed(desc.pixel_format) {
            @V(is_immutable, VALIDATE_IMAGEDESC_COMPRESSED_IMMUTABLE);
        }
        if (!injected && is_immutable) {
            // image desc must have valid data
            validate(&desc.data,
                desc.pixel_format,
                desc.width,
                desc.height,
                (desc.type == SG_IMAGETYPE_CUBE) ? 6 : 1,
                desc.num_mipmaps,
                desc.num_slices);
        } else {
            // image desc must not have data
            for (int face_index = 0; face_index < SG_CUBEFACE_NUM; face_index++) {
                for (int mip_index = 0; mip_index < SG_MAX_MIPMAPS; mip_index++) {
                    const bool no_data = 0 == desc.data.subimage[face_index][mip_index].ptr;
                    const bool no_size = 0 == desc.data.subimage[face_index][mip_index].size;
                    if (injected) {
                        @V(no_data && no_size, VALIDATE_IMAGEDESC_INJECTED_NO_DATA);
                    }
                    if (!is_immutable) {
                        @V(no_data && no_size, VALIDATE_IMAGEDESC_DYNAMIC_NO_DATA);
                    }
                }
            }
        }
    }
};

fn validate(sg: *Self, desc: *Sampler.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    @V(desc._start_canary == 0, VALIDATE_SAMPLERDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_SAMPLERDESC_CANARY);
    // restriction from WebGPU: when anisotropy > 1, all filters must be linear
    if (desc.max_anisotropy > 1) {
        @V((desc.min_filter == SG_FILTER_LINEAR)
                    && (desc.mag_filter == SG_FILTER_LINEAR)
                    && (desc.mipmap_filter == SG_FILTER_LINEAR),
                    VALIDATE_SAMPLERDESC_ANISTROPIC_REQUIRES_LINEAR_FILTERING);
    }
};

_sg_u128_t :: @struct {
    lo: i64;
    hi: i64;
};

fn _sg_validate_set_slot_bit(bits: _sg_u128_t, stage: sg_shader_stage, slot: u8) _sg_u128_t = {
    switch (stage) {
        case SG_SHADERSTAGE_NONE:
            @debug_assert(slot < 128);
            if (slot < 64) {
                bits.lo |= 1ULL << slot;
            } else {
                bits.hi |= 1ULL << (slot - 64);
            }
            break();
        case SG_SHADERSTAGE_VERTEX:
            @debug_assert(slot < 64);
            bits.lo |= 1ULL << slot;
            break();
        case SG_SHADERSTAGE_FRAGMENT:
            @debug_assert(slot < 64);
            bits.hi |= 1ULL << slot;
            break();
        case SG_SHADERSTAGE_COMPUTE:
            @debug_assert(slot < 64);
            bits.lo |= 1ULL << slot;
            break();
        default:
            SOKOL_UNREACHABLE;
            break();
    }
    return bits;
}

fn _sg_validate_slot_bits(bits: _sg_u128_t, stage: sg_shader_stage, slot: u8) bool = {
    _sg_u128_t mask = zeroed(_sg_u128_t);
    switch (stage) {
        case SG_SHADERSTAGE_NONE:
            @debug_assert(slot < 128);
            if (slot < 64) {
                mask.lo = 1ULL << slot;
            } else {
                mask.hi = 1ULL << (slot - 64);
            }
            break();
        case SG_SHADERSTAGE_VERTEX:
            @debug_assert(slot < 64);
            mask.lo = 1ULL << slot;
            break();
        case SG_SHADERSTAGE_FRAGMENT:
            @debug_assert(slot < 64);
            mask.hi = 1ULL << slot;
            break();
        case SG_SHADERSTAGE_COMPUTE:
            @debug_assert(slot < 64);
            mask.lo = 1ULL << slot;
            break();
        default:
            SOKOL_UNREACHABLE;
            break();
    }
    return ((bits.lo & mask.lo) == 0) && ((bits.hi & mask.hi) == 0);
}

fn validate(sg: FatExpr, body: FatExpr) FatExpr #outputs(bool) = @{
    @if(!SOKOL_DEBUG, true, {
        @[sg].desc.disable_validation || {
            _sg_validate_begin();
            @[body];
            _sg_validate_end()
        }
    })
}

V :: fn(a: FatExpr) FatExpr #macro = @{ 
    if !@[a&.items()[0]] {
        panic(@[@literal a&.items()[1].ident().expect("@V(cond, MSG)").str()]);
    };
};

fn validate(sg: *Self, desc: *Shader.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    bool is_compute_shader = (desc.compute_func.source != 0) || (desc.compute_func.bytecode.ptr != 0);
    _sg_validate_begin();
    @V(desc._start_canary == 0, VALIDATE_SHADERDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_SHADERDESC_CANARY);
    ASSERT_METAL();
    // on Metal or D3D11, must provide shader source code or byte code
    if (is_compute_shader) {
        @V((0 != desc.compute_func.source) || (0 != desc.compute_func.bytecode.ptr), VALIDATE_SHADERDESC_COMPUTE_SOURCE_OR_BYTECODE);
    } else {
        @V((0 != desc.vertex_func.source)|| (0 != desc.vertex_func.bytecode.ptr), VALIDATE_SHADERDESC_VERTEX_SOURCE_OR_BYTECODE);
        @V((0 != desc.fragment_func.source) || (0 != desc.fragment_func.bytecode.ptr), VALIDATE_SHADERDESC_FRAGMENT_SOURCE_OR_BYTECODE);
    }
    if (is_compute_shader) {
        @V((0 == desc.vertex_func.source) && (0 == desc.vertex_func.bytecode.ptr), VALIDATE_SHADERDESC_INVALID_SHADER_COMBO);
        @V((0 == desc.fragment_func.source) && (0 == desc.fragment_func.bytecode.ptr), VALIDATE_SHADERDESC_INVALID_SHADER_COMBO);
    } else {
        @V((0 == desc.compute_func.source) && (0 == desc.compute_func.bytecode.ptr), VALIDATE_SHADERDESC_INVALID_SHADER_COMBO);
    }
    ASSERT_METAL();
    if (is_compute_shader) {
        @V(desc.mtl_threads_per_threadgroup.x > 0, VALIDATE_SHADERDESC_METAL_THREADS_PER_THREADGROUP);
        @V(desc.mtl_threads_per_threadgroup.y > 0, VALIDATE_SHADERDESC_METAL_THREADS_PER_THREADGROUP);
        @V(desc.mtl_threads_per_threadgroup.z > 0, VALIDATE_SHADERDESC_METAL_THREADS_PER_THREADGROUP);
    }
    for (size_t i = 0; i < SG_MAX_VERTEX_ATTRIBUTES; i++) {
        if (desc.attrs[i].glsl_name) {
            @V(strlen(desc.attrs[i].glsl_name) < _SG_STRING_SIZE, VALIDATE_SHADERDESC_ATTR_STRING_TOO_LONG);
        }
        if (desc.attrs[i].hlsl_sem_name) {
            @V(strlen(desc.attrs[i].hlsl_sem_name) < _SG_STRING_SIZE, VALIDATE_SHADERDESC_ATTR_STRING_TOO_LONG);
        }
    }
    // if shader byte code, the size must also be provided
    if (0 != desc.vertex_func.bytecode.ptr) {
        @V(desc.vertex_func.bytecode.size > 0, VALIDATE_SHADERDESC_NO_BYTECODE_SIZE);
    }
    if (0 != desc.fragment_func.bytecode.ptr) {
        @V(desc.fragment_func.bytecode.size > 0, VALIDATE_SHADERDESC_NO_BYTECODE_SIZE);
    }
    if (0 != desc.compute_func.bytecode.ptr) {
        @V(desc.compute_func.bytecode.size > 0, VALIDATE_SHADERDESC_NO_BYTECODE_SIZE);
    }

    _sg_u128_t msl_buf_bits = zeroed(_sg_u128_t);
    _sg_u128_t msl_tex_bits = zeroed(_sg_u128_t);
    _sg_u128_t msl_smp_bits = zeroed(_sg_u128_t);
    ASSERT_METAL();
    for (size_t ub_idx = 0; ub_idx < SG_MAX_UNIFORMBLOCK_BINDSLOTS; ub_idx++) {
        const sg_shader_uniform_block* ub_desc = &desc.uniform_blocks[ub_idx];
        if (ub_desc.stage == SG_SHADERSTAGE_NONE) {
            continue();
        }
        @V(ub_desc.size > 0, VALIDATE_SHADERDESC_UNIFORMBLOCK_SIZE_IS_ZERO);
        @V(ub_desc.msl_buffer_n < _SG_MTL_MAX_STAGE_UB_BINDINGS, VALIDATE_SHADERDESC_UNIFORMBLOCK_METAL_BUFFER_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_buf_bits, ub_desc.stage, ub_desc.msl_buffer_n), VALIDATE_SHADERDESC_UNIFORMBLOCK_METAL_BUFFER_SLOT_COLLISION);
        msl_buf_bits = _sg_validate_set_slot_bit(msl_buf_bits, ub_desc.stage, ub_desc.msl_buffer_n);
        ASSERT_METAL();
        ASSERT_METAL();
    }

    for (size_t sbuf_idx = 0; sbuf_idx < SG_MAX_STORAGEBUFFER_BINDSLOTS; sbuf_idx++) {
        const sg_shader_storage_buffer* sbuf_desc = &desc.storage_buffers[sbuf_idx];
        if (sbuf_desc.stage == SG_SHADERSTAGE_NONE) {
            continue();
        }
        @V((sbuf_desc.msl_buffer_n >= _SG_MTL_MAX_STAGE_UB_BINDINGS) && (sbuf_desc.msl_buffer_n < _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS), VALIDATE_SHADERDESC_STORAGEBUFFER_METAL_BUFFER_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_buf_bits, sbuf_desc.stage, sbuf_desc.msl_buffer_n), VALIDATE_SHADERDESC_STORAGEBUFFER_METAL_BUFFER_SLOT_COLLISION);
        msl_buf_bits = _sg_validate_set_slot_bit(msl_buf_bits, sbuf_desc.stage, sbuf_desc.msl_buffer_n);
        ASSERT_METAL();
    }

    uint32_t img_slot_mask = 0;
    for (size_t img_idx = 0; img_idx < SG_MAX_IMAGE_BINDSLOTS; img_idx++) {
        const sg_shader_image* img_desc = &desc.images[img_idx];
        if (img_desc.stage == SG_SHADERSTAGE_NONE) {
            continue();
        }
        img_slot_mask |= (1 << img_idx);
        @V(img_desc.msl_texture_n < _SG_MTL_MAX_STAGE_IMAGE_BINDINGS, VALIDATE_SHADERDESC_IMAGE_METAL_TEXTURE_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_tex_bits, img_desc.stage, img_desc.msl_texture_n), VALIDATE_SHADERDESC_IMAGE_METAL_TEXTURE_SLOT_COLLISION);
        msl_tex_bits = _sg_validate_set_slot_bit(msl_tex_bits, img_desc.stage, img_desc.msl_texture_n);
        ASSERT_METAL();
    }

    uint32_t smp_slot_mask = 0;
    for (size_t smp_idx = 0; smp_idx < SG_MAX_SAMPLER_BINDSLOTS; smp_idx++) {
        const sg_shader_sampler* smp_desc = &desc.samplers[smp_idx];
        if (smp_desc.stage == SG_SHADERSTAGE_NONE) {
            continue();
        }
        smp_slot_mask |= (1 << smp_idx);
        @V(smp_desc.msl_sampler_n < _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS, VALIDATE_SHADERDESC_SAMPLER_METAL_SAMPLER_SLOT_OUT_OF_RANGE);
        @V(_sg_validate_slot_bits(msl_smp_bits, smp_desc.stage, smp_desc.msl_sampler_n), VALIDATE_SHADERDESC_SAMPLER_METAL_SAMPLER_SLOT_COLLISION);
        msl_smp_bits = _sg_validate_set_slot_bit(msl_smp_bits, smp_desc.stage, smp_desc.msl_sampler_n);
        ASSERT_METAL();
    }

    uint32_t ref_img_slot_mask = 0;
    uint32_t ref_smp_slot_mask = 0;
    for (size_t img_smp_idx = 0; img_smp_idx < SG_MAX_IMAGE_SAMPLER_PAIRS; img_smp_idx++) {
        const sg_shader_image_sampler_pair* img_smp_desc = &desc.image_sampler_pairs[img_smp_idx];
        if (img_smp_desc.stage == SG_SHADERSTAGE_NONE) {
            continue();
        }
        ASSERT_METAL();
        const bool img_slot_in_range = img_smp_desc.image_slot < SG_MAX_IMAGE_BINDSLOTS;
        const bool smp_slot_in_range = img_smp_desc.sampler_slot < SG_MAX_SAMPLER_BINDSLOTS;
        @V(img_slot_in_range, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_IMAGE_SLOT_OUT_OF_RANGE);
        @V(smp_slot_in_range, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_SAMPLER_SLOT_OUT_OF_RANGE);
        if (img_slot_in_range && smp_slot_in_range) {
            ref_img_slot_mask |= 1 << img_smp_desc.image_slot;
            ref_smp_slot_mask |= 1 << img_smp_desc.sampler_slot;
            const sg_shader_image* img_desc = &desc.images[img_smp_desc.image_slot];
            const sg_shader_sampler* smp_desc = &desc.samplers[img_smp_desc.sampler_slot];
            @V(img_desc.stage == img_smp_desc.stage, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_IMAGE_STAGE_MISMATCH);
            @V(smp_desc.stage == img_smp_desc.stage, VALIDATE_SHADERDESC_IMAGE_SAMPLER_PAIR_SAMPLER_STAGE_MISMATCH);
            const bool needs_nonfiltering = (img_desc.sample_type == SG_IMAGESAMPLETYPE_UINT)
                                            || (img_desc.sample_type == SG_IMAGESAMPLETYPE_SINT)
                                            || (img_desc.sample_type == SG_IMAGESAMPLETYPE_UNFILTERABLE_FLOAT);
            const bool needs_comparison = img_desc.sample_type == SG_IMAGESAMPLETYPE_DEPTH;
            if (needs_nonfiltering) {
                @V(needs_nonfiltering && (smp_desc.sampler_type == SG_SAMPLERTYPE_NONFILTERING), VALIDATE_SHADERDESC_NONFILTERING_SAMPLER_REQUIRED);
            }
            if (needs_comparison) {
                @V(needs_comparison && (smp_desc.sampler_type == SG_SAMPLERTYPE_COMPARISON), VALIDATE_SHADERDESC_COMPARISON_SAMPLER_REQUIRED);
            }
        }
    }
    // each image and sampler must be referenced by an image sampler
    @V(img_slot_mask == ref_img_slot_mask, VALIDATE_SHADERDESC_IMAGE_NOT_REFERENCED_BY_IMAGE_SAMPLER_PAIRS);
    @V(smp_slot_mask == ref_smp_slot_mask, VALIDATE_SHADERDESC_SAMPLER_NOT_REFERENCED_BY_IMAGE_SAMPLER_PAIRS);

    _sg_validate_end()
}

fn validate(sg: *Self, desc: *Pipeline.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    @V(desc._start_canary == 0, VALIDATE_PIPELINEDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_PIPELINEDESC_CANARY);
    @V(desc.shader.id != SG_INVALID_ID, VALIDATE_PIPELINEDESC_SHADER);
    const _sg_shader_t* shd = _sg_lookup_shader(&sg.pools, desc.shader.id);
    @V(0 != shd, VALIDATE_PIPELINEDESC_SHADER);
    if (shd) {
        @V(shd.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_PIPELINEDESC_SHADER);
        if (desc.compute) {
            @V(shd.cmn.is_compute, VALIDATE_PIPELINEDESC_COMPUTE_SHADER_EXPECTED);
        } else {
            @V(!shd.cmn.is_compute, VALIDATE_PIPELINEDESC_NO_COMPUTE_SHADER_EXPECTED);
            bool attrs_cont = true;
            for (size_t attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
                const sg_vertex_attr_state* a_state = &desc.layout.attrs[attr_index];
                if (a_state.format == SG_VERTEXFORMAT_INVALID) {
                    attrs_cont = false;
                    continue();
                }
                @V(attrs_cont, VALIDATE_PIPELINEDESC_NO_CONT_ATTRS);
                @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
                // vertex format must match expected shader attribute base type (if provided)
                if (shd.cmn.attrs[attr_index].base_type != SG_SHADERATTRBASETYPE_UNDEFINED) {
                    if (vertexformat_basetype(a_state.format) != shd.cmn.attrs[attr_index].base_type) {
                        @V(false, VALIDATE_PIPELINEDESC_ATTR_BASETYPE_MISMATCH);
                        _SG_LOGMSG(VALIDATE_PIPELINEDESC_ATTR_BASETYPE_MISMATCH, "attr format:");
                        _SG_LOGMSG(VALIDATE_PIPELINEDESC_ATTR_BASETYPE_MISMATCH, _sg_vertexformat_to_string(a_state.format));
                        _SG_LOGMSG(VALIDATE_PIPELINEDESC_ATTR_BASETYPE_MISMATCH, "shader attr base type:");
                        _SG_LOGMSG(VALIDATE_PIPELINEDESC_ATTR_BASETYPE_MISMATCH, _sg_shaderattrbasetype_to_string(shd.cmn.attrs[attr_index].base_type));
                    }
                }
                #if defined(SOKOL_D3D11)
                // on D3D11, semantic names (and semantic indices) must be provided
                @V(!_sg_strempty(&shd.d3d11.attrs[attr_index].sem_name), VALIDATE_PIPELINEDESC_ATTR_SEMANTICS);
                #endif
            }
            // must only use readonly storage buffer bindings in render pipelines
            for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
                if (shd.cmn.storage_buffers[i].stage != SG_SHADERSTAGE_NONE) {
                    @V(shd.cmn.storage_buffers[i].readonly, VALIDATE_PIPELINEDESC_SHADER_READONLY_STORAGEBUFFERS);
                }
            }
            for (int buf_index = 0; buf_index < SG_MAX_VERTEXBUFFER_BINDSLOTS; buf_index++) {
                const sg_vertex_buffer_layout_state* l_state = &desc.layout.buffers[buf_index];
                if (l_state.stride == 0) {
                    continue();
                }
                @V(_sg_multiple_u64((uint64_t)l_state.stride, 4), VALIDATE_PIPELINEDESC_LAYOUT_STRIDE4);
            }
        }
    }
    for (size_t color_index = 0; color_index < (size_t)desc.color_count; color_index++) {
        const sg_blend_state* bs = &desc.colors[color_index].blend;
        if ((bs.op_rgb == SG_BLENDOP_MIN) || (bs.op_rgb == SG_BLENDOP_MAX)) {
            @V((bs.src_factor_rgb == SG_BLENDFACTOR_ONE) && (bs.dst_factor_rgb == SG_BLENDFACTOR_ONE), VALIDATE_PIPELINEDESC_BLENDOP_MINMAX_REQUIRES_BLENDFACTOR_ONE);
        }
        if ((bs.op_alpha == SG_BLENDOP_MIN) || (bs.op_alpha == SG_BLENDOP_MAX)) {
            @V((bs.src_factor_alpha == SG_BLENDFACTOR_ONE) && (bs.dst_factor_alpha == SG_BLENDFACTOR_ONE), VALIDATE_PIPELINEDESC_BLENDOP_MINMAX_REQUIRES_BLENDFACTOR_ONE);
        }
    }
};

fn validate(sg: *Self, desc: *Attachments.Desc) bool = @validate(sg) {
    @debug_assert(desc);
    @V(desc._start_canary == 0, VALIDATE_ATTACHMENTSDESC_CANARY);
    @V(desc._end_canary == 0, VALIDATE_ATTACHMENTSDESC_CANARY);
    atts_cont := true;
    int color_width = -1, color_height = -1, color_sample_count = -1;
    bool has_color_atts = false;
    for (int att_index = 0; att_index < SG_MAX_COLOR_ATTACHMENTS; att_index++) {
        const sg_attachment_desc* att = &desc.colors[att_index];
        if (att.image.id == SG_INVALID_ID) {
            atts_cont = false;
            continue();
        }
        @V(atts_cont, VALIDATE_ATTACHMENTSDESC_NO_CONT_COLOR_ATTS);
        has_color_atts = true;
        const *Image.T img = _sg_lookup_image(&sg.pools, att.image.id);
        @V(img, VALIDATE_ATTACHMENTSDESC_IMAGE);
        if (0 != img) {
            @V(img.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_ATTACHMENTSDESC_IMAGE);
            @V(img.cmn.render_target, VALIDATE_ATTACHMENTSDESC_IMAGE_NO_RT);
            @V(att.mip_level < img.cmn.num_mipmaps, VALIDATE_ATTACHMENTSDESC_MIPLEVEL);
            if (img.cmn.type == SG_IMAGETYPE_CUBE) {
                @V(att.slice < 6, VALIDATE_ATTACHMENTSDESC_FACE);
            } else if (img.cmn.type == SG_IMAGETYPE_ARRAY) {
                @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_LAYER);
            } else if (img.cmn.type == SG_IMAGETYPE_3D) {
                @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_SLICE);
            }
            if (att_index == 0) {
                color_width = _sg_miplevel_dim(img.cmn.width, att.mip_level);
                color_height = _sg_miplevel_dim(img.cmn.height, att.mip_level);
                color_sample_count = img.cmn.sample_count;
            } else {
                @V(color_width == _sg_miplevel_dim(img.cmn.width, att.mip_level), VALIDATE_ATTACHMENTSDESC_IMAGE_SIZES);
                @V(color_height == _sg_miplevel_dim(img.cmn.height, att.mip_level), VALIDATE_ATTACHMENTSDESC_IMAGE_SIZES);
                @V(color_sample_count == img.cmn.sample_count, VALIDATE_ATTACHMENTSDESC_IMAGE_SAMPLE_COUNTS);
            }
            @V(is_valid_rendertarget_color_format(img.cmn.pixel_format), VALIDATE_ATTACHMENTSDESC_COLOR_INV_PIXELFORMAT);

            // check resolve attachment
            const sg_attachment_desc* res_att = &desc.resolves[att_index];
            if (res_att.image.id != SG_INVALID_ID) {
                // associated color attachment must be MSAA
                @V(img.cmn.sample_count > 1, VALIDATE_ATTACHMENTSDESC_RESOLVE_COLOR_IMAGE_MSAA);
                const *Image.T res_img = _sg_lookup_image(&sg.pools, res_att.image.id);
                @V(res_img, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE);
                if (res_img != 0) {
                    @V(res_img.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE);
                    @V(res_img.cmn.render_target, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_NO_RT);
                    @V(res_img.cmn.sample_count == 1, VALIDATE_ATTACHMENTSDESC_RESOLVE_SAMPLE_COUNT);
                    @V(res_att.mip_level < res_img.cmn.num_mipmaps, VALIDATE_ATTACHMENTSDESC_RESOLVE_MIPLEVEL);
                    if (res_img.cmn.type == SG_IMAGETYPE_CUBE) {
                        @V(res_att.slice < 6, VALIDATE_ATTACHMENTSDESC_RESOLVE_FACE);
                    } else if (res_img.cmn.type == SG_IMAGETYPE_ARRAY) {
                        @V(res_att.slice < res_img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_RESOLVE_LAYER);
                    } else if (res_img.cmn.type == SG_IMAGETYPE_3D) {
                        @V(res_att.slice < res_img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_RESOLVE_SLICE);
                    }
                    @V(img.cmn.pixel_format == res_img.cmn.pixel_format, VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_FORMAT);
                    @V(color_width == _sg_miplevel_dim(res_img.cmn.width, res_att.mip_level), VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_SIZES);
                    @V(color_height == _sg_miplevel_dim(res_img.cmn.height, res_att.mip_level), VALIDATE_ATTACHMENTSDESC_RESOLVE_IMAGE_SIZES);
                }
            }
        }
    }
    bool has_depth_stencil_att = false;
    if (desc.depth_stencil.image.id != SG_INVALID_ID) {
        const sg_attachment_desc* att = &desc.depth_stencil;
        const *Image.T img = _sg_lookup_image(&sg.pools, att.image.id);
        @V(img, VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE);
        has_depth_stencil_att = true;
        if (img) {
            @V(img.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE);
            @V(att.mip_level < img.cmn.num_mipmaps, VALIDATE_ATTACHMENTSDESC_DEPTH_MIPLEVEL);
            if (img.cmn.type == SG_IMAGETYPE_CUBE) {
                @V(att.slice < 6, VALIDATE_ATTACHMENTSDESC_DEPTH_FACE);
            } else if (img.cmn.type == SG_IMAGETYPE_ARRAY) {
                @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_DEPTH_LAYER);
            } else if (img.cmn.type == SG_IMAGETYPE_3D) {
                // NOTE: this can't actually happen because of VALIDATE_IMAGEDESC_DEPTH_3D_IMAGE
                @V(att.slice < img.cmn.num_slices, VALIDATE_ATTACHMENTSDESC_DEPTH_SLICE);
            }
            @V(img.cmn.render_target, VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_NO_RT);
            @V((color_width == -1) || (color_width == _sg_miplevel_dim(img.cmn.width, att.mip_level)), VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_SIZES);
            @V((color_height == -1) || (color_height == _sg_miplevel_dim(img.cmn.height, att.mip_level)), VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_SIZES);
            @V((color_sample_count == -1) || (color_sample_count == img.cmn.sample_count), VALIDATE_ATTACHMENTSDESC_DEPTH_IMAGE_SAMPLE_COUNT);
            @V(sg.is_valid_rendertarget_depth_format(img.cmn.pixel_format), VALIDATE_ATTACHMENTSDESC_DEPTH_INV_PIXELFORMAT);
        }
    }
    @V(has_color_atts || has_depth_stencil_att, VALIDATE_ATTACHMENTSDESC_NO_ATTACHMENTS);
};

fn validate_begin_pass(sg: *Self, pass: *SgPass) bool = @validate(sg) {
    const bool is_compute_pass = pass.compute;
    const bool is_swapchain_pass = !is_compute_pass && (pass.attachments.id == SG_INVALID_ID);
    const bool is_offscreen_pass = !(is_compute_pass || is_swapchain_pass);
    _sg_validate_begin();
    @V(pass._start_canary == 0, VALIDATE_BEGINPASS_CANARY);
    @V(pass._end_canary == 0, VALIDATE_BEGINPASS_CANARY);
    if (is_compute_pass) {
        // this is a compute pass
        @V(pass.attachments.id == SG_INVALID_ID, VALIDATE_BEGINPASS_EXPECT_NO_ATTACHMENTS);
    } else if (is_swapchain_pass) {
        // this is a swapchain pass
        @V(pass.swapchain.width > 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_WIDTH);
        @V(pass.swapchain.height > 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_HEIGHT);
        @V(pass.swapchain.sample_count > 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_SAMPLECOUNT);
        @V(pass.swapchain.color_format > SG_PIXELFORMAT_NONE, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_COLORFORMAT);
        // NOTE: depth buffer is optional, so depth_format is allowed to be invalid
        // NOTE: the GL framebuffer handle may actually be 0
        @V(pass.swapchain.metal.current_drawable != 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_CURRENTDRAWABLE);
        if (pass.swapchain.depth_format == SG_PIXELFORMAT_NONE) {
            @V(pass.swapchain.metal.depth_stencil_texture == 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_DEPTHSTENCILTEXTURE_NOTSET);
        } else {
            @V(pass.swapchain.metal.depth_stencil_texture != 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_DEPTHSTENCILTEXTURE);
        }
        if (pass.swapchain.sample_count > 1) {
            @V(pass.swapchain.metal.msaa_color_texture != 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_MSAACOLORTEXTURE);
        } else {
            @V(pass.swapchain.metal.msaa_color_texture == 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_MSAACOLORTEXTURE_NOTSET);
        }
        ASSERT_METAL();
    } else {
        // this is an 'offscreen pass'
        const _sg_attachments_t* atts = _sg_lookup_attachments(&sg.pools, pass.attachments.id);
        if (atts) {
            @V(atts.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_BEGINPASS_ATTACHMENTS_VALID);
            for (int i = 0; i < SG_MAX_COLOR_ATTACHMENTS; i++) {
                const _sg_attachment_common_t* color_att = &atts.cmn.colors[i];
                color_img := sg.impl().attachments_color_image(atts, i);
                if (color_img) {
                    @V(color_img.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_BEGINPASS_COLOR_ATTACHMENT_IMAGE);
                    @V(color_img.slot.id == color_att.image_id.id, VALIDATE_BEGINPASS_COLOR_ATTACHMENT_IMAGE);
                }
                const _sg_attachment_common_t* resolve_att = &atts.cmn.resolves[i];
                const *Image.T resolve_img = sg.impl().attachments_resolve_image(atts, i);
                if (resolve_img) {
                    @V(resolve_img.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_BEGINPASS_RESOLVE_ATTACHMENT_IMAGE);
                    @V(resolve_img.slot.id == resolve_att.image_id.id, VALIDATE_BEGINPASS_RESOLVE_ATTACHMENT_IMAGE);
                }
            }
            const *Image.T ds_img = sg.impl().attachments_ds_image(atts);
            if (ds_img) {
                const _sg_attachment_common_t* att = &atts.cmn.depth_stencil;
                @V(ds_img.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_BEGINPASS_DEPTHSTENCIL_ATTACHMENT_IMAGE);
                @V(ds_img.slot.id == att.image_id.id, VALIDATE_BEGINPASS_DEPTHSTENCIL_ATTACHMENT_IMAGE);
            }
        } else {
            @V(atts != 0, VALIDATE_BEGINPASS_ATTACHMENTS_EXISTS);
        }
    }
    if (is_compute_pass || is_offscreen_pass) {
        it := pass.swapchain&;
        @V(it.width == 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_WIDTH_NOTSET);
        @V(it.height == 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_HEIGHT_NOTSET);
        @V(it.sample_count == 0, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_SAMPLECOUNT_NOTSET);
        @V(it.color_format == _SG_PIXELFORMAT_DEFAULT, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_COLORFORMAT_NOTSET);
        @V(it.depth_format == _SG_PIXELFORMAT_DEFAULT, VALIDATE_BEGINPASS_SWAPCHAIN_EXPECT_DEPTHFORMAT_NOTSET);
        @V(it.metal.current_drawable == 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_CURRENTDRAWABLE_NOTSET);
        @V(it.metal.depth_stencil_texture == 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_DEPTHSTENCILTEXTURE_NOTSET);
        @V(it.metal.msaa_color_texture == 0, VALIDATE_BEGINPASS_SWAPCHAIN_METAL_EXPECT_MSAACOLORTEXTURE_NOTSET);
        ASSERT_METAL();
    }
};

fn validate_apply_viewport(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && !sg.cur_pass.is_compute, VALIDATE_AVP_RENDERPASS_EXPECTED);
};

fn _sg_validate_apply_scissor_rect(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && !sg.cur_pass.is_compute, VALIDATE_ASR_RENDERPASS_EXPECTED);
};

fn _sg_validate_apply_pipeline(sg: *Self, pip_id: Pipeline) bool = @validate(sg) {
    // the pipeline object must be alive and valid
    @V(pip_id.id != SG_INVALID_ID, VALIDATE_APIP_PIPELINE_VALID_ID);
    const _sg_pipeline_t* pip = _sg_lookup_pipeline(&sg.pools, pip_id.id);
    @V(pip != 0, VALIDATE_APIP_PIPELINE_EXISTS);
    if (!pip) {
        return _sg_validate_end();
    }
    @V(pip.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_APIP_PIPELINE_VALID);
    // the pipeline's shader must be alive and valid
    @debug_assert(pip.shader);
    @V(sg.cur_pass.in_pass, VALIDATE_APIP_PASS_EXPECTED);
    @V(pip.shader.slot.id == pip.cmn.shader_id.id, VALIDATE_APIP_SHADER_EXISTS);
    @V(pip.shader.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_APIP_SHADER_VALID);
    if (pip.cmn.is_compute) {
        @V(sg.cur_pass.is_compute, VALIDATE_APIP_COMPUTEPASS_EXPECTED);
    } else {
        @V(!sg.cur_pass.is_compute, VALIDATE_APIP_RENDERPASS_EXPECTED);
        // check that pipeline attributes match current pass attributes
        if (sg.cur_pass.atts_id.id != SG_INVALID_ID) {
            // an offscreen pass
            const _sg_attachments_t* atts = sg.cur_pass.atts;
            @debug_assert(atts);
            @V(atts.slot.id == sg.cur_pass.atts_id.id, VALIDATE_APIP_CURPASS_ATTACHMENTS_EXISTS);
            @V(atts.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_APIP_CURPASS_ATTACHMENTS_VALID);

            @V(pip.cmn.color_count == atts.cmn.num_colors, VALIDATE_APIP_ATT_COUNT);
            for (int i = 0; i < pip.cmn.color_count; i++) {
                const *Image.T att_img = sg.impl().attachments_color_image(atts, i);
                @V(pip.cmn.colors[i].pixel_format == att_img.cmn.pixel_format, VALIDATE_APIP_COLOR_FORMAT);
                @V(pip.cmn.sample_count == att_img.cmn.sample_count, VALIDATE_APIP_SAMPLE_COUNT);
            }
            const *Image.T att_dsimg = sg.impl().attachments_ds_image(atts);
            if (att_dsimg) {
                @V(pip.cmn.depth.pixel_format == att_dsimg.cmn.pixel_format, VALIDATE_APIP_DEPTH_FORMAT);
            } else {
                @V(pip.cmn.depth.pixel_format == SG_PIXELFORMAT_NONE, VALIDATE_APIP_DEPTH_FORMAT);
            }
        } else {
            // default pass
            @V(pip.cmn.color_count == 1, VALIDATE_APIP_ATT_COUNT);
            @V(pip.cmn.colors[0].pixel_format == sg.cur_pass.swapchain.color_fmt, VALIDATE_APIP_COLOR_FORMAT);
            @V(pip.cmn.depth.pixel_format == sg.cur_pass.swapchain.depth_fmt, VALIDATE_APIP_DEPTH_FORMAT);
            @V(pip.cmn.sample_count == sg.cur_pass.swapchain.sample_count, VALIDATE_APIP_SAMPLE_COUNT);
        }
    }
}

fn _sg_validate_apply_bindings(sg: *Self, bindings: *sg_bindings) bool = @validate(sg) {
    // must be called in a pass
    @V(sg.cur_pass.in_pass, VALIDATE_ABND_PASS_EXPECTED);

    // bindings must not be empty
    bool has_any_bindings = bindings.index_buffer.id != SG_INVALID_ID;
    if (!has_any_bindings) for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
        has_any_bindings |= bindings.vertex_buffers[i].id != SG_INVALID_ID;
    }
    if (!has_any_bindings) for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        has_any_bindings |= bindings.images[i].id != SG_INVALID_ID;
    }
    if (!has_any_bindings) for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        has_any_bindings |= bindings.samplers[i].id != SG_INVALID_ID;
    }
    if (!has_any_bindings) for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        has_any_bindings |= bindings.storage_buffers[i].id != SG_INVALID_ID;
    }
    @V(has_any_bindings, VALIDATE_ABND_EMPTY_BINDINGS);

    // a pipeline object must have been applied
    @V(sg.cur_pipeline.id != SG_INVALID_ID, VALIDATE_ABND_PIPELINE);
    const _sg_pipeline_t* pip = _sg_lookup_pipeline(&sg.pools, sg.cur_pipeline.id);
    @V(pip != 0, VALIDATE_ABND_PIPELINE_EXISTS);
    if (!pip) {
        return _sg_validate_end();
    }
    @V(pip.slot.state == SG_RESOURCESTATE_VALID, VALIDATE_ABND_PIPELINE_VALID);
    @debug_assert(pip.shader && (pip.cmn.shader_id.id == pip.shader.slot.id));
    const _sg_shader_t* shd = pip.shader;

    if (sg.cur_pass.is_compute) {
        for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
            @V(bindings.vertex_buffers[i].id == SG_INVALID_ID, VALIDATE_ABND_COMPUTE_EXPECTED_NO_VBS);
        }
    } else {
        // has expected vertex buffers, and vertex buffers still exist
        for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
            if (pip.cmn.vertex_buffer_layout_active[i]) {
                @V(bindings.vertex_buffers[i].id != SG_INVALID_ID, VALIDATE_ABND_EXPECTED_VB);
                // buffers in vertex-buffer-slots must be of type SG_BUFFERTYPE_VERTEXBUFFER
                if (bindings.vertex_buffers[i].id != SG_INVALID_ID) {
                    const *Buffer.T buf = _sg_lookup_buffer(&sg.pools, bindings.vertex_buffers[i].id);
                    @V(buf != 0, VALIDATE_ABND_VB_EXISTS);
                    if (buf && buf.slot.state == SG_RESOURCESTATE_VALID) {
                        @V(SG_BUFFERTYPE_VERTEXBUFFER == buf.cmn.type, VALIDATE_ABND_VB_TYPE);
                        @V(!buf.cmn.append_overflow, VALIDATE_ABND_VB_OVERFLOW);
                    }
                }
            }
        }
    }

    if (sg.cur_pass.is_compute) {
        @V(bindings.index_buffer.id == SG_INVALID_ID, VALIDATE_ABND_COMPUTE_EXPECTED_NO_IB);
    } else {
        // index buffer expected or not, and index buffer still exists
        if (pip.cmn.index_type == SG_INDEXTYPE_NONE) {
            // pipeline defines non-indexed rendering, but index buffer provided
            @V(bindings.index_buffer.id == SG_INVALID_ID, VALIDATE_ABND_IB);
        } else {
            // pipeline defines indexed rendering, but no index buffer provided
            @V(bindings.index_buffer.id != SG_INVALID_ID, VALIDATE_ABND_NO_IB);
        }
        if (bindings.index_buffer.id != SG_INVALID_ID) {
            // buffer in index-buffer-slot must be of type SG_BUFFERTYPE_INDEXBUFFER
            const *Buffer.T buf = _sg_lookup_buffer(&sg.pools, bindings.index_buffer.id);
            @V(buf != 0, VALIDATE_ABND_IB_EXISTS);
            if (buf && buf.slot.state == SG_RESOURCESTATE_VALID) {
                @V(SG_BUFFERTYPE_INDEXBUFFER == buf.cmn.type, VALIDATE_ABND_IB_TYPE);
                @V(!buf.cmn.append_overflow, VALIDATE_ABND_IB_OVERFLOW);
            }
        }
    }

    // has expected images
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage != SG_SHADERSTAGE_NONE) {
            @V(bindings.images[i].id != SG_INVALID_ID, VALIDATE_ABND_EXPECTED_IMAGE_BINDING);
            if (bindings.images[i].id != SG_INVALID_ID) {
                const *Image.T img = _sg_lookup_image(&sg.pools, bindings.images[i].id);
                @V(img != 0, VALIDATE_ABND_IMG_EXISTS);
                if (img && img.slot.state == SG_RESOURCESTATE_VALID) {
                    @V(img.cmn.type == shd.cmn.images[i].image_type, VALIDATE_ABND_IMAGE_TYPE_MISMATCH);
                    if (!sg.features.msaa_image_bindings) {
                        @V(img.cmn.sample_count == 1, VALIDATE_ABND_IMAGE_MSAA);
                    }
                    if (shd.cmn.images[i].multisampled) {
                        @V(img.cmn.sample_count > 1, VALIDATE_ABND_EXPECTED_MULTISAMPLED_IMAGE);
                    }
                    const _sg_pixelformat_info_t* info = &sg.formats[img.cmn.pixel_format];
                    switch (shd.cmn.images[i].sample_type) {
                        case SG_IMAGESAMPLETYPE_FLOAT:
                            @V(info.filter, VALIDATE_ABND_EXPECTED_FILTERABLE_IMAGE);
                            break();
                        case SG_IMAGESAMPLETYPE_DEPTH:
                            @V(info.depth, VALIDATE_ABND_EXPECTED_DEPTH_IMAGE);
                            break();
                        default:
                            break();
                    }
                }
            }
        }
    }

    // has expected samplers
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage != SG_SHADERSTAGE_NONE) {
            @V(bindings.samplers[i].id != SG_INVALID_ID, VALIDATE_ABND_EXPECTED_SAMPLER_BINDING);
            if (bindings.samplers[i].id != SG_INVALID_ID) {
                const *Sampler.T smp = _sg_lookup_sampler(&sg.pools, bindings.samplers[i].id);
                @V(smp != 0, VALIDATE_ABND_SMP_EXISTS);
                if (smp) {
                    if (shd.cmn.samplers[i].sampler_type == SG_SAMPLERTYPE_COMPARISON) {
                        @V(smp.cmn.compare != SG_COMPAREFUNC_NEVER, VALIDATE_ABND_UNEXPECTED_SAMPLER_COMPARE_NEVER);
                    } else {
                        @V(smp.cmn.compare == SG_COMPAREFUNC_NEVER, VALIDATE_ABND_EXPECTED_SAMPLER_COMPARE_NEVER);
                    }
                    if (shd.cmn.samplers[i].sampler_type == SG_SAMPLERTYPE_NONFILTERING) {
                        const bool nonfiltering = (smp.cmn.min_filter != SG_FILTER_LINEAR)
                                                && (smp.cmn.mag_filter != SG_FILTER_LINEAR)
                                                && (smp.cmn.mipmap_filter != SG_FILTER_LINEAR);
                        @V(nonfiltering, VALIDATE_ABND_EXPECTED_NONFILTERING_SAMPLER);
                    }
                }
            }
        }
    }

    // has expected storage buffers
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage != SG_SHADERSTAGE_NONE) {
            @V(bindings.storage_buffers[i].id != SG_INVALID_ID, VALIDATE_ABND_EXPECTED_STORAGEBUFFER_BINDING);
            if (bindings.storage_buffers[i].id != SG_INVALID_ID) {
                const *Buffer.T sbuf = _sg_lookup_buffer(&sg.pools, bindings.storage_buffers[i].id);
                @V(sbuf != 0, VALIDATE_ABND_STORAGEBUFFER_EXISTS);
                if (sbuf) {
                    @V(sbuf.cmn.type == SG_BUFFERTYPE_STORAGEBUFFER, VALIDATE_ABND_STORAGEBUFFER_BINDING_BUFFERTYPE);
                    // read/write bindings are only allowed for immutable buffers
                    if (!shd.cmn.storage_buffers[i].readonly) {
                        @V(sbuf.cmn.usage == SG_USAGE_IMMUTABLE, VALIDATE_ABND_STORAGEBUFFER_READWRITE_IMMUTABLE);
                    }
                }
            }
        }
    }
};

fn _sg_validate_apply_uniforms(sg: *Self, ub_slot: i32, data: []u8) bool = @validate(sg) {
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @V(sg.cur_pass.in_pass, VALIDATE_AU_PASS_EXPECTED);
    @V(sg.cur_pipeline.id != SG_INVALID_ID, VALIDATE_AU_NO_PIPELINE);
    const _sg_pipeline_t* pip = _sg_lookup_pipeline(&sg.pools, sg.cur_pipeline.id);
    @debug_assert(pip && (pip.slot.id == sg.cur_pipeline.id));
    @debug_assert(pip.shader && (pip.shader.slot.id == pip.cmn.shader_id.id));

    const _sg_shader_t* shd = pip.shader;
    @V(shd.cmn.uniform_blocks[ub_slot].stage != SG_SHADERSTAGE_NONE, VALIDATE_AU_NO_UNIFORMBLOCK_AT_SLOT);
    @V(data.size == shd.cmn.uniform_blocks[ub_slot].size, VALIDATE_AU_SIZE);
};

fn _sg_validate_draw(sg: *Self, base_element: i32, num_elements: i32, num_instances: i32) bool = @validate(sg) {
    @V(sg.cur_pass.in_pass && !sg.cur_pass.is_compute, VALIDATE_DRAW_RENDERPASS_EXPECTED);
    @V(base_element >= 0, VALIDATE_DRAW_BASEELEMENT);
    @V(num_elements >= 0, VALIDATE_DRAW_NUMELEMENTS);
    @V(num_instances >= 0, VALIDATE_DRAW_NUMINSTANCES);
    @V(sg.required_bindings_and_uniforms == sg.applied_bindings_and_uniforms, VALIDATE_DRAW_REQUIRED_BINDINGS_OR_UNIFORMS_MISSING);
};

fn _sg_validate_dispatch(sg: *Self, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) bool = @validate(sg) = {
    @V(sg.cur_pass.in_pass && sg.cur_pass.is_compute, VALIDATE_DISPATCH_COMPUTEPASS_EXPECTED);
    @V((num_groups_x >= 0) && (num_groups_x < (1<<16)), VALIDATE_DISPATCH_NUMGROUPSX);
    @V((num_groups_y >= 0) && (num_groups_y < (1<<16)), VALIDATE_DISPATCH_NUMGROUPSY);
    @V((num_groups_z >= 0) && (num_groups_z < (1<<16)), VALIDATE_DISPATCH_NUMGROUPSZ);
    @V(sg.required_bindings_and_uniforms == sg.applied_bindings_and_uniforms, VALIDATE_DRAW_REQUIRED_BINDINGS_OR_UNIFORMS_MISSING);
};

fn _sg_validate_update_buffer(sg: *Self, buf: *Buffer.T, data: []u8) bool = @validate(sg) = {
    @debug_assert(buf && data && data.ptr);
    @V(buf.cmn.usage != SG_USAGE_IMMUTABLE, VALIDATE_UPDATEBUF_USAGE);
    @V(buf.cmn.size >= (int)data.size, VALIDATE_UPDATEBUF_SIZE);
    @V(buf.cmn.update_frame_index != sg.frame_index, VALIDATE_UPDATEBUF_ONCE);
    @V(buf.cmn.append_frame_index != sg.frame_index, VALIDATE_UPDATEBUF_APPEND);
}

fn _sg_validate_append_buffer(sg: *Self, buf: *Buffer.T, data: []u8) bool = @validate(sg) = {
    @debug_assert(buf && data && data.ptr);
    @V(buf.cmn.usage != SG_USAGE_IMMUTABLE, VALIDATE_APPENDBUF_USAGE);
    @V(buf.cmn.size >= (buf.cmn.append_pos + (int)data.size), VALIDATE_APPENDBUF_SIZE);
    @V(buf.cmn.update_frame_index != sg.frame_index, VALIDATE_APPENDBUF_UPDATE);
}

fn validate(sg: *Self, img: *Image.T, data: *sg_image_data) bool = @validate(sg) = {
    @debug_assert(img && data);
    @V(img.cmn.usage != SG_USAGE_IMMUTABLE, VALIDATE_UPDIMG_USAGE);
    @V(img.cmn.upd_frame_index != sg.frame_index, VALIDATE_UPDIMG_ONCE);
    _sg_validate_image_data(data,
        img.cmn.pixel_format,
        img.cmn.width,
        img.cmn.height,
        (img.cmn.type == SG_IMAGETYPE_CUBE) ? 6 : 1,
        img.cmn.num_mipmaps,
        img.cmn.num_slices);
}

fn desc_defaults(desc: *Buffer.Desc) Buffer.Desc = {
    sg_buffer_desc def = *desc;
    def.type = _sg_def(def.type, SG_BUFFERTYPE_VERTEXBUFFER);
    def.usage = _sg_def(def.usage, SG_USAGE_IMMUTABLE);
    if (def.size == 0) {
        def.size = def.data.size;
    }
    return def;
}

fn desc_defaults(desc: *Image.Desc) Image.Desc = {
    sg_image_desc def = *desc;
    def.type = _sg_def(def.type, SG_IMAGETYPE_2D);
    def.num_slices = _sg_def(def.num_slices, 1);
    def.num_mipmaps = _sg_def(def.num_mipmaps, 1);
    def.usage = _sg_def(def.usage, SG_USAGE_IMMUTABLE);
    if (desc.render_target) {
        def.pixel_format = _sg_def(def.pixel_format, sg.desc.environment.defaults.color_format);
        def.sample_count = _sg_def(def.sample_count, sg.desc.environment.defaults.sample_count);
    } else {
        def.pixel_format = _sg_def(def.pixel_format, SG_PIXELFORMAT_RGBA8);
        def.sample_count = _sg_def(def.sample_count, 1);
    }
    return def;
}

fn desc_defaults(desc: *Sampler.Desc) Sampler.Desc {
    sg_sampler_desc def = *desc;
    def.min_filter = _sg_def(def.min_filter, SG_FILTER_NEAREST);
    def.mag_filter = _sg_def(def.mag_filter, SG_FILTER_NEAREST);
    def.mipmap_filter = _sg_def(def.mipmap_filter, SG_FILTER_NEAREST);
    def.wrap_u = _sg_def(def.wrap_u, SG_WRAP_REPEAT);
    def.wrap_v = _sg_def(def.wrap_v, SG_WRAP_REPEAT);
    def.wrap_w = _sg_def(def.wrap_w, SG_WRAP_REPEAT);
    def.max_lod = _sg_def_flt(def.max_lod, FLT_MAX);
    def.border_color = _sg_def(def.border_color, SG_BORDERCOLOR_OPAQUE_BLACK);
    def.compare = _sg_def(def.compare, SG_COMPAREFUNC_NEVER);
    def.max_anisotropy = _sg_def(def.max_anisotropy, 1);
    return def;
}

fn desc_defaults(desc: *Shader.Desc) Shader.Desc = {
    sg_shader_desc def = *desc;
    def.vertex_func.entry = _sg_def(def.vertex_func.entry, "_main");
    def.fragment_func.entry = _sg_def(def.fragment_func.entry, "_main");
    def.compute_func.entry = _sg_def(def.compute_func.entry, "_main");
    ASSERT_METAL();
    def.mtl_threads_per_threadgroup.y = _sg_def(desc.mtl_threads_per_threadgroup.y, 1);
    def.mtl_threads_per_threadgroup.z = _sg_def(desc.mtl_threads_per_threadgroup.z, 1);
    for (size_t ub_index = 0; ub_index < SG_MAX_UNIFORMBLOCK_BINDSLOTS; ub_index++) {
        sg_shader_uniform_block* ub_desc = &def.uniform_blocks[ub_index];
        if (ub_desc.stage != SG_SHADERSTAGE_NONE) {
            ub_desc.layout = _sg_def(ub_desc.layout, SG_UNIFORMLAYOUT_NATIVE);
            for (size_t u_index = 0; u_index < SG_MAX_UNIFORMBLOCK_MEMBERS; u_index++) {
                sg_glsl_shader_uniform* u_desc = &ub_desc.glsl_uniforms[u_index];
                if (u_desc.type == SG_UNIFORMTYPE_INVALID) {
                    break();
                }
                u_desc.array_count = _sg_def(u_desc.array_count, 1);
            }
        }
    }
    for (size_t img_index = 0; img_index < SG_MAX_IMAGE_BINDSLOTS; img_index++) {
        sg_shader_image* img_desc = &def.images[img_index];
        if (img_desc.stage != SG_SHADERSTAGE_NONE) {
            img_desc.image_type = _sg_def(img_desc.image_type, SG_IMAGETYPE_2D);
            img_desc.sample_type = _sg_def(img_desc.sample_type, SG_IMAGESAMPLETYPE_FLOAT);
        }
    }
    for (size_t smp_index = 0; smp_index < SG_MAX_SAMPLER_BINDSLOTS; smp_index++) {
        sg_shader_sampler* smp_desc = &def.samplers[smp_index];
        if (smp_desc.stage != SG_SHADERSTAGE_NONE) {
            smp_desc.sampler_type = _sg_def(smp_desc.sampler_type, SG_SAMPLERTYPE_FILTERING);
        }
    }
    return def;
}

fn desc_defaults(desc: *Pipeline.Desc) Pipeline.Desc = {
    sg_pipeline_desc def = *desc;

    // FIXME: should we actually do all this stuff for a compute pipeline?

    def.primitive_type = _sg_def(def.primitive_type, SG_PRIMITIVETYPE_TRIANGLES);
    def.index_type = _sg_def(def.index_type, SG_INDEXTYPE_NONE);
    def.cull_mode = _sg_def(def.cull_mode, SG_CULLMODE_NONE);
    def.face_winding = _sg_def(def.face_winding, SG_FACEWINDING_CW);
    def.sample_count = _sg_def(def.sample_count, sg.desc.environment.defaults.sample_count);

    def.stencil.front.compare = _sg_def(def.stencil.front.compare, SG_COMPAREFUNC_ALWAYS);
    def.stencil.front.fail_op = _sg_def(def.stencil.front.fail_op, SG_STENCILOP_KEEP);
    def.stencil.front.depth_fail_op = _sg_def(def.stencil.front.depth_fail_op, SG_STENCILOP_KEEP);
    def.stencil.front.pass_op = _sg_def(def.stencil.front.pass_op, SG_STENCILOP_KEEP);
    def.stencil.back.compare = _sg_def(def.stencil.back.compare, SG_COMPAREFUNC_ALWAYS);
    def.stencil.back.fail_op = _sg_def(def.stencil.back.fail_op, SG_STENCILOP_KEEP);
    def.stencil.back.depth_fail_op = _sg_def(def.stencil.back.depth_fail_op, SG_STENCILOP_KEEP);
    def.stencil.back.pass_op = _sg_def(def.stencil.back.pass_op, SG_STENCILOP_KEEP);

    def.depth.compare = _sg_def(def.depth.compare, SG_COMPAREFUNC_ALWAYS);
    def.depth.pixel_format = _sg_def(def.depth.pixel_format, sg.desc.environment.defaults.depth_format);
    if (def.colors[0].pixel_format == SG_PIXELFORMAT_NONE) {
        // special case depth-only rendering, enforce a color count of 0
        def.color_count = 0;
    } else {
        def.color_count = _sg_def(def.color_count, 1);
    }
    if (def.color_count > SG_MAX_COLOR_ATTACHMENTS) {
        def.color_count = SG_MAX_COLOR_ATTACHMENTS;
    }
    for (int i = 0; i < def.color_count; i++) {
        sg_color_target_state* cs = &def.colors[i];
        cs.pixel_format = _sg_def(cs.pixel_format, sg.desc.environment.defaults.color_format);
        cs.write_mask = _sg_def(cs.write_mask, SG_COLORMASK_RGBA);
        sg_blend_state* bs = &def.colors[i].blend;
        bs.op_rgb = _sg_def(bs.op_rgb, SG_BLENDOP_ADD);
        bs.src_factor_rgb = _sg_def(bs.src_factor_rgb, SG_BLENDFACTOR_ONE);
        if ((bs.op_rgb == SG_BLENDOP_MIN) || (bs.op_rgb == SG_BLENDOP_MAX)) {
            bs.dst_factor_rgb = _sg_def(bs.dst_factor_rgb, SG_BLENDFACTOR_ONE);
        } else {
            bs.dst_factor_rgb = _sg_def(bs.dst_factor_rgb, SG_BLENDFACTOR_ZERO);
        }
        bs.op_alpha = _sg_def(bs.op_alpha, SG_BLENDOP_ADD);
        bs.src_factor_alpha = _sg_def(bs.src_factor_alpha, SG_BLENDFACTOR_ONE);
        if ((bs.op_alpha == SG_BLENDOP_MIN) || (bs.op_alpha == SG_BLENDOP_MAX)) {
            bs.dst_factor_alpha = _sg_def(bs.dst_factor_alpha, SG_BLENDFACTOR_ONE);
        } else {
            bs.dst_factor_alpha = _sg_def(bs.dst_factor_alpha, SG_BLENDFACTOR_ZERO);
        }
    }

    for (int attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
        sg_vertex_attr_state* a_state = &def.layout.attrs[attr_index];
        if (a_state.format == SG_VERTEXFORMAT_INVALID) {
            break();
        }
        @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
        sg_vertex_buffer_layout_state* l_state = &def.layout.buffers[a_state.buffer_index];
        l_state.step_func = _sg_def(l_state.step_func, SG_VERTEXSTEP_PER_VERTEX);
        l_state.step_rate = _sg_def(l_state.step_rate, 1);
    }

    // resolve vertex layout strides and offsets
    int auto_offset[SG_MAX_VERTEXBUFFER_BINDSLOTS];
    _sg_clear(auto_offset, sizeof(auto_offset));
    bool use_auto_offset = true;
    for (int attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
        // to use computed offsets, *all* attr offsets must be 0
        if (def.layout.attrs[attr_index].offset != 0) {
            use_auto_offset = false;
        }
    }
    for (int attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
        sg_vertex_attr_state* a_state = &def.layout.attrs[attr_index];
        if (a_state.format == SG_VERTEXFORMAT_INVALID) {
            break();
        }
        @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
        if (use_auto_offset) {
            a_state.offset = auto_offset[a_state.buffer_index];
        }
        auto_offset[a_state.buffer_index] += vertexformat_bytesize(a_state.format);
    }
    // compute vertex strides if needed
    for (int buf_index = 0; buf_index < SG_MAX_VERTEXBUFFER_BINDSLOTS; buf_index++) {
        sg_vertex_buffer_layout_state* l_state = &def.layout.buffers[buf_index];
        if (l_state.stride == 0) {
            l_state.stride = auto_offset[buf_index];
        }
    }

    return def;
}

fn desc_defaults(desc: *Attachments.Desc) Attachments.Desc = {
    desc[]
}

Metal :: import("@/graphics/macos/gfx.fr");

// https://floooh.github.io/2018/06/17/handles-vs-pointers.html
/*
    The 32-bit resource id is split into a 16-bit pool index in the lower bits,
    and a 16-bit 'generation counter' in the upper bits. The index allows fast
    pool lookups, and combined with the generation-counter it allows to detect
    'dangling accesses' (trying to use an object which no longer exists, and
    its pool slot has been reused for a new object)
*/

Buffer :: Resource(.Buffer, SgBufferDesc, BufferCommon, Metal.Buffer);  // vertex- and index-buffers
Image :: Resource(.Image, SgImageDesc, ImageCommon, Metal.Image);  // images used as textures and render-pass attachments
Sampler :: Resource(.Sampler, SgSamplerDesc, SamplerCommon, Metal.Sampler);  // sampler objects describing how a texture is sampled in a shader
Shader :: Resource(.Shader, SgShaderDesc, ShaderCommon, Metal.Shader);  // vertex- and fragment-shaders and shader interface information
Pipeline :: Resource(.Pipeline, SgPipelineDesc, PipelineCommon, Metal.Pipeline);  // associated shader and vertex-layouts, and render states
Attachments :: Resource(.Attachments, SgAttachmentsDec, AttachmentsCommon, Metal.Attachments);  // a baked collection of render pass attachment images

PoolType :: @enum(u8) (Buffer, Image, Sampler, Shader, Pipeline, Attatchments);

DynPool :: @struct {
    size: i32;
    queue_top: i32;
    gen_ctrs: []u32;
    free_queue: []i32;
    sizeof_t: i64;
    data: rawptr;  // first of []T
};

fn Resource($_Tag: PoolType, $_Desc: Type, $_Common: Type, $_Impl: Type) Type = {
    I :: @struct {
        id: u32;
        Tag :: _Tag;
        // The internal data we store in the pool
        T :: @struct {
            slot: SlotHeader;
            cmn: Common;
            mtl: Impl;
        };     
        Desc :: _Desc;  // Info the user gives us to create one 
        Common :: _Common;
        Impl :: _Impl;
    };
    
    fn alloc(sg: *Self) I #trace_args = {
        p := sg.pools&.index(I.Tag);
        slot_index := p.alloc_index();
        id := if (_SG_INVALID_SLOT_INDEX != slot_index) {
            p.slot_alloc(slot_index)
        } else {
            res.id = SG_INVALID_ID;
            _SG_ERROR(SHADER_POOL_EXHAUSTED);
            SG_INVALID_ID
        };
        (id = id)
    }
    
    fn make(sg: *Self, desc: *I.Desc) I #trace_args = {
        @debug_assert(sg.valid);
        @debug_assert(desc);
        desc_def: I.Desc = desc_defaults(desc);
        id: I = sg.alloc();
        if (id.id != SG_INVALID_ID) {
            _sg_shader_t* shd = _sg_shader_at(&sg.pools, shd_id.id);
            @debug_assert(shd && (shd.slot.state == SG_RESOURCESTATE_ALLOC));
            _sg_init_shader(shd, &desc_def);
            @debug_assert((shd.slot.state == SG_RESOURCESTATE_VALID) || (shd.slot.state == SG_RESOURCESTATE_FAILED));
        };
        id
    }
    
    // returns pointer to resource by id without matching id check
    fn at(sg: *Self, id: u32) *I.T = {
        p := sg.pools&.index(I.Tag);
        @debug_assert(SG_INVALID_ID != id);
        slot_index := _sg_slot_index(id);
        @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < p.shader_pool.size));
        I.T.ptr_from_raw(p.data.offset(slot_index.intcast() * size_of(I.T)))
    }
    
    // returns pointer to resource with matching id check, may return 0
    fn lookup(sg: *Self, id: I) ?*I.T = {
        if(SG_INVALID_ID != id.id, => return(.None));
        it: *I.T = sg.at(id.id);
        if(it.slot.id != id.id, => return(.None));
        (Some = it)
    }
    
    fn dealloc(sg: *Self, id: I) void #trace_args = {
        it := sg.lookup(id) || return();
        if (it.slot.state == SG_RESOURCESTATE_ALLOC) {
            sg.dealloc(it);
        } else {
            _SG_ERROR(DEALLOC_SHADER_INVALID_STATE);
        }
    }
    
    fn dealloc(sg: *Self, it: *I.T) void = {
        p := sg.pools&.index(I.Tag);
        SOKOL_ASSERT((it->slot.state == SG_RESOURCESTATE_ALLOC) && (it->slot.id != SG_INVALID_ID));
        p.free_index(_sg_slot_index(it.slot.id));
        p.slot = zeroed SlotHeader;
    }
    
    fn init(sg: *Self, id: I, desc: *I.Desc) void #trace_args = {
        @debug_assert(sg.valid);
        desc_def := sg.desc_defaults(desc);
        it := sg.lookup(id) || return();
        if (it.slot.state == SG_RESOURCESTATE_ALLOC) {
            _sg_init_shader(it, &desc_def);
            @debug_assert((shd.slot.state == SG_RESOURCESTATE_VALID) || (shd.slot.state == SG_RESOURCESTATE_FAILED));
        } else {
            _SG_ERROR(INIT_SHADER_INVALID_STATE);
        }
    }
    
    // TODO: sampler/shader/pipeline/attatchments are the same but buffer/image are different
    fn sg_shader_info sg_query_shader_info(sg_shader smp_id) {
        @debug_assert(sg.valid);
        info := zeroed sg_shader_info;
        smp := _sg_lookup_shader(&sg.pools, smp_id.id) || return(info);
        info.slot.state = smp.slot.state;
        info.slot.res_id = smp.slot.id;
        info
    }
    
    fn query_state(sg: *Self, id: I) sg_resource_state = {
        @debug_assert(sg.valid);
        it := sg.lookup(id) || return(.INVALID);
        it.slot.state
    }
    
    fn uninit(sg: *Self, id: I) void #trace_args = {
        @debug_assert(sg.valid);
        it := sg.lookup(id) || return();
        if ((it.slot.state == SG_RESOURCESTATE_VALID) || (it.slot.state == SG_RESOURCESTATE_FAILED)) {
            sg.impl().discard(it);
            reset_to_alloc_state(shd);
            @debug_assert(shd.slot.state == SG_RESOURCESTATE_ALLOC);
        } else {
            _SG_ERROR(UNINIT_SHADER_INVALID_STATE);
        }
    }
    
    fn sg_fail_shader(sg_shader shd_id) void #trace_args = {
        @debug_assert(sg.valid);
        shd := _sg_lookup_shader(&sg.pools, shd_id.id) || return();
        if (shd.slot.state == SG_RESOURCESTATE_ALLOC) {
            shd.slot.state = SG_RESOURCESTATE_FAILED;
        } else {
            _SG_ERROR(FAIL_SHADER_INVALID_STATE);
        }
    }
    
    // TODO: buffer/image/sampler/shader are the same but pipeline/attatchments is different
    fn _sg_init_shader(_sg_shader_t* shd, const sg_shader_desc* desc) {
        @debug_assert(shd && (shd.slot.state == SG_RESOURCESTATE_ALLOC));
        @debug_assert(desc);
        if (validate(desc)) {
            _sg_shader_common_init(&shd.cmn, desc);
            shd.slot.state = _sg_create_shader(shd, desc);
        } else {
            shd.slot.state = SG_RESOURCESTATE_FAILED;
        }
        @debug_assert((shd.slot.state == SG_RESOURCESTATE_VALID)||(shd.slot.state == SG_RESOURCESTATE_FAILED));
    }
    
    fn destroy(sg: *Self, id: I) void #trace_args(.before) = {
        @debug_assert(sg.valid);
        it := sg.lookup(id) || return();
        if ((it.slot.state == SG_RESOURCESTATE_VALID) || (it.slot.state == SG_RESOURCESTATE_FAILED)) {
            sg.uninit(it);
            @debug_assert(it.slot.state == SG_RESOURCESTATE_ALLOC);
        }
        if (it.slot.state == SG_RESOURCESTATE_ALLOC) {
            sg.dealloc(it);
            @debug_assert(it.slot.state == SG_RESOURCESTATE_INITIAL);
        }
    }
    
    fn reset_to_alloc_state(it: *I.T) void = {
        id := it.slot.id;
        it[] = zeroed I.T;
        it.slot = (id = id, state = .ALLOC);
    }
    
    I
}

fn init_pipeline(sg: *Self, atts: *Pipeline.T, desc: *Pipeline.Desc) void = {
    @debug_assert(pip && (pip.slot.state == SG_RESOURCESTATE_ALLOC));
    @debug_assert(desc);
    if (_sg_validate_pipeline_desc(desc)) {
        _sg_shader_t* shd = _sg_lookup_shader(&sg.pools, desc.shader.id);
        if (shd && (shd.slot.state == SG_RESOURCESTATE_VALID)) {
            _sg_pipeline_common_init(&pip.cmn, desc);
            pip.slot.state = sg.impl().create_pipeline(pip, shd, desc);
        } else {
            pip.slot.state = SG_RESOURCESTATE_FAILED;
        }
    } else {
        pip.slot.state = SG_RESOURCESTATE_FAILED;
    }
    @debug_assert((pip.slot.state == SG_RESOURCESTATE_VALID)||(pip.slot.state == SG_RESOURCESTATE_FAILED));
}

fn init_attachments(sg: *Self, atts: *Attachments.T, desc: *Attachments.Desc) void = {
    @debug_assert(atts && atts.slot.state == SG_RESOURCESTATE_ALLOC);
    @debug_assert(desc);
    if !_sg_validate_attachments_desc(desc)) {
        atts.slot.state = SG_RESOURCESTATE_FAILED;
        return();
    };
    // lookup pass attachment image pointers
    color_images := zeroed Array(*Image.T, SG_MAX_COLOR_ATTACHMENTS);
    resolve_images := zeroed Array(*Image.T, SG_MAX_COLOR_ATTACHMENTS);
    ds_image := zeroed(*Image.T);
    // NOTE: validation already checked that all surfaces are same width/height
    width, height := (0, 0);
    range(0, SG_MAX_COLOR_ATTACHMENTS) { i |
        if (desc.colors[i].image.id) {
            color_images[i] = _sg_lookup_image(&sg.pools, desc.colors[i].image.id);
            if (!(color_images[i] && color_images[i].slot.state == SG_RESOURCESTATE_VALID)) {
                atts.slot.state = SG_RESOURCESTATE_FAILED;
                return();
            }
            const int mip_level = desc.colors[i].mip_level;
            width = _sg_miplevel_dim(color_images[i].cmn.width, mip_level);
            height = _sg_miplevel_dim(color_images[i].cmn.height, mip_level);
        }
        if (desc.resolves[i].image.id) {
            resolve_images[i] = _sg_lookup_image(&sg.pools, desc.resolves[i].image.id);
            if (!(resolve_images[i] && resolve_images[i].slot.state == SG_RESOURCESTATE_VALID)) {
                atts.slot.state = SG_RESOURCESTATE_FAILED;
                return();
            }
        }
    }
    if (desc.depth_stencil.image.id) {
        ds_image = _sg_lookup_image(&sg.pools, desc.depth_stencil.image.id);
        if (!(ds_image && ds_image.slot.state == SG_RESOURCESTATE_VALID)) {
            atts.slot.state = SG_RESOURCESTATE_FAILED;
            return();
        }
        mip_level := desc.depth_stencil.mip_level;
        width = _sg_miplevel_dim(ds_image.cmn.width, mip_level);
        height = _sg_miplevel_dim(ds_image.cmn.height, mip_level);
    }
    _sg_attachments_common_init(&atts.cmn, desc, width, height);
    atts.slot.state = sg.impl().create_attachments(atts, color_images, resolve_images, ds_image, desc);
    @debug_assert((atts.slot.state == SG_RESOURCESTATE_VALID)||(atts.slot.state == SG_RESOURCESTATE_FAILED));
}

fn _sg_notify_commit_listeners(sg: *Self) void = {
    @debug_assert(sg.commit_listeners.items);
    for (int i = 0; i < sg.commit_listeners.upper; i++) {
        const sg_commit_listener* listener = &sg.commit_listeners.items[i];
        if (listener.func) {
            listener.func(listener.user_data);
        }
    }
}

fn _sg_add_commit_listener(sg: *Self, new_listener: *sg_commit_listener) bool = {
    @debug_assert(new_listener && new_listener.func);
    @debug_assert(sg.commit_listeners.items);
    // first check if the listener hadn't been added already
    for (int i = 0; i < sg.commit_listeners.upper; i++) {
        const sg_commit_listener* slot = &sg.commit_listeners.items[i];
        if ((slot.func == new_listener.func) && (slot.user_data == new_listener.user_data)) {
            _SG_ERROR(IDENTICAL_COMMIT_LISTENER);
            return false;
        }
    }
    // first try to plug a hole
    sg_commit_listener* slot = 0;
    for (int i = 0; i < sg.commit_listeners.upper; i++) {
        if (sg.commit_listeners.items[i].func == 0) {
            slot = &sg.commit_listeners.items[i];
            break();
        }
    }
    if (!slot) {
        // append to end
        if (sg.commit_listeners.upper < sg.commit_listeners.num) {
            slot = &sg.commit_listeners.items[sg.commit_listeners.upper++];
        }
    }
    if (!slot) {
        _SG_ERROR(COMMIT_LISTENER_ARRAY_FULL);
        return false;
    }
    *slot = *new_listener;
    return true;
}

fn remove_commit_listener(sg: *Self, listener: *sg_commit_listener) bool = {
    @debug_assert(sg.valid && listener && listener.func);
    @debug_assert(sg.commit_listeners.items);
    for (int i = 0; i < sg.commit_listeners.upper; i++) {
        sg_commit_listener* slot = &sg.commit_listeners.items[i];
        // both the function pointer and user data must match!
        if ((slot.func == listener.func) && (slot.user_data == listener.user_data)) {
            slot.func = 0;
            slot.user_data = 0;
            // NOTE: since _sg_add_commit_listener() already catches duplicates,
            // we don't need to worry about them here
            return true;
        }
    }
    return false;
}

fn setup_compute(sg: *Self, desc: *SgDesc) void = {
    @debug_assert(desc && (desc.max_dispatch_calls_per_pass > 0));
    max_tracked_sbufs := desc.max_dispatch_calls_per_pass * SG_MAX_STORAGEBUFFER_BINDSLOTS;
    sg.compute.readwrite_sbufs = init(sg.allocator, max_tracked_sbufs);
}

fn discard_compute(sg: *Self) void = {
    sg.compute.readwrite_sbufs&.drop(sg.allocator);
}

fn compute_pass_track_storage_buffer(sg: *Self, sbuf: *Buffer.T, readonly: bool) void = {
    if !readonly {
        try_push(sg.compute.readwrite_sbufs&, sbuf.slot.id);
    }
}

fn compute_on_endpass(sg: *Self) void = {
    @debug_assert(sg.cur_pass.in_pass && sg.cur_pass.is_compute);
    sg.compute.readwrite_sbufs.len = 0;
}

fn desc_defaults(desc: *SgDesc) SgDesc = {
    /*
        NOTE: on WebGPU, the default color pixel format MUST be provided,
        cannot be a default compile-time constant.
    */
    sg_desc res = *desc;
    ASSERT_METAL();
    res.environment.defaults.color_format = _sg_def(res.environment.defaults.color_format, SG_PIXELFORMAT_BGRA8);
    res.environment.defaults.depth_format = _sg_def(res.environment.defaults.depth_format, SG_PIXELFORMAT_DEPTH_STENCIL);
    res.environment.defaults.sample_count = _sg_def(res.environment.defaults.sample_count, 1);
    res.buffer_pool_size = _sg_def(res.buffer_pool_size, _SG_DEFAULT_BUFFER_POOL_SIZE);
    res.image_pool_size = _sg_def(res.image_pool_size, _SG_DEFAULT_IMAGE_POOL_SIZE);
    res.sampler_pool_size = _sg_def(res.sampler_pool_size, _SG_DEFAULT_SAMPLER_POOL_SIZE);
    res.shader_pool_size = _sg_def(res.shader_pool_size, _SG_DEFAULT_SHADER_POOL_SIZE);
    res.pipeline_pool_size = _sg_def(res.pipeline_pool_size, _SG_DEFAULT_PIPELINE_POOL_SIZE);
    res.attachments_pool_size = _sg_def(res.attachments_pool_size, _SG_DEFAULT_ATTACHMENTS_POOL_SIZE);
    res.uniform_buffer_size = _sg_def(res.uniform_buffer_size, _SG_DEFAULT_UB_SIZE);
    res.max_dispatch_calls_per_pass = _sg_def(res.max_dispatch_calls_per_pass, _SG_DEFAULT_MAX_DISPATCH_CALLS_PER_PASS);
    res.max_commit_listeners = _sg_def(res.max_commit_listeners, _SG_DEFAULT_MAX_COMMIT_LISTENERS);
    res.wgpu_bindgroups_cache_size = _sg_def(res.wgpu_bindgroups_cache_size, _SG_DEFAULT_WGPU_BINDGROUP_CACHE_SIZE);
    return res;
}

fn pass_defaults(pass: *sg_pass) sg_pass = {
    sg_pass res = *pass;
    // ... this sure isn't a program?
    if (!res.compute) {
        if (res.compute && res.attachments.id == SG_INVALID_ID) {
            // this is a swapchain-pass
            res.swapchain.sample_count = _sg_def(res.swapchain.sample_count, sg.desc.environment.defaults.sample_count);
            res.swapchain.color_format = _sg_def(res.swapchain.color_format, sg.desc.environment.defaults.color_format);
            res.swapchain.depth_format = _sg_def(res.swapchain.depth_format, sg.desc.environment.defaults.depth_format);
        }
        res.action = _sg_pass_action_defaults(&res.action);
    }
    res
}

fn setup(sg: *Self, desc: SgDesc) void = {
    @debug_assert(desc);
    @debug_assert((desc._start_canary == 0) && (desc._end_canary == 0));
    @debug_assert((desc.allocator.alloc_fn && desc.allocator.free_fn) || (!desc.allocator.alloc_fn && !desc.allocator.free_fn));
    _SG_CLEAR_ARC_STRUCT(_sg_state_t, _sg);
    sg.desc = _sg_desc_defaults(desc); desc := ();
    _sg_setup_pools(&sg.pools, &sg.desc);
    _sg_setup_compute(&sg.desc);
    @debug_assert(sg.desc.max_commit_listeners > 0);
    sg.commit_listeners = init(sg.allocator, sg.desc.max_commit_listeners);
    sg.frame_index = 1;
    sg.stats_enabled = true;
    sg.impl().setup_backend(&sg.desc);
    sg.valid = true;
}

fn shutdown(sg: *Self) void = {
    sg.discard_all_resources();
    sg.impl().discard_backend();
    sg.commit_listeners&.drop(sg.allocator);
    sg.discard_compute();
    sg.discard_pools();
    sg[] = zereod Self;
}

fn query_pixelformat(sg: *Self, fmt: SgPixelFormat) SgPixelFormatInfo = {
    @debug_assert(sg.valid);
    c := is_compressed(fmt);
    (
        flags = sg.formats[@as(i64) fmt], 
        compressed = c, 
        bytes_per_pixel = @if(c, 0, pixelformat_bytesize(fmt)),
    )
}

fn sg_query_row_pitch(fmt: SgPixelFormat, width: i32, row_align_bytes: i32) i32 = {
    @debug_assert(sg.valid);
    @debug_assert(width > 0);
    @debug_assert((row_align_bytes > 0) && _sg_ispow2(row_align_bytes));
    @debug_assert(((int)fmt > SG_PIXELFORMAT_NONE) && ((int)fmt < _SG_PIXELFORMAT_NUM));
    row_pitch(fmt, width, row_align_bytes)
}

fn sg_query_surface_pitch(fmt: SgPixelFormat, width: i32, height i32, row_align_bytes: i32) i32 = {
    @debug_assert(sg.valid);
    @debug_assert((width > 0) && (height > 0));
    @debug_assert((row_align_bytes > 0) && _sg_ispow2(row_align_bytes));
    @debug_assert(((int)fmt > SG_PIXELFORMAT_NONE) && ((int)fmt < _SG_PIXELFORMAT_NUM));
    _sg_surface_pitch(fmt, width, height, row_align_bytes)
}

fn install_trace_hooks(sg: *Self, trace_hooks: *sg_trace_hooks) sg_trace_hooks = {
    @debug_assert(sg.valid);
    @debug_assert(trace_hooks);
    sg_trace_hooks old_hooks = sg.hooks;
    if SOKOL_TRACE_HOOKS {
        sg.hooks = trace_hooks[];
    } else {
        _SG_WARN(TRACE_HOOKS_NOT_ENABLED);
    }
    old_hooks
}

fn sg_begin_pass(sg: *Self, pass: *sg_pass) void #trace_args(.tail) = {
    @debug_assert(sg.valid);
    cur := sg.cur_pass&;
    @debug_assert(!cur.valid);
    @debug_assert(!cur.in_pass);
    @debug_assert(pass);
    @debug_assert((pass._start_canary == 0) && (pass._end_canary == 0));
    const sg_pass pass_def = _sg_pass_defaults(pass);
    if (!_sg_validate_begin_pass(&pass_def)) {
        return();
    }
    if (!pass_def.compute) {
        if (pass_def.attachments.id != SG_INVALID_ID) {
            // an offscreen pass
            @debug_assert(cur.atts == 0);
            cur.atts = _sg_lookup_attachments(&sg.pools, pass_def.attachments.id);
            if (0 == cur.atts) {
                _SG_ERROR(BEGINPASS_ATTACHMENT_INVALID);
                return();
            }
            cur.atts_id = pass_def.attachments;
            cur.width = cur.atts.cmn.width;
            cur.height = cur.atts.cmn.height;
        } else {
            // a swapchain pass
            sc := pass_def.swapchain&;
            @debug_assert(sc.width > 0);
            @debug_assert(sc.height > 0);
            @debug_assert(sc.color_format > SG_PIXELFORMAT_NONE);
            @debug_assert(sc.sample_count > 0);
            cur.width = sc.swapchain.width;
            cur.height = sc.height;
            cur.swapchain.color_fmt = sc.color_format;
            cur.swapchain.depth_fmt = sc.depth_format;
            cur.swapchain.sample_count = sc.sample_count;
        }
    }
    cur.valid = true;  // may be overruled by backend begin-pass functions
    cur.in_pass = true;
    cur.is_compute = pass_def.compute;
    sg.impl().begin_pass(&pass_def);
}

fn apply_viewport(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) void #trace_args = {
    @debug_assert(sg.valid);
    if(!sg.validate_apply_viewport(x, y, width, height, origin_top_left), => return());
    _sg_stats_add(num_apply_viewport, 1);
    if(!sg.cur_pass.valid, => return());
    sg.impl().apply_viewport(x, y, width, height, origin_top_left);
}

fn apply_viewportf(sg: *Self, x: f32, y: f32, width: f32, height: f32, origin_top_left: bool) void = {
    sg.apply_viewport((int)x, (int)y, (int)width, (int)height, origin_top_left);
}

fn apply_scissor_rect(sg: *Self, x: i32, y: i32, width: i32, height: i32, origin_top_left: bool) void #trace_args(.tail) = {
    @debug_assert(sg.valid);
    if(!_sg_validate_apply_scissor_rect(x, y, width, height, origin_top_left), => return());
    _sg_stats_add(num_apply_scissor_rect, 1);
    if(!sg.cur_pass.valid, => return());
    sg.impl().apply_scissor_rect(x, y, width, height, origin_top_left);
}

fn apply_scissor_rectf(sg: *Self, x: f32, y: f32, width: f32, height: f32, origin_top_left: bool) void = {
    sg.apply_scissor_rect((int)x, (int)y, (int)width, (int)height, origin_top_left);
}

fn sg_apply_pipeline(sg: *Self, pip_id: Pipeline) void #trace(.tail) = {
    @debug_assert(sg.valid);
    _sg_stats_add(num_apply_pipeline, 1);
    if (!_sg_validate_apply_pipeline(pip_id)) {
        sg.next_draw_valid = false;
        return();
    }
    if(!sg.cur_pass.valid, => return());
    sg.cur_pipeline = pip_id;
    _sg_pipeline_t* pip = _sg_lookup_pipeline(&sg.pools, pip_id.id);
    @debug_assert(pip);

    sg.next_draw_valid = (SG_RESOURCESTATE_VALID == pip.slot.state);
    if(!sg.next_draw_valid, => return());

    @debug_assert(pip.shader && (pip.shader.slot.id == pip.cmn.shader_id.id));
    sg.impl().apply_pipeline(pip);

    // set the expected bindings and uniform block flags
    sg.required_bindings_and_uniforms = pip.cmn.required_bindings_and_uniforms | pip.shader.cmn.required_bindings_and_uniforms;
    sg.applied_bindings_and_uniforms = 0;
}

fn apply_bindings(sg: *Self, bindings: *sg_bindings) void = {
    @debug_assert(sg.valid);
    @debug_assert(bindings);
    @debug_assert((bindings._start_canary == 0) && (bindings._end_canary==0));
    _sg_stats_add(num_apply_bindings, 1);
    sg.applied_bindings_and_uniforms |= (1 << SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    if (!_sg_validate_apply_bindings(bindings)) {
        sg.next_draw_valid = false;
        return();
    }
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());

    bnd := zeroed _sg_bindings_t;
    bnd.pip = sg.lookup(sg.cur_pipeline.id)  || {
        sg.next_draw_valid = false;
        return()
    };
    @debug_assert(bnd.pip.shader && (bnd.pip.cmn.shader_id.id == bnd.pip.shader.slot.id));
    shd := bnd.pip.shader;

    if (!sg.cur_pass.is_compute) {
        for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
            if (bnd.pip.cmn.vertex_buffer_layout_active[i]) {
                @debug_assert(bindings.vertex_buffers[i].id != SG_INVALID_ID);
                bnd.vbs[i] = _sg_lookup_buffer(&sg.pools, bindings.vertex_buffers[i].id);
                bnd.vb_offsets[i] = bindings.vertex_buffer_offsets[i];
                if (bnd.vbs[i]) {
                    sg.next_draw_valid &= (SG_RESOURCESTATE_VALID == bnd.vbs[i].slot.state);
                    sg.next_draw_valid &= !bnd.vbs[i].cmn.append_overflow;
                } else {
                    sg.next_draw_valid = false;
                }
            }
        }
        if (bindings.index_buffer.id) {
            bnd.ib = _sg_lookup_buffer(&sg.pools, bindings.index_buffer.id);
            bnd.ib_offset = bindings.index_buffer_offset;
            if (bnd.ib) {
                sg.next_draw_valid &= (SG_RESOURCESTATE_VALID == bnd.ib.slot.state);
                sg.next_draw_valid &= !bnd.ib.cmn.append_overflow;
            } else {
                sg.next_draw_valid = false;
            }
        }
    }

    for (int i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage != SG_SHADERSTAGE_NONE) {
            @debug_assert(bindings.images[i].id != SG_INVALID_ID);
            bnd.imgs[i] = _sg_lookup_image(&sg.pools, bindings.images[i].id);
            if (bnd.imgs[i]) {
                sg.next_draw_valid &= (SG_RESOURCESTATE_VALID == bnd.imgs[i].slot.state);
            } else {
                sg.next_draw_valid = false;
            }
        }
    }

    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage != SG_SHADERSTAGE_NONE) {
            @debug_assert(bindings.samplers[i].id != SG_INVALID_ID);
            bnd.smps[i] = _sg_lookup_sampler(&sg.pools, bindings.samplers[i].id);
            if (bnd.smps[i]) {
                sg.next_draw_valid &= (SG_RESOURCESTATE_VALID == bnd.smps[i].slot.state);
            } else {
                sg.next_draw_valid = false;
            }
        }
    }

    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage != SG_SHADERSTAGE_NONE) {
            @debug_assert(bindings.storage_buffers[i].id != SG_INVALID_ID);
            bnd.sbufs[i] = _sg_lookup_buffer(&sg.pools, bindings.storage_buffers[i].id);
            if (bnd.sbufs[i]) {
                sg.next_draw_valid &= (SG_RESOURCESTATE_VALID == bnd.sbufs[i].slot.state);
                if (sg.cur_pass.is_compute) {
                    sg.compute_pass_track_storage_buffer(bnd.sbufs[i], shd.cmn.storage_buffers[i].readonly);
                }
            } else {
                sg.next_draw_valid = false;
            }
        }
    }

    if (sg.next_draw_valid) {
        sg.next_draw_valid &= sg.impl().apply_bindings(&bnd);
        _SG_TRACE_ARGS(apply_bindings, bindings);  // TODO
    }
}

fn apply_uniforms(sg: *Self, ub_slot: i32, data: []u8) void #trace(.tail) = {
    @debug_assert(sg.valid);
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert(data && data.ptr && (data.size > 0));
    _sg_stats_add(num_apply_uniforms, 1);
    _sg_stats_add(size_apply_uniforms, (uint32_t)data.size);
    sg.applied_bindings_and_uniforms |= 1 << ub_slot;
    if (!_sg_validate_apply_uniforms(ub_slot, data)) {
        sg.next_draw_valid = false;
        return();
    }
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());
    sg.impl().apply_uniforms(ub_slot, data);
}

fn draw(sg: *Self, base_element: i32, num_elements: i32, num_instances: i32) void #trace(.tail) = {
    @debug_assert(sg.valid);
    if(!_sg_validate_draw(base_element, num_elements, num_instances), => return());
    _sg_stats_add(num_draw, 1);
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return()) {
    // skip no-op draws
    if((0 == num_elements) || (0 == num_instances, => return()) {
    sg.impl().draw(base_element, num_elements, num_instances);
}

fn dispatch(sg: *Self, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void #trace(.tail) = {
    @debug_assert(sg.valid);
    if(!_sg_validate_dispatch(num_groups_x, num_groups_y, num_groups_z), => return());
    _sg_stats_add(num_dispatch, 1);
    if(!sg.cur_pass.valid || !sg.next_draw_valid, => return());
    // skip no-op dispatches
    if((0 == num_groups_x) || (0 == num_groups_y) || (0 == num_groups_z), => return());
    sg.impl().dispatch(num_groups_x, num_groups_y, num_groups_z);
}

fn end_pass(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    @debug_assert(sg.cur_pass.in_pass);
    _sg_stats_add(num_passes, 1);
    // NOTE: don't exit early if !sg.cur_pass.valid
    sg.impl().end_pass();
    sg.cur_pipeline.id = SG_INVALID_ID;
    if (sg.cur_pass.is_compute) {
        sg.compute_on_endpass();
    }
    sg.cur_pass = zeroed @type sg.cur_pass;
}

fn commit(sg: *Self) void = {
    @debug_assert(sg.valid);
    @debug_assert(!sg.cur_pass.valid);
    @debug_assert(!sg.cur_pass.in_pass);
    sg.impl().commit();
    sg.stats.frame_index = sg.frame_index;
    sg.prev_stats = sg.stats;
    _sg_clear(&sg.stats, sizeof(sg.stats));
    _sg_notify_commit_listeners();
    _SG_TRACE_NOARGS(commit);
    sg.frame_index += 1;
}

fn reset_state_cache(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    sg.impl().reset_state_cache();
}

fn update_buffer(sg: *Self, buf_id: Buffer, data: []u8) void #trace = {
    @debug_assert(sg.valid);
    @debug_assert(data && data.ptr && (data.size > 0));
    _sg_stats_add(num_update_buffer, 1);
    _sg_stats_add(size_update_buffer, (uint32_t)data.size);
    buf := sg.lookup(buf_id);
    if ((data.size > 0) && buf && (buf.slot.state == SG_RESOURCESTATE_VALID)) {
        if (_sg_validate_update_buffer(buf, data)) {
            @debug_assert(data.size <= (size_t)buf.cmn.size);
            // only one update allowed per buffer and frame
            @debug_assert(buf.cmn.update_frame_index != sg.frame_index);
            // update and append on same buffer in same frame not allowed
            @debug_assert(buf.cmn.append_frame_index != sg.frame_index);
            sg.impl().update_buffer(buf, data);
            buf.cmn.update_frame_index = sg.frame_index;
        }
    }
}

fn append_buffer(sg: *Self, buf_id: Buffer, data: []u8) i64 #trace = {
    @debug_assert(sg.valid);
    @debug_assert(data && data.ptr);
    _sg_stats_add(num_append_buffer, 1);
    _sg_stats_add(size_append_buffer, (uint32_t)data.size);
    buf := _sg_lookup_buffer(&sg.pools, buf_id.id) || return(0);  // FIXME: should we return -1 here?
    // rewind append cursor in a new frame
    if (buf.cmn.append_frame_index != sg.frame_index) {
        buf.cmn.append_pos = 0;
        buf.cmn.append_overflow = false;
    }
    if (((size_t)buf.cmn.append_pos + data.size) > (size_t)buf.cmn.size) {
        buf.cmn.append_overflow = true;
    }
    const int start_pos = buf.cmn.append_pos;
    // NOTE: the multiple-of-4 requirement for the buffer offset is coming
    // from WebGPU, but we want identical behaviour between backends
    @debug_assert(_sg_multiple_u64((uint64_t)start_pos, 4));
    if (buf.slot.state == SG_RESOURCESTATE_VALID) {
        if (_sg_validate_append_buffer(buf, data)) {
            if (!buf.cmn.append_overflow && (data.size > 0)) {
                // update and append on same buffer in same frame not allowed
                @debug_assert(buf.cmn.update_frame_index != sg.frame_index);
                sg.impl().append_buffer(buf, data, buf.cmn.append_frame_index != sg.frame_index);
                buf.cmn.append_pos += (int) _sg_roundup_u64(data.size, 4);
                buf.cmn.append_frame_index = sg.frame_index;
            }
        }
    }
    start_pos
}

fn query_buffer_overflow(sg: *Self, buf_id: Buffer) bool = 
    sg.query(buf_id, fn(it) => it.cmn.append_overflow, false);

fn query_buffer_will_overflow(sg: *Self, buf_id: Buffer, size: i64) bool = {
    @debug_assert(sg.valid);
    buf := _sg_lookup_buffer(&sg.pools, buf_id.id) || return(false);
    bool result = false;
    int append_pos = buf.cmn.append_pos;
    // rewind append cursor in a new frame
    if (buf.cmn.append_frame_index != sg.frame_index) {
        append_pos = 0;
    }
    append_pos + _sg_roundup((int)size, 4)) > buf.cmn.size
}

fn update_image(sg: *Self, img_id: Image, data: *sg_image_data) void #trace = {
    @debug_assert(sg.valid);
    _sg_stats_add(num_update_image, 1);
    range(0, SG_CUBEFACE_NUM) { face_index |
        range(0, SG_MAX_MIPMAPS) { mip_index |
            if (data.subimage[face_index][mip_index].size == 0) {
                break();
            }
            _sg_stats_add(size_update_image, (uint32_t)data.subimage[face_index][mip_index].size);
        }
    }
    img := _sg_lookup_image(&sg.pools, img_id.id) || return();
    if img.slot.state == SG_RESOURCESTATE_VALID {
        if (validate(img, data)) {
            @debug_assert(img.cmn.upd_frame_index != sg.frame_index);
            sg.impl().update_image(img, data);
            img.cmn.upd_frame_index = sg.frame_index;
        }
    }
}

fn push_debug_group(sg: *Self, name: CStr) void #trace = {
    @debug_assert(sg.valid);
    @debug_assert(name);
    sg.impl().push_debug_group(name);
}

fn pop_debug_group(sg: *Self) void #trace = {
    @debug_assert(sg.valid);
    sg.impl().pop_debug_group();
}

// just use the field for:
// sg_enable_frame_stats, sg_disable_frame_stats, sg_frame_stats_enabled, sg_query_frame_stats
// sg_isvalid, sg_query_desc, sg_query_backend, sg_query_features, sg_query_limits

fn query_buffer_info(sg: *Self, buf_id: Buffer) sg_buffer_info = {
    @debug_assert(sg.valid);
    sg_buffer_info info;
    _sg_clear(&info, sizeof(info));
    const *Buffer.T buf = _sg_lookup_buffer(&sg.pools, buf_id.id);
    if(buf.is_null(), => return(info));
    info.slot.state = buf.slot.state;
    info.slot.res_id = buf.slot.id;
    info.update_frame_index = buf.cmn.update_frame_index;
    info.append_frame_index = buf.cmn.append_frame_index;
    info.append_pos = buf.cmn.append_pos;
    info.append_overflow = buf.cmn.append_overflow;
    ASSERT_METAL();
    info.num_slots = buf.cmn.num_slots;
    info.active_slot = buf.cmn.active_slot;
    info
}

fn query_image_info(sg: *Self, img_id: Image) sg_image_info = {
    @debug_assert(sg.valid);
    info := zeroed sg_image_info;
    img := _sg_lookup_image(&sg.pools, img_id.id) || return(info);
    info.slot.state = img.slot.state;
    info.slot.res_id = img.slot.id;
    info.upd_frame_index = img.cmn.upd_frame_index;
    ASSERT_METAL();
    info.num_slots = img.cmn.num_slots;
    info.active_slot = img.cmn.active_slot;
    info
}

fn query_buffer_desc(sg: *Self, buf_id: Buffer) Buffer.Desc = {
    @debug_assert(sg.valid);
    d := zeroed sg_buffer_desc;
    buf := _sg_lookup_buffer(&sg.pools, buf_id.id) || return(d);
    d.size = (size_t)buf.cmn.size;
    d.type = buf.cmn.type;
    d.usage = buf.cmn.usage;
    d
}

fn query_buffer_size(sg: *Self, buf_id: Buffer) i64 = 
    sg.query(buf_id, fn(it) => it.cmn.size, 0);

fn query_buffer_type(sg: *Self, buf_id: Buffer) SgBufferType = 
    sg.query(buf_id, fn(it) => it.cmn.type, _SG_BUFFERTYPE_DEFAULT);

fn query_buffer_usage(sg: *Self, buf_id: Buffer) SgUsage = 
    sg.query(buf_id, fn(it) => it.cmn.usage, _SG_USAGE_DEFAULT);

fn query_image_desc(sg: *Self, img_id: Image) Image.Desc = {
    @debug_assert(sg.valid);
    d := zeroed sg_image_desc;
    img := _sg_lookup_image(&sg.pools, img_id.id) || return(d);
    d.type = img.cmn.type;
    d.render_target = img.cmn.render_target;
    d.width = img.cmn.width;
    d.height = img.cmn.height;
    d.num_slices = img.cmn.num_slices;
    d.num_mipmaps = img.cmn.num_mipmaps;
    d.usage = img.cmn.usage;
    d.pixel_format = img.cmn.pixel_format;
    d.sample_count = img.cmn.sample_count;
    d
}

fn query(sg: *Self, id: ~Handle, $body: @Fn(it: *Handle.T) Out, default: ~Out) Out #where = {
    @debug_assert(sg.valid);
    it := sg.lookup(id) || return(default);
    body(it)
}

fn query_image_type(sg: *Self, img_id: Image) SgImageType = 
    sg.query(img_id, fn(it) => it.cmn.type, .DEFAULT);

fn query_image_width(sg: *Self, img_id: Image) i32 = 
    sg.query(img_id, fn(it) => it.cmn.width, 0);

fn query_image_height(sg: *Self, img_id: Image) i32 = 
    sg.query(img_id, fn(it) => it.cmn.height, 0);

fn query_image_num_slices(sg: *Self, img_id: Image) i32 = 
    sg.query(img_id, fn(it) => it.cmn.num_slices, 0);

fn sg_query_image_num_mipmaps(sg: *Self, img_id: Image) i32 = 
    sg.query(img_id, fn(it) => it.cmn.num_mipmaps, 0);

fn query_image_pixelformat(sg: *Self, img_id: Image) SgPixelFormat = 
    sg.query(img_id, fn(it) => it.cmn.pixel_format, .DEFAULT);

fn query_image_usage(sg: *Self, img_id: Image) SgUsage = 
    sg.query(img_id, fn(it) => it.cmn.usage, .DEFAULT);

fn query_image_sample_count(sg: *Self, img_id: Image) i32 = 
    sg.query(img_id, fn(it) => it.cmn.sample_count, 0);

fn query_sampler_desc(sg: *Self, smp_id: Sampler) Sampler.Desc = {
    @debug_assert(sg.valid);
    smp := _sg_lookup_sampler(&sg.pools, smp_id.id) || return(zeroed(@type smp.cmn));
    smp.cmn
}

fn query_shader_desc(sg: *Self, shd_id: Shader) Shader.Desc = {
    @debug_assert(sg.valid);
    desc := zeroed sg_shader_desc;
    shd := _sg_lookup_shader(&sg.pools, shd_id.id) || return(desc);
    for (size_t ub_idx = 0; ub_idx < SG_MAX_UNIFORMBLOCK_BINDSLOTS; ub_idx++) {
        sg_shader_uniform_block* ub_desc = &desc.uniform_blocks[ub_idx];
        const _sg_shader_uniform_block_t* ub = &shd.cmn.uniform_blocks[ub_idx];
        ub_desc.stage = ub.stage;
        ub_desc.size = ub.size;
    }
    for (size_t sbuf_idx = 0; sbuf_idx < SG_MAX_STORAGEBUFFER_BINDSLOTS; sbuf_idx++) {
        sg_shader_storage_buffer* sbuf_desc = &desc.storage_buffers[sbuf_idx];
        const _sg_shader_storage_buffer_t* sbuf = &shd.cmn.storage_buffers[sbuf_idx];
        sbuf_desc.stage = sbuf.stage;
        sbuf_desc.readonly = sbuf.readonly;
    }
    for (size_t img_idx = 0; img_idx < SG_MAX_IMAGE_BINDSLOTS; img_idx++) {
        sg_shader_image* img_desc = &desc.images[img_idx];
        const _sg_shader_image_t* img = &shd.cmn.images[img_idx];
        img_desc.stage = img.stage;
        img_desc.image_type = img.image_type;
        img_desc.sample_type = img.sample_type;
        img_desc.multisampled = img.multisampled;
    }
    for (size_t smp_idx = 0; smp_idx < SG_MAX_SAMPLER_BINDSLOTS; smp_idx++) {
        sg_shader_sampler* smp_desc = &desc.samplers[smp_idx];
        const _sg_shader_sampler_t* smp = &shd.cmn.samplers[smp_idx];
        smp_desc.stage = smp.stage;
        smp_desc.sampler_type = smp.sampler_type;
    }
    for (size_t img_smp_idx = 0; img_smp_idx < SG_MAX_IMAGE_SAMPLER_PAIRS; img_smp_idx++) {
        sg_shader_image_sampler_pair* img_smp_desc = &desc.image_sampler_pairs[img_smp_idx];
        const _sg_shader_image_sampler_t* img_smp = &shd.cmn.image_samplers[img_smp_idx];
        img_smp_desc.stage = img_smp.stage;
        img_smp_desc.image_slot = img_smp.image_slot;
        img_smp_desc.sampler_slot = img_smp.sampler_slot;
    }
    desc
}

fn query_pipeline_desc(sg: *Self, pip_id: Pipeline) Pipeline.Desc = {
    @debug_assert(sg.valid);
    pip := _sg_lookup_pipeline(&sg.pools, pip_id.id) || return(zeroed SgPipelineDesc);
    pip.cmn.common
}

fn query_attachments_desc(sg: *Self, atts_id: Attachments) Attachments.Desc = {
    @debug_assert(sg.valid);
    desc := zeroed sg_attachments_desc;
    atts := _sg_lookup_attachments(&sg.pools, atts_id.id) || return(desc);
    range(0, atts.cmn.num_colors) { i |
        desc.colors&[i] = atts.cmn.colors&[i];
    }
    desc.depth_stencil.image = atts.cmn.depth_stencil.image_id;
    desc.depth_stencil.mip_level = atts.cmn.depth_stencil.mip_level;
    desc.depth_stencil.slice = atts.cmn.depth_stencil.slice;
}
