// WebGPU 3D API wrapper 
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.

WGPU :: import("@/graphics/web/webgpu.fr");

ROWPITCH_ALIGN :: (256);
MAX_UNIFORM_UPDATE_SIZE :: 1.shift_left(16); // also see WGPU.Limits.maxUniformBufferBindingSize
NUM_BINDGROUPS :: (2); // 0: uniforms, 1: images, samplers, storage buffers
UB_BINDGROUP_INDEX :: (0);
IMG_SMP_SBUF_BINDGROUP_INDEX :: (1);
MAX_UB_BINDGROUP_ENTRIES :: (Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
MAX_UB_BINDGROUP_BIND_SLOTS :: (2 * Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES :: (Sg.SG_MAX_IMAGE_BINDSLOTS + Sg.SG_MAX_SAMPLER_BINDSLOTS + Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS);
MAX_IMG_SMP_SBUF_BIND_SLOTS :: (128);

// TODO: maybe this should live in webgpu.fr for people who want to use that directly 
// generate nicer overloads of the imported wgpu methods 
// `wgpuFooBarBaz :: fn(foo: Foo);` -> `fn barBaz(foo: Foo);`
// This relies on TypeMeta.inferred_name + typename() working properly. 
:: @if(@run(SOKOL_BACKEND == .WGPU)) {
    // TODO: probably just add the source directly (add_file) instead of writing it like this
    // TODO: why doesn't this work? Poison expression InProgressMacro on PipelineDesc :compilerbug
    //@run import("@/graphics/web/webgpu_api.fr")'main();
    
    #use("@/compiler/ast_external.fr");
    //WGPU :: import("@/graphics/web/webgpu.fr");
    // TODO: get_constants doesn't see #reexport
    WGPU :: import("@/target/franca/webgpu.g.fr");
    c := current_compiler_context();
    skips := 0;
    for WGPU.get_constants() { n |
        name := n.str();
        if name.starts_with("wgpu") {
            name = name.rest(4);
            fid := get_constant(FuncId, WGPU, n).unwrap();
            func := get_function_ast(fid, true, false, true, false);
            arg_types := c.arg_types(func.finished_arg.unwrap());
            first_arg_name := arg_types[0].typename().str();
            if name.starts_with(first_arg_name) {
                name = name.rest(first_arg_name.len);
                name := name.shallow_copy(ast_alloc()); // DO NOT MUTATE THE COMPILER'S STRINGS
                name[0] += 32;  // to_lower
                os := get_or_create_overloads(name.sym(), TOP_LEVEL_SCOPE, func.loc);
                add_to_overload_set(os, fid);
            } else {
                skips += 1;
            }
        };
    };
    //@assert_eq(skips, 5, "new fuctions added to @/graphics/web/webgpu.fr ?")
};  // TODO: don't silently not run if you forget this semicolon

Buffer :: @struct {
    buf: WGPU.Buffer;
};

Image :: @struct {
    tex: WGPU.Texture;
    view: WGPU.TextureView;
};

Sampler :: @struct {
    smp: WGPU.Sampler;
};

shader_func_t :: @struct {
    module: WGPU.ShaderModule;
    entry: []u8;  // :LEAK
};

Shader :: @struct {
    vertex_func: shader_func_t;
    fragment_func: shader_func_t;
    compute_func: shader_func_t;
    bgl_ub: WGPU.BindGroupLayout;
    bg_ub: WGPU.BindGroup;
    bgl_img_smp_sbuf: WGPU.BindGroupLayout;
    // a mapping of sokol-gfx bind slots to setBindGroup dynamic-offset-array indices
    ub_num_dynoffsets: u8;
    ub_dynoffsets: Array(u8, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    // indexed by sokol-gfx bind slot:
    ub_grp0_bnd_n: Array(u8, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    img_grp1_bnd_n: Array(u8, Sg.SG_MAX_IMAGE_BINDSLOTS);
    smp_grp1_bnd_n: Array(u8, Sg.SG_MAX_SAMPLER_BINDSLOTS);
    sbuf_grp1_bnd_n: Array(u8, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Pipeline :: @struct {
    rpip: WGPU.RenderPipeline;
    cpip: WGPU.ComputePipeline;
    blend_color: WGPU.Color;
};

Attachments :: Sg'AttachmentsImpl(Impl, @struct {
    image: *Sg.Image.T;
    view: WGPU.TextureView;
});

// a pool of per-frame uniform buffers
uniform_buffer_t :: @struct {
    offset: i64;    // current offset into buf
    staging: []u8;   // intermediate buffer for uniform data updates (same size as gpu buffer)
    buf: WGPU.Buffer;     // the GPU-side uniform buffer
    bind_offsets: Array(u32, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);   // NOTE: index is sokol-gfx ub slot index!
};

bindgroup_handle_t :: @struct(id: u32);

bindgroups_cache_item_type_t :: @enum(u32) 
    (None = 0, Image = 0x00001111, Sampler = 0x00002222, StorageBuffer = 0x00003333, Pipeline = 0x00004444);

BINDGROUPSCACHEKEY_NUM_ITEMS :: (1 + MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);

bindgroups_cache_key_t :: @struct {
    hash: u64;
    // the format of cache key items is BBBBTTTTIIIIIIII
    // where
    //  - BBBB is 2x the WGPU. binding
    //  - TTTT is the bindgroups_cache_item_type_t
    //  - IIIIIIII is the resource id
    //
    // where the item type is a per-resource-type bit pattern
    items: Array(u64, BINDGROUPSCACHEKEY_NUM_ITEMS);
};

bindgroups_cache_t :: @struct {
    index_mask: i64;    // mask to turn hash into valid index
    items: []bindgroup_handle_t;  // len must be 2^n
};

bindgroup_t :: @struct {
    slot: Sg.SlotHeader;
    bindgroup: WGPU.BindGroup;
    key: bindgroups_cache_key_t;
};

bindings_cache_t :: @struct {
    ib: @struct {
        buffer: Sg.Buffer;
        offset: i64;
    };
    bg: bindgroup_handle_t;
};

// the WGPU backend state
Impl :: @struct {
    sg: *Sg.Self;
    valid: bool;
    use_indexed_draw: bool;
    dev: WGPU.Device;
    limits: WGPU.Limits;
    queue: WGPU.Queue;
    cmd_enc: WGPU.CommandEncoder;
    rpass_enc: ?WGPU.RenderPassEncoder;
    cpass_enc: ?WGPU.ComputePassEncoder;
    empty_bind_group: WGPU.BindGroup;
    cur_pipeline: ?*Sg.Pipeline.T;
    cur_pipeline_id: Sg.Pipeline;
    uniform: uniform_buffer_t;
    bindings_cache: bindings_cache_t;
    bindgroups_cache: bindgroups_cache_t;
    bindgroups_pool: Sg.DynPool;
};

:: ASSERT_NOT_HTML();

fn to_wgpu(b: bool) WGPU.OptionalBool = 
    @if(b, .True, .False);

fn to_wgpu(b: bool) WGPU.Bool = 
    @if(b, .True, .False);

fn to_wgpu(t: Sg.BufferType, u: Sg.Usage) WGPU.BufferUsage = {
    // FIXME: change to WGPU.BufferUsage once Emscripten and Dawn webgpu.h agree
    // TODO: ?^ sure seems like this is using WGPU.BufferUsage?
    
    // TODO: better things for flags and then put these numbers in the bindings instead so 
    //       you can sanely use wgpu from franca without the gfx abstraction layer. :LazyMagicNumbers
    res: i64 = @map_enum(t) (STORAGEBUFFER = 0x80, INDEXBUFFER = 0x10);
    (_ = res.bit_or(@if(u == .IMMUTABLE, 0, 0x8))) // CopyDst
}

fn to_wgpu(view: WGPU.TextureView, a: Sg.LoadAction) WGPU.LoadOp = 
    @if(view._.is_null(), .Undefined,
        @map_enum(a) (CLEAR = .Clear, DONTCARE = .Clear, LOAD = .Load));

fn to_wgpu(view: WGPU.TextureView, it: Sg.StoreAction) WGPU.StoreOp = 
    @if(view._.is_null(), .Undefined, @map_enum(it) 
        (STORE = .Store, DONTCARE = .Discard));

fn to_wgpu(it: Sg.ImageType) WGPU.TextureViewDimension = @map_enum(it)
    (_2D = ._2D, CUBE = .Cube, _3D = ._3D, ARRAY = ._2DArray);

fn to_wgpu(it: Sg.ImageType) WGPU.TextureDimension = 
    @if(it == ._3D, ._3D, ._2D);

fn to_wgpu(it: Sg.ImageSampleType, msaa: bool) WGPU.TextureSampleType = @map_enum(it)
    (FLOAT = @if(msaa, .UnfilterableFloat, .Float), DEPTH = .Depth, SINT = .Sint, UINT = .Uint, UNFILTERABLE_FLOAT = .UnfilterableFloat);

fn to_wgpu(it: Sg.SamplerType) WGPU.SamplerBindingType = @map_enum(it)
    (FILTERING = .Filtering, COMPARISON = .Comparison, NONFILTERING = .NonFiltering);

fn to_wgpu(it: Sg.Wrap) WGPU.AddressMode = @map_enum(it)    // vvvvvv not supported?
    (REPEAT = .Repeat, CLAMP_TO_EDGE = .ClampToEdge, CLAMP_TO_BORDER = .ClampToEdge, MIRRORED_REPEAT = .MirrorRepeat);

fn to_wgpu(it: Sg.Filter) WGPU.FilterMode = @map_enum(it)
    (NEAREST = .Nearest, LINEAR = .Linear);

fn to_wgpu(it: Sg.Filter) WGPU.MipmapFilterMode = @map_enum(it)
    (NEAREST = .Nearest, LINEAR = .Linear);

// NOTE: there's no WGPU.IndexFormat_None
fn to_wgpu(it: Sg.IndexType) WGPU.IndexFormat = {
    @if(it == .UINT16, .Uint16, .Uint32)
}

fn to_wgpu(prim_type: Sg.PrimitiveType, idx_type: Sg.IndexType) WGPU.IndexFormat = {
    if(idx_type == .NONE, => return(.Undefined));
    if(@is(prim_type, .LINE_STRIP, .TRIANGLE_STRIP), => return(to_wgpu(idx_type)));
    .Undefined
}

fn to_wgpu(it: Sg.PrimitiveType) WGPU.PrimitiveTopology = @map_enum(it)
    (POINTS = .PointList, LINES = .LineList, LINE_STRIP = .LineStrip, TRIANGLES = .TriangleList, TRIANGLE_STRIP = .TriangleStrip);

fn to_wgpu(it: Sg.FaceWinding) WGPU.FrontFace = 
    @if(it == .CCW, .CCW, .CW);

fn to_wgpu(it: Sg.CullMode) WGPU.CullMode = @map_enum(it)
    (NONE = .None, FRONT = .Front, BACK = .Back);

fn to_wgpu(it: Sg.PixelFormat) WGPU.TextureFormat = @map_enum(it) (
    NONE = .Undefined, R8 = .R8Unorm, R8SN = .R8Snorm, 
    R8UI = .R8Uint, R8SI = .R8Sint, R16UI = .R16Uint, R16SI = .R16Sint, 
    R16F = .R16Float, RG8 = .RG8Unorm, RG8SN = .RG8Snorm, RG8UI = .RG8Uint, 
    RG8SI = .RG8Sint, R32UI = .R32Uint, R32SI = .R32Sint, R32F = .R32Float, 
    RG16UI = .RG16Uint, RG16SI = .RG16Sint, RG16F = .RG16Float, RGBA8 = .RGBA8Unorm, 
    SRGB8A8 = .RGBA8UnormSrgb, RGBA8SN = .RGBA8Snorm, RGBA8UI = .RGBA8Uint, 
    RGBA8SI = .RGBA8Sint, BGRA8 = .BGRA8Unorm, RGB10A2 = .RGB10A2Unorm, 
    RG11B10F = .RG11B10Ufloat, RG32UI = .RG32Uint, RG32SI = .RG32Sint, 
    RG32F = .RG32Float, RGBA16UI = .RGBA16Uint, RGBA16SI = .RGBA16Sint, 
    RGBA16F = .RGBA16Float, RGBA32UI = .RGBA32Uint, RGBA32SI = .RGBA32Sint, 
    RGBA32F = .RGBA32Float, DEPTH = .Depth32Float, 
    DEPTH_STENCIL = .Depth32FloatStencil8, BC1_RGBA = .BC1RGBAUnorm, 
    BC2_RGBA = .BC2RGBAUnorm, BC3_RGBA = .BC3RGBAUnorm, BC3_SRGBA = .BC3RGBAUnormSrgb, 
    BC4_R = .BC4RUnorm, BC4_RSN = .BC4RSnorm, BC5_RG = .BC5RGUnorm, 
    BC5_RGSN = .BC5RGSnorm, BC6H_RGBF = .BC6HRGBFloat, BC6H_RGBUF = .BC6HRGBUfloat,
    BC7_RGBA = .BC7RGBAUnorm, BC7_SRGBA = .BC7RGBAUnormSrgb, ETC2_RGB8 = .ETC2RGB8Unorm, 
    ETC2_RGB8A1 = .ETC2RGB8A1Unorm, ETC2_RGBA8 = .ETC2RGBA8Unorm, 
    ETC2_SRGB8 = .ETC2RGB8UnormSrgb, ETC2_SRGB8A8 = .ETC2RGBA8UnormSrgb, 
    EAC_R11 = .EACR11Unorm, EAC_R11SN = .EACR11Snorm, EAC_RG11 = .EACRG11Unorm,
    EAC_RG11SN = .EACRG11Snorm, RGB9E5 = .RGB9E5Ufloat, ASTC_4x4_RGBA = .ASTC4x4Unorm,
    ASTC_4x4_SRGBA = .ASTC4x4UnormSrgb,
    // NOT SUPPORTED
    R16 = .Undefined, R16SN = .Undefined, RG16 = .Undefined, RG16SN = .Undefined, 
    RGBA16 = .Undefined, RGBA16SN = .Undefined,
);

fn to_wgpu(it: Sg.CompareFunc) WGPU.CompareFunction = @map_enum(it)
    (NEVER = .Never, LESS = .Less, EQUAL = .Equal, LESS_EQUAL = .LessEqual, GREATER = .Greater, NOT_EQUAL = .NotEqual, GREATER_EQUAL = .GreaterEqual, ALWAYS = .Always);

fn to_wgpu(it: Sg.StencilOp) WGPU.StencilOperation = @map_enum(it)
    (KEEP = .Keep, ZERO = .Zero, REPLACE = .Replace, INCR_CLAMP = .IncrementClamp, DECR_CLAMP = .DecrementClamp, INVERT = .Invert, INCR_WRAP = .IncrementWrap, DECR_WRAP = .DecrementWrap);

fn to_wgpu(it: Sg.BlendOp) WGPU.BlendOperation = @map_enum(it)
    (ADD = .Add, SUBTRACT = .Subtract, REVERSE_SUBTRACT = .ReverseSubtract, MIN = .Min, MAX = .Max);

fn to_wgpu(it: Sg.BlendFactor) WGPU.BlendFactor = @map_enum(it) (
    ZERO = .Zero, ONE = .One, 
    SRC_COLOR = .Src, ONE_MINUS_SRC_COLOR = .OneMinusSrc,
    SRC_ALPHA = .SrcAlpha, ONE_MINUS_SRC_ALPHA = .OneMinusSrcAlpha,
    DST_COLOR = .Dst, ONE_MINUS_DST_COLOR = .OneMinusDst,
    DST_ALPHA = .DstAlpha, ONE_MINUS_DST_ALPHA = .OneMinusDstAlpha,
    SRC_ALPHA_SATURATED = .SrcAlphaSaturated,
    BLEND_COLOR = .Constant, ONE_MINUS_BLEND_COLOR = .OneMinusConstant,
    // FIXME: separate blend alpha value not supported?
    BLEND_ALPHA = .Constant, ONE_MINUS_BLEND_ALPHA = .OneMinusConstant,
);

fn to_wgpu(m: Sg.ColorMask) WGPU.ColorWriteMask = {
    // FIXME: change to WGPU.ColorWriteMask once Emscripten and Dawn webgpu.h agree
    
    res := 0;
    X :: fn(s: Sg.ColorMask, w) => if bit_and((@as(u32) m), @as(u32) s) != 0 {
        res = res.bit_or(w);
    };
    X(.R, WGPU.ColorWriteMask.Red);
    X(.G, WGPU.ColorWriteMask.Green);
    X(.B, WGPU.ColorWriteMask.Blue);
    X(.A, WGPU.ColorWriteMask.Alpha);
    (_ = res)
}

fn to_wgpu(it: Sg.ShaderStage) WGPU.ShaderStage = (_ = @map_enum(it)
    (VERTEX = 1, FRAGMENT = 2, COMPUTE = 4));

fn init_caps(wgpu: *Impl) void = {
    wgpu.sg.features = (
        origin_top_left = true,
        image_clamp_to_border = false,
        mrt_independent_blend_state = true,
        mrt_independent_write_mask = true,
        compute = true,
        msaa_image_bindings = true,
    );

    canary := wgpu.queue;
    getLimits(wgpu.dev, wgpu.limits&);
    @debug_assert(identical(canary._, wgpu.queue._), "they grew the limits struct and overwrote our other fields");
    
    l := wgpu.limits&;
    wgpu.sg.limits = (
        max_image_size_2d = bitcast l.maxTextureDimension2D,
        max_image_size_cube = bitcast l.maxTextureDimension2D, // not a bug, see: https://github.com/gpuweb/gpuweb/issues/1327
        max_image_size_3d = bitcast l.maxTextureDimension3D,
        max_image_size_array = bitcast l.maxTextureDimension2D,
        max_image_array_layers = bitcast l.maxTextureArrayLayers,
    );
    
    f := wgpu.sg.formats&;
    
    // NOTE: no WGPU.TextureFormat_R16Unorm
    @fill_formats(f) (
        sfbrm = (.R8, .RG8, .RGBA8, .SRGB8A8, .BGRA8, .R16F, .RG16F, .RGBA16F, .RGB10A2),
        // NOTE: msaa rendering is possible in WebGPU, but no resolve
        // which is a combination that's not currently supported in sokol-gfx
        sr = (.R8UI, .R8SI, .RG8UI, .RG8SI, .RGBA8UI, .RGBA8SI, .R16UI, .R16SI, .RG16UI, 
            .RG16SI, .RGBA16UI, .RGBA16SI, .R32UI, .R32SI, .RG32UI, .RG32SI, .RGBA32UI, .RGBA32SI,
        ),
        // FIXME: can be made renderable via extension VV 
        sf = (.R8SN, .RG8SN, .RGBA8SN, .RGB9E5, .RG11B10F),   
        srmd = (.DEPTH, .DEPTH_STENCIL),
    );
    
    // TODO: ask for these features in web/app.fr/pre_init ?
    
    if hasFeature(wgpu.dev, .Float32Filterable) == .True {
        @fill_formats(f) (sfr = (.R32F, .RG32F, .RGBA32F));
    } else {
        @fill_formats(f) (sr = (.R32F, .RG32F, .RGBA32F));
    };

    @if(hasFeature(wgpu.dev, .TextureCompressionBC) == .True) @fill_formats(f) (
        sf = (.BC1_RGBA, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC4_R, .BC4_RSN, .BC5_RG, .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA),
    );
    @if(hasFeature(wgpu.dev, .TextureCompressionETC2) == .True) @fill_formats(f) (
        sf = (.ETC2_RGB8, .ETC2_SRGB8, .ETC2_RGB8A1, .ETC2_RGBA8, .ETC2_SRGB8A8, .EAC_R11, .EAC_R11SN, .EAC_RG11, .EAC_RG11SN),
    );
    @if(hasFeature(wgpu.dev, .TextureCompressionASTC) == .True) @fill_formats(f) (
        sf = (.ASTC_4x4_RGBA, .ASTC_4x4_SRGBA),
    );
}

fn uniform_buffer_init(wgpu: *Impl, desc: *Sg.Desc) void #once = {
    // Add the max-uniform-update size (64 KB) to the requested buffer size,
    // this is to prevent validation errors in the WebGPU implementation
    // if the entire buffer size is used per frame. 64 KB is the allowed
    // max uniform update size on NVIDIA
    //
    // FIXME: is this still needed?
    num_bytes := desc.uniform_buffer_size + MAX_UNIFORM_UPDATE_SIZE;
    wgpu.uniform.staging = wgpu.sg.desc.allocator.alloc_uninit(u8, intcast num_bytes);

    ub_desc: WGPU.BufferDescriptor = (
        size = num_bytes.intcast(),
        usage = (_ = @or(WGPU.BufferUsage.Uniform, WGPU.BufferUsage.CopyDst)),
    );
    wgpu.uniform.buf = createBuffer(wgpu.dev, ub_desc&);
    @debug_assert(!wgpu.uniform.buf._.is_null());
}

fn uniform_buffer_discard(wgpu: *Impl) void = {
    @debug_assert(!wgpu.uniform.buf.is_null(), "don't discard backend twice");
    wgpuBufferRelease(wgpu.uniform.buf);
    wgpu.sg.allocator.dealloc(u8, wgpu.uniform.staging);
}

fn uniform_buffer_on_commit(wgpu: *Impl) void = {
    writeBuffer(wgpu.queue, wgpu.uniform.buf, 0, u8.raw_from_ptr(wgpu.uniform.staging.ptr), wgpu.uniform.offset);
    wgpu.sg.stat(.wgpu_uniforms_size_write_buffer, trunc wgpu.uniform.offset);
    wgpu.uniform.offset = 0;
    wgpu.uniform.bind_offsets&.items().set_zeroed();
}

fn bindgroups_pool_discard() void = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    @debug_assert(p.bindgroups);
    _sg_free(p.bindgroups); p.bindgroups = 0;
    _sg_pool_discard(&p.pool);
}

fn bindgroup_at(wgpu: *Impl, bg_id: u32) *bindgroup_t = {
    @debug_assert(bg_id != Sg.Sg.INVALID_ID);
    p := wgpu.bindgroups_pool&;
    slot_index := _sg_slot_index(bg_id);
    @debug_assert((slot_index > Sg._SG_INVALID_SLOT_INDEX) && (slot_index < p.size));
    ptr_cast_unchecked(u8, bindgroup_t, p.data.ptr.offset(slot_index.intcast() * size_of(bindgroup_t)))
}

fn lookup_bindgroup(wgpu: *Impl, bg_id: u32) ?*bindgroup_t = {
    if(bg_id == Sg.INVALID_ID, => return(.None));
    bg := bindgroup_at(bg_id);
    if(bg.slot.id != bg_id, => return(.None));
    (Some = bg)
}

fn alloc_bindgroup(wgpu: *Impl) bindgroup_handle_t = {
    p := wgpu.bindgroups_pool&;
    slot_index := alloc_index(p);
    ::if(u32);
    (id = if (Sg._SG_INVALID_SLOT_INDEX != slot_index) {
        p.slot_alloc(slot_index)
    } else {
        _SG_ERROR("WGPU_BINDGROUPS_POOL_EXHAUSTED");
        Sg.INVALID_ID
    })
}

fn dealloc_bindgroup(wgpu: *Impl, bg: *bindgroup_t) void = {
    @debug_assert(bg.slot.state == .ALLOC && bg.slot.id != Sg.INVALID_ID);
    p := wgpu.bindgroups_pool&;
    p.free_index(_sg_slot_index(bg.slot.id));
    bg.slot = zeroed Sg.SlotHeader;
}

fn hash(key: []u64) u64 = {
    // TODO: sokol uses MurmurHash64B. i can earn a better hash function by producing a benchmark where it matters. 
    x: u64 = 1;
    for key { k |
        // :CompilerBug shouldn't need this `bitcast` but the high bit is set so it freaks out
        x += k * bitcast 15120273018600372767;  // high quality prime number chosen by fair die roll (certainly not the nsa) (i googled "64 bit primes")
    };
    x
}

fn bindgroups_cache_item(type: bindgroups_cache_item_type_t, wgpu_binding: u8, id: u32) u64 = {
    // key pattern is bbbbttttiiiiiiii
    bb: u64 = @as(u64) zext wgpu_binding;
    tttt: u64 = @as(u64) uext @as(u32) type;
    iiiiiiii: u64 = uext id;
    bb.shift_left(56).bit_or(bb.shift_left(48)).bit_or(tttt.shift_left(32)).bit_or(iiiiiiii)
}

fn init_bindgroups_cache_key(wgpu: *Impl, key: *bindgroups_cache_key_t, bnd: *Sg.ResolvedBindings) void = {
    @debug_assert(!bnd.pip.is_null());
    shd := bnd.pip.shader_t;
    @debug_assert(shd.slot.id == bnd.pip.cmn.shader.id);

    set_zeroed(key.items&.items());
    key.items&[0] = bindgroups_cache_item(.Pipeline, 0xFF, bnd.pip.slot.id);
    rangec(0, Sg.SG_MAX_IMAGE_BINDSLOTS) { i, $continue |
        if(shd.cmn.images&[i].stage == .NONE, => continue());
        @debug_assert(!bnd.imgs&[i].is_null());
        item_idx := i + 1;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(key.items&[item_idx] == 0);
        wgpu_binding := shd.mtl.img_grp1_bnd_n&[i];
        id := bnd.imgs&[i].slot.id;
        key.items&[item_idx] = bindgroups_cache_item(.Image, wgpu_binding, id);
    }
    rangec(0, Sg.SG_MAX_SAMPLER_BINDSLOTS) { i, $continue |
        if(shd.cmn.samplers&[i].stage == .NONE, => continue());
        @debug_assert(!bnd.smps&[i].is_null());
        item_idx := i + 1 + Sg.SG_MAX_IMAGE_BINDSLOTS;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(key.items&[item_idx] == 0);
        wgpu_binding := shd.mtl.smp_grp1_bnd_n&[i];
        id := bnd.smps&[i].slot.id;
        key.items&[item_idx] = bindgroups_cache_item(.Sampler, wgpu_binding, id);
    }
    rangec(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i, $continue |
        if(shd.cmn.storage_buffers&[i].stage == .NONE, => continue());
        @debug_assert(!bnd.sbufs&[i].is_null());
        item_idx := i + 1 + Sg.SG_MAX_IMAGE_BINDSLOTS + Sg.SG_MAX_SAMPLER_BINDSLOTS;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(key.items&[item_idx] == 0);
        wgpu_binding := shd.mtl.sbuf_grp1_bnd_n&[i];
        id := bnd.sbufs&[i].slot.id;
        key.items&[item_idx] = bindgroups_cache_item(.StorageBuffer, wgpu_binding, id);
    }
    key.hash = hash(key.items&.items());
}

// TODO
fn compare_bindgroups_cache_key(wgpu: *Impl, k0: *bindgroups_cache_key_t, k1: *bindgroups_cache_key_t) bool = {
    @debug_assert(k0 && k1);
    if (k0.hash != k1.hash) {
        return false;
    }
    if (memcmp(&k0.items, &k1.items, sizeof(k0.items)) != 0) {
        wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_hash_vs_key_mismatch, 1);
        return false;
    }
    true
}

// .None means pool full. can return Some with state=.FAILED which is confusing perhaps
fn create_bindgroup(wgpu: *Impl, bnd: *Sg.ResolvedBindings) ?*bindgroup_t = {
    @debug_assert(!wgpu.dev._.is_null(), "missing device");
    shd := bnd.pip.cmn.shader_t;
    @debug_assert_eq(shd.slot.id, bnd.pip.cmn.shader.id, "stale shader");
    wgpu.sg.stat(.wgpu_bindings_num_create_bindgroup, 1);
    bg_id := wgpu.alloc_bindgroup();
    if(bg_id.id == Sg.INVALID_ID, => return(.None));
    bg := wgpu.bindgroup_at(bg_id.id);
    @debug_assert_eq(bg.slot.state, .ALLOC);

    // create wgpu bindgroup object (also see create_shader())
    bgl := bnd.pip.shader_t.mtl.bgl_img_smp_sbuf;
    @debug_assert(!bgl._.is_null(), "expected call create_shader first");
    bg_entries := zeroed Array(WGPU.BindGroupEntry, MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    bgl_index := 0;
    rangec(0, Sg.SG_MAX_IMAGE_BINDSLOTS) { i, $continue |
        if(shd.cmn.images&[i].stage == .NONE, => continue());
        @debug_assert(!bnd.imgs&[i].is_null());
        bg_entries&[bgl_index] = (
            binding = zext shd.mtl.img_grp1_bnd_n&[i],
            textureView = bnd.imgs&[i].mtl.view,
        );
        bgl_index += 1;
    }
    rangec(0, Sg.SG_MAX_SAMPLER_BINDSLOTS) { i, $continue |
        if(shd.cmn.samplers&[i].stage == .NONE, => continue());
        @debug_assert(!bnd.smps&[i].is_null());
        bg_entries&[bgl_index] = (
            binding = zext shd.mtl.smp_grp1_bnd_n&[i],
            sampler = bnd.smps&[i].mtl.smp,
        );
        bgl_index += 1;
    }
    rangec(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i, $continue |
        if(shd.cmn.storage_buffers&[i].stage == .NONE, => continue());
        @debug_assert(!bnd.sbufs&[i].is_null());
        bg_entries&[bgl_index] = (
            binding = zext shd.mtl.sbuf_grp1_bnd_n&[i],
            buffer = bnd.sbufs&[i].mtl.buf,
            size = intcast bnd.sbufs&[i].cmn.size,
        );
        bgl_index += 1;
    }
    bg.bindgroup = wgpu.dev.createBindGroup(@ref @as(WGPU.BindGroupDescriptor) (
        layout = bgl,
        entryCount = bgl_index,
        entries = bg_entries&.as_ptr(),
    ));
    if bg.bindgroup._.is_null() {
        _SG_ERROR("WGPU_CREATEBINDGROUP_FAILED");
        bg.slot.state = .FAILED;
    } else {
        wgpu.init_bindgroups_cache_key(bg.key&, bnd);
        bg.slot.state = .VALID;
    };
    (Some = bg)
}

fn discard(wgpu: *Impl, bg: *bindgroup_t) void = {
    wgpu.sg.stat(.wgpu_bindings_num_discard_bindgroup, 1);
    if (bg.slot.state == .VALID) {
        if !bg.bindgroup._.is_null() {
            release(bg.bindgroup);
            bg.bindgroup = zeroed WGPU.BindGroup;
        };
        id := bg.slot.id;
        bg[] = zeroed @type bg[];
        bg.slot = (id = id, state = .ALLOC);
    }
    if bg.slot.state == .ALLOC {
        wgpu.dealloc_bindgroup(bg);
        @debug_assert(bg.slot.state == .INITIAL);
    }
}

fn discard_all_bindgroups(wgpu: *Impl) void = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    for (int i = 0; i < p.pool.size; i++) {
        sg_resource_state state = p.bindgroups[i].slot.state;
        if ((state == SG_RESOURCESTATE_VALID) || (state == SG_RESOURCESTATE_FAILED)) {
            discard_bindgroup(&p.bindgroups[i]);
        }
    }
}

_SG_PANIC :: _SG_ERROR;

// TODO: generics again

fn bindgroups_cache_init(wgpu: *Impl, desc: *Sg.Desc) void #once = {
    num := desc.wgpu_bindgroups_cache_size;
    @if(num <= 1) _SG_PANIC("WGPU_BINDGROUPSCACHE_SIZE_GREATER_ONE");
    @if(!Sg'ispow2(num)) _SG_PANIC("WGPU_BINDGROUPSCACHE_SIZE_POW2");
    num: i64 = intcast desc.wgpu_bindgroups_cache_size;
    wgpu.bindgroups_cache = (
        index_mask = num - 1,
        items = desc.allocator.alloc_zeroed(bindgroup_handle_t, num),
    );
}

fn bindgroups_cache_set(wgpu: *Impl, hash: uint64_t, bg_id: uint32_t) void = {
    uint32_t index := hash & wgpu.bindgroups_cache.index_mask;
    wgpu.bindgroups_cache.items[index].id = bg_id;
}

fn bindgroups_cache_get(wgpu: *Impl, hash: u64) u32 = {
    index := hash.bit_and(wgpu.bindgroups_cache.index_mask);
    wgpu.bindgroups_cache.items[index].id
}

// called from wgpu resource destroy functions to also invalidate any
// bindgroups cache slot and bindgroup referencing that resource
fn bindgroups_cache_invalidate(wgpu: *Impl, type: bindgroups_cache_item_type_t, id: uint32_t) void = {
    const uint64_t key_mask = 0x0000FFFFFFFFFFFF;
    const uint64_t key_item = bindgroups_cache_item(type, 0, id) & key_mask;
    @debug_assert(wgpu.bindgroups_cache.items);
    for (uint32_t cache_item_idx = 0; cache_item_idx < wgpu.bindgroups_cache.num; cache_item_idx++) {
        const uint32_t bg_id = wgpu.bindgroups_cache.items[cache_item_idx].id;
        if (bg_id != Sg.INVALID_ID) {
            bindgroup_t* bg = lookup_bindgroup(bg_id);
            @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_VALID));
            // check if resource is in bindgroup, if yes discard bindgroup and invalidate cache slot
            bool invalidate_cache_item = false;
            for (int key_item_idx = 0; key_item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS; key_item_idx++) {
                if ((bg.key.items[key_item_idx] & key_mask) == key_item) {
                    invalidate_cache_item = true;
                    break;
                }
            }
            if (invalidate_cache_item) {
                discard_bindgroup(bg); bg = 0;
                bindgroups_cache_set(cache_item_idx, Sg.INVALID_ID);
                wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_invalidates, 1);
            }
        }
    }
}

fn bindings_cache_clear(wgpu: *Impl) void = {
    wgpu.bindings_cache = zeroed @type wgpu.bindings_cache;
}

fn bindings_cache_ib_dirty(wgpu: *Impl, ib: *Sg.Buffer.T, offset: i64) bool = {
    dest := wgpu.bindings_cache.ib&;
    if !ib.is_null() {
        dest.buffer.id != ib.slot.id || dest.offset != offset
    } else {
        dest.buffer.id != 0
    }
}

fn bindings_cache_ib_update(wgpu: *Impl, ib: *Sg.Buffer.T, offset: i64) void = {
    dest := wgpu.bindings_cache.ib&;
    if !ib.is_null() {
        dest[] = (buffer = (id = ib.slot.id), offset = offset);
    } else {
        dest[] = zeroed @type dest[];
    }
}

fn bindings_cache_bg_dirty(wgpu: *Impl, bg: ?*bindgroup_t) bool = {
    wgpu.bindings_cache.bg.id != if(bg, fn(bg) => bg.slot.id, => 0)
}

fn bindings_cache_bg_update(wgpu: *Impl, bg: ?*bindgroup_t) void = {
    ::if_opt(*bindgroup_t, u32);
    wgpu.bindings_cache.bg.id = if(bg, fn(bg) => bg.slot.id, => 0);
}

fn set_img_smp_sbuf_bindgroup(wgpu: *Impl, bg: ?*bindgroup_t) void = {
    if !wgpu.bindings_cache_bg_dirty(bg) {
        wgpu.sg.stat(.wgpu_bindings_num_skip_redundant_bindgroup, 1);
        return();
    };
    // else:
    
    wgpu.bindings_cache_bg_update(bg);
    wgpu.sg.stat(.wgpu_bindings_num_set_bindgroup, 1);
    if (wgpu.sg.cur_pass.is_compute) {
        cpass_enc := wgpu.cpass_enc.unwrap();
        group := wgpu.empty_bind_group;
        if bg { bg |
            @debug_assert(bg.slot.state == .VALID && !bg.bindgroup._.is_null());
            group = bg.bindgroup;
        };
        cpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, group, 0, zeroed(*u32));
    } else {
        rpass_enc := wgpu.rpass_enc.unwrap();
        group := wgpu.empty_bind_group;
        if bg { bg |
            @debug_assert(bg.slot.state == .VALID && !bg.bindgroup._.is_null());
            group = bg.bindgroup;
        };
        rpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, group, 0, zeroed(*u32));
    }
}

fn apply_bindgroup(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    if true /*TODO :SLOW TODO*/ || wgpu.sg.desc.wgpu_disable_bindgroups_cache {
        // bindgroups cache disabled, create and destroy bindgroup on the fly (expensive!)
        bg := wgpu.create_bindgroup(bnd).expect("pool can't be full when cache disabled");
        if bg.slot.state == .VALID {
            wgpu.set_img_smp_sbuf_bindgroup(Some = bg);
        }
        wgpu.discard(bg);
        return true;
    };
    unreachable();
    // else, check the cache

    bg: ?*bindgroup_t = .None;
    key := zeroed bindgroups_cache_key_t;
    wgpu.init_bindgroups_cache_key(key&, bnd);
    bg_id := wgpu.bindgroups_cache_get(key.hash);
    if (bg_id != Sg.INVALID_ID) {
        // potential cache hit
        bg = wgpu.lookup_bindgroup(bg_id);
        @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_VALID));
        if !compare_bindgroups_cache_key(key&, bg.key&) {
            // cache collision, need to delete cached bindgroup
            wgpu.sg.stat(.wgpu_bindings_num_bindgroup_cache_collisions, 1);
            discard(bg);
            bindgroups_cache_set(key.hash, Sg.INVALID_ID);
            bg = .None;
        } else {
            wgpu.sg.stat(.wgpu_bindings_num_bindgroup_cache_hits, 1);
        }
    } else {
        wgpu.sg.stat(.wgpu_bindings_num_bindgroup_cache_misses, 1);
    }
    if (bg == 0) {
        // either no cache entry yet, or cache collision, create new bindgroup and store in cache
        bg = create_bindgroup(bnd);
        bindgroups_cache_set(key.hash, bg.slot.id);
    }
    if (bg && bg.slot.state == .VALID) {
        set_img_smp_sbuf_bindgroup(bg);
    } else {
        return false;
    };
    true
}

fn apply_index_buffer(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    rpass_enc := wgpu.rpass_enc.expect("in render pass");
    ib := bnd.ib;
    offset := intcast bnd.ib_offset;
    if !wgpu.bindings_cache_ib_dirty(ib, offset) {
        wgpu.sg.stat(.wgpu_bindings_num_skip_redundant_index_buffer, 1);
        return true;
    }
    wgpu.bindings_cache_ib_update(ib, offset);
    if !ib.is_null() {
        format := to_wgpu(bnd.pip.cmn.index_type);
        buf_size := intcast ib.cmn.size;
        @debug_assert(buf_size > offset);
        max_bytes := buf_size - offset;
        setIndexBuffer(rpass_enc, ib.mtl.buf, format, offset, max_bytes);
    /* FIXME: the else-pass should actually set a null index buffer, but that doesn't seem to work yet
    } else {
        setIndexBuffer(rpass_enc, 0, WGPU.IndexFormat_Undefined, 0, 0);
    */
    }
    wgpu.sg.stat(.wgpu_bindings_num_set_index_buffer, 1);
    true
}

fn setup_backend(wgpu: *Impl, desc: *Sg.Desc) void = {
    @debug_assert(!desc.environment.wgpu.device._.is_null());
    @debug_assert(desc.uniform_buffer_size > 0);
    //wgpu.sg.backend = .WGPU;
    wgpu.valid = true;
    wgpu.dev = desc.environment.wgpu.device;
    wgpu.queue = getQueue(wgpu.dev);
    @debug_assert(!wgpu.queue._.is_null());
    wgpu.rpass_enc = .None;
    wgpu.cpass_enc = .None;

    wgpu.init_caps();
    wgpu.uniform_buffer_init(desc);
    init_pool(wgpu.bindgroups_pool&, desc.allocator, intcast desc.wgpu_bindgroups_cache_size, size_of bindgroup_t);
    wgpu.bindgroups_cache_init(desc);
    wgpu.bindings_cache_clear();

    // create an empty bind group
    empty_bgl := createBindGroupLayout(wgpu.dev, @ref WGPU.BindGroupLayoutDescriptor.zeroed());
    @debug_assert(!empty_bgl._.is_null());
    wgpu.empty_bind_group = createBindGroup(wgpu.dev, @ref @as(WGPU.BindGroupDescriptor) (
        layout = empty_bgl,
        entryCount = 0,
    ));
    @debug_assert(!wgpu.empty_bind_group._.is_null());
    release(empty_bgl);

    // create initial per-frame command encoder
    wgpu.cmd_enc = createCommandEncoder(wgpu.dev, @ref WGPU.CommandEncoderDescriptor.zeroed());
    @debug_assert(!wgpu.cmd_enc._.is_null());
}

// TODO
fn discard_backend(wgpu: *Impl) void = {
    @debug_assert(wgpu.valid);
    @debug_assert(wgpu.cmd_enc);
    wgpu.valid = false;
    discard_all_bindgroups();
    wgpu.sg.desc.allocator.dealloc(bindgroup_handle_t, wgpu.bindgroups_cache.items);
    bindgroups_pool_discard();
    uniform_buffer_discard();
    wgpuBindGroupRelease(wgpu.empty_bind_group); wgpu.empty_bind_group = 0;
    wgpuCommandEncoderRelease(wgpu.cmd_enc); wgpu.cmd_enc = 0;
    wgpuQueueRelease(wgpu.queue); wgpu.queue = 0;
    
    wgpu = zeroed Impl;
}

fn reset_state_cache(wgpu: *Impl) void = {
    bindings_cache_clear();
}

fn create(wgpu: *Impl, buf: *Sg.Buffer.T, desc: *Sg.Buffer.Desc) Sg.ResourceState #once = {
    @debug_assert(buf.cmn.size > 0);
    injected := !desc.wgpu_buffer.is_null();
    if (injected) {
        buf.mtl.buf = bit_cast_unchecked(rawptr, WGPU.Buffer, desc.wgpu_buffer);
        addRef(buf.mtl.buf);
    } else {
        // buffer mapping size must be multiple of 4, so round up buffer size (only a problem
        // with index buffers containing odd number of indices)
        wgpu_buf_size := Sg'roundup(buf.cmn.size, 4);
        map_at_creation := buf.cmn.usage == .IMMUTABLE && !desc.data.ptr.is_null();
        @debug_assert(map_at_creation || desc.data.len == 0, "mutable buffer created with data is not automatically initialized");
        if map_at_creation && GRAPHICS_TARGET_WEB {
            // TODO: im not sure if you ever need it to not be CopyDst, 
            //       then i have to do a convoluted thing on the js side 
            //       because i can't map it into linear memory.  :WebBufferInitHack 
            buf.cmn.usage = .DYNAMIC;  
        };
        buf.mtl.buf = createBuffer(wgpu.dev, @ref @as(WGPU.BufferDescriptor) (
            usage = to_wgpu(buf.cmn.type, buf.cmn.usage),
            size = wgpu_buf_size.zext(),
            mappedAtCreation = to_wgpu (map_at_creation && !GRAPHICS_TARGET_WEB /*:WebBufferInitHack*/),
            label = str(desc.label),
        ));
        if buf.mtl.buf._.is_null() {
            _SG_ERROR("WGPU_CREATE_BUFFER_FAILED");
            return(.FAILED);
        }
        // NOTE: assume that WebGPU creates zero-initialized buffers
        if map_at_creation {
            @debug_assert(!desc.data.ptr.is_null() && (desc.data.len > 0));
            @debug_assert(desc.data.len <= intcast buf.cmn.size);
            @if(GRAPHICS_TARGET_WEB, {
                // :WebBufferInitHack
                wgpu.copy_buffer_data(buf, 0, desc.data);
            }, {
                ptr := getMappedRange(buf.mtl.buf, 0, intcast wgpu_buf_size);
                @debug_assert(!ptr.is_null());
                ptr := u8.ptr_from_raw(ptr);
                ptr.slice(desc.data.len).copy_from(desc.data);
                unmap(buf.mtl.buf);
            });
        }
    };
    .VALID
}

// TODO
fn discard(wgpu: *Impl, buf: *Sg.Buffer.T) void = {
    @debug_assert(!buf.is_null());
    if (buf.cmn.type == .STORAGEBUFFER) {
        bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_STORAGEBUFFER, buf.slot.id);
    }
    if (buf.wgpu.buf) {
        wgpuBufferRelease(buf.wgpu.buf);
    }
}

fn copy_buffer_data(wgpu: *Impl, buf: *Sg.Buffer.T, offset: i64, data: []u8) void = {
    @debug_assert(offset + data.len <= buf.cmn.size.intcast());
    @debug_assert(!data.ptr.is_null() && data.len > 0);
    
    // WebGPU's write-buffer requires the size to be a multiple of four, so we may need to split the copy
    // operation into two writeBuffer calls
    clamped_size, extra_size := (data.len.align_to(4), data.len.mod(4));
    wgpu.queue.writeBuffer(buf.mtl.buf, offset, u8.raw_from_ptr(data.ptr), clamped_size);
    if extra_size > 0 {
        extra_src_offset := clamped_size;
        extra_dst_offset := offset + clamped_size;
        extra_data := items(@ref Array(u8, 4).zeroed());
        extra_data.slice(0, extra_size).copy_from(data.subslice(extra_src_offset, extra_size));
        wgpu.queue.writeBuffer(buf.mtl.buf, extra_dst_offset, u8.raw_from_ptr(extra_data.ptr), 4);
    }
}

fn copy_image_data(wgpu: *Impl, img: *Sg.Image.T, wgpu_tex: WGPU.Texture, data: *Sg.ImageData) void = {
    wgpu_copy_tex := zeroed WGPU.TexelCopyTextureInfo;
    wgpu_copy_tex.texture = wgpu_tex;
    wgpu_copy_tex.aspect = .All;
    num_faces := @if(img.cmn.type == .CUBE, 6, 1);
    range(0, num_faces) { face_index |
        range(0, intcast img.cmn.num_mipmaps) { mip_index |
            wgpu_copy_tex.mipLevel = trunc mip_index;
            wgpu_copy_tex.origin.z = trunc face_index;
            mip_width := Sg'miplevel_dim(img.cmn.width, intcast mip_index);
            mip_height := Sg'miplevel_dim(img.cmn.height, intcast mip_index);
            wgpu_layout: WGPU.TexelCopyBufferLayout = (
                offset = 0,
                bytesPerRow = bitcast Sg'row_pitch(img.cmn.pixel_format, mip_width, 1),
                rowsPerImage = bitcast Sg'num_rows(img.cmn.pixel_format, mip_height),
            );
            if is_compressed(img.cmn.pixel_format) {
                mip_width = Sg'roundup(mip_width, 4);
                mip_height = Sg'roundup(mip_height, 4);
            }
            wgpu_extent: WGPU.Extent3D = (
                width = bitcast mip_width,
                height = bitcast mip_height,
                depthOrArrayLayers = @match(img.cmn.type) {
                    fn CUBE() => 1;
                    fn _3D() => bitcast(Sg'miplevel_dim(img.cmn.num_slices, intcast mip_index));
                    @default => bitcast(img.cmn.num_slices);
                },
            );
            mip_data := data.subimage&[face_index]&[mip_index];
            ptr := u8.raw_from_ptr(mip_data.ptr);
            writeTexture(wgpu.queue, wgpu_copy_tex&, ptr, mip_data.len, wgpu_layout&, wgpu_extent&);
        }
    }
}

fn create(wgpu: *Impl, img: *Sg.Image.T, desc: *Sg.Image.Desc) Sg.ResourceState ={
    injected := !desc.wgpu_texture.is_null();
    if injected {
        img.mtl.tex._ = desc.wgpu_texture;
        addRef(img.mtl.tex);
        img.mtl.view._ = desc.wgpu_texture_view;
        if !img.mtl.view._.is_null() {
            addRef(img.mtl.view);
        }
    } else {
        wgpu_tex_desc: WGPU.TextureDescriptor = (
            label = str(desc.label),
            usage = {
                u := @or(WGPU.TextureUsage.CopyDst, WGPU.TextureUsage.TextureBinding);
                if desc.render_target {
                    u = u.bit_or(WGPU.TextureUsage.RenderAttachment);
                };
                (_ = u)
            },
            dimension = to_wgpu(img.cmn.type),
            size = (
                width = bitcast img.cmn.width, 
                height = bitcast img.cmn.height,
                depthOrArrayLayers = @if(desc.type == .CUBE, 6, bitcast img.cmn.num_slices),
            ),
            format = to_wgpu(img.cmn.pixel_format),
            mipLevelCount = bitcast img.cmn.num_mipmaps,
            sampleCount = bitcast img.cmn.sample_count,
        );
        img.mtl.tex = createTexture(wgpu.dev, wgpu_tex_desc&);
        if img.mtl.tex._.is_null() {
            _SG_ERROR("CREATE_TEXTURE_FAILED");
            return(.FAILED);
        }
        if ((img.cmn.usage == .IMMUTABLE) && !img.cmn.render_target) {
            wgpu.copy_image_data(img, img.mtl.tex, desc.data&);
        }
        wgpu_texview_desc: WGPU.TextureViewDescriptor = (
            usage = (_ = 0),
            label = str(desc.label),
            dimension = to_wgpu(img.cmn.type),
            mipLevelCount = bitcast img.cmn.num_mipmaps,
            arrayLayerCount = @match(img.cmn.type) {
                fn CUBE() => 6;
                fn ARRAY() => bitcast img.cmn.num_slices;
                @default => 1;
            },
            aspect = @if(is_depth_or_depth_stencil_format(img.cmn.pixel_format), .DepthOnly, .All),
        );
        img.mtl.view = createView(img.mtl.tex, wgpu_texview_desc&);
        if img.mtl.view._.is_null() {
            _SG_ERROR("CREATE_TEXTURE_VIEW_FAILED");
            return(.FAILED);
        }
    };
    .VALID
}

// TODO
fn discard(wgpu: *Impl, img: *Sg.Image.T) void = {
    @debug_assert(img);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_IMAGE, img.slot.id);
    if (img.wgpu.view) {
        wgpuTextureViewRelease(img.wgpu.view);
        img.wgpu.view = 0;
    }
    if (img.wgpu.tex) {
        wgpuTextureRelease(img.wgpu.tex);
        img.wgpu.tex = 0;
    }
}

fn create(wgpu: *Impl, smp: *Sg.Sampler.T, desc: *Sg.Sampler.Desc) Sg.ResourceState = {
    injected := !desc.wgpu_sampler.is_null();
    if injected {
        smp.mtl.smp._ = desc.wgpu_sampler;
        addRef(smp.mtl.smp);
    } else {
        @debug_assert_ge(desc.max_lod, desc.min_lod);
        wgpu_desc: WGPU.SamplerDescriptor = (
            label = str(desc.label),
            addressModeU = to_wgpu(desc.wrap_u),
            addressModeV = to_wgpu(desc.wrap_v),
            addressModeW = to_wgpu(desc.wrap_w),
            magFilter = to_wgpu(desc.mag_filter),
            minFilter = to_wgpu(desc.min_filter),
            mipmapFilter = to_wgpu(desc.mipmap_filter),
            lodMinClamp = desc.min_lod,
            lodMaxClamp = desc.max_lod,
            maxAnisotropy = desc.max_anisotropy.trunc(),
            compare = {
                it := to_wgpu(desc.compare);
                @if(it == .Never, .Undefined, it)
            },
        );
        smp.mtl.smp = wgpu.dev.createSampler(wgpu_desc&);
        if smp.mtl.smp._.is_null() {
            _SG_ERROR("CREATE_SAMPLER_FAILED");
            return(.FAILED);
        }
    };
    .VALID
}

// TODO
fn discard(wgpu: *Impl, smp: *Sg.Sampler.T) void = {
    @debug_assert(smp);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_SAMPLER, smp.slot.id);
    if (smp.wgpu.smp) {
        wgpuSamplerRelease(smp.wgpu.smp);
        smp.wgpu.smp = 0;
    }
}

fn create_shader_func(wgpu: *Impl, func: *Sg.ShaderFunction, label: CStr) shader_func_t = {
    @debug_assert(!func.entry.is_null());

    wgpu_shdmod_wgsl_desc: WGPU.ShaderSourceWGSL = (
        chain = (sType = .ShaderSourceWGSL),
        code = str(func.source),
    );

    wgpu_shdmod_desc: WGPU.ShaderModuleDescriptor = (
        nextInChain = wgpu_shdmod_wgsl_desc.chain&,
        label = str(label),
    );

    // NOTE: if compilation fails we won't actually find out in this call since
    // it always returns a valid module handle, and the GetCompilationInfo() call
    // is asynchronous
    res: shader_func_t = (
        module = createShaderModule(wgpu.dev, wgpu_shdmod_desc&),
        entry = func.entry.str().shallow_copy(wgpu.sg.desc.allocator), // :LEAK
    );
    if res.module._.is_null() {
        _SG_ERROR("WGPU_CREATE_SHADER_MODULE_FAILED");
    };
    res
}

fn discard(wgpu: *Impl, func: *shader_func_t) void = {
    if !func.module._.is_null() {
        release(func.module);
        func.module = zeroed @type func.module;
    }
}

dynoffset_mapping_t :: @struct(sokol_slot: u8, wgpu_slot: u8);

// TODO: this does not spark joy. it's a paste from the metal one. 
//       make the field names the same and then use our surprise tool (generics). 
// NOTE: this is an out-of-range check for WGSL bindslots that's also active in release mode
fn ensure_wgsl_bindslot_ranges(wgpu: *Impl, desc: *Sg.Shader.Desc) bool = {
    X :: fn(count: i64, $get: @Fn(i: i64) u8, limit: u8, error) => 
        range(0, count) { i |
            if get(i) >= limit {
                _SG_ERROR(error);
                return false;
            };
        };
    X(Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS, fn(i) => desc.uniform_blocks&[i].wgsl_group0_binding_n, MAX_UB_BINDGROUP_BIND_SLOTS, "WGPU_UNIFORMBLOCK_WGSL_GROUP0_BINDING_OUT_OF_RANGE");
    X(Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS, fn(i) => desc.storage_buffers&[i].wgsl_group1_binding_n, MAX_IMG_SMP_SBUF_BIND_SLOTS, "WGPU_STORAGEBUFFER_WGSL_GROUP1_BINDING_OUT_OF_RANGE");
    X(Sg.SG_MAX_IMAGE_BINDSLOTS, fn(i) => desc.images&[i].wgsl_group1_binding_n, MAX_IMG_SMP_SBUF_BIND_SLOTS, "WGPU_IMAGE_WGSL_GROUP1_BINDING_OUT_OF_RANGE");
    X(Sg.SG_MAX_SAMPLER_BINDSLOTS, fn(i) => desc.samplers&[i].wgsl_group1_binding_n, MAX_IMG_SMP_SBUF_BIND_SLOTS, "WGPU_SAMPLER_WGSL_GROUP1_BINDING_OUT_OF_RANGE");
    true
}

fn create(wgpu: *Impl, shd: *Sg.Shader.T, desc: *Sg.Shader.Desc) Sg.ResourceState #once = {
    // do a release-mode bounds-check on wgsl bindslots, even though out-of-range
    // bindslots can't cause out-of-bounds accesses in the wgpu backend, this
    // is done to be consistent with the other backends
    if(!wgpu.ensure_wgsl_bindslot_ranges(desc), => return(.FAILED));

    // build shader modules
    shd_valid := true;
    
    X :: fn(func, dest) => if !func.source.is_null() {
        dest[] = wgpu.create_shader_func(func, desc.label);
        shd_valid = shd_valid && !dest[].module._.is_null();
    };
    
    // TODO: these could be EnumMap(Sg.ShaderStage, _)
    X(desc.vertex_func&, shd.mtl.vertex_func&);
    X(desc.fragment_func&, shd.mtl.fragment_func&);
    X(desc.compute_func&, shd.mtl.compute_func&);

    if (!shd_valid) {
        wgpu.discard(shd.mtl.vertex_func&);
        wgpu.discard(shd.mtl.fragment_func&);
        wgpu.discard(shd.mtl.compute_func&);
        return(.FAILED);
    };

    // create bind group layout and bind group for uniform blocks
    // NOTE also need to create a mapping of sokol ub bind slots to array indices
    // for the dynamic offsets array in the setBindGroup call
    @debug_assert(MAX_UB_BINDGROUP_ENTRIES <= MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    bgl_entries := zeroed Array(WGPU.BindGroupLayoutEntry, MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    bg_entries  := zeroed Array(WGPU.BindGroupEntry, MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    dynoffset_map := zeroed Array(dynoffset_mapping_t, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    bgl_index := 0;
    rangec(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { i, $continue |
        if(shd.cmn.uniform_blocks&[i].stage == .NONE, => continue());
        shd.mtl.ub_grp0_bnd_n&[i] = desc.uniform_blocks&[i].wgsl_group0_binding_n;
        bgl_entry := bgl_entries&[bgl_index]&;
        bgl_entry[] = (
            binding = zext shd.mtl.ub_grp0_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.uniform_blocks&[i].stage),
            buffer = (
                type = .Uniform,
                hasDynamicOffset = .True,
            ),
        );
        bg_entries&[bgl_index] = (
            binding = bgl_entry.binding,
            buffer = wgpu.uniform.buf,
            size = MAX_UNIFORM_UPDATE_SIZE,
        );
        dynoffset_map&[i] = (sokol_slot = trunc i, wgpu_slot = trunc bgl_entry.binding);
        bgl_index += 1;
    }
    
    bgl_desc: WGPU.BindGroupLayoutDescriptor = (entryCount = bgl_index, entries = bgl_entries&.as_ptr());
    shd.mtl.bgl_ub = createBindGroupLayout(wgpu.dev, bgl_desc&);
    @debug_assert(!shd.mtl.bgl_ub._.is_null());
    shd.mtl.bg_ub = createBindGroup(wgpu.dev, @ref @as(WGPU.BindGroupDescriptor) (
        layout = shd.mtl.bgl_ub,
        entryCount = bgl_index,
        entries = bg_entries&.as_ptr(),
    ));
    @debug_assert(!shd.mtl.bg_ub._.is_null());

    // sort the dynoffset_map by wgpu bindings, this is because the
    // dynamic offsets of the WebGPU setBindGroup call must be in
    // 'binding order', not 'bindgroup entry order'
    // TODO: did i flip it?
    sort :: import("@/lib/sort.fr")'quicksort(dynoffset_mapping_t, fn(a, b) => a.wgpu_slot < b.wgpu_slot);
    sort(dynoffset_map&.items().slice(0, bgl_index));

    shd.mtl.ub_num_dynoffsets = trunc bgl_index;
    range(0, bgl_index) { i |
        sokol_slot := dynoffset_map&[i].sokol_slot;
        shd.mtl.ub_dynoffsets&[zext sokol_slot] = trunc i;
    }

    // create bind group layout for images, samplers and storage buffers
    bgl_entries&.items().set_zeroed();
    bgl_index = 0;
    rangec(0, Sg.SG_MAX_IMAGE_BINDSLOTS) { i, $continue |
        if(shd.cmn.images&[i].stage == .NONE, => continue());
        
        msaa := shd.cmn.images&[i].multisampled;
        shd.mtl.img_grp1_bnd_n&[i] = desc.images&[i].wgsl_group1_binding_n;
        bgl_entries&[bgl_index] = (
            binding = zext shd.mtl.img_grp1_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.images&[i].stage),
            texture = (
                viewDimension = to_wgpu(shd.cmn.images&[i].image_type),
                sampleType = to_wgpu(shd.cmn.images&[i].sample_type, msaa),
                multisampled = to_wgpu msaa,
            )
        );
        bgl_index += 1;
    }
    rangec(0, Sg.SG_MAX_SAMPLER_BINDSLOTS) { i, $continue |
        if(shd.cmn.samplers&[i].stage == .NONE, => continue());
        
        shd.mtl.smp_grp1_bnd_n&[i] = desc.samplers&[i].wgsl_group1_binding_n;
        bgl_entries&[bgl_index] = (
            binding = zext shd.mtl.smp_grp1_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.samplers&[i].stage),
            sampler = (
                type = to_wgpu(shd.cmn.samplers&[i].sampler_type),
            ),
        );
        bgl_index += 1;
    }
    rangec(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i, $continue |
        if(shd.cmn.storage_buffers&[i].stage == .NONE, => continue());
        
        shd.mtl.sbuf_grp1_bnd_n&[i] = desc.storage_buffers&[i].wgsl_group1_binding_n;
        bgl_entries&[bgl_index] = (
            binding = zext shd.mtl.sbuf_grp1_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.storage_buffers&[i].stage),
            buffer = (
                type = @if(shd.cmn.storage_buffers&[i].readonly, .ReadOnlyStorage, .Storage),
            ),
        );
        bgl_index += 1;
    }
    bgl_desc: WGPU.BindGroupLayoutDescriptor = (entryCount = bgl_index, entries = bgl_entries&.as_ptr());
    shd.mtl.bgl_img_smp_sbuf = createBindGroupLayout(wgpu.dev, bgl_desc&);
    if shd.mtl.bgl_img_smp_sbuf._.is_null() {
        _SG_ERROR("WGPU_SHADER_CREATE_BINDGROUP_LAYOUT_FAILED");
        return(.FAILED);
    };
    .VALID
}

fn discard(wgpu: *Impl, shd: *Sg.Shader.T) void = {
    @debug_assert(!shd.is_null());
    wgpu.discard(shd.mtl.vertex_func&);
    wgpu.discard(shd.mtl.fragment_func&);
    wgpu.discard(shd.mtl.compute_func&);
    if !shd.mtl.bgl_ub._.is_null() {
        release(shd.mtl.bgl_ub);
        shd.mtl.bgl_ub._ = zeroed(rawptr);
    }
    if !shd.mtl.bg_ub._.is_null() {
        release(shd.mtl.bg_ub);
        shd.mtl.bg_ub._ = zeroed(rawptr);
    }
    if !shd.mtl.bgl_img_smp_sbuf._.is_null() {
        release(shd.mtl.bgl_img_smp_sbuf);
        shd.mtl.bgl_img_smp_sbuf._ = zeroed(rawptr);
    }
}

fn to_wgpu(it: Sg.Color) WGPU.Color = (
    r = cast it.r,
    g = cast it.g,
    b = cast it.b,
    a = cast it.a,
);

fn create(wgpu: *Impl, pip: *Sg.Pipeline.T, shd: *Sg.Shader.T, desc: *Sg.Pipeline.Desc) Sg.ResourceState #once = {
    @debug_assert(desc.shader.id == shd.slot.id);
    @debug_assert(!shd.mtl.bgl_ub._.is_null());
    @debug_assert(!shd.mtl.bgl_img_smp_sbuf._.is_null());
    pip.cmn.shader_t = shd;

    pip.mtl.blend_color = to_wgpu desc.blend_color;

    // - @group(0) for uniform blocks
    // - @group(1) for all image, sampler and storagebuffer resources
    wgpu_bgl := zeroed Array(WGPU.BindGroupLayout, NUM_BINDGROUPS);
    wgpu_bgl&[UB_BINDGROUP_INDEX] = shd.mtl.bgl_ub;
    wgpu_bgl&[IMG_SMP_SBUF_BINDGROUP_INDEX] = shd.mtl.bgl_img_smp_sbuf;
    wgpu_pl_desc: WGPU.PipelineLayoutDescriptor = (
        bindGroupLayoutCount = NUM_BINDGROUPS,
        bindGroupLayouts = wgpu_bgl&.as_ptr(),
    );
    wgpu_pip_layout := createPipelineLayout(wgpu.dev, wgpu_pl_desc&);
    if wgpu_pip_layout._.is_null() {
        _SG_ERROR("WGPU_CREATE_PIPELINE_LAYOUT_FAILED");
        return(.FAILED);
    }

    if pip.cmn.compute {
        pip.mtl.cpip = createComputePipeline(wgpu.dev, @ref @as(WGPU.ComputePipelineDescriptor) (
            label = str(desc.label),
            layout = wgpu_pip_layout,
            compute = (
                module = shd.mtl.compute_func.module,
                entryPoint = shd.mtl.compute_func.entry,
            ),
        ));
        release(wgpu_pip_layout);
        if pip.mtl.cpip._.is_null() {
            _SG_ERROR("WGPU_CREATE_COMPUTE_PIPELINE_FAILED");
            return(.FAILED);
        }
    } else {
        ::if(*WGPU.FragmentState);
        ::if(*WGPU.DepthStencilState);
        pip.mtl.rpip = createRenderPipeline(wgpu.dev, @ref @as(WGPU.RenderPipelineDescriptor) (
            label = str(desc.label),
            layout = wgpu_pip_layout,
            vertex = (
                module = shd.mtl.vertex_func.module,
                entryPoint = shd.mtl.vertex_func.entry,
            ),
            primitive = (
                topology = to_wgpu(desc.primitive_type),
                stripIndexFormat = to_wgpu(desc.primitive_type, desc.index_type),
                frontFace = to_wgpu(desc.face_winding),
                cullMode = to_wgpu(desc.cull_mode),
            ),
            depthStencil = if desc.depth.pixel_format == .NONE {
                WGPU.DepthStencilState.ptr_from_int(0)
            } else {
                wgpu_ds_state: WGPU.DepthStencilState = (
                    format = to_wgpu(desc.depth.pixel_format),
                    depthWriteEnabled = to_wgpu(desc.depth.write_enabled),
                    depthCompare = to_wgpu(desc.depth.compare),
                    stencilFront = to_wgpu desc.stencil.front&,
                    stencilBack = to_wgpu desc.stencil.back&,
                    stencilReadMask = zext desc.stencil.read_mask,
                    stencilWriteMask = zext desc.stencil.write_mask,
                    depthBias = desc.depth.bias.cast().int().intcast(),
                    depthBiasSlopeScale = desc.depth.bias_slope_scale,
                    depthBiasClamp = desc.depth.bias_clamp,
                );
                wgpu_ds_state&
            },
            multisample = (
                count = bitcast desc.sample_count,
                mask = 0xFFFFFFFF,
                alphaToCoverageEnabled = to_wgpu desc.alpha_to_coverage_enabled,
            ),
            fragment = if desc.color_count == 0 {
                WGPU.FragmentState.ptr_from_int(0)
            } else {
                wgpu_ctgt_state := zeroed Array(WGPU.ColorTargetState, Sg.SG_MAX_COLOR_ATTACHMENTS);
                wgpu_blend_state := @uninitialized Array(WGPU.BlendState, Sg.SG_MAX_COLOR_ATTACHMENTS);
                @debug_assert(desc.color_count < Sg.SG_MAX_COLOR_ATTACHMENTS);
                range(0, intcast desc.color_count) { i |
                    dest := wgpu_ctgt_state&[i]&;
                    dest.format = to_wgpu desc.colors&[i].pixel_format;
                    dest.writeMask = to_wgpu(desc.colors&[i].write_mask);
                    it := desc.colors&[i].blend&;
                    if it.enabled {
                        dest.blend = wgpu_blend_state&[i]&;
                        dest.blend[] = (
                            color = (
                                operation = to_wgpu it.op_rgb,
                                srcFactor = to_wgpu it.src_factor_rgb,
                                dstFactor = to_wgpu it.dst_factor_rgb,
                            ),
                            alpha = (
                                operation = to_wgpu it.op_alpha,
                                srcFactor = to_wgpu it.src_factor_alpha,
                                dstFactor = to_wgpu it.dst_factor_alpha,
                            ),
                        );
                    }
                };
                @ref @as(WGPU.FragmentState) (
                    module = shd.mtl.fragment_func.module,
                    entryPoint = shd.mtl.fragment_func.entry,
                    targetCount = intcast desc.color_count,
                    targets = wgpu_ctgt_state&.as_ptr(), 
                )
            },
        ));
        release(wgpu_pip_layout);
        if pip.mtl.rpip._.is_null() {
            _SG_ERROR("WGPU_CREATE_RENDER_PIPELINE_FAILED");
            return(.FAILED);
        }
    };
    .VALID
}

fn to_wgpu(it: *Sg.StencilFaceState) WGPU.StencilFaceState = (
    compare = to_wgpu it.compare,
    failOp = to_wgpu it.fail_op,
    depthFailOp = to_wgpu it.depth_fail_op,
    passOp = to_wgpu it.pass_op,
);

// TODO
fn discard(wgpu: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(!pip.is_null());
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_PIPELINE, pip.slot.id);
    if wgpu.cur_pipeline { cur |
        if cur.identical(pip) {
            wgpu.cur_pipeline = .None;
            wgpu.cur_pipeline_id.id = 0;
        }
    }
    if (pip.mtl.rpip) {
        release(pip.mtl.rpip);
        pip.mtl.rpip = 0;
    }
    if (pip.mtl.cpip) {
        release(pip.mtl.cpip);
        pip.mtl.cpip = 0;
    }
}

// TODO
fn create(wgpu: *Impl, atts: *Sg.Attachments.T, color_images: **Sg.Image.T, resolve_images: **Sg.Image.T, ds_img: *Sg.Image.T, desc: *Sg.Attachments.Desc) Sg.ResourceState = {
    @debug_assert(atts && desc);
    @debug_assert(color_images && resolve_images);

    // copy image pointers and create renderable wgpu texture views
    for (int i = 0; i < atts.cmn.num_colors; i++) {
        const sg_attachment_desc* color_desc = &desc.colors[i];
        _SOKOL_UNUSED(color_desc);
        @debug_assert(color_desc.image.id != Sg.INVALID_ID);
        @debug_assert(0 == atts.wgpu.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        @debug_assert(color_images[i].wgpu.tex);
        atts.wgpu.colors[i].image = color_images[i];

        wgpu_color_view_desc := color_desc.to_wgpu();
        atts.wgpu.colors[i].view = wgpuTextureCreateView(color_images[i].wgpu.tex, &wgpu_color_view_desc);
        if (0 == atts.wgpu.colors[i].view) {
            _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }

        const sg_attachment_desc* resolve_desc = &desc.resolves[i];
        if (resolve_desc.image.id != Sg.INVALID_ID) {
            @debug_assert(0 == atts.wgpu.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            @debug_assert(resolve_images[i].wgpu.tex);
            atts.wgpu.resolves[i].image = resolve_images[i];

            wgpu_resolve_view_desc := resolve_desc.to_wgpu();
            atts.wgpu.resolves[i].view = wgpuTextureCreateView(resolve_images[i].wgpu.tex, &wgpu_resolve_view_desc);
            if (0 == atts.wgpu.resolves[i].view) {
                _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
        }
    }
    @debug_assert(0 == atts.wgpu.depth_stencil.image);
    const sg_attachment_desc* ds_desc = &desc.depth_stencil;
    if (ds_desc.image.id != Sg.INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        @debug_assert(ds_img.wgpu.tex);
        atts.wgpu.depth_stencil.image = ds_img;
        wgpu_ds_view_desc: WGPU.TextureViewDescriptor = (
            baseMipLevel = (uint32_t) ds_desc.mip_level,
            mipLevelCount = 1,
            baseArrayLayer = (uint32_t) ds_desc.slice,
            arrayLayerCount = 1,
        );
        atts.wgpu.depth_stencil.view = wgpuTextureCreateView(ds_img.wgpu.tex, wgpu_ds_view_desc&);
        if (0 == atts.wgpu.depth_stencil.view) {
            _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    }
    return SG_RESOURCESTATE_VALID;
}

fn to_wgpu(it: *Sg.AttachmentDesc) WGPU.TextureViewDescriptor = (
    baseMipLevel = bitcast it.mip_level,
    mipLevelCount = 1,
    baseArrayLayer = bitcast it.slice,
    arrayLayerCount = 1,
);

// TODO
fn discard(wgpu: *Impl, atts: *Sg.Attachments.T) void = {
    @debug_assert(atts);
    for (int i = 0; i < atts.cmn.num_colors; i++) {
        if (atts.wgpu.colors[i].view) {
            wgpuTextureViewRelease(atts.wgpu.colors[i].view);
            atts.wgpu.colors[i].view = 0;
        }
        if (atts.wgpu.resolves[i].view) {
            wgpuTextureViewRelease(atts.wgpu.resolves[i].view);
            atts.wgpu.resolves[i].view = 0;
        }
    }
    if (atts.wgpu.depth_stencil.view) {
        wgpuTextureViewRelease(atts.wgpu.depth_stencil.view);
        atts.wgpu.depth_stencil.view = 0;
    }
}

fn init_color_att(action: *Sg.ColorAttachmentAction, color_view: WGPU.TextureView, resolve_view: WGPU.TextureView) WGPU.RenderPassColorAttachment = (
    view = color_view,
    resolveTarget = resolve_view,
    loadOp = to_wgpu(color_view, action.load_action),
    storeOp = to_wgpu(color_view, action.store_action),
    clearValue = to_wgpu action.clear_value,
);

fn init_ds_att(action: *Sg.PassAction, fmt: Sg.PixelFormat, view: WGPU.TextureView) WGPU.RenderPassDepthStencilAttachment = {
    wgpu_att := zeroed WGPU.RenderPassDepthStencilAttachment;
    wgpu_att.view = view;
    wgpu_att.depthLoadOp = to_wgpu(view, action.depth.load_action);
    wgpu_att.depthStoreOp = to_wgpu(view, action.depth.store_action);
    wgpu_att.depthClearValue = action.depth.clear_value;
    wgpu_att.depthReadOnly = to_wgpu false;
    if is_depth_stencil_format(fmt) {
        wgpu_att.stencilLoadOp = to_wgpu(view, action.stencil.load_action);
        wgpu_att.stencilStoreOp = to_wgpu(view, action.stencil.store_action);
    } else {
        wgpu_att.stencilLoadOp = .Undefined;
        wgpu_att.stencilStoreOp = .Undefined;
    }
    wgpu_att.stencilClearValue = zext action.stencil.clear_value;
    wgpu_att.stencilReadOnly = to_wgpu false;
    wgpu_att
}

fn begin_compute_pass(wgpu: *Impl, pass: *Sg.Pass) void = {
    wgpu_pass_desc: WGPU.ComputePassDescriptor = (label = str(pass.label));
    cpass_enc := beginComputePass(wgpu.cmd_enc, wgpu_pass_desc&);
    @debug_assert(!cpass_enc._.is_null());
    wgpu.cpass_enc = (Some = cpass_enc);
    // clear initial bindings
    cpass_enc.setBindGroup(UB_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    cpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    wgpu.sg.stat(.wgpu_bindings_num_set_bindgroup, 1);
}

fn begin_render_pass(wgpu: *Impl, pass: *Sg.Pass) void = {
    atts := wgpu.sg.cur_pass.atts;
    swapchain := pass.swapchain&;
    action := pass.action&;

    wgpu_pass_desc := zeroed WGPU.RenderPassDescriptor;
    wgpu_color_att := zeroed Array(WGPU.RenderPassColorAttachment, Sg.SG_MAX_COLOR_ATTACHMENTS);
    wgpu_pass_desc.label = str(pass.label);
    if !atts.is_null() {
        @debug_assert(atts.slot.state == .VALID);
        range(0, intcast atts.cmn.num_colors) { i | 
            wgpu_color_att&[i] = init_color_att(action.colors&[i]&, atts.mtl.colors&[i].view, atts.mtl.resolves&[i].view);
        }
        wgpu_pass_desc.colorAttachmentCount = intcast atts.cmn.num_colors;
        wgpu_pass_desc.colorAttachments = wgpu_color_att&.as_ptr();
        if !atts.mtl.depth_stencil.image.is_null() {
            wgpu_ds_att := init_ds_att(action, atts.mtl.depth_stencil.image.cmn.pixel_format, atts.mtl.depth_stencil.view);
            wgpu_pass_desc.depthStencilAttachment = wgpu_ds_att&;
        }
    } else {
        wgpu_color_att&[0] = init_color_att(action.colors&[0]&, swapchain.wgpu.render_view, swapchain.wgpu.resolve_view);
        wgpu_pass_desc.colorAttachmentCount = 1;
        wgpu_pass_desc.colorAttachments = wgpu_color_att&.as_ptr();
        depth_view := swapchain.wgpu.depth_stencil_view;
        if !depth_view._.is_null() {
            wgpu_ds_att := init_ds_att(action, swapchain.depth_format, depth_view);
            wgpu_pass_desc.depthStencilAttachment = wgpu_ds_att&;
        }
    }
    rpass_enc := beginRenderPass(wgpu.cmd_enc, wgpu_pass_desc&);
    @debug_assert(!rpass_enc._.is_null());
    wgpu.rpass_enc = (Some = rpass_enc);

    rpass_enc.setBindGroup(UB_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    rpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    wgpu.sg.stat(.wgpu_bindings_num_set_bindgroup, 1);
}

fn begin_pass(wgpu: *Impl, pass: *Sg.Pass) void #once = {
    @debug_assert(!wgpu.dev._.is_null() && !wgpu.cmd_enc._.is_null());
    @debug_assert(wgpu.rpass_enc.is_none() && wgpu.cpass_enc.is_none());

    wgpu.cur_pipeline = .None;
    wgpu.cur_pipeline_id.id = Sg.INVALID_ID;
    wgpu.bindings_cache_clear();

    if pass.compute {
        wgpu.begin_compute_pass(pass);
    } else {
        wgpu.begin_render_pass(pass);
    }
}

fn end_pass(wgpu: *Impl) void = {
    if wgpu.rpass_enc { rpass_enc |
        end(rpass_enc);
        release(rpass_enc);
        wgpu.rpass_enc = .None;
    }
    if wgpu.cpass_enc { cpass_enc |
        end(cpass_enc);
        release(cpass_enc);
        wgpu.cpass_enc = .None;
    }
}

fn commit(wgpu: *Impl, _stall: bool) void = {
    @debug_assert(!wgpu.cmd_enc._.is_null());
    wgpu.uniform_buffer_on_commit();

    cmd_buf_desc := zeroed WGPU.CommandBufferDescriptor;
    wgpu_cmd_buf := finish(wgpu.cmd_enc, cmd_buf_desc&);
    @debug_assert(!wgpu_cmd_buf._.is_null());
    release(wgpu.cmd_enc);
    wgpu.cmd_enc = zeroed @type wgpu.cmd_enc;

    submit(wgpu.queue, 1, wgpu_cmd_buf&);
    release(wgpu_cmd_buf);

    // create a new render-command-encoder for next frame
    cmd_enc_desc := zeroed WGPU.CommandEncoderDescriptor;
    wgpu.cmd_enc = createCommandEncoder(wgpu.dev, cmd_enc_desc&);
}

fn apply_viewport(wgpu: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    rpass_enc := wgpu.rpass_enc.unwrap();
    // FIXME FIXME FIXME: CLIPPING THE VIEWPORT HERE IS WRONG!!!
    // (but currently required because WebGPU insists that the viewport rectangle must be
    // fully contained inside the framebuffer, but this doesn't make any sense, and also
    // isn't required by the backend APIs)
    clip := Sg'clipi(x, y, w, h, wgpu.sg.cur_pass.width, wgpu.sg.cur_pass.height);
    f :: fn(x: i32) f32 = x.intcast().float().cast();
    xf := f clip.x;
    yf := f @if(origin_top_left, clip.y, (wgpu.sg.cur_pass.height - (clip.y + clip.h)));
    wf := f clip.w;
    hf := f clip.h;
    setViewport(rpass_enc, xf, yf, wf, hf, 0.0, 1.0);
}

fn apply_scissor_rect(wgpu: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    rpass_enc := wgpu.rpass_enc.unwrap();
    clip := Sg'clipi(x, y, w, h, wgpu.sg.cur_pass.width, wgpu.sg.cur_pass.height);
    sx := bitcast clip.x;
    sy := bitcast @if(origin_top_left, clip.y, (wgpu.sg.cur_pass.height - (clip.y + clip.h)));
    sw := bitcast clip.w;
    sh := bitcast clip.h;
    setScissorRect(rpass_enc, sx, sy, sw, sh);
}

fn set_ub_bindgroup(wgpu: *Impl, shd: *Sg.Shader.T) void = {
    // NOTE: dynamic offsets must be in binding order, not in BindGroupEntry order
    @debug_assert(shd.mtl.ub_num_dynoffsets < Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    dyn_offsets := zeroed Array(u32, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    rangec(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { i, $continue |
        if(shd.cmn.uniform_blocks&[i].stage == .NONE, => continue());
        dynoffset_index := shd.mtl.ub_dynoffsets&[i];
        @debug_assert(dynoffset_index < shd.mtl.ub_num_dynoffsets);
        dyn_offsets&[zext dynoffset_index] = wgpu.uniform.bind_offsets&[i];
    }
    a, b, c, d := (@as(u32) UB_BINDGROUP_INDEX, shd.mtl.bg_ub, @as(i64) zext shd.mtl.ub_num_dynoffsets, dyn_offsets&.as_ptr());
    if wgpu.sg.cur_pass.is_compute {
        enc := wgpu.cpass_enc.unwrap();
        enc.setBindGroup(a, b, c, d);
    } else {
        enc := wgpu.rpass_enc.unwrap();
        enc.setBindGroup(a, b, c, d);
    }
}

fn apply_pipeline(wgpu: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip.cmn.shader_t.slot.id == pip.cmn.shader.id);
    wgpu.cur_pipeline = (Some = pip);
    wgpu.cur_pipeline_id.id = pip.slot.id;
    @debug_assert(pip.cmn.compute == wgpu.sg.cur_pass.is_compute);
    if pip.cmn.compute {
        @debug_assert(!pip.mtl.cpip._.is_null());
        cpass_enc := wgpu.cpass_enc.unwrap();
        setPipeline(cpass_enc, pip.mtl.cpip);
    } else {
        @debug_assert(!pip.mtl.rpip._.is_null());
        rpass_enc := wgpu.rpass_enc.unwrap();
        wgpu.use_indexed_draw = pip.cmn.index_type != .NONE;
        setPipeline(rpass_enc, pip.mtl.rpip);
        setBlendConstant(rpass_enc, pip.mtl.blend_color&);
        setStencilReference(rpass_enc, zext pip.cmn.stencil.ref);
    }
    // bind groups must be set because pipelines without uniform blocks or resource bindings
    // will still create 'empty' BindGroupLayouts
    wgpu.set_ub_bindgroup(pip.cmn.shader_t);
    wgpu.set_img_smp_sbuf_bindgroup(.None); // this will set the 'empty bind group'
}

fn apply_bindings(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(bnd.pip.cmn.shader.id == bnd.pip.cmn.shader_t.slot.id);
    retval := true;
    if (!wgpu.sg.cur_pass.is_compute) {
        retval = wgpu.apply_index_buffer(bnd) && retval;
    }
    retval = wgpu.apply_bindgroup(bnd) && retval;
    retval
}

fn apply_uniforms(wgpu: *Impl, ub_slot: i32, data: []u8) void #once = {
    alignment: i64 = wgpu.limits.minUniformBufferOffsetAlignment.zext();
    @debug_assert(!wgpu.uniform.staging.ptr.is_null(), "uninit");
    @debug_assert(wgpu.uniform.offset + data.len <= wgpu.uniform.staging.len, "you can't write more bytes than the whole buffer in one commit()");
    @debug_assert((wgpu.uniform.offset.bit_and(alignment - 1)) == 0);
    @debug_assert(alignment > 0);
    pip := wgpu.cur_pipeline.unwrap();
    @debug_assert(pip.slot.id == wgpu.cur_pipeline_id.id);
    shd := pip.cmn.shader_t;
    @debug_assert(shd.slot.id == pip.cmn.shader.id);
    @debug_assert(data.len == zext shd.cmn.uniform_blocks&[zext ub_slot].size);
    @debug_assert(data.len <= MAX_UNIFORM_UPDATE_SIZE);

    wgpu.sg.stat(.wgpu_uniforms_num_set_bindgroup, 1);
    wgpu.uniform.staging.subslice(wgpu.uniform.offset, data.len).copy_from(data);
    wgpu.uniform.bind_offsets&[zext ub_slot] = trunc wgpu.uniform.offset;
    new_off := wgpu.uniform.offset + data.len;
    wgpu.uniform.offset = Sg'roundup(new_off, alignment);

    wgpu.set_ub_bindgroup(shd);
}

fn draw(wgpu: *Impl, base_element: i32, num_elements: i32, num_instances: i32) void = {
    rpass_enc := wgpu.rpass_enc.unwrap();
    pip := wgpu.cur_pipeline.unwrap();
    @debug_assert(pip.slot.id == wgpu.cur_pipeline_id.id);
    if pip.cmn.index_type != .NONE {
        drawIndexed(rpass_enc, bitcast num_elements, bitcast num_instances, bitcast base_element, 0, 0);
    } else {
        draw(rpass_enc, bitcast num_elements, bitcast num_instances, bitcast base_element, 0);
    }
}

fn dispatch(wgpu: *Impl, num_groups_x: i64, num_groups_y: i64, num_groups_z: i64) void = {
    cpass_enc := wgpu.cpass_enc.unwrap();
    cpass_enc.dispatchWorkgroups(num_groups_x.trunc(), num_groups_y.trunc(), num_groups_z.trunc());
}

fn update_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, data: []u8) void =
    wgpu.copy_buffer_data(buf, 0, data);

fn append_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, data: []u8, _new_frame: bool) void = 
    copy_buffer_data(buf, zext buf.cmn.append_pos, data);

fn update_image(wgpu: *Impl, img: *Sg.Image.T, data: *Sg.ImageData) void #once = 
    wgpu.copy_image_data(img, img.wgpu.tex, data);

fn push_debug_group(_: *Impl, __: CStr) void = ();
fn pop_debug_group(wgpu: *Impl) void = ();
