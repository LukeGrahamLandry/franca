// WebGPU 3D API wrapper 
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.

fn ASSERT_NOT_WEBSITE() void = ();
WGPU. :: import("@/graphics/web/webgpu.fr");

ROWPITCH_ALIGN :: (256);
MAX_UNIFORM_UPDATE_SIZE :: (1<<16); // also see WGPU.Limits.maxUniformBufferBindingSize
NUM_BINDGROUPS :: (2); // 0: uniforms, 1: images, samplers, storage buffers
UB_BINDGROUP_INDEX :: (0);
IMG_SMP_SBUF_BINDGROUP_INDEX :: (1);
MAX_UB_BINDGROUP_ENTRIES :: (SG_MAX_UNIFORMBLOCK_BINDSLOTS);
MAX_UB_BINDGROUP_BIND_SLOTS :: (2 * SG_MAX_UNIFORMBLOCK_BINDSLOTS);
MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES :: (SG_MAX_IMAGE_BINDSLOTS + SG_MAX_SAMPLER_BINDSLOTS + SG_MAX_STORAGEBUFFER_BINDSLOTS);
MAX_IMG_SMP_SBUF_BIND_SLOTS :: (128);

Buffer :: @struct {
    buf: WGPU.Buffer;
};

Image :: @struct {
    tex: WGPU.Texture;
    view: WGPU.TextureView;
};

Sampler :: @struct {
    smp: WGPU.Sampler;
};

shader_func_t :: @struct {
    module: WGPU.ShaderModule;
    entry: _sg_str_t;
};

Shader :: @struct {
    vertex_func: shader_func_t;
    fragment_func: shader_func_t;
    compute_func: shader_func_t;
    bgl_ub: WGPU.BindGroupLayout;
    bg_ub: WGPU.BindGroup;
    bgl_img_smp_sbuf: WGPU.BindGroupLayout;
    // a mapping of sokol-gfx bind slots to setBindGroup dynamic-offset-array indices
    ub_num_dynoffsets: u8;
    ub_dynoffsets: Array(u8, SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    // indexed by sokol-gfx bind slot:
    ub_grp0_bnd_n: Array(u8, SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    img_grp1_bnd_n: Array(u8, SG_MAX_IMAGE_BINDSLOTS);
    smp_grp1_bnd_n: Array(u8, SG_MAX_SAMPLER_BINDSLOTS);
    sbuf_grp1_bnd_n: Array(u8, SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Pipeline :: @struct {
    rpip: WGPU.RenderPipeline;
    cpip: WGPU.ComputePipeline;
    blend_color: WGPU.Color;
};

Attachment :: @struct {
    image: *Sg.Image.T;
    view: WGPU.TextureView;
};

Attachments :: @struct {
    colors: Array(Attachment, SG_MAX_COLOR_ATTACHMENTS);
    resolves: Array(Attachment, SG_MAX_COLOR_ATTACHMENTS);
    depth_stencil: Attachment;
};

// a pool of per-frame uniform buffers
typedef struct {
    uint32_t num_bytes;
    uint32_t offset;    // current offset into buf
    uint8_t* staging;   // intermediate buffer for uniform data updates
    WGPU.Buffer buf;     // the GPU-side uniform buffer
    uint32_t bind_offsets[SG_MAX_UNIFORMBLOCK_BINDSLOTS];   // NOTE: index is sokol-gfx ub slot index!
} uniform_buffer_t;

typedef struct {
    uint32_t id;
} bindgroup_handle_t;

typedef enum {
    BINDGROUPSCACHEITEMTYPE_NONE           = 0,
    BINDGROUPSCACHEITEMTYPE_IMAGE          = 0x00001111,
    BINDGROUPSCACHEITEMTYPE_SAMPLER        = 0x00002222,
    BINDGROUPSCACHEITEMTYPE_STORAGEBUFFER  = 0x00003333,
    BINDGROUPSCACHEITEMTYPE_PIPELINE       = 0x00004444,
} bindgroups_cache_item_type_t;

#define BINDGROUPSCACHEKEY_NUM_ITEMS (1 + MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES)
typedef struct {
    uint64_t hash;
    // the format of cache key items is BBBBTTTTIIIIIIII
    // where
    //  - BBBB is 2x the WGPU. binding
    //  - TTTT is the bindgroups_cache_item_type_t
    //  - IIIIIIII is the resource id
    //
    // where the item type is a per-resource-type bit pattern
    uint64_t items[BINDGROUPSCACHEKEY_NUM_ITEMS];
} bindgroups_cache_key_t;

typedef struct {
    uint32_t num;           // must be 2^n
    uint32_t index_mask;    // mask to turn hash into valid index
    bindgroup_handle_t* items;
} bindgroups_cache_t;

typedef struct {
    _sg_slot_t slot;
    WGPU.BindGroup bindgroup;
    bindgroups_cache_key_t key;
} bindgroup_t;

typedef struct {
    _sg_pool_t pool;
    bindgroup_t* bindgroups;
} bindgroups_pool_t;

typedef struct {
    struct {
        sg_buffer buffer;
        uint64_t offset;
    } vbs[SG_MAX_VERTEXBUFFER_BINDSLOTS];
    struct {
        sg_buffer buffer;
        uint64_t offset;
    } ib;
    bindgroup_handle_t bg;
} bindings_cache_t;

// the WGPU backend state
Impl :: @struct {
    sg: *Sg.Self;
    valid: bool;
    use_indexed_draw: bool;
    dev: WGPU.Device;
    limits: WGPU.SupportedLimits;
    queue: WGPU.Queue;
    cmd_enc: WGPU.CommandEncoder;
    rpass_enc: WGPU.RenderPassEncoder;
    cpass_enc: WGPU.ComputePassEncoder;
    empty_bind_group: WGPU.BindGroup;
    cur_pipeline: *Sg.Pipeline.T;
    cur_pipeline_id: Sg.Pipeline;
    uniform: uniform_buffer_t;
    bindings_cache: bindings_cache_t;
    bindgroups_cache: bindgroups_cache_t;
    bindgroups_pool: bindgroups_pool_t;
};

:: ASSERT_NOT_WEBSITE();
// FIXME: webgpu.h differences between Dawn and Emscripten webgpu.h
wgpuBufferReference :: wgpuBufferAddRef;
wgpuTextureReference :: wgpuTextureAddRef;
wgpuTextureViewReference :: wgpuTextureViewAddRef;
wgpuSamplerReference :: wgpuSamplerAddRef;
SType_ShaderModuleWGSLDescriptor :: WGPU.SType.ShaderSourceWGSL;

fn to_wgpu(b: bool) WGPU.OptionalBool = 
    @if(b, True, False);

fn to_wgpu(t: SgBufferType, u: SgUsage) WGPU.BufferUsage = {
    // FIXME: change to WGPU.BufferUsage once Emscripten and Dawn webgpu.h agree
    int res = 0;
    if (SG_BUFFERTYPE_VERTEXBUFFER == t) {
        res = WGPU.BufferUsage_Vertex;
    } else if (SG_BUFFERTYPE_STORAGEBUFFER == t) {
        res = WGPU.BufferUsage_Storage;
    } else {
        res = WGPU.BufferUsage_Index;
    }
    if (SG_USAGE_IMMUTABLE != u) {
        res |= WGPU.BufferUsage_CopyDst;
    }
    return (WGPU.BufferUsage)res;
}

fn to_wgpu(view: WGPU.TextureView, a: SgLoadAction) WGPU.LoadOp = 
    @if(view.is_null(), .Undefined,
        @map_enum(a) (CLEAR = .Clear, DONTCARE = .Clear, LOAD = .Load));

fn to_wgpu(view: WGPU.TextureView, it: SgStoreAction) WGPU.StoreOp = 
    @if(view.is_null(), Undefined, @map_enum(it) 
        (STORE = .Store, DONTCARE = .Discard));

fn to_wgpu(it: SgImageType) WGPU.TextureViewDimension = @map_enum(it)
    (_2D = ._2D, CUBE = .CUBE, _3D = ._3D, ARRAY = .2DArray);

fn to_wgpu(it: SgImageType) WGPU.TextureDimension = 
    @if(it == ._3D, ._3D, ._2D);

fn to_wgpu(it: SgImageSampleType, msaa: bool) WGPU.TextureSampleType = @map_enum(it)
    (FLOAT = @if(msaa, UnfilterableFloat, Float), DEPTH = .Depth, SINT = .Sint, UINT = .Uint, UNFILTERABLE_FLOAT = .UnfilterableFloat);

fn to_wgpu(t: SgSamplerType) WGPU.SamplerBindingType = @map_enum(it)
    (FILTERING = .Filtering, COMPARISON = .Comparison, NONFILTERING = .NonFiltering);

fn to_wgpu(it: SgWrap) WGPU.AddressMode = @map_enum(it)    // vvvvvv not supported?
    (REPEAT = .Repeat, CLAMP_TO_EDGE = .ClampToEdge, CLAMP_TO_BORDER = .ClampToEdge, MIRRORED_REPEAT = .MirrorRepeat);

fn to_wgpu(it: SgFilter) WGPU.FilterMode = @map_enum(it)
    (NEAREST = .Nearest, LINEAR = .Linear);

fn to_wgpu(it: SgFilter) WGPU.MipmapFilterMode = @map_enum(it)
    (NEAREST = .Nearest, LINEAR = .Linear);

// NOTE: there's no WGPU.IndexFormat_None
fn to_wgpu(it: SgIndexType) WGPU.IndexFormat = 
    @if(it == .UInt16, .Uint16, .Uint32);

fn to_wgpu(prim_type: SgPrimitiveType, idx_type: SgIndexType) WGPU.IndexFormat = {
    if (idx_type == SG_INDEXTYPE_NONE) {
        return WGPU.IndexFormat_Undefined;
    } else if ((prim_type == SG_PRIMITIVETYPE_LINE_STRIP) || (prim_type == SG_PRIMITIVETYPE_TRIANGLE_STRIP)) {
        return indexformat(idx_type);
    } else {
        return WGPU.IndexFormat_Undefined;
    }
}

fn to_wgpu(it: SgVertexStep) WGPU.VertexStepMode = 
    @if(it == .PER_VERTEX, .Vertex, .Instance);

fn to_wgpu(it: SgVertexFormat) WGPU.VertexFormat = @map_enum(it) (
    FLOAT = .Float32, FLOAT2 = .Float32x2, FLOAT3 = .Float32x3, FLOAT4 = .Float32x4, 
    INT   = .Sint32,  INT2   = .Sint32x2,  INT3   = .Sint32x3,  INT4   = .Sint32x4, 
    UINT  = .Uint32,  UINT2  = .Uint32x2,  UINT3  = .Uint32x3,  UINT4  = .Uint32x4, 
    BYTE4 = .Sint8x4, BYTE4N = .Snorm8x4,  UBYTE4 = .Uint8x4,  UBYTE4N = .Unorm8x4, 
    SHORT2 = .Sint16x2, SHORT2N = .Snorm16x2, USHORT2 = .Uint16x2, USHORT2N = .Unorm16x2, 
    SHORT4 = .Sint16x4, SHORT4N = .Snorm16x4, USHORT4 = .Uint16x4, USHORT4N = .Unorm16x4, 
    UINT10_N2 = .Unorm10_10_10_2, HALF2 = .Float16x2, HALF4 = .Float16x4,
);

fn to_wgpu(t: SgPrimitiveType) WGPU.PrimitiveTopology = @map_enum(it)
    (POINTS = .PointList, LINES = .LineList, LINE_STRIP = .LineStrip, TRIANGLES = .TriangleList, TRIANGLE_STRIP = .TriangleStrip);

fn to_wgpu(it: SgFaceWinding) WGPU.FrontFace = 
    @if(it == .CCW, .CCW, .CW);

fn to_wgpu(it: SgCullMode) WGPU.CullMode = @enum_map(it)
    (NONE = .None, FRONT = .Front, BACK = .Back);

fn to_wgpu(p: SgPixelFormat) WGPU.TextureFormat = @map_enum(it) (
    NONE = .Undefined, R8 = .R8Unorm, R8SN = .R8Snorm, 
    R8UI = .R8Uint, R8SI = .R8Sint, R16UI = .R16Uint, R16SI = .R16Sint, 
    R16F = .R16Float, RG8 = .RG8Unorm, RG8SN = .RG8Snorm, RG8UI = .RG8Uint, 
    RG8SI = .RG8Sint, R32UI = .R32Uint, R32SI = .R32Sint, R32F = .R32Float, 
    RG16UI = .RG16Uint, RG16SI = .RG16Sint, RG16F = .RG16Float, RGBA8 = .RGBA8Unorm, 
    SRGB8A8 = .RGBA8UnormSrgb, RGBA8SN = .RGBA8Snorm, RGBA8UI = .RGBA8Uint, 
    RGBA8SI = .RGBA8Sint, BGRA8 = .BGRA8Unorm, RGB10A2 = .RGB10A2Unorm, 
    RG11B10F = .RG11B10Ufloat, RG32UI = .RG32Uint, RG32SI = .RG32Sint, 
    RG32F = .RG32Float, RGBA16UI = .RGBA16Uint, RGBA16SI = .RGBA16Sint, 
    RGBA16F = .RGBA16Float, RGBA32UI = .RGBA32Uint, RGBA32SI = .RGBA32Sint, 
    RGBA32F = .RGBA32Float, DEPTH = .Depth32Float, 
    DEPTH_STENCIL = .Depth32FloatStencil8, BC1_RGBA = .BC1RGBAUnorm, 
    BC2_RGBA = .BC2RGBAUnorm, BC3_RGBA = .BC3RGBAUnorm, BC3_SRGBA = .BC3RGBAUnormSrgb, 
    BC4_R = .BC4RUnorm, BC4_RSN = .BC4RSnorm, BC5_RG = .BC5RGUnorm, 
    BC5_RGSN = .BC5RGSnorm, BC6H_RGBF = .BC6HRGBFloat, BC6H_RGBUF = .BC6HRGBUfloat,
    BC7_RGBA = .BC7RGBAUnorm, BC7_SRGBA = .BC7RGBAUnormSrgb, ETC2_RGB8 = .ETC2RGB8Unorm, 
    ETC2_RGB8A1 = .ETC2RGB8A1Unorm, ETC2_RGBA8 = .ETC2RGBA8Unorm, 
    ETC2_SRGB8 = .ETC2RGB8UnormSrgb, ETC2_SRGB8A8 = .ETC2RGBA8UnormSrgb, 
    EAC_R11 = .EACR11Unorm, EAC_R11SN = .EACR11Snorm, EAC_RG11 = .EACRG11Unorm,
    EAC_RG11SN = .EACRG11Snorm, RGB9E5 = .RGB9E5Ufloat, ASTC_4x4_RGBA = .ASTC4x4Unorm,
    ASTC_4x4_SRGBA = .ASTC4x4UnormSrgb,
    // NOT SUPPORTED
    R16 = .Undefined, .R16SN = .Undefined, RG16 = .Undefined, RG16SN = .Undefined, 
    RGBA16 = .Undefined, RGBA16SN = .Undefined,
);

fn to_wgpu(it: SgCompareFunc) WGPU.CompareFunction = 
    (NEVER = .Never, LESS = .Less, EQUAL = .Equal, LESS_EQUAL = .LessEqual, GREATER = .Greater, NOT_EQUAL = .NotEqual, GREATER_EQUAL = .GreaterEqual, ALWAYS = .Always);

fn to_wgpu(it: SgStencilOp) WGPU.StencilOperation = @map_enum(it)
    (KEEP = .Keep, ZERO = .Zero, REPLACE = .Replace, INCR_CLAMP = .IncrementClamp, DECR_CLAMP = .DecrementClamp, INVERT = .Infert, INCR_WRAP = .IncrementWrap, DECR_WRAP = .DecrementWrap);

fn to_wgpu(it: SgBlendOp) WGPU.BlendOperation = @map_enum(it)
    (ADD = .Add, SUBTRACT = .Subtract, REVERSE_SUBTRACT = .ReverseSubtract, MIN = .Min, MAX = .Max);

fn to_wgpu(it: SgBlendFactor) WGPU.BlendFactor = @map_enum(it) (
    ZERO = .Zero, ONE = .One, 
    SRC_COLOR = .Src, ONE_MINUS_SRC_COLOR = .OneMinusSrc,
    SRC_ALPHA = .SrcAlpha, ONE_MINUS_SRC_ALPHA = .OneMinusSrcAlpha,
    DST_COLOR = .Dst, ONE_MINUS_DST_COLOR = .OneMinusDst,
    DST_ALPHA = .DstAlpha, ONE_MINUS_DST_ALPHA = .OneMinusDstAlpha,
    SRC_ALPHA_SATURATED = .SrcAlphaSaturated,
    BLEND_COLOR = .Constant, ONE_MINUS_BLEND_COLOR = .OneMinusConstant,
    // FIXME: separate blend alpha value not supported?
    BLEND_ALPHA = .Constant, ONE_MINUS_BLEND_ALPHA = .OneMinusConstant,
);

fn colorwritemask(uint8_t m) WGPU.ColorWriteMask = {
    // FIXME: change to WGPU.ColorWriteMask once Emscripten and Dawn webgpu.h agree
    int res = 0;
    if (0 != (m & SG_COLORMASK_R)) {
        res |= WGPU.ColorWriteMask_Red;
    }
    if (0 != (m & SG_COLORMASK_G)) {
        res |= WGPU.ColorWriteMask_Green;
    }
    if (0 != (m & SG_COLORMASK_B)) {
        res |= WGPU.ColorWriteMask_Blue;
    }
    if (0 != (m & SG_COLORMASK_A)) {
        res |= WGPU.ColorWriteMask_Alpha;
    }
    return (WGPU.ColorWriteMask)res;
}

fn to_wgpu(it: SgShaderStage) WGPU.ShaderStage = @map_enum(it)
    (VERTEX = .Vertex, FRAGMENT = .Fragment, COMPUTE = .Compute);

fn init_caps(wgpu: *Impl) void = {
    wgpu.sg.backend = SG_BACKEND_WGPU;
    wgpu.sg.features = (
        origin_top_left = true,
        image_clamp_to_border = false,
        mrt_independent_blend_state = true,
        mrt_independent_write_mask = true,
        compute = true,
        msaa_image_bindings = true,
    );

    wgpuDeviceGetLimits(wgpu.dev, wgpu.limits&);
    l := wgpu.limits.limits&;
    wgpu.sg.limits = (
        max_image_size_2d = (int) l.maxTextureDimension2D,
        max_image_size_cube = (int) l.maxTextureDimension2D, // not a bug, see: https://github.com/gpuweb/gpuweb/issues/1327
        max_image_size_3d = (int) l.maxTextureDimension3D,
        max_image_size_array = (int) l.maxTextureDimension2D,
        max_image_array_layers = (int) l.maxTextureArrayLayers,
        max_vertex_attrs = SG_MAX_VERTEX_ATTRIBUTES,
    );
    
    f := mtl.sg.formats&;
    
    // NOTE: no WGPU.TextureFormat_R16Unorm
    @fill_formats(f) (
        sfbrm = (.R8, RG8, RGBA8, SRGB8A8, BGRA8, R16F, RG16F, RGBA16F, RGB10A2),
        srm = (),
        // NOTE: msaa rendering is possible in WebGPU, but no resolve
        // which is a combination that's not currently supported in sokol-gfx
        sr = (.R8UI, R8SI, RG8UI, RG8SI, RGBA8UI, RGBA8SI, R16UI, R16SI, RG16UI, 
            .RG16SI, RGBA16UI, RGBA16SI, R32UI, R32SI, RG32UI, RG32SI,RGBA32UI, RGBA32SI,
        ),
        // FIXME: can be made renderable via extension VV 
        sf = (.R8SN, RG8SN, RGBA8SN, RGB9E5, RG11B10F),   
        srmd = (.DEPTH, DEPTH_STENCIL),
    );
    
    if wgpuDeviceHasFeature(wgpu.dev, Float32Filterable) {
        @fill_formats(f) (sfr = (.R32F, RG32F, RGBA32F));
    } else {
        @fill_formats(f) (sr = (.R32F, RG32F, RGBA32F));
    };

    @if(wgpuDeviceHasFeature(wgpu.dev, TextureCompressionBC)) @fill_formats(f) (
        sf = (.BC1_RGBA, BC2_RGBA, BC3_RGBA, BC3_SRGBA, BC4_R, BC4_RSN, BC5_RG, BC5_RGSN, BC6H_RGBF, BC6H_RGBUF, BC7_RGBA, BC7_SRGBA),
    );
    @if(wgpuDeviceHasFeature(wgpu.dev, TextureCompressionETC2)) @fill_formats(f) (
        sf = (.ETC2_RGB8, ETC2_SRGB8, ETC2_RGB8A1, ETC2_RGBA8, ETC2_SRGB8A8, EAC_R11, EAC_R11SN, EAC_RG11, EAC_RG11SN);
    );
    @if(wgpuDeviceHasFeature(wgpu.dev, TextureCompressionASTC)) @fill_formats(f) (
        sf = (.ASTC_4x4_RGBA, ASTC_4x4_SRGBA),
    );
}

fn uniform_buffer_init(wgpu: *Impl, desc: *SgDesc) void = {
    @debug_assert(0 == wgpu.uniform.staging);
    @debug_assert(0 == wgpu.uniform.buf);

    // Add the max-uniform-update size (64 KB) to the requested buffer size,
    // this is to prevent validation errors in the WebGPU implementation
    // if the entire buffer size is used per frame. 64 KB is the allowed
    // max uniform update size on NVIDIA
    //
    // FIXME: is this still needed?
    wgpu.uniform.num_bytes = (uint32_t)(desc.uniform_buffer_size + MAX_UNIFORM_UPDATE_SIZE);
    wgpu.uniform.staging = (uint8_t*)_sg_malloc(wgpu.uniform.num_bytes);

    ub_desc: WGPU.BufferDescriptor = (
        size = wgpu.uniform.num_bytes,
        usage = WGPU.BufferUsage_Uniform|WGPU.BufferUsage_CopyDst,
    );
    wgpu.uniform.buf = wgpuDeviceCreateBuffer(wgpu.dev, ub_desc&);
    @debug_assert(wgpu.uniform.buf);
}

fn uniform_buffer_discard(wgpu: *Impl) void = {
    if (wgpu.uniform.buf) {
        wgpuBufferRelease(wgpu.uniform.buf);
        wgpu.uniform.buf = 0;
    }
    if (wgpu.uniform.staging) {
        _sg_free(wgpu.uniform.staging);
        wgpu.uniform.staging = 0;
    }
}

fn uniform_buffer_on_commit(wgpu: *Impl) void = {
    wgpuQueueWriteBuffer(wgpu.queue, wgpu.uniform.buf, 0, wgpu.uniform.staging, wgpu.uniform.offset);
    _sg_stats_add(wgpu.uniforms.size_write_buffer, wgpu.uniform.offset);
    wgpu.uniform.offset = 0;
    _sg_clear(wgpu.uniform.bind_offsets, sizeof(wgpu.uniform.bind_offsets));
}

fn bindgroups_pool_init(desc: *SgDesc) void = {
    @debug_assert((desc.wgpu_bindgroups_cache_size > 0) && (desc.wgpu_bindgroups_cache_size < _SG_MAX_POOL_SIZE));
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    @debug_assert(0 == p.bindgroups);
    const int pool_size = desc.wgpu_bindgroups_cache_size;
    _sg_pool_init(&p.pool, pool_size);
    size_t pool_byte_size = sizeof(bindgroup_t) * (size_t)p.pool.size;
    p.bindgroups = (bindgroup_t*) _sg_malloc_clear(pool_byte_size);
}

fn bindgroups_pool_discard() void = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    @debug_assert(p.bindgroups);
    _sg_free(p.bindgroups); p.bindgroups = 0;
    _sg_pool_discard(&p.pool);
}

fn bindgroup_at(bg_id: uint32_t) *bindgroup_t = {
    @debug_assert(SG_INVALID_ID != bg_id);
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    int slot_index = _sg_slot_index(bg_id);
    @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < p.pool.size));
    return &p.bindgroups[slot_index];
}

fn lookup_bindgroup(bg_id: uint32_t) *bindgroup_t = {
    if (SG_INVALID_ID != bg_id) {
        bindgroup_t* bg = bindgroup_at(bg_id);
        if (bg.slot.id == bg_id) {
            return bg;
        }
    }
    return 0;
}

fn alloc_bindgroup() bindgroup_handle_t = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    bindgroup_handle_t res;
    int slot_index = _sg_pool_alloc_index(&p.pool);
    if (_SG_INVALID_SLOT_INDEX != slot_index) {
        res.id = _sg_slot_alloc(&p.pool, &p.bindgroups[slot_index].slot, slot_index);
    } else {
        res.id = SG_INVALID_ID;
        _SG_ERROR(WGPU._BINDGROUPS_POOL_EXHAUSTED);
    }
    res
}

fn dealloc_bindgroup(bg: *bindgroup_t) void = {
    @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_ALLOC) && (bg.slot.id != SG_INVALID_ID));
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    _sg_pool_free_index(&p.pool, _sg_slot_index(bg.slot.id));
    _sg_slot_reset(&bg.slot);
}

fn reset_bindgroup_to_alloc_state(bg: *bindgroup_t) void = {
    @debug_assert(bg);
    _sg_slot_t slot = bg.slot;
    _sg_clear(bg, sizeof(bindgroup_t));
    bg.slot = slot;
    bg.slot.state = SG_RESOURCESTATE_ALLOC;
}

// MurmurHash64B (see: https://github.com/aappleby/smhasher/blob/61a0530f28277f2e850bfc39600ce61d02b518de/src/MurmurHash2.cpp#L142)
fn hash(const void* key: rawptr, len: int, seed: uint64_t) uint64_t = {
    const uint32_t m = 0x5bd1e995;
    const int r = 24;
    uint32_t h1 = (uint32_t)seed ^ (uint32_t)len;
    uint32_t h2 = (uint32_t)(seed >> 32);
    const uint32_t * data = (const uint32_t *)key;
    while (len >= 8) {
        uint32_t k1 = *data++;
        k1 *= m; k1 ^= k1 >> r; k1 *= m;
        h1 *= m; h1 ^= k1;
        len -= 4;
        uint32_t k2 = *data++;
        k2 *= m; k2 ^= k2 >> r; k2 *= m;
        h2 *= m; h2 ^= k2;
        len -= 4;
    }
    if (len >= 4) {
        uint32_t k1 = *data++;
        k1 *= m; k1 ^= k1 >> r; k1 *= m;
        h1 *= m; h1 ^= k1;
        len -= 4;
    }
    switch(len) {
        case 3: h2 ^= (uint32_t)(((unsigned char*)data)[2] << 16);
        case 2: h2 ^= (uint32_t)(((unsigned char*)data)[1] << 8);
        case 1: h2 ^= ((unsigned char*)data)[0];
        h2 *= m;
    };
    h1 ^= h2 >> 18; h1 *= m;
    h2 ^= h1 >> 22; h2 *= m;
    h1 ^= h2 >> 17; h1 *= m;
    h2 ^= h1 >> 19; h2 *= m;
    uint64_t h = h1;
    h = (h << 32) | h2;
    return h;
}

fn bindgroups_cache_item(type: bindgroups_cache_item_type_t, wgpu_binding: u8, id: u32) u64 = {
    // key pattern is bbbbttttiiiiiiii
    const uint64_t bb = (uint64_t)wgpu_binding;
    const uint64_t tttt = (uint64_t)type;
    const uint64_t iiiiiiii = (uint64_t)id;
    return (bb << 56) | (bb << 48) | (tttt << 32) | iiiiiiii;
}

fn void init_bindgroups_cache_key(wgpu: *Impl, key: *bindgroups_cache_key_t, bnd: *Sg.ResolvedBindings) void = {
    @debug_assert(bnd);
    @debug_assert(bnd.pip);
    const _sg_shader_t* shd = bnd.pip.shader;
    @debug_assert(shd && shd.slot.id == bnd.pip.cmn.shader_id.id);

    _sg_clear(key.items, sizeof(key.items));
    key.items[0] = bindgroups_cache_item(.PIPELINE, 0xFF, bnd.pip.slot.id);
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.imgs[i]);
        const size_t item_idx = i + 1;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(0 == key.items[item_idx]);
        const uint8_t wgpu_binding = shd.wgpu.img_grp1_bnd_n[i];
        const uint32_t id = bnd.imgs[i].slot.id;
        key.items[item_idx] = bindgroups_cache_item(.IMAGE, wgpu_binding, id);
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.smps[i]);
        const size_t item_idx = i + 1 + SG_MAX_IMAGE_BINDSLOTS;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(0 == key.items[item_idx]);
        const uint8_t wgpu_binding = shd.wgpu.smp_grp1_bnd_n[i];
        const uint32_t id = bnd.smps[i].slot.id;
        key.items[item_idx] = bindgroups_cache_item(.SAMPLER, wgpu_binding, id);
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.sbufs[i]);
        const size_t item_idx = i + 1 + SG_MAX_IMAGE_BINDSLOTS + SG_MAX_SAMPLER_BINDSLOTS;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(0 == key.items[item_idx]);
        const uint8_t wgpu_binding = shd.wgpu.sbuf_grp1_bnd_n[i];
        const uint32_t id = bnd.sbufs[i].slot.id;
        key.items[item_idx] = bindgroups_cache_item(.STORAGEBUFFER, wgpu_binding, id);
    }
    key.hash = hash(&key.items, (int)sizeof(key.items), 0x1234567887654321);
}

fn compare_bindgroups_cache_key(wgpu: *Impl, k0: *bindgroups_cache_key_t, k1: *bindgroups_cache_key_t) bool = {
    @debug_assert(k0 && k1);
    if (k0.hash != k1.hash) {
        return false;
    }
    if (memcmp(&k0.items, &k1.items, sizeof(k0.items)) != 0) {
        _sg_stats_add(wgpu.bindings.num_bindgroup_cache_hash_vs_key_mismatch, 1);
        return false;
    }
    true
}

fn create_bindgroup(wgpu: *Impl, bnd: *Sg.ResolvedBindings) *bindgroup_t = {
    @debug_assert(wgpu.dev);
    @debug_assert(bnd.pip);
    const _sg_shader_t* shd = bnd.pip.shader;
    @debug_assert(shd && (shd.slot.id == bnd.pip.cmn.shader_id.id));
    _sg_stats_add(wgpu.bindings.num_create_bindgroup, 1);
    bindgroup_handle_t bg_id = alloc_bindgroup();
    if (bg_id.id == SG_INVALID_ID) {
        return 0;
    }
    bindgroup_t* bg = bindgroup_at(bg_id.id);
    @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_ALLOC));

    // create wgpu bindgroup object (also see create_shader())
    WGPU.BindGroupLayout bgl = bnd.pip.shader.wgpu.bgl_img_smp_sbuf;
    @debug_assert(bgl);
    WGPU.BindGroupEntry bg_entries[MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES];
    _sg_clear(&bg_entries, sizeof(bg_entries));
    size_t bgl_index = 0;
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.imgs[i]);
        @debug_assert(bgl_index < MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
        WGPU.BindGroupEntry* bg_entry = &bg_entries[bgl_index];
        bg_entry.binding = shd.wgpu.img_grp1_bnd_n[i];
        bg_entry.textureView = bnd.imgs[i].wgpu.view;
        bgl_index += 1;
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.smps[i]);
        @debug_assert(bgl_index < MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
        WGPU.BindGroupEntry* bg_entry = &bg_entries[bgl_index];
        bg_entry.binding = shd.wgpu.smp_grp1_bnd_n[i];
        bg_entry.sampler = bnd.smps[i].wgpu.smp;
        bgl_index += 1;
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.sbufs[i]);
        @debug_assert(bgl_index < MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
        bg_entry := bg_entries&[bgl_index]&;
        bg_entry.binding = shd.wgpu.sbuf_grp1_bnd_n[i];
        bg_entry.buffer = bnd.sbufs[i].wgpu.buf;
        bg_entry.size = (uint64_t) bnd.sbufs[i].cmn.size;
        bgl_index += 1;
    }
    bg_desc: WGPU.BindGroupDescriptor = (
        layout = bgl;
        entryCount = bgl_index;
        entries = bg_entries;
    );
    bg.bindgroup = wgpuDeviceCreateBindGroup(wgpu.dev, bg_desc&);
    if (bg.bindgroup == 0) {
        _SG_ERROR(WGPU._CREATEBINDGROUP_FAILED);
        bg.slot.state = SG_RESOURCESTATE_FAILED;
        return bg;
    }
    init_bindgroups_cache_key(bg.key&, bnd);
    bg.slot.state = .VALID;
    bg
}

fn discard_bindgroup(wgpu: *Impl, bg: *bindgroup_t) void = {
    @debug_assert(bg);
    _sg_stats_add(wgpu.bindings.num_discard_bindgroup, 1);
    if (bg.slot.state == SG_RESOURCESTATE_VALID) {
        if (bg.bindgroup) {
            wgpuBindGroupRelease(bg.bindgroup);
            bg.bindgroup = 0;
        }
        reset_bindgroup_to_alloc_state(bg);
        @debug_assert(bg.slot.state == SG_RESOURCESTATE_ALLOC);
    }
    if (bg.slot.state == SG_RESOURCESTATE_ALLOC) {
        dealloc_bindgroup(bg);
        @debug_assert(bg.slot.state == SG_RESOURCESTATE_INITIAL);
    }
}

fn discard_all_bindgroups(wgpu: *Impl) void = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    for (int i = 0; i < p.pool.size; i++) {
        sg_resource_state state = p.bindgroups[i].slot.state;
        if ((state == SG_RESOURCESTATE_VALID) || (state == SG_RESOURCESTATE_FAILED)) {
            discard_bindgroup(&p.bindgroups[i]);
        }
    }
}

fn bindgroups_cache_init(wgpu: *Impl, desc: *SgDesc) void = {
    @debug_assert(desc);
    @debug_assert(wgpu.bindgroups_cache.num == 0);
    @debug_assert(wgpu.bindgroups_cache.index_mask == 0);
    @debug_assert(wgpu.bindgroups_cache.items == 0);
    const int num = desc.wgpu_bindgroups_cache_size;
    if (num <= 1) {
        _SG_PANIC(WGPU._BINDGROUPSCACHE_SIZE_GREATER_ONE);
    }
    if (!_sg_ispow2(num)) {
        _SG_PANIC(WGPU._BINDGROUPSCACHE_SIZE_POW2);
    }
    wgpu.bindgroups_cache.num = (uint32_t)desc.wgpu_bindgroups_cache_size;
    wgpu.bindgroups_cache.index_mask = wgpu.bindgroups_cache.num - 1;
    size_t size_in_bytes = sizeof(bindgroup_handle_t) * (size_t)num;
    wgpu.bindgroups_cache.items = (bindgroup_handle_t*)_sg_malloc_clear(size_in_bytes);
}

fn bindgroups_cache_discard(wgpu: *Impl) void = {
    if (wgpu.bindgroups_cache.items) {
        _sg_free(wgpu.bindgroups_cache.items);
        wgpu.bindgroups_cache.items = 0;
    }
    wgpu.bindgroups_cache.num = 0;
    wgpu.bindgroups_cache.index_mask = 0;
}

fn bindgroups_cache_set(wgpu: *Impl, hash: uint64_t, bg_id: uint32_t) void = {
    uint32_t index = hash & wgpu.bindgroups_cache.index_mask;
    @debug_assert(index < wgpu.bindgroups_cache.num);
    @debug_assert(wgpu.bindgroups_cache.items);
    wgpu.bindgroups_cache.items[index].id = bg_id;
}

fn bindgroups_cache_get(wgpu: *Impl, hash: uint64_t) uint32_t = {
    uint32_t index = hash & wgpu.bindgroups_cache.index_mask;
    @debug_assert(index < wgpu.bindgroups_cache.num);
    @debug_assert(wgpu.bindgroups_cache.items);
    return wgpu.bindgroups_cache.items[index].id;
}

// called from wgpu resource destroy functions to also invalidate any
// bindgroups cache slot and bindgroup referencing that resource
fn bindgroups_cache_invalidate(wgpu: *Impl, type: bindgroups_cache_item_type_t, id: uint32_t) void = {
    const uint64_t key_mask = 0x0000FFFFFFFFFFFF;
    const uint64_t key_item = bindgroups_cache_item(type, 0, id) & key_mask;
    @debug_assert(wgpu.bindgroups_cache.items);
    for (uint32_t cache_item_idx = 0; cache_item_idx < wgpu.bindgroups_cache.num; cache_item_idx++) {
        const uint32_t bg_id = wgpu.bindgroups_cache.items[cache_item_idx].id;
        if (bg_id != SG_INVALID_ID) {
            bindgroup_t* bg = lookup_bindgroup(bg_id);
            @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_VALID));
            // check if resource is in bindgroup, if yes discard bindgroup and invalidate cache slot
            bool invalidate_cache_item = false;
            for (int key_item_idx = 0; key_item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS; key_item_idx++) {
                if ((bg.key.items[key_item_idx] & key_mask) == key_item) {
                    invalidate_cache_item = true;
                    break;
                }
            }
            if (invalidate_cache_item) {
                discard_bindgroup(bg); bg = 0;
                bindgroups_cache_set(cache_item_idx, SG_INVALID_ID);
                _sg_stats_add(wgpu.bindings.num_bindgroup_cache_invalidates, 1);
            }
        }
    }
}

fn bindings_cache_clear(wgpu: *Impl) void = {
    memset(&wgpu.bindings_cache, 0, sizeof(wgpu.bindings_cache));
}

fn bindings_cache_vb_dirty(wgpu: *Impl, index: i64, vb: *Sg.Buffer.T, offset: uint64_t) bool = {
    @debug_assert((index >= 0) && (index < SG_MAX_VERTEXBUFFER_BINDSLOTS));
    if (vb) {
        return (wgpu.bindings_cache.vbs[index].buffer.id != vb.slot.id)
            || (wgpu.bindings_cache.vbs[index].offset != offset);
    } else {
        return wgpu.bindings_cache.vbs[index].buffer.id != SG_INVALID_ID;
    }
}

fn bindings_cache_vb_update(wgpu: *Impl, index: i64, vb: *Sg.Buffer.T, offset: u64) void = {
    @debug_assert((index >= 0) && (index < SG_MAX_VERTEXBUFFER_BINDSLOTS));
    if (vb) {
        wgpu.bindings_cache.vbs[index].buffer.id = vb.slot.id;
        wgpu.bindings_cache.vbs[index].offset = offset;
    } else {
        wgpu.bindings_cache.vbs[index].buffer.id = SG_INVALID_ID;
        wgpu.bindings_cache.vbs[index].offset = 0;
    }
}

fn bindings_cache_ib_dirty(wgpu: *Impl, ib: *Sg.Buffer.T, offset: u64) bool = {
    if (ib) {
        return (wgpu.bindings_cache.ib.buffer.id != ib.slot.id)
            || (wgpu.bindings_cache.ib.offset != offset);
    } else {
        return wgpu.bindings_cache.ib.buffer.id != SG_INVALID_ID;
    }
}

fn bindings_cache_ib_update(wgpu: *Impl, ib: *Sg.Buffer.T, offset: u64) void = {
    if (ib) {
        wgpu.bindings_cache.ib.buffer.id = ib.slot.id;
        wgpu.bindings_cache.ib.offset = offset;
    } else {
        wgpu.bindings_cache.ib.buffer.id = SG_INVALID_ID;
        wgpu.bindings_cache.ib.offset = 0;
    }
}

fn bindings_cache_bg_dirty(wgpu: *Impl, bg: *bindgroup_t) bool = {
    wgpu.bindings_cache.bg.id != @if(bg.is_null(), 0, bg.slot.id)
}

fn bindings_cache_bg_update(wgpu: *Impl, bg: *bindgroup_t) void = {
    wgpu.bindings_cache.bg.id = @if(bg.is_null(), 0, bg.slot.id);
}

fn set_img_smp_sbuf_bindgroup(wgpu: *Impl, bg: *bindgroup_t) void = {
    if !bindings_cache_bg_dirty(bg)) {
        _sg_stats_add(wgpu.bindings.num_skip_redundant_bindgroup, 1);
        return();
    };
    // else:
    
    bindings_cache_bg_update(bg);
    _sg_stats_add(wgpu.bindings.num_set_bindgroup, 1);
    if (wgpu.sg.cur_pass.is_compute) {
        @debug_assert(wgpu.cpass_enc);
        if (bg) {
            @debug_assert(bg.slot.state == SG_RESOURCESTATE_VALID);
            @debug_assert(bg.bindgroup);
            wgpuComputePassEncoderSetBindGroup(wgpu.cpass_enc, IMG_SMP_SBUF_BINDGROUP_INDEX, bg.bindgroup, 0, 0);
        } else {
            wgpuComputePassEncoderSetBindGroup(wgpu.cpass_enc, IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, 0);
        }
    } else {
        @debug_assert(wgpu.rpass_enc);
        if (bg) {
            @debug_assert(bg.slot.state == SG_RESOURCESTATE_VALID);
            @debug_assert(bg.bindgroup);
            wgpuRenderPassEncoderSetBindGroup(wgpu.rpass_enc, _SG_WGPU_IMG_SMP_SBUF_BINDGROUP_INDEX, bg.bindgroup, 0, 0);
        } else {
            wgpuRenderPassEncoderSetBindGroup(wgpu.rpass_enc, _SG_WGPU_IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, 0);
        }
    }
}

fn apply_bindgroup(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    if (!wgpu.sg.desc.wgpu_disable_bindgroups_cache) {
        bindgroup_t* bg = 0;
        bindgroups_cache_key_t key;
        init_bindgroups_cache_key(&key, bnd);
        uint32_t bg_id = bindgroups_cache_get(key.hash);
        if (bg_id != SG_INVALID_ID) {
            // potential cache hit
            bg = lookup_bindgroup(bg_id);
            @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_VALID));
            if (!compare_bindgroups_cache_key(&key, &bg.key)) {
                // cache collision, need to delete cached bindgroup
                _sg_stats_add(wgpu.bindings.num_bindgroup_cache_collisions, 1);
                discard_bindgroup(bg);
                bindgroups_cache_set(key.hash, SG_INVALID_ID);
                bg = 0;
            } else {
                _sg_stats_add(wgpu.bindings.num_bindgroup_cache_hits, 1);
            }
        } else {
            _sg_stats_add(wgpu.bindings.num_bindgroup_cache_misses, 1);
        }
        if (bg == 0) {
            // either no cache entry yet, or cache collision, create new bindgroup and store in cache
            bg = create_bindgroup(bnd);
            bindgroups_cache_set(key.hash, bg.slot.id);
        }
        if (bg && bg.slot.state == SG_RESOURCESTATE_VALID) {
            set_img_smp_sbuf_bindgroup(bg);
        } else {
            return false;
        }
    } else {
        // bindgroups cache disabled, create and destroy bindgroup on the fly (expensive!)
        bindgroup_t* bg = create_bindgroup(bnd);
        if (bg) {
            if (bg.slot.state == SG_RESOURCESTATE_VALID) {
                set_img_smp_sbuf_bindgroup(bg);
            }
            discard_bindgroup(bg);
        } else {
            return false;
        }
    };
    true
}

fn apply_index_buffer(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(wgpu.rpass_enc);
    const _sg_buffer_t* ib = bnd.ib;
    uint64_t offset = (uint64_t)bnd.ib_offset;
    if (bindings_cache_ib_dirty(ib, offset)) {
        bindings_cache_ib_update(ib, offset);
        if (ib) {
            const WGPU.IndexFormat format = indexformat(bnd.pip.cmn.index_type);
            const uint64_t buf_size = (uint64_t)ib.cmn.size;
            @debug_assert(buf_size > offset);
            const uint64_t max_bytes = buf_size - offset;
            wgpuRenderPassEncoderSetIndexBuffer(wgpu.rpass_enc, ib.wgpu.buf, format, offset, max_bytes);
        /* FIXME: the else-pass should actually set a null index buffer, but that doesn't seem to work yet
        } else {
            wgpuRenderPassEncoderSetIndexBuffer(wgpu.rpass_enc, 0, WGPU.IndexFormat_Undefined, 0, 0);
        */
        }
        _sg_stats_add(wgpu.bindings.num_set_index_buffer, 1);
    } else {
        _sg_stats_add(wgpu.bindings.num_skip_redundant_index_buffer, 1);
    };
    true
}

fn apply_vertex_buffers(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(wgpu.rpass_enc);
    for (uint32_t slot = 0; slot < SG_MAX_VERTEXBUFFER_BINDSLOTS; slot++) {
        const _sg_buffer_t* vb = bnd.vbs[slot];
        const uint64_t offset = (uint64_t)bnd.vb_offsets[slot];
        if (bindings_cache_vb_dirty(slot, vb, offset)) {
            bindings_cache_vb_update(slot, vb, offset);
            if (vb) {
                const uint64_t buf_size = (uint64_t)vb.cmn.size;
                @debug_assert(buf_size > offset);
                const uint64_t max_bytes = buf_size - offset;
                wgpuRenderPassEncoderSetVertexBuffer(wgpu.rpass_enc, slot, vb.wgpu.buf, offset, max_bytes);
            /* FIXME: the else-pass should actually set a null vertex buffer, but that doesn't seem to work yet
            } else {
                wgpuRenderPassEncoderSetVertexBuffer(wgpu.rpass_enc, slot, 0, 0, 0);
            */
            }
            _sg_stats_add(wgpu.bindings.num_set_vertex_buffer, 1);
        } else {
            _sg_stats_add(wgpu.bindings.num_skip_redundant_vertex_buffer, 1);
        }
    };
    true
}

fn setup_backend(wgpu: *Impl, desc: *SgDesc) void = {
    @debug_assert(!desc.environment.wgpu.device.is_null());
    @debug_assert(desc.uniform_buffer_size > 0);
    wgpu.sg.backend = SG_BACKEND_WGPU.;
    wgpu.valid = true;
    wgpu.dev = (WGPU.Device) desc.environment.wgpu.device;
    wgpu.queue = wgpuDeviceGetQueue(wgpu.dev);
    @debug_assert(wgpu.queue);

    init_caps();
    uniform_buffer_init(desc);
    bindgroups_pool_init(desc);
    bindgroups_cache_init(desc);
    bindings_cache_clear();

    // create an empty bind group
    bgl_desc := zeroed WGPU.BindGroupLayoutDescriptor;
    empty_bgl := wgpuDeviceCreateBindGroupLayout(wgpu.dev, &bgl_desc);
    @debug_assert(!empty_bgl.is_null());
    bg_desc := zeroed WGPU.BindGroupDescriptor;
    bg_desc.layout = empty_bgl;
    wgpu.empty_bind_group = wgpuDeviceCreateBindGroup(wgpu.dev, bg_desc&);
    @debug_assert(!wgpu.empty_bind_group.is_null());
    wgpuBindGroupLayoutRelease(empty_bgl);

    // create initial per-frame command encoder
    cmd_enc_desc := zereod WGPU.CommandEncoderDescriptor;
    wgpu.cmd_enc = wgpuDeviceCreateCommandEncoder(wgpu.dev, &cmd_enc_desc);
    @debug_assert(!wgpu.cmd_enc.is_null());
}

fn discard_backend(wgpu: *Impl) void = {
    @debug_assert(wgpu.valid);
    @debug_assert(wgpu.cmd_enc);
    wgpu.valid = false;
    discard_all_bindgroups();
    bindgroups_cache_discard();
    bindgroups_pool_discard();
    uniform_buffer_discard();
    wgpuBindGroupRelease(wgpu.empty_bind_group); wgpu.empty_bind_group = 0;
    wgpuCommandEncoderRelease(wgpu.cmd_enc); wgpu.cmd_enc = 0;
    wgpuQueueRelease(wgpu.queue); wgpu.queue = 0;
}

fn reset_state_cache(wgpu: *Impl) void = {
    bindings_cache_clear();
}

fn create_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, desc: *Sg.Buffer.Desc) SgResourceState = {
    @debug_assert(buf && desc);
    @debug_assert(buf.cmn.size > 0);
    const bool injected = (0 != desc.wgpu_buffer);
    if (injected) {
        buf.wgpu.buf = (WGPU.Buffer) desc.wgpu_buffer;
        wgpuBufferReference(buf.wgpu.buf);
    } else {
        // buffer mapping size must be multiple of 4, so round up buffer size (only a problem
        // with index buffers containing odd number of indices)
        const uint64_t wgpu_buf_size = _sg_roundup_u64((uint64_t)buf.cmn.size, 4);
        const bool map_at_creation = (SG_USAGE_IMMUTABLE == buf.cmn.usage) && (desc.data.ptr);
        wgpu_buf_desc: WGPU.BufferDescriptor = (
            usage = buffer_usage(buf.cmn.type, buf.cmn.usage),
            size = wgpu_buf_size,
            mappedAtCreation = map_at_creation,
            label = stringview(desc.label),
        );
        buf.wgpu.buf = wgpuDeviceCreateBuffer(wgpu.dev, wgpu_buf_desc&);
        if (0 == buf.wgpu.buf) {
            _SG_ERROR(WGPU._CREATE_BUFFER_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
        // NOTE: assume that WebGPU creates zero-initialized buffers
        if (map_at_creation) {
            @debug_assert(desc.data.ptr && (desc.data.size > 0));
            @debug_assert(desc.data.size <= (size_t)buf.cmn.size);
            // FIXME: inefficient on WASM
            void* ptr = wgpuBufferGetMappedRange(buf.wgpu.buf, 0, wgpu_buf_size);
            @debug_assert(ptr);
            memcpy(ptr, desc.data.ptr, desc.data.size);
            wgpuBufferUnmap(buf.wgpu.buf);
        }
    };
    .Valid
}

fn discard_buffer(wgpu: *Impl, buf: *Sg.Buffer.T) void = {
    @debug_assert(!buf.is_null());
    if (buf.cmn.type == .STORAGEBUFFER) {
        bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_STORAGEBUFFER, buf.slot.id);
    }
    if (buf.wgpu.buf) {
        wgpuBufferRelease(buf.wgpu.buf);
    }
}

fn copy_buffer_data(wgpu: *Impl, buf: *Sg.Buffer.T, offset: u64, data: []u8) void = {
    @debug_assert((offset + data.size) <= (size_t)buf.cmn.size);
    @debug_assert(!data.ptr.is_null() && data.size > 0);
    
    // WebGPU's write-buffer requires the size to be a multiple of four, so we may need to split the copy
    // operation into two writeBuffer calls
    uint64_t clamped_size = data.size & ~3UL;
    uint64_t extra_size = data.size & 3UL;
    @debug_assert(extra_size < 4);
    wgpuQueueWriteBuffer(wgpu.queue, buf.wgpu.buf, offset, data.ptr, clamped_size);
    if (extra_size > 0) {
        const uint64_t extra_src_offset = clamped_size;
        const uint64_t extra_dst_offset = offset + clamped_size;
        uint8_t extra_data[4] = { 0 };
        uint8_t* extra_src_ptr = ((uint8_t*)data.ptr) + extra_src_offset;
        for (size_t i = 0; i < extra_size; i++) {
            extra_data[i] = extra_src_ptr[i];
        }
        wgpuQueueWriteBuffer(wgpu.queue, buf.wgpu.buf, extra_dst_offset, extra_src_ptr, 4);
    }
}

fn copy_image_data(wgpu: *Impl, img: *Sg.Image.T, wgpu_tex: *WGPU.Texture, data: *SgImageData) void = {
    wgpu_copy_tex := zeroed WGPU.ImageCopyTexture;
    wgpu_copy_tex.texture = wgpu_tex;
    wgpu_copy_tex.aspect = .All;
    num_faces := @if(img.cmn.type == .CUBE, 6, 1);
    range(0, num_faces) { face_index |
        range(0, img.cmn.num_mipmaps) { mip_index |
            wgpu_copy_tex.mipLevel = (uint32_t)mip_index;
            wgpu_copy_tex.origin.z = (uint32_t)face_index;
            int mip_width = _sg_miplevel_dim(img.cmn.width, mip_index);
            int mip_height = _sg_miplevel_dim(img.cmn.height, mip_index);
            wgpu_layout: WGPU.TextureDataLayout = (
                offset = 0,
                bytesPerRow = (uint32_t)sg_row_pitch(img.cmn.pixel_format, mip_width, 1),
                rowsPerImage = (uint32_t)_sg_num_rows(img.cmn.pixel_format, mip_height),
            );
            if (_sg_is_compressed_pixel_format(img.cmn.pixel_format)) {
                mip_width = _sg_roundup(mip_width, 4);
                mip_height = _sg_roundup(mip_height, 4);
            }
            wgpu_extent: WGPU.Extent3D = (
                width = (uint32_t)mip_width,
                height = (uint32_t)mip_height,
                depthOrArrayLayers = (uint32_t) @match(img.cmn.type) {
                    fn CUBE() => 1;
                    fn _3D() => _sg_miplevel_dim(img.cmn.num_slices, mip_index);
                    @default => img.cmn.num_slices;
                },
            );
            mip_data := data.subimage&[face_index]&[mip_index];
            wgpuQueueWriteTexture(wgpu.queue, wgpu_copy_tex&, mip_data.ptr, mip_data.len, wgpu_layout&, wgpu_extent&);
        }
    }
}

fn create(wgpu: *Impl, img: *Sg.Image.T, desc: *Sg.Image.Desc) SgResourceState ={
    @debug_assert(img && desc);
    const bool injected = (0 != desc.wgpu_texture);
    if (injected) {
        img.wgpu.tex = bit_cast_unchecked(rawptr, WGPU.Texture, desc.wgpu_texture);
        wgpuTextureReference(img.wgpu.tex);
        img.wgpu.view = bit_cast_unchecked(rawptr, WGPU.TextureView, desc.wgpu_texture_view);
        if (img.wgpu.view) {
            wgpuTextureViewReference(img.wgpu.view);
        }
    } else {
        wgpu_tex_desc: WGPU.TextureDescriptor = (
            label = str(desc.label),
            usage = {
                WGPU.TextureUsage_TextureBinding|WGPU.TextureUsage_CopyDst;
                if (desc.render_target) {
                    wgpu_tex_desc.usage |= WGPU.TextureUsage_RenderAttachment;
                }
            }
            dimension = texture_dimension(img.cmn.type),
            size = (
                width = (uint32_t) img.cmn.width, 
                height = (uint32_t) img.cmn.height,
                depthOrArrayLayers = @if(desc.type == .CUBE, 6, (uint32_t) img.cmn.num_slices),
            ),
            format = textureformat(img.cmn.pixel_format),
            mipLevelCount = (uint32_t) img.cmn.num_mipmaps,
            sampleCount = (uint32_t) img.cmn.sample_count,
        );
        img.wgpu.tex = wgpuDeviceCreateTexture(wgpu.dev, wgpu_tex_desc&);
        if (0 == img.wgpu.tex) {
            _SG_ERROR(WGPU._CREATE_TEXTURE_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
        if ((img.cmn.usage == SG_USAGE_IMMUTABLE) && !img.cmn.render_target) {
            copy_image_data(img, img.wgpu.tex, &desc.data);
        }
        wgpu_texview_desc: WGPU.TextureViewDescriptor = (
            label = stringview(desc.label),
            dimension = texture_view_dimension(img.cmn.type),
            mipLevelCount = (uint32_t)img.cmn.num_mipmaps;
            arrayLayerCount = @match(img.cmn.type) {
                fn CUBE() => 6;
                fn ARRAY() => (uint32_t)img.cmn.num_slices;
                @default => 1;
            },
            aspect = if (_sg_is_depth_or_depth_stencil_format(img.cmn.pixel_format)) {
                .DepthOnly;
            } else {
               .All
            },
        );
        img.wgpu.view = wgpuTextureCreateView(img.wgpu.tex, wgpu_texview_desc&);
        if (0 == img.wgpu.view) {
            _SG_ERROR(WGPU._CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    };
    .VALID
}

fn discard_image(wgpu: *Impl, img: *Sg.Image.T) void = {
    @debug_assert(img);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_IMAGE, img.slot.id);
    if (img.wgpu.view) {
        wgpuTextureViewRelease(img.wgpu.view);
        img.wgpu.view = 0;
    }
    if (img.wgpu.tex) {
        wgpuTextureRelease(img.wgpu.tex);
        img.wgpu.tex = 0;
    }
}

fn sg_resource_state create_sampler(wgpu: *Impl, _sg_sampler_t* smp, const sg_sampler_desc* desc) {
    @debug_assert(smp && desc);
    @debug_assert(wgpu.dev);
    const bool injected = (0 != desc.wgpu_sampler);
    if (injected) {
        smp.wgpu.smp = (WGPU.Sampler) desc.wgpu_sampler;
        wgpuSamplerReference(smp.wgpu.smp);
    } else {
        wgpu_desc: WGPU.SamplerDescriptor = (
            label = str(desc.label);
            addressModeU = to_wgpu(desc.wrap_u),
            addressModeV = to_wgpu(desc.wrap_v),
            addressModeW = to_wgpu(desc.wrap_w),
            magFilter = sampler_minmag_filter(desc.mag_filter),
            minFilter = sampler_minmag_filter(desc.min_filter),
            mipmapFilter = sampler_mipmap_filter(desc.mipmap_filter),
            lodMinClamp = desc.min_lod,
            lodMaxClamp = desc.max_lod,
            maxAnisotropy = (uint16_t)desc.max_anisotropy,
            compare = {
                it := to_wgpu(desc.compare);
                @if(it == .Never, .Undefined, it)
            },
        );
        smp.wgpu.smp = wgpuDeviceCreateSampler(wgpu.dev, wgpu_desc&);
        if (0 == smp.wgpu.smp) {
            _SG_ERROR(WGPU._CREATE_SAMPLER_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    };
    .VALID
}

fn discard_sampler(wgpu: *Impl, smp: *Sg.Sampler.T) void = {
    @debug_assert(smp);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_SAMPLER, smp.slot.id);
    if (smp.wgpu.smp) {
        wgpuSamplerRelease(smp.wgpu.smp);
        smp.wgpu.smp = 0;
    }
}

fn create_shader_func(wgpu: *Impl, func: *sg_shader_function, label: CStr) shader_func_t = {
    @debug_assert(func);
    @debug_assert(func.source);
    @debug_assert(func.entry);

    shader_func_t res;
    _sg_clear(&res, sizeof(res));
    _sg_strcpy(&res.entry, func.entry);

    wgpu_shdmod_wgsl_desc: WGPU.ShaderModuleWGSLDescriptor = (
        chain = (sType = .ShaderModuleWGSLDescriptor),
        code = str(func.source),
    );

    wgpu_shdmod_desc: WGPU.ShaderModuleDescriptor = (
        nextInChain = wgpu_shdmod_wgsl_desc.chain&;
        label = str(label);
    );

    // NOTE: if compilation fails we won't actually find out in this call since
    // it always returns a valid module handle, and the GetCompilationInfo() call
    // is asynchronous
    res.module = wgpuDeviceCreateShaderModule(wgpu.dev, wgpu_shdmod_desc&);
    if (0 == res.module) {
        _SG_ERROR(WGPU._CREATE_SHADER_MODULE_FAILED);
    }
    return res;
}

fn discard_shader_func(wgpu: *Impl, func: *shader_func_t) void = {
    if (func.module) {
        wgpuShaderModuleRelease(func.module);
        func.module = 0;
    }
}

typedef struct { uint8_t sokol_slot, wgpu_slot; } dynoffset_mapping_t;

fn int dynoffset_cmp(const void* a, const void* b) {
    const dynoffset_mapping_t* aa = (const dynoffset_mapping_t*)a;
    const dynoffset_mapping_t* bb = (const dynoffset_mapping_t*)b;
    if (aa.wgpu_slot < bb.wgpu_slot) return -1;
    else if (aa.wgpu_slot > bb.wgpu_slot) return 1;
    return 0;
}

// TODO: paste from the metal one?
// NOTE: this is an out-of-range check for WGSL bindslots that's also active in release mode
fn bool ensure_wgsl_bindslot_ranges(const sg_shader_desc* desc) {
    @debug_assert(desc);
    for (size_t i = 0; i < SG_MAX_UNIFORMBLOCK_BINDSLOTS; i++) {
        if (desc.uniform_blocks[i].wgsl_group0_binding_n >= MAX_UB_BINDGROUP_BIND_SLOTS) {
            _SG_ERROR(WGPU._UNIFORMBLOCK_WGSL_GROUP0_BINDING_OUT_OF_RANGE);
            return false;
        }
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (desc.storage_buffers[i].wgsl_group1_binding_n >= MAX_IMG_SMP_SBUF_BIND_SLOTS) {
            _SG_ERROR(WGPU._STORAGEBUFFER_WGSL_GROUP1_BINDING_OUT_OF_RANGE);
            return false;
        }
    }
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (desc.images[i].wgsl_group1_binding_n >= MAX_IMG_SMP_SBUF_BIND_SLOTS) {
            _SG_ERROR(WGPU._IMAGE_WGSL_GROUP1_BINDING_OUT_OF_RANGE);
            return false;
        }
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (desc.samplers[i].wgsl_group1_binding_n >= MAX_IMG_SMP_SBUF_BIND_SLOTS) {
            _SG_ERROR(WGPU._SAMPLER_WGSL_GROUP1_BINDING_OUT_OF_RANGE);
            return false;
        }
    }
    return true;
}

fn sg_resource_state create_shader(wgpu: *Impl, _sg_shader_t* shd, const sg_shader_desc* desc) {
    @debug_assert(shd && desc);
    @debug_assert(shd.wgpu.vertex_func.module == 0);
    @debug_assert(shd.wgpu.fragment_func.module == 0);
    @debug_assert(shd.wgpu.compute_func.module == 0);
    @debug_assert(shd.wgpu.bgl_ub == 0);
    @debug_assert(shd.wgpu.bg_ub == 0);
    @debug_assert(shd.wgpu.bgl_img_smp_sbuf == 0);

    // do a release-mode bounds-check on wgsl bindslots, even though out-of-range
    // bindslots can't cause out-of-bounds accesses in the wgpu backend, this
    // is done to be consistent with the other backends
    if (!ensure_wgsl_bindslot_ranges(desc)) {
        return SG_RESOURCESTATE_FAILED;
    }

    // build shader modules
    shd_valid := true;
    if (desc.vertex_func.source) {
        shd.wgpu.vertex_func = create_shader_func(&desc.vertex_func, desc.label);
        shd_valid &= shd.wgpu.vertex_func.module != 0;
    }
    if (desc.fragment_func.source) {
        shd.wgpu.fragment_func = create_shader_func(&desc.fragment_func, desc.label);
        shd_valid &= shd.wgpu.fragment_func.module != 0;
    }
    if (desc.compute_func.source) {
        shd.wgpu.compute_func = create_shader_func(&desc.compute_func, desc.label);
        shd_valid &= shd.wgpu.compute_func.module != 0;
    }
    if (!shd_valid) {
        discard_shader_func(shd.wgpu.vertex_func&);
        discard_shader_func(shd.wgpu.fragment_func&);
        discard_shader_func(shd.wgpu.compute_func&);
        return SG_RESOURCESTATE_FAILED;
    }

    // create bind group layout and bind group for uniform blocks
    // NOTE also need to create a mapping of sokol ub bind slots to array indices
    // for the dynamic offsets array in the setBindGroup call
    @debug_assert(MAX_UB_BINDGROUP_ENTRIES <= MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    WGPU.BindGroupLayoutEntry bgl_entries[MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES];
    _sg_clear(bgl_entries, sizeof(bgl_entries));
    WGPU.BindGroupLayoutDescriptor bgl_desc;
    _sg_clear(&bgl_desc, sizeof(bgl_desc));
    WGPU.BindGroupEntry bg_entries[MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES];
    _sg_clear(&bg_entries, sizeof(bg_entries));
    WGPU.BindGroupDescriptor bg_desc;
    _sg_clear(&bg_desc, sizeof(bg_desc));
    dynoffset_mapping_t dynoffset_map[SG_MAX_UNIFORMBLOCK_BINDSLOTS];
    _sg_clear(dynoffset_map, sizeof(dynoffset_map));
    size_t bgl_index = 0;
    for (size_t i = 0; i < SG_MAX_UNIFORMBLOCK_BINDSLOTS; i++) {
        if (shd.cmn.uniform_blocks[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        shd.wgpu.ub_grp0_bnd_n[i] = desc.uniform_blocks[i].wgsl_group0_binding_n;
        WGPU.BindGroupEntry* bg_entry = &bg_entries[bgl_index];
        WGPU.BindGroupLayoutEntry* bgl_entry = &bgl_entries[bgl_index];
        bgl_entry.binding = shd.wgpu.ub_grp0_bnd_n[i];
        bgl_entry.visibility = shader_stage(shd.cmn.uniform_blocks[i].stage);
        bgl_entry.buffer.type = WGPU.BufferBindingType_Uniform;
        bgl_entry.buffer.hasDynamicOffset = true;
        bg_entry.binding = bgl_entry.binding;
        bg_entry.buffer = wgpu.uniform.buf;
        bg_entry.size = MAX_UNIFORM_UPDATE_SIZE;
        dynoffset_map[i].sokol_slot = i;
        dynoffset_map[i].wgpu_slot = bgl_entry.binding;
        bgl_index += 1;
    }
    bgl_desc.entryCount = bgl_index;
    bgl_desc.entries = bgl_entries;
    shd.wgpu.bgl_ub = wgpuDeviceCreateBindGroupLayout(wgpu.dev, &bgl_desc);
    @debug_assert(shd.wgpu.bgl_ub);
    bg_desc.layout = shd.wgpu.bgl_ub;
    bg_desc.entryCount = bgl_index;
    bg_desc.entries = bg_entries;
    shd.wgpu.bg_ub = wgpuDeviceCreateBindGroup(wgpu.dev, &bg_desc);
    @debug_assert(shd.wgpu.bg_ub);

    // sort the dynoffset_map by wgpu bindings, this is because the
    // dynamic offsets of the WebGPU setBindGroup call must be in
    // 'binding order', not 'bindgroup entry order'
    qsort(dynoffset_map, bgl_index, sizeof(dynoffset_mapping_t), dynoffset_cmp);
    shd.wgpu.ub_num_dynoffsets = bgl_index;
    for (uint8_t i = 0; i < bgl_index; i++) {
        const uint8_t sokol_slot = dynoffset_map[i].sokol_slot;
        shd.wgpu.ub_dynoffsets[sokol_slot] = i;
    }

    // create bind group layout for images, samplers and storage buffers
    _sg_clear(bgl_entries, sizeof(bgl_entries));
    _sg_clear(&bgl_desc, sizeof(bgl_desc));
    bgl_index = 0;
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        const bool msaa = shd.cmn.images[i].multisampled;
        shd.wgpu.img_grp1_bnd_n[i] = desc.images[i].wgsl_group1_binding_n;
        WGPU.BindGroupLayoutEntry* bgl_entry = &bgl_entries[bgl_index];
        bgl_entry.binding = shd.wgpu.img_grp1_bnd_n[i];
        bgl_entry.visibility = shader_stage(shd.cmn.images[i].stage);
        bgl_entry.texture.viewDimension = texture_view_dimension(shd.cmn.images[i].image_type);
        bgl_entry.texture.sampleType = texture_sample_type(shd.cmn.images[i].sample_type, msaa);
        bgl_entry.texture.multisampled = msaa;
        bgl_index += 1;
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        shd.wgpu.smp_grp1_bnd_n[i] = desc.samplers[i].wgsl_group1_binding_n;
        WGPU.BindGroupLayoutEntry* bgl_entry = &bgl_entries[bgl_index];
        bgl_entry.binding = shd.wgpu.smp_grp1_bnd_n[i];
        bgl_entry.visibility = shader_stage(shd.cmn.samplers[i].stage);
        bgl_entry.sampler.type = sampler_binding_type(shd.cmn.samplers[i].sampler_type);
        bgl_index += 1;
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        shd.wgpu.sbuf_grp1_bnd_n[i] = desc.storage_buffers[i].wgsl_group1_binding_n;
        WGPU.BindGroupLayoutEntry* bgl_entry = &bgl_entries[bgl_index];
        bgl_entry.binding = shd.wgpu.sbuf_grp1_bnd_n[i];
        bgl_entry.visibility = shader_stage(shd.cmn.storage_buffers[i].stage);
        if (shd.cmn.storage_buffers[i].readonly) {
            bgl_entry.buffer.type = WGPU.BufferBindingType_ReadOnlyStorage;
        } else {
            bgl_entry.buffer.type = WGPU.BufferBindingType_Storage;
        }
        bgl_index += 1;
    }
    bgl_desc.entryCount = bgl_index;
    bgl_desc.entries = bgl_entries;
    shd.wgpu.bgl_img_smp_sbuf = wgpuDeviceCreateBindGroupLayout(wgpu.dev, &bgl_desc);
    if (shd.wgpu.bgl_img_smp_sbuf == 0) {
        _SG_ERROR(WGPU._SHADER_CREATE_BINDGROUP_LAYOUT_FAILED);
        return SG_RESOURCESTATE_FAILED;
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard_shader(wgpu: *Impl, shd: *Sg.Shader.T) void = {
    @debug_assert(shd);
    discard_shader_func(shd.wgpu.vertex_func&);
    discard_shader_func(shd.wgpu.fragment_func&);
    discard_shader_func(shd.wgpu.compute_func&);
    if (shd.wgpu.bgl_ub) {
        wgpuBindGroupLayoutRelease(shd.wgpu.bgl_ub);
        shd.wgpu.bgl_ub = 0;
    }
    if (shd.wgpu.bg_ub) {
        wgpuBindGroupRelease(shd.wgpu.bg_ub);
        shd.wgpu.bg_ub = 0;
    }
    if (shd.wgpu.bgl_img_smp_sbuf) {
        wgpuBindGroupLayoutRelease(shd.wgpu.bgl_img_smp_sbuf);
        shd.wgpu.bgl_img_smp_sbuf = 0;
    }
}

fn to_wgpu(it: SgColor) GPU.Color = (
    r = cast it.r;
    g = cast it.g;
    b = cast it.b;
    a = cast it.a;
);

fn sg_resource_state create_pipeline(wgpu: *Impl, _sg_pipeline_t* pip, _sg_shader_t* shd, const sg_pipeline_desc* desc) {
    @debug_assert(pip && shd && desc);
    @debug_assert(desc.shader.id == shd.slot.id);
    @debug_assert(shd.wgpu.bgl_ub);
    @debug_assert(shd.wgpu.bgl_img_smp_sbuf);
    pip.shader = shd;

    pip.wgpu.blend_color = to_wgpu desc.blend_color;

    // - @group(0) for uniform blocks
    // - @group(1) for all image, sampler and storagebuffer resources
    WGPU.BindGroupLayout wgpu_bgl[NUM_BINDGROUPS];
    _sg_clear(&wgpu_bgl, sizeof(wgpu_bgl));
    wgpu_bgl[UB_BINDGROUP_INDEX ] = shd.wgpu.bgl_ub;
    wgpu_bgl[IMG_SMP_SBUF_BINDGROUP_INDEX] = shd.wgpu.bgl_img_smp_sbuf;
    WGPU.PipelineLayoutDescriptor wgpu_pl_desc;
    _sg_clear(&wgpu_pl_desc, sizeof(wgpu_pl_desc));
    wgpu_pl_desc.bindGroupLayoutCount = NUM_BINDGROUPS;
    wgpu_pl_desc.bindGroupLayouts = &wgpu_bgl[0];
    const WGPU.PipelineLayout wgpu_pip_layout = wgpuDeviceCreatePipelineLayout(wgpu.dev, &wgpu_pl_desc);
    if (0 == wgpu_pip_layout) {
        _SG_ERROR(WGPU._CREATE_PIPELINE_LAYOUT_FAILED);
        return SG_RESOURCESTATE_FAILED;
    }

    if (pip.cmn.is_compute) {
        WGPU.ComputePipelineDescriptor wgpu_pip_desc;
        _sg_clear(&wgpu_pip_desc, sizeof(wgpu_pip_desc));
        wgpu_pip_desc.label = stringview(desc.label);
        wgpu_pip_desc.layout = wgpu_pip_layout;
        wgpu_pip_desc.compute.module = shd.wgpu.compute_func.module;
        wgpu_pip_desc.compute.entryPoint = shd.wgpu.compute_func.entry.buf;
        pip.wgpu.cpip = wgpuDeviceCreateComputePipeline(wgpu.dev, &wgpu_pip_desc);
        wgpuPipelineLayoutRelease(wgpu_pip_layout);
        if (0 == pip.wgpu.cpip) {
            _SG_ERROR(WGPU._CREATE_COMPUTE_PIPELINE_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    } else {
        wgpu_vb_layouts := zeroed Array(WGPU.VertexBufferLayout, SG_MAX_VERTEXBUFFER_BINDSLOTS);
        wgpu_vtx_attrs := zeroed Array(Array(WGPU.VertexAttribute, SG_MAX_VERTEXBUFFER_BINDSLOTS), SG_MAX_VERTEX_ATTRIBUTES);
        int wgpu_vb_num = 0;
        for (int vb_idx = 0; vb_idx < SG_MAX_VERTEXBUFFER_BINDSLOTS; vb_idx++, wgpu_vb_num++) {
            const sg_vertex_buffer_layout_state* vbl_state = &desc.layout.buffers[vb_idx];
            if (0 == vbl_state.stride) {
                break;
            }
            wgpu_vb_layouts[vb_idx].arrayStride = (uint64_t)vbl_state.stride;
            wgpu_vb_layouts[vb_idx].stepMode = stepmode(vbl_state.step_func);
            wgpu_vb_layouts[vb_idx].attributes = &wgpu_vtx_attrs[vb_idx][0];
        }
        for (int va_idx = 0; va_idx < SG_MAX_VERTEX_ATTRIBUTES; va_idx++) {
            const sg_vertex_attr_state* va_state = &desc.layout.attrs[va_idx];
            if (SG_VERTEXFORMAT_INVALID == va_state.format) {
                break;
            }
            const int vb_idx = va_state.buffer_index;
            @debug_assert(vb_idx < SG_MAX_VERTEXBUFFER_BINDSLOTS);
            @debug_assert(pip.cmn.vertex_buffer_layout_active[vb_idx]);
            const size_t wgpu_attr_idx = wgpu_vb_layouts[vb_idx].attributeCount;
            wgpu_vb_layouts[vb_idx].attributeCount += 1;
            wgpu_vtx_attrs[vb_idx][wgpu_attr_idx].format = vertexformat(va_state.format);
            wgpu_vtx_attrs[vb_idx][wgpu_attr_idx].offset = (uint64_t)va_state.offset;
            wgpu_vtx_attrs[vb_idx][wgpu_attr_idx].shaderLocation = (uint32_t)va_idx;
        }

        // TODO: I think my policy is that stack frame is valid for the whole function regardless 
        //       of if the block ends. are we sure that's what i want to commit to? 
        //       tho since that's not true for loops it's kinda incoherent to make it true for 
        //       other blocks i guess? :Compiler
        
        wgpu_pip_desc: WGPU.RenderPipelineDescriptor = (
            label = str(desc.label);
            layout = wgpu_pip_layout;
            vertex = (
                module = shd.wgpu.vertex_func.module,
                entryPoint = shd.wgpu.vertex_func.entry.buf,
                bufferCount = (size_t)wgpu_vb_num,
                buffers = wgpu_vb_layouts&.as_ptr(),
            ),
            primitive = (
                topology = topology(desc.primitive_type),
                stripIndexFormat = stripindexformat(desc.primitive_type, desc.index_type),
                frontFace = frontface(desc.face_winding),
                cullMode = cullmode(desc.cull_mode),
            ),
            depthStencil = if desc.depth.pixel_format == .NONE {
                WGPU.DepthStencilState.ptr_from_int(0)
            } else {
                wgpu_ds_state: WGPU.DepthStencilState = (
                    format = to_wgpu(desc.depth.pixel_format),
                    depthWriteEnabled = to_wgpu(desc.depth.write_enabled),
                    depthCompare = to_wgpu(desc.depth.compare),
                    stencilFront = to_wgpu desc.stencil.front&,
                    stencilBack = to_wgpu desc.stencil.back&,
                    stencilReadMask = desc.stencil.read_mask,
                    stencilWriteMask = desc.stencil.write_mask,
                    depthBias = (int32_t)desc.depth.bias,
                    depthBiasSlopeScale = desc.depth.bias_slope_scale,
                    depthBiasClamp = desc.depth.bias_clamp,
                );
                wgpu_ds_state&
            },
            multisample = (
                count = (uint32_t)desc.sample_count;
                mask = 0xFFFFFFFF;
                alphaToCoverageEnabled = desc.alpha_to_coverage_enabled;
            ),
            fragment = if desc.color_count == 0 {
                WGPU.FragmentState.ptr_from_int(0)
            } else {
                wgpu_ctgt_state := zeroed Array(WGPU.ColorTargetState, SG_MAX_COLOR_ATTACHMENTS);
                wgpu_frag_state: WGPU.FragmentState = (
                    module = shd.wgpu.fragment_func.module,
                    entryPoint = shd.wgpu.fragment_func.entry.buf,
                    targetCount = (size_t)desc.color_count,
                    targets = wgpu_ctgt_state&.as_ptr(), 
                );
            
                wgpu_blend_state := @uninitialized Array(WGPU.BlendState, SG_MAX_COLOR_ATTACHMENTS);
                @debug_assert(desc.color_count < SG_MAX_COLOR_ATTACHMENTS);
                range(0, desc.color_count) { i |
                    dest := wgpu_ctgt_state&[i]&;
                    dest.format = textureformat(desc.colors[i].pixel_format);
                    dest.writeMask = colorwritemask(desc.colors[i].write_mask);
                    it := desc.colors[i].blend&;
                    if it.enabled {
                        dest.blend = wgpu_blend_state&[i]&;
                        dest.blend[] = (
                            color = (
                                operation = to_wgpu it.op_rgb,
                                srcFactor = to_wgpu it.src_factor_rgb,
                                dstFactor = to_wgpu it.dst_factor_rgb,
                            ),
                            alpha = (
                                operation = to_wgpu it.op_alpha,
                                srcFactor = to_wgpu it.src_factor_alpha,
                                dstFactor = to_wgpu it.dst_factor_alpha,
                            ),
                        );
                    }
                };
                wgpu_frag_state&
            },
        );
        pip.wgpu.rpip = wgpuDeviceCreateRenderPipeline(wgpu.dev, wgpu_pip_desc&);
        wgpuPipelineLayoutRelease(wgpu_pip_layout);
        if (0 == pip.wgpu.rpip) {
            _SG_ERROR(WGPU._CREATE_RENDER_PIPELINE_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    };
    .Valid
}

fn to_wgpu(it: *SgStencilFaceState) WGPU.StencilFaceState = (
    compare = to_wgpu it.compare,
    failOp = to_wgpu it.fail_op,
    depthFailOp = to_wgpu it.depth_fail_op,
    passOp = to_wgpu it.pass_op,
);

fn discard_pipeline(wgpu: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_PIPELINE, pip.slot.id);
    if (pip == wgpu.cur_pipeline) {
        wgpu.cur_pipeline = 0;
        wgpu.cur_pipeline_id.id = SG_INVALID_ID;
    }
    if (pip.wgpu.rpip) {
        wgpuRenderPipelineRelease(pip.wgpu.rpip);
        pip.wgpu.rpip = 0;
    }
    if (pip.wgpu.cpip) {
        wgpuComputePipelineRelease(pip.wgpu.cpip);
        pip.wgpu.cpip = 0;
    }
}

fn create_attachments(wgpu: *Impl, atts: *Sg.Attachments.T, color_images: **Sg.Image.T, resolve_images: **Sg.Image.T, ds_img: *Sg.Image.T, desc: *Sg.Attachments.Desc) SgResourceState = {
    @debug_assert(atts && desc);
    @debug_assert(color_images && resolve_images);

    // copy image pointers and create renderable wgpu texture views
    for (int i = 0; i < atts.cmn.num_colors; i++) {
        const sg_attachment_desc* color_desc = &desc.colors[i];
        _SOKOL_UNUSED(color_desc);
        @debug_assert(color_desc.image.id != SG_INVALID_ID);
        @debug_assert(0 == atts.wgpu.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        @debug_assert(color_images[i].wgpu.tex);
        atts.wgpu.colors[i].image = color_images[i];

        wgpu_color_view_desc := color_desc.to_wgpu();
        atts.wgpu.colors[i].view = wgpuTextureCreateView(color_images[i].wgpu.tex, &wgpu_color_view_desc);
        if (0 == atts.wgpu.colors[i].view) {
            _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }

        const sg_attachment_desc* resolve_desc = &desc.resolves[i];
        if (resolve_desc.image.id != SG_INVALID_ID) {
            @debug_assert(0 == atts.wgpu.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            @debug_assert(resolve_images[i].wgpu.tex);
            atts.wgpu.resolves[i].image = resolve_images[i];

            wgpu_resolve_view_desc := resolve_desc.to_wgpu();
            atts.wgpu.resolves[i].view = wgpuTextureCreateView(resolve_images[i].wgpu.tex, &wgpu_resolve_view_desc);
            if (0 == atts.wgpu.resolves[i].view) {
                _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
        }
    }
    @debug_assert(0 == atts.wgpu.depth_stencil.image);
    const sg_attachment_desc* ds_desc = &desc.depth_stencil;
    if (ds_desc.image.id != SG_INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        @debug_assert(ds_img.wgpu.tex);
        atts.wgpu.depth_stencil.image = ds_img;
        wgpu_ds_view_desc: WGPU.TextureViewDescriptor = (
            baseMipLevel = (uint32_t) ds_desc.mip_level,
            mipLevelCount = 1,
            baseArrayLayer = (uint32_t) ds_desc.slice,
            arrayLayerCount = 1,
        );
        atts.wgpu.depth_stencil.view = wgpuTextureCreateView(ds_img.wgpu.tex, wgpu_ds_view_desc&);
        if (0 == atts.wgpu.depth_stencil.view) {
            _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    }
    return SG_RESOURCESTATE_VALID;
}

fn to_wgpu(it: *SgAttachmentDesc) WGPU.TextureViewDescriptor = (
    baseMipLevel = (uint32_t) it.mip_level,
    mipLevelCount = 1,
    baseArrayLayer = (uint32_t) it.slice,
    arrayLayerCount = 1,
);

fn discard_attachments(wgpu: *Impl, atts: *Sg.Attachments.T) void = {
    @debug_assert(atts);
    for (int i = 0; i < atts.cmn.num_colors; i++) {
        if (atts.wgpu.colors[i].view) {
            wgpuTextureViewRelease(atts.wgpu.colors[i].view);
            atts.wgpu.colors[i].view = 0;
        }
        if (atts.wgpu.resolves[i].view) {
            wgpuTextureViewRelease(atts.wgpu.resolves[i].view);
            atts.wgpu.resolves[i].view = 0;
        }
    }
    if (atts.wgpu.depth_stencil.view) {
        wgpuTextureViewRelease(atts.wgpu.depth_stencil.view);
        atts.wgpu.depth_stencil.view = 0;
    }
}

fn attachments_color_image(wgpu: *Impl, atts: *Sg.Attachments.T, index: i32) ?*Sg.Image.T {
    @debug_assert(atts && (index >= 0) && (index < SG_MAX_COLOR_ATTACHMENTS));
    // NOTE: may return null
    return atts.wgpu.colors[index].image;
}

fn attachments_resolve_image(wgpu: *Impl, atts: *Sg.Attachments.T, index: i32) ?*Sg.Image.T {
    @debug_assert(atts && (index >= 0) && (index < SG_MAX_COLOR_ATTACHMENTS));
    // NOTE: may return null
    return atts.wgpu.resolves[index].image;
}

fn attachments_ds_image(wgpu: *Impl, atts: *Sg.Attachments.T) ?*Sg.Image.T {
    // NOTE: may return null
    @debug_assert(atts);
    return atts.wgpu.depth_stencil.image;
}

fn init_color_att(wgpu_att: *WGPU.RenderPassColorAttachment, action: *SgColorAttachmentAction, color_view: *WGPU.TextureView, resolve_view: *WGPU.TextureView) void = (
    view = color_view,
    resolveTarget = resolve_view,
    loadOp = load_op(color_view, action.load_action),
    storeOp = store_op(color_view, action.store_action),
    clearValue = to_wgpu action.clear_value,
);

fn init_ds_att(wgpu_att: *WGPU.RenderPassDepthStencilAttachment, action: *SgPassAction, fmt: SgPixelFormat, view: WGPU.TextureView) void = {
    wgpu_att.view = view;
    wgpu_att.depthLoadOp = load_op(view, action.depth.load_action);
    wgpu_att.depthStoreOp = store_op(view, action.depth.store_action);
    wgpu_att.depthClearValue = action.depth.clear_value;
    wgpu_att.depthReadOnly = false;
    if (_sg_is_depth_stencil_format(fmt)) {
        wgpu_att.stencilLoadOp = load_op(view, action.stencil.load_action);
        wgpu_att.stencilStoreOp = store_op(view, action.stencil.store_action);
    } else {
        wgpu_att.stencilLoadOp = .Undefined;
        wgpu_att.stencilStoreOp = .Undefined;
    }
    wgpu_att.stencilClearValue = action.stencil.clear_value;
    wgpu_att.stencilReadOnly = false;
}

fn begin_compute_pass(wgpu: *Impl, pass: *SgPass) void = {
    wgpu_pass_desc: WGPU.ComputePassDescriptor = (label = str(pass.label));
    wgpu.cpass_enc = wgpuCommandEncoderBeginComputePass(wgpu.cmd_enc, wgpu_pass_desc&);
    @debug_assert(wgpu.cpass_enc);
    // clear initial bindings
    wgpuComputePassEncoderSetBindGroup(wgpu.cpass_enc, UB_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, 0);
    wgpuComputePassEncoderSetBindGroup(wgpu.cpass_enc, IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, 0);
    _sg_stats_add(wgpu.bindings.num_set_bindgroup, 1);
}

fn begin_render_pass(wgpu: *Impl, pass: *SgPass) void = {
    atts := wgpu.sg.cur_pass.atts;
    swapchain := pass.swapchain&;
    action := pass.action&;

    WGPU.RenderPassDescriptor wgpu_pass_desc;
    WGPU.RenderPassColorAttachment wgpu_color_att[SG_MAX_COLOR_ATTACHMENTS];
    WGPU.RenderPassDepthStencilAttachment wgpu_ds_att;
    _sg_clear(&wgpu_pass_desc, sizeof(wgpu_pass_desc));
    _sg_clear(&wgpu_color_att, sizeof(wgpu_color_att));
    _sg_clear(&wgpu_ds_att, sizeof(wgpu_ds_att));
    wgpu_pass_desc.label = stringview(pass.label);
    if (atts) {
        @debug_assert(atts.slot.state == SG_RESOURCESTATE_VALID);
        for (int i = 0; i < atts.cmn.num_colors; i++) {
            init_color_att(&wgpu_color_att[i], &action.colors[i], atts.wgpu.colors[i].view, atts.wgpu.resolves[i].view);
        }
        wgpu_pass_desc.colorAttachmentCount = (size_t)atts.cmn.num_colors;
        wgpu_pass_desc.colorAttachments = &wgpu_color_att[0];
        if (atts.wgpu.depth_stencil.image) {
            init_ds_att(&wgpu_ds_att, action, atts.wgpu.depth_stencil.image.cmn.pixel_format, atts.wgpu.depth_stencil.view);
            wgpu_pass_desc.depthStencilAttachment = &wgpu_ds_att;
        }
    } else {
        WGPU.TextureView wgpu_color_view = (WGPU.TextureView) swapchain.wgpu.render_view;
        WGPU.TextureView wgpu_resolve_view = (WGPU.TextureView) swapchain.wgpu.resolve_view;
        WGPU.TextureView wgpu_depth_stencil_view = (WGPU.TextureView) swapchain.wgpu.depth_stencil_view;
        init_color_att(&wgpu_color_att[0], &action.colors[0], wgpu_color_view, wgpu_resolve_view);
        wgpu_pass_desc.colorAttachmentCount = 1;
        wgpu_pass_desc.colorAttachments = &wgpu_color_att[0];
        if (wgpu_depth_stencil_view) {
            @debug_assert(swapchain.depth_format > SG_PIXELFORMAT_NONE);
            init_ds_att(&wgpu_ds_att, action, swapchain.depth_format, wgpu_depth_stencil_view);
            wgpu_pass_desc.depthStencilAttachment = &wgpu_ds_att;
        }
    }
    wgpu.rpass_enc = wgpuCommandEncoderBeginRenderPass(wgpu.cmd_enc, wgpu_pass_desc&);
    @debug_assert(wgpu.rpass_enc);

    wgpuRenderPassEncoderSetBindGroup(wgpu.rpass_enc, UB_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, 0);
    wgpuRenderPassEncoderSetBindGroup(wgpu.rpass_enc, IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, 0);
    _sg_stats_add(wgpu.bindings.num_set_bindgroup, 1);
}

fn begin_pass(wgpu: *Impl, pass: *SgPass) void = {
    @debug_assert(pass);
    @debug_assert(wgpu.dev);
    @debug_assert(wgpu.cmd_enc);
    @debug_assert(0 == wgpu.rpass_enc);
    @debug_assert(0 == wgpu.cpass_enc);

    wgpu.cur_pipeline = 0;
    wgpu.cur_pipeline_id.id = SG_INVALID_ID;
    bindings_cache_clear();

    if (pass.compute) {
        begin_compute_pass(pass);
    } else {
        begin_render_pass(pass);
    }
}

fn end_pass(wgpu: *Impl) void = {
    if (wgpu.rpass_enc) {
        wgpuRenderPassEncoderEnd(wgpu.rpass_enc);
        wgpuRenderPassEncoderRelease(wgpu.rpass_enc);
        wgpu.rpass_enc = 0;
    }
    if (wgpu.cpass_enc) {
        wgpuComputePassEncoderEnd(wgpu.cpass_enc);
        wgpuComputePassEncoderRelease(wgpu.cpass_enc);
        wgpu.cpass_enc = 0;
    }
}

fn commit(wgpu: *Impl) void = {
    @debug_assert(wgpu.cmd_enc);

    uniform_buffer_on_commit();

    cmd_buf_desc := zeroed WGPU.CommandBufferDescriptor;
    wgpu_cmd_buf := wgpuCommandEncoderFinish(wgpu.cmd_enc, cmd_buf_desc&);
    @debug_assert(!wgpu_cmd_buf.is_null());
    wgpuCommandEncoderRelease(wgpu.cmd_enc);
    wgpu.cmd_enc = 0;

    wgpuQueueSubmit(wgpu.queue, 1, wgpu_cmd_buf&);
    wgpuCommandBufferRelease(wgpu_cmd_buf);

    // create a new render-command-encoder for next frame
    cmd_enc_desc := zeroed WGPU.CommandEncoderDescriptor;
    wgpu.cmd_enc = wgpuDeviceCreateCommandEncoder(wgpu.dev, cmd_enc_desc&);
}

fn apply_viewport(wgpu: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(wgpu.rpass_enc);
    // FIXME FIXME FIXME: CLIPPING THE VIEWPORT HERE IS WRONG!!!
    // (but currently required because WebGPU insists that the viewport rectangle must be
    // fully contained inside the framebuffer, but this doesn't make any sense, and also
    // isn't required by the backend APIs)
    const _sg_recti_t clip = _sg_clipi(x, y, w, h, wgpu.sg.cur_pass.width, wgpu.sg.cur_pass.height);
    float xf = (float) clip.x;
    float yf = (float) (origin_top_left ? clip.y : (wgpu.sg.cur_pass.height - (clip.y + clip.h)));
    float wf = (float) clip.w;
    float hf = (float) clip.h;
    wgpuRenderPassEncoderSetViewport(wgpu.rpass_enc, xf, yf, wf, hf, 0.0f, 1.0f);
}

fn apply_scissor_rect(wgpu: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(wgpu.rpass_enc);
    const _sg_recti_t clip = _sg_clipi(x, y, w, h, wgpu.sg.cur_pass.width, wgpu.sg.cur_pass.height);
    uint32_t sx = (uint32_t) clip.x;
    uint32_t sy = (uint32_t) (origin_top_left ? clip.y : (wgpu.sg.cur_pass.height - (clip.y + clip.h)));
    uint32_t sw = (uint32_t) clip.w;
    uint32_t sh = (uint32_t) clip.h;
    wgpuRenderPassEncoderSetScissorRect(wgpu.rpass_enc, sx, sy, sw, sh);
}

fn set_ub_bindgroup(wgpu: *Impl, shd: *Sg.Shader.T) void = {
    // NOTE: dynamic offsets must be in binding order, not in BindGroupEntry order
    @debug_assert(shd.wgpu.ub_num_dynoffsets < SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    uint32_t dyn_offsets[SG_MAX_UNIFORMBLOCK_BINDSLOTS];
    _sg_clear(dyn_offsets, sizeof(dyn_offsets));
    for (size_t i = 0; i < SG_MAX_UNIFORMBLOCK_BINDSLOTS; i++) {
        if (shd.cmn.uniform_blocks[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        uint8_t dynoffset_index = shd.wgpu.ub_dynoffsets[i];
        @debug_assert(dynoffset_index < shd.wgpu.ub_num_dynoffsets);
        dyn_offsets[dynoffset_index] = wgpu.uniform.bind_offsets[i];
    }
    if (wgpu.sg.cur_pass.is_compute) {
        @debug_assert(wgpu.cpass_enc);
        wgpuComputePassEncoderSetBindGroup(wgpu.cpass_enc,
            UB_BINDGROUP_INDEX,
            shd.wgpu.bg_ub,
            shd.wgpu.ub_num_dynoffsets,
            dyn_offsets);
    } else {
        @debug_assert(wgpu.rpass_enc);
        wgpuRenderPassEncoderSetBindGroup(wgpu.rpass_enc,
            UB_BINDGROUP_INDEX,
            shd.wgpu.bg_ub,
            shd.wgpu.ub_num_dynoffsets,
            dyn_offsets);
    }
}

fn apply_pipeline(wgpu: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip);
    @debug_assert(pip.shader && (pip.shader.slot.id == pip.cmn.shader_id.id));
    wgpu.cur_pipeline = pip;
    wgpu.cur_pipeline_id.id = pip.slot.id;
    if (pip.cmn.is_compute) {
        @debug_assert(wgpu.sg.cur_pass.is_compute);
        @debug_assert(pip.wgpu.cpip);
        @debug_assert(wgpu.cpass_enc);
        wgpuComputePassEncoderSetPipeline(wgpu.cpass_enc, pip.wgpu.cpip);
    } else {
        @debug_assert(!wgpu.sg.cur_pass.is_compute);
        @debug_assert(pip.wgpu.rpip);
        @debug_assert(wgpu.rpass_enc);
        wgpu.use_indexed_draw = (pip.cmn.index_type != SG_INDEXTYPE_NONE);
        wgpuRenderPassEncoderSetPipeline(wgpu.rpass_enc, pip.wgpu.rpip);
        wgpuRenderPassEncoderSetBlendConstant(wgpu.rpass_enc, &pip.wgpu.blend_color);
        wgpuRenderPassEncoderSetStencilReference(wgpu.rpass_enc, pip.cmn.stencil.ref);
    }
    // bind groups must be set because pipelines without uniform blocks or resource bindings
    // will still create 'empty' BindGroupLayouts
    set_ub_bindgroup(pip.shader);
    set_img_smp_sbuf_bindgroup(0); // this will set the 'empty bind group'
}

fn apply_bindings(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(bnd);
    @debug_assert(bnd.pip.shader && (bnd.pip.cmn.shader_id.id == bnd.pip.shader.slot.id));
    bool retval = true;
    if (!wgpu.sg.cur_pass.is_compute) {
        retval &= apply_index_buffer(bnd);
        retval &= apply_vertex_buffers(bnd);
    }
    retval &= apply_bindgroup(bnd);
    retval
}

fn apply_uniforms(wgpu: *Impl, ub_slot: i32, data: []u8) void = {
    const uint32_t alignment = wgpu.limits.limits.minUniformBufferOffsetAlignment;
    @debug_assert(wgpu.uniform.staging);
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert((wgpu.uniform.offset + data.size) <= wgpu.uniform.num_bytes);
    @debug_assert((wgpu.uniform.offset & (alignment - 1)) == 0);
    const _sg_pipeline_t* pip = wgpu.cur_pipeline;
    @debug_assert(pip && pip.shader);
    @debug_assert(pip.slot.id == wgpu.cur_pipeline_id.id);
    const _sg_shader_t* shd = pip.shader;
    @debug_assert(shd.slot.id == pip.cmn.shader_id.id);
    @debug_assert(data.size == shd.cmn.uniform_blocks[ub_slot].size);
    @debug_assert(data.size <= MAX_UNIFORM_UPDATE_SIZE);

    _sg_stats_add(wgpu.uniforms.num_set_bindgroup, 1);
    memcpy(wgpu.uniform.staging + wgpu.uniform.offset, data.ptr, data.size);
    wgpu.uniform.bind_offsets[ub_slot] = wgpu.uniform.offset;
    wgpu.uniform.offset = _sg_roundup_u32(wgpu.uniform.offset + (uint32_t)data.size, alignment);

    set_ub_bindgroup(shd);
}

fn draw(wgpu: *Impl, base_element: i32, num_elements: i32, num_instances: i32) void = {
    @debug_assert(wgpu.rpass_enc);
    @debug_assert(wgpu.cur_pipeline && (wgpu.cur_pipeline.slot.id == wgpu.cur_pipeline_id.id));
    if (SG_INDEXTYPE_NONE != wgpu.cur_pipeline.cmn.index_type) {
        wgpuRenderPassEncoderDrawIndexed(wgpu.rpass_enc, (uint32_t)num_elements, (uint32_t)num_instances, (uint32_t)base_element, 0, 0);
    } else {
        wgpuRenderPassEncoderDraw(wgpu.rpass_enc, (uint32_t)num_elements, (uint32_t)num_instances, (uint32_t)base_element, 0);
    }
}

fn dispatch(wgpu: *Impl, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void = {
    @debug_assert(!wgpu.cpass_enc.is_null());
    wgpuComputePassEncoderDispatchWorkgroups(wgpu.cpass_enc, num_groups_x, num_groups_y, num_groups_z);
}

fn update_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, data: []u8) void =
    wgpu.copy_buffer_data(buf, 0, data);

fn append_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, data: []u8, _new_frame: bool) void = 
    copy_buffer_data(buf, zext buf.cmn.append_pos, data);

fn update_image(wgpu: *Impl, img: *Sg.Image.T, data: *SgImageData) void #once = 
    copy_image_data(img, img.wgpu.tex, data);
