// WebGPU 3D API wrapper 
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.

WGPU :: import("@/graphics/web/webgpu.fr");

ROWPITCH_ALIGN :: (256);
MAX_UNIFORM_UPDATE_SIZE :: 1.shift_left(16); // also see WGPU.Limits.maxUniformBufferBindingSize
NUM_BINDGROUPS :: (2); // 0: uniforms, 1: images, samplers, storage buffers
UB_BINDGROUP_INDEX :: (0);
IMG_SMP_SBUF_BINDGROUP_INDEX :: (1);
MAX_UB_BINDGROUP_ENTRIES :: (Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
MAX_UB_BINDGROUP_BIND_SLOTS :: (2 * Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES :: (Sg.SG_MAX_IMAGE_BINDSLOTS + Sg.SG_MAX_SAMPLER_BINDSLOTS + Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS);
MAX_IMG_SMP_SBUF_BIND_SLOTS :: (128);

// TODO: maybe this should live in webgpu.fr for people who want to use that directly 
// generate nicer overloads of the imported wgpu methods 
// `wgpuFooBarBaz :: fn(foo: Foo);` -> `fn barBaz(foo: Foo);`
// This relies on TypeMeta.inferred_name + typename() working properly. 
:: @if(SOKOL_BACKEND == .WGPU) {
    #use("@/compiler/ast_external.fr");
    WGPU :: import("@/graphics/web/webgpu.fr");
    c := current_compiler_context();
    skips := 0;
    for WGPU.get_constants() { n |
        name := n.str();
        if name.starts_with("wgpu") {
            name = name.rest(4);
            fid := get_constant(FuncId, WGPU, n).unwrap();
            func := get_function_ast(fid, true, false, true, false);
            arg_types := c.arg_types(func.finished_arg.unwrap());
            first_arg_name := arg_types[0].typename().str();
            if name.starts_with(first_arg_name) {
                name = name.rest(first_arg_name.len);
                name := name.shallow_copy(ast_alloc()); // DO NOT MUTATE THE COMPILER'S STRINGS
                name[0] += 32;  // to_lower
                os := get_or_create_overloads(name.sym(), TOP_LEVEL_SCOPE, func.loc);
                add_to_overload_set(os, fid);
            } else {
                skips += 1;
            }
        };
    };
    @assert_eq(skips, 5, "new fuctions added to @/graphics/web/webgpu.fr ?")
};  // TODO: don't silently not run if you forget this semicolon

Buffer :: @struct {
    buf: WGPU.Buffer;
};

Image :: @struct {
    tex: WGPU.Texture;
    view: WGPU.TextureView;
};

Sampler :: @struct {
    smp: WGPU.Sampler;
};

shader_func_t :: @struct {
    module: WGPU.ShaderModule;
    entry: []u8;  // :LEAK
};

Shader :: @struct {
    vertex_func: shader_func_t;
    fragment_func: shader_func_t;
    compute_func: shader_func_t;
    bgl_ub: WGPU.BindGroupLayout;
    bg_ub: WGPU.BindGroup;
    bgl_img_smp_sbuf: WGPU.BindGroupLayout;
    // a mapping of sokol-gfx bind slots to setBindGroup dynamic-offset-array indices
    ub_num_dynoffsets: u8;
    ub_dynoffsets: Array(u8, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    // indexed by sokol-gfx bind slot:
    ub_grp0_bnd_n: Array(u8, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    img_grp1_bnd_n: Array(u8, Sg.SG_MAX_IMAGE_BINDSLOTS);
    smp_grp1_bnd_n: Array(u8, Sg.SG_MAX_SAMPLER_BINDSLOTS);
    sbuf_grp1_bnd_n: Array(u8, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Pipeline :: @struct {
    rpip: WGPU.RenderPipeline;
    cpip: WGPU.ComputePipeline;
    blend_color: WGPU.Color;
};

Attachments :: Sg'AttachmentsImpl(Impl, @struct {
    image: *Sg.Image.T;
    view: WGPU.TextureView;
});

// a pool of per-frame uniform buffers
uniform_buffer_t :: @struct {
    offset: i64;    // current offset into buf
    staging: []u8;   // intermediate buffer for uniform data updates (same size as gpu buffer)
    buf: WGPU.Buffer;     // the GPU-side uniform buffer
    bind_offsets: Array(u32, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);   // NOTE: index is sokol-gfx ub slot index!
};

bindgroup_handle_t :: @struct(id: u32);

bindgroups_cache_item_type_t :: @enum(u32) 
    (None = 0, Image = 0x00001111, Sampler = 0x00002222, StorageBuffer = 0x00003333, Pipeline = 0x00004444);

BINDGROUPSCACHEKEY_NUM_ITEMS :: (1 + MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);

bindgroups_cache_key_t :: @struct {
    hash: u64;
    // the format of cache key items is BBBBTTTTIIIIIIII
    // where
    //  - BBBB is 2x the WGPU. binding
    //  - TTTT is the bindgroups_cache_item_type_t
    //  - IIIIIIII is the resource id
    //
    // where the item type is a per-resource-type bit pattern
    items: Array(u64, BINDGROUPSCACHEKEY_NUM_ITEMS);
};

bindgroups_cache_t :: @struct {
    index_mask: i64;    // mask to turn hash into valid index
    items: []bindgroup_handle_t;  // len must be 2^n
} ;

bindgroup_t :: @struct {
    slot: Sg.SlotHeader;
    bindgroup: WGPU.BindGroup;
    key: bindgroups_cache_key_t;
};

bindings_cache_t :: @struct {
    vbs: Array(@struct {
        buffer: Sg.Buffer;
        offset: u64;
    }, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS);
    ib: @struct {
        buffer: Sg.Buffer;
        offset: u64;
    };
    bg: bindgroup_handle_t;
};

// the WGPU backend state
Impl :: @struct {
    sg: *Sg.Self;
    valid: bool;
    use_indexed_draw: bool;
    dev: WGPU.Device;
    limits: WGPU.Limits;
    queue: WGPU.Queue;
    cmd_enc: WGPU.CommandEncoder;
    rpass_enc: ?WGPU.RenderPassEncoder;
    cpass_enc: ?WGPU.ComputePassEncoder;
    empty_bind_group: WGPU.BindGroup;
    cur_pipeline: ?*Sg.Pipeline.T;
    cur_pipeline_id: Sg.Pipeline;
    uniform: uniform_buffer_t;
    bindings_cache: bindings_cache_t;
    bindgroups_cache: bindgroups_cache_t;
    bindgroups_pool: Sg.DynPool;
};

:: ASSERT_NOT_WEBSITE();

fn to_wgpu(b: bool) WGPU.OptionalBool = 
    @if(b, .True, .False);

fn to_wgpu(b: bool) WGPU.Bool = 
    @if(b, .True, .False);

fn to_wgpu(t: SgBufferType, u: SgUsage) WGPU.BufferUsage = {
    // FIXME: change to WGPU.BufferUsage once Emscripten and Dawn webgpu.h agree
    // TODO: ?^ sure seems like this is using WGPU.BufferUsage?
    
    // TODO: better things for flags and then put these numbers in the bindings instead so 
    //       you can sanely use wgpu from franca without the gfx abstraction layer. :LazyMagicNumbers
    res: u64 = @map_enum(t) (VERTEXBUFFER = 0x20, STORAGEBUFFER = 0x80, INDEXBUFFER = 0x10);
    (_ = res.bit_or(@if(u == .IMMUTABLE, 0, 0x8))) // CopyDst
}

fn to_wgpu(view: WGPU.TextureView, a: SgLoadAction) WGPU.LoadOp = 
    @if(view._.is_null(), .Undefined,
        @map_enum(a) (CLEAR = .Clear, DONTCARE = .Clear, LOAD = .Load));

fn to_wgpu(view: WGPU.TextureView, it: SgStoreAction) WGPU.StoreOp = 
    @if(view._.is_null(), .Undefined, @map_enum(it) 
        (STORE = .Store, DONTCARE = .Discard));

fn to_wgpu(it: SgImageType) WGPU.TextureViewDimension = @map_enum(it)
    (_2D = ._2D, CUBE = .Cube, _3D = ._3D, ARRAY = ._2DArray);

fn to_wgpu(it: SgImageType) WGPU.TextureDimension = 
    @if(it == ._3D, ._3D, ._2D);

fn to_wgpu(it: SgImageSampleType, msaa: bool) WGPU.TextureSampleType = @map_enum(it)
    (FLOAT = @if(msaa, .UnfilterableFloat, .Float), DEPTH = .Depth, SINT = .Sint, UINT = .Uint, UNFILTERABLE_FLOAT = .UnfilterableFloat);

fn to_wgpu(it: SgSamplerType) WGPU.SamplerBindingType = @map_enum(it)
    (FILTERING = .Filtering, COMPARISON = .Comparison, NONFILTERING = .NonFiltering);

fn to_wgpu(it: SgWrap) WGPU.AddressMode = @map_enum(it)    // vvvvvv not supported?
    (REPEAT = .Repeat, CLAMP_TO_EDGE = .ClampToEdge, CLAMP_TO_BORDER = .ClampToEdge, MIRRORED_REPEAT = .MirrorRepeat);

fn to_wgpu(it: SgFilter) WGPU.FilterMode = @map_enum(it)
    (NEAREST = .Nearest, LINEAR = .Linear);

fn to_wgpu(it: SgFilter) WGPU.MipmapFilterMode = @map_enum(it)
    (NEAREST = .Nearest, LINEAR = .Linear);

// NOTE: there's no WGPU.IndexFormat_None
fn to_wgpu(it: SgIndexType) WGPU.IndexFormat = {
    ::enum(@type it);
    @if(it == .UINT16, .Uint16, .Uint32)
}

fn to_wgpu(prim_type: SgPrimitiveType, idx_type: SgIndexType) WGPU.IndexFormat = {
    if(idx_type == .NONE, => return(.Undefined));
    ::enum(@type prim_type);
    if(@is(prim_type, .LINE_STRIP, .TRIANGLE_STRIP), => return(to_wgpu(idx_type)));
    .Undefined
}

fn to_wgpu(it: SgVertexStep) WGPU.VertexStepMode = {
    ::enum(@type it);
    @if(it == .PER_VERTEX, .Vertex, .Instance)
}

fn to_wgpu(it: SgVertexFormat) WGPU.VertexFormat = @map_enum(it) (
    FLOAT = .Float32, FLOAT2 = .Float32x2, FLOAT3 = .Float32x3, FLOAT4 = .Float32x4, 
    INT   = .Sint32,  INT2   = .Sint32x2,  INT3   = .Sint32x3,  INT4   = .Sint32x4, 
    UINT  = .Uint32,  UINT2  = .Uint32x2,  UINT3  = .Uint32x3,  UINT4  = .Uint32x4, 
    BYTE4 = .Sint8x4, BYTE4N = .Snorm8x4,  UBYTE4 = .Uint8x4,  UBYTE4N = .Unorm8x4, 
    SHORT2 = .Sint16x2, SHORT2N = .Snorm16x2, USHORT2 = .Uint16x2, USHORT2N = .Unorm16x2, 
    SHORT4 = .Sint16x4, SHORT4N = .Snorm16x4, USHORT4 = .Uint16x4, USHORT4N = .Unorm16x4, 
    UINT10_N2 = .Unorm10_10_10_2, HALF2 = .Float16x2, HALF4 = .Float16x4,
);

fn to_wgpu(it: SgPrimitiveType) WGPU.PrimitiveTopology = @map_enum(it)
    (POINTS = .PointList, LINES = .LineList, LINE_STRIP = .LineStrip, TRIANGLES = .TriangleList, TRIANGLE_STRIP = .TriangleStrip);

fn to_wgpu(it: SgFaceWinding) WGPU.FrontFace = 
    @if(it == .CCW, .CCW, .CW);

fn to_wgpu(it: SgCullMode) WGPU.CullMode = @map_enum(it)
    (NONE = .None, FRONT = .Front, BACK = .Back);

fn to_wgpu(it: SgPixelFormat) WGPU.TextureFormat = @map_enum(it) (
    NONE = .Undefined, R8 = .R8Unorm, R8SN = .R8Snorm, 
    R8UI = .R8Uint, R8SI = .R8Sint, R16UI = .R16Uint, R16SI = .R16Sint, 
    R16F = .R16Float, RG8 = .RG8Unorm, RG8SN = .RG8Snorm, RG8UI = .RG8Uint, 
    RG8SI = .RG8Sint, R32UI = .R32Uint, R32SI = .R32Sint, R32F = .R32Float, 
    RG16UI = .RG16Uint, RG16SI = .RG16Sint, RG16F = .RG16Float, RGBA8 = .RGBA8Unorm, 
    SRGB8A8 = .RGBA8UnormSrgb, RGBA8SN = .RGBA8Snorm, RGBA8UI = .RGBA8Uint, 
    RGBA8SI = .RGBA8Sint, BGRA8 = .BGRA8Unorm, RGB10A2 = .RGB10A2Unorm, 
    RG11B10F = .RG11B10Ufloat, RG32UI = .RG32Uint, RG32SI = .RG32Sint, 
    RG32F = .RG32Float, RGBA16UI = .RGBA16Uint, RGBA16SI = .RGBA16Sint, 
    RGBA16F = .RGBA16Float, RGBA32UI = .RGBA32Uint, RGBA32SI = .RGBA32Sint, 
    RGBA32F = .RGBA32Float, DEPTH = .Depth32Float, 
    DEPTH_STENCIL = .Depth32FloatStencil8, BC1_RGBA = .BC1RGBAUnorm, 
    BC2_RGBA = .BC2RGBAUnorm, BC3_RGBA = .BC3RGBAUnorm, BC3_SRGBA = .BC3RGBAUnormSrgb, 
    BC4_R = .BC4RUnorm, BC4_RSN = .BC4RSnorm, BC5_RG = .BC5RGUnorm, 
    BC5_RGSN = .BC5RGSnorm, BC6H_RGBF = .BC6HRGBFloat, BC6H_RGBUF = .BC6HRGBUfloat,
    BC7_RGBA = .BC7RGBAUnorm, BC7_SRGBA = .BC7RGBAUnormSrgb, ETC2_RGB8 = .ETC2RGB8Unorm, 
    ETC2_RGB8A1 = .ETC2RGB8A1Unorm, ETC2_RGBA8 = .ETC2RGBA8Unorm, 
    ETC2_SRGB8 = .ETC2RGB8UnormSrgb, ETC2_SRGB8A8 = .ETC2RGBA8UnormSrgb, 
    EAC_R11 = .EACR11Unorm, EAC_R11SN = .EACR11Snorm, EAC_RG11 = .EACRG11Unorm,
    EAC_RG11SN = .EACRG11Snorm, RGB9E5 = .RGB9E5Ufloat, ASTC_4x4_RGBA = .ASTC4x4Unorm,
    ASTC_4x4_SRGBA = .ASTC4x4UnormSrgb,
    // NOT SUPPORTED
    R16 = .Undefined, R16SN = .Undefined, RG16 = .Undefined, RG16SN = .Undefined, 
    RGBA16 = .Undefined, RGBA16SN = .Undefined,
);

fn to_wgpu(it: SgCompareFunc) WGPU.CompareFunction = @map_enum(it)
    (NEVER = .Never, LESS = .Less, EQUAL = .Equal, LESS_EQUAL = .LessEqual, GREATER = .Greater, NOT_EQUAL = .NotEqual, GREATER_EQUAL = .GreaterEqual, ALWAYS = .Always);

fn to_wgpu(it: SgStencilOp) WGPU.StencilOperation = @map_enum(it)
    (KEEP = .Keep, ZERO = .Zero, REPLACE = .Replace, INCR_CLAMP = .IncrementClamp, DECR_CLAMP = .DecrementClamp, INVERT = .Invert, INCR_WRAP = .IncrementWrap, DECR_WRAP = .DecrementWrap);

fn to_wgpu(it: SgBlendOp) WGPU.BlendOperation = @map_enum(it)
    (ADD = .Add, SUBTRACT = .Subtract, REVERSE_SUBTRACT = .ReverseSubtract, MIN = .Min, MAX = .Max);

fn to_wgpu(it: SgBlendFactor) WGPU.BlendFactor = @map_enum(it) (
    ZERO = .Zero, ONE = .One, 
    SRC_COLOR = .Src, ONE_MINUS_SRC_COLOR = .OneMinusSrc,
    SRC_ALPHA = .SrcAlpha, ONE_MINUS_SRC_ALPHA = .OneMinusSrcAlpha,
    DST_COLOR = .Dst, ONE_MINUS_DST_COLOR = .OneMinusDst,
    DST_ALPHA = .DstAlpha, ONE_MINUS_DST_ALPHA = .OneMinusDstAlpha,
    SRC_ALPHA_SATURATED = .SrcAlphaSaturated,
    BLEND_COLOR = .Constant, ONE_MINUS_BLEND_COLOR = .OneMinusConstant,
    // FIXME: separate blend alpha value not supported?
    BLEND_ALPHA = .Constant, ONE_MINUS_BLEND_ALPHA = .OneMinusConstant,
);

fn to_wgpu(m: SgColorMask) WGPU.ColorWriteMask = {
    // FIXME: change to WGPU.ColorWriteMask once Emscripten and Dawn webgpu.h agree
    //int res = 0;
    //if (0 != (m & SG_COLORMASK_R)) {
    //    res |= WGPU.ColorWriteMask_Red;
    //}
    //if (0 != (m & SG_COLORMASK_G)) {
    //    res |= WGPU.ColorWriteMask_Green;
    //}
    //if (0 != (m & SG_COLORMASK_B)) {
    //    res |= WGPU.ColorWriteMask_Blue;
    //}
    //if (0 != (m & SG_COLORMASK_A)) {
    //    res |= WGPU.ColorWriteMask_Alpha;
    //}
    //return (WGPU.ColorWriteMask)res;
    todo()
}

fn to_wgpu(it: SgShaderStage) WGPU.ShaderStage = (_ = @map_enum(it)
    (VERTEX = 1, FRAGMENT = 2, COMPUTE = 4));

fn init_caps(wgpu: *Impl) void = {
    wgpu.sg.features = (
        origin_top_left = true,
        image_clamp_to_border = false,
        mrt_independent_blend_state = true,
        mrt_independent_write_mask = true,
        compute = true,
        msaa_image_bindings = true,
    );

    getLimits(wgpu.dev, wgpu.limits&);
    l := wgpu.limits&;
    wgpu.sg.limits = (
        max_image_size_2d = bitcast l.maxTextureDimension2D,
        max_image_size_cube = bitcast l.maxTextureDimension2D, // not a bug, see: https://github.com/gpuweb/gpuweb/issues/1327
        max_image_size_3d = bitcast l.maxTextureDimension3D,
        max_image_size_array = bitcast l.maxTextureDimension2D,
        max_image_array_layers = bitcast l.maxTextureArrayLayers,
        max_vertex_attrs = Sg.SG_MAX_VERTEX_ATTRIBUTES,
    );
    
    f := wgpu.sg.formats&;
    
    // NOTE: no WGPU.TextureFormat_R16Unorm
    @fill_formats(f) (
        sfbrm = (.R8, .RG8, .RGBA8, .SRGB8A8, .BGRA8, .R16F, .RG16F, .RGBA16F, .RGB10A2),
        // NOTE: msaa rendering is possible in WebGPU, but no resolve
        // which is a combination that's not currently supported in sokol-gfx
        sr = (.R8UI, .R8SI, .RG8UI, .RG8SI, .RGBA8UI, .RGBA8SI, .R16UI, .R16SI, .RG16UI, 
            .RG16SI, .RGBA16UI, .RGBA16SI, .R32UI, .R32SI, .RG32UI, .RG32SI, .RGBA32UI, .RGBA32SI,
        ),
        // FIXME: can be made renderable via extension VV 
        sf = (.R8SN, .RG8SN, .RGBA8SN, .RGB9E5, .RG11B10F),   
        srmd = (.DEPTH, .DEPTH_STENCIL),
    );
    
    ::enum(WGPU.Bool);
    if hasFeature(wgpu.dev, .Float32Filterable) == .True {
        @fill_formats(f) (sfr = (.R32F, .RG32F, .RGBA32F));
    } else {
        @fill_formats(f) (sr = (.R32F, .RG32F, .RGBA32F));
    };

    @if(hasFeature(wgpu.dev, .TextureCompressionBC) == .True) @fill_formats(f) (
        sf = (.BC1_RGBA, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC4_R, .BC4_RSN, .BC5_RG, .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA),
    );
    @if(hasFeature(wgpu.dev, .TextureCompressionETC2) == .True) @fill_formats(f) (
        sf = (.ETC2_RGB8, .ETC2_SRGB8, .ETC2_RGB8A1, .ETC2_RGBA8, .ETC2_SRGB8A8, .EAC_R11, .EAC_R11SN, .EAC_RG11, .EAC_RG11SN),
    );
    @if(hasFeature(wgpu.dev, .TextureCompressionASTC) == .True) @fill_formats(f) (
        sf = (.ASTC_4x4_RGBA, .ASTC_4x4_SRGBA),
    );
}

fn uniform_buffer_init(wgpu: *Impl, desc: *SgDesc) void #once = {
    // Add the max-uniform-update size (64 KB) to the requested buffer size,
    // this is to prevent validation errors in the WebGPU implementation
    // if the entire buffer size is used per frame. 64 KB is the allowed
    // max uniform update size on NVIDIA
    //
    // FIXME: is this still needed?
    num_bytes := desc.uniform_buffer_size + MAX_UNIFORM_UPDATE_SIZE;
    wgpu.uniform.staging = wgpu.sg.desc.allocator.alloc(u8, intcast num_bytes);

    ub_desc: WGPU.BufferDescriptor = (
        size = num_bytes.intcast().bitcast(),
        usage = (_ = 0x48), // :LazyMagicNumbers WGPU.BufferUsage_Uniform|WGPU.BufferUsage_CopyDst,
    );
    wgpu.uniform.buf = createBuffer(wgpu.dev, ub_desc&);
    @debug_assert(!wgpu.uniform.buf._.is_null());
}

fn uniform_buffer_discard(wgpu: *Impl) void = {
    @debug_assert(!wgpu.uniform.buf.is_null(), "don't discard backend twice");
    wgpuBufferRelease(wgpu.uniform.buf);
    wgpu.sg.allocator.dealloc(u8, wgpu.uniform.staging);
}

fn uniform_buffer_on_commit(wgpu: *Impl) void = {
    writeBuffer(wgpu.queue, wgpu.uniform.buf, 0, wgpu.uniform.staging.ptr, wgpu.uniform.offset);
    wgpu.sg.stat(.wgpu_uniforms_size_write_buffer, trunc wgpu.uniform.offset);
    wgpu.uniform.offset = 0;
    wgpu.uniform.bind_offsets&.items().set_zeroed();
}

fn bindgroups_pool_discard() void = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    @debug_assert(p.bindgroups);
    _sg_free(p.bindgroups); p.bindgroups = 0;
    _sg_pool_discard(&p.pool);
}

fn bindgroup_at(bg_id: uint32_t) *bindgroup_t = {
    @debug_assert(SG_INVALID_ID != bg_id);
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    int slot_index = _sg_slot_index(bg_id);
    @debug_assert((slot_index > _SG_INVALID_SLOT_INDEX) && (slot_index < p.pool.size));
    return &p.bindgroups[slot_index];
}

fn lookup_bindgroup(bg_id: uint32_t) *bindgroup_t = {
    if (SG_INVALID_ID != bg_id) {
        bindgroup_t* bg = bindgroup_at(bg_id);
        if (bg.slot.id == bg_id) {
            return bg;
        }
    }
    return 0;
}

fn alloc_bindgroup() bindgroup_handle_t = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    bindgroup_handle_t res;
    int slot_index = _sg_pool_alloc_index(&p.pool);
    if (_SG_INVALID_SLOT_INDEX != slot_index) {
        res.id = _sg_slot_alloc(&p.pool, &p.bindgroups[slot_index].slot, slot_index);
    } else {
        res.id = SG_INVALID_ID;
        _SG_ERROR(WGPU._BINDGROUPS_POOL_EXHAUSTED);
    }
    res
}

fn dealloc_bindgroup(bg: *bindgroup_t) void = {
    @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_ALLOC) && (bg.slot.id != SG_INVALID_ID));
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    _sg_pool_free_index(&p.pool, _sg_slot_index(bg.slot.id));
    _sg_slot_reset(&bg.slot);
}

fn reset_bindgroup_to_alloc_state(bg: *bindgroup_t) void = {
    @debug_assert(bg);
    _sg_slot_t slot = bg.slot;
    _sg_clear(bg, sizeof(bindgroup_t));
    bg.slot = slot;
    bg.slot.state = SG_RESOURCESTATE_ALLOC;
}

// MurmurHash64B (see: https://github.com/aappleby/smhasher/blob/61a0530f28277f2e850bfc39600ce61d02b518de/src/MurmurHash2.cpp#L142)
fn hash(key: rawptr, len: int, seed: uint64_t) uint64_t = {
    const uint32_t m = 0x5bd1e995;
    const int r = 24;
    uint32_t h1 = (uint32_t)seed ^ (uint32_t)len;
    uint32_t h2 = (uint32_t)(seed >> 32);
    const uint32_t * data = (const uint32_t *)key;
    while (len >= 8) {
        uint32_t k1 = *data++;
        k1 *= m; k1 ^= k1 >> r; k1 *= m;
        h1 *= m; h1 ^= k1;
        len -= 4;
        uint32_t k2 = *data++;
        k2 *= m; k2 ^= k2 >> r; k2 *= m;
        h2 *= m; h2 ^= k2;
        len -= 4;
    }
    if (len >= 4) {
        uint32_t k1 = *data++;
        k1 *= m; k1 ^= k1 >> r; k1 *= m;
        h1 *= m; h1 ^= k1;
        len -= 4;
    }
    switch(len) {
        case 3: h2 ^= (uint32_t)(((unsigned char*)data)[2] << 16);
        case 2: h2 ^= (uint32_t)(((unsigned char*)data)[1] << 8);
        case 1: h2 ^= ((unsigned char*)data)[0];
        h2 *= m;
    };
    h1 ^= h2 >> 18; h1 *= m;
    h2 ^= h1 >> 22; h2 *= m;
    h1 ^= h2 >> 17; h1 *= m;
    h2 ^= h1 >> 19; h2 *= m;
    uint64_t h = h1;
    h = (h << 32) | h2;
    return h;
}

fn bindgroups_cache_item(type: bindgroups_cache_item_type_t, wgpu_binding: u8, id: u32) u64 = {
    // key pattern is bbbbttttiiiiiiii
    const uint64_t bb = (uint64_t)wgpu_binding;
    const uint64_t tttt = (uint64_t)type;
    const uint64_t iiiiiiii = (uint64_t)id;
    return (bb << 56) | (bb << 48) | (tttt << 32) | iiiiiiii;
}

fn init_bindgroups_cache_key(wgpu: *Impl, key: *bindgroups_cache_key_t, bnd: *Sg.ResolvedBindings) void = {
    @debug_assert(bnd);
    @debug_assert(bnd.pip);
    const _sg_shader_t* shd = bnd.pip.shader;
    @debug_assert(shd && shd.slot.id == bnd.pip.cmn.shader_id.id);

    _sg_clear(key.items, sizeof(key.items));
    key.items[0] = bindgroups_cache_item(.PIPELINE, 0xFF, bnd.pip.slot.id);
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.imgs[i]);
        const size_t item_idx = i + 1;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(0 == key.items[item_idx]);
        const uint8_t wgpu_binding = shd.mtl.img_grp1_bnd_n[i];
        const uint32_t id = bnd.imgs[i].slot.id;
        key.items[item_idx] = bindgroups_cache_item(.IMAGE, wgpu_binding, id);
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.smps[i]);
        const size_t item_idx = i + 1 + SG_MAX_IMAGE_BINDSLOTS;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(0 == key.items[item_idx]);
        const uint8_t wgpu_binding = shd.mtl.smp_grp1_bnd_n[i];
        const uint32_t id = bnd.smps[i].slot.id;
        key.items[item_idx] = bindgroups_cache_item(.SAMPLER, wgpu_binding, id);
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.sbufs[i]);
        const size_t item_idx = i + 1 + SG_MAX_IMAGE_BINDSLOTS + SG_MAX_SAMPLER_BINDSLOTS;
        @debug_assert(item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS);
        @debug_assert(0 == key.items[item_idx]);
        const uint8_t wgpu_binding = shd.mtl.sbuf_grp1_bnd_n[i];
        const uint32_t id = bnd.sbufs[i].slot.id;
        key.items[item_idx] = bindgroups_cache_item(.STORAGEBUFFER, wgpu_binding, id);
    }
    key.hash = hash(&key.items, (int)sizeof(key.items), 0x1234567887654321);
}

fn compare_bindgroups_cache_key(wgpu: *Impl, k0: *bindgroups_cache_key_t, k1: *bindgroups_cache_key_t) bool = {
    @debug_assert(k0 && k1);
    if (k0.hash != k1.hash) {
        return false;
    }
    if (memcmp(&k0.items, &k1.items, sizeof(k0.items)) != 0) {
        wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_hash_vs_key_mismatch, 1);
        return false;
    }
    true
}

fn create_bindgroup(wgpu: *Impl, bnd: *Sg.ResolvedBindings) *bindgroup_t = {
    @debug_assert(wgpu.dev);
    @debug_assert(bnd.pip);
    const _sg_shader_t* shd = bnd.pip.shader;
    @debug_assert(shd && (shd.slot.id == bnd.pip.cmn.shader_id.id));
    wgpu.sg.stat(wgpu.bindings.num_create_bindgroup, 1);
    bindgroup_handle_t bg_id = alloc_bindgroup();
    if (bg_id.id == SG_INVALID_ID) {
        return 0;
    }
    bindgroup_t* bg = bindgroup_at(bg_id.id);
    @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_ALLOC));

    // create wgpu bindgroup object (also see create_shader())
    WGPU.BindGroupLayout bgl = bnd.pip.shader.wgpu.bgl_img_smp_sbuf;
    @debug_assert(bgl);
    WGPU.BindGroupEntry bg_entries[MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES];
    _sg_clear(&bg_entries, sizeof(bg_entries));
    size_t bgl_index = 0;
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        if (shd.cmn.images[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.imgs[i]);
        @debug_assert(bgl_index < MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
        WGPU.BindGroupEntry* bg_entry = &bg_entries[bgl_index];
        bg_entry.binding = shd.mtl.img_grp1_bnd_n[i];
        bg_entry.textureView = bnd.imgs[i].wgpu.view;
        bgl_index += 1;
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        if (shd.cmn.samplers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.smps[i]);
        @debug_assert(bgl_index < MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
        WGPU.BindGroupEntry* bg_entry = &bg_entries[bgl_index];
        bg_entry.binding = shd.mtl.smp_grp1_bnd_n[i];
        bg_entry.sampler = bnd.smps[i].wgpu.smp;
        bgl_index += 1;
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        if (shd.cmn.storage_buffers[i].stage == SG_SHADERSTAGE_NONE) {
            continue;
        }
        @debug_assert(bnd.sbufs[i]);
        @debug_assert(bgl_index < MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
        bg_entry := bg_entries&[bgl_index]&;
        bg_entry.binding = shd.mtl.sbuf_grp1_bnd_n[i];
        bg_entry.buffer = bnd.sbufs[i].wgpu.buf;
        bg_entry.size = (uint64_t) bnd.sbufs[i].cmn.size;
        bgl_index += 1;
    }
    bg_desc: WGPU.BindGroupDescriptor = (
        layout = bgl;
        entryCount = bgl_index;
        entries = bg_entries;
    );
    bg.bindgroup = wgpuDeviceCreateBindGroup(wgpu.dev, bg_desc&);
    if (bg.bindgroup == 0) {
        _SG_ERROR(WGPU._CREATEBINDGROUP_FAILED);
        bg.slot.state = SG_RESOURCESTATE_FAILED;
        return bg;
    }
    init_bindgroups_cache_key(bg.key&, bnd);
    bg.slot.state = .VALID;
    bg
}

fn discard_bindgroup(wgpu: *Impl, bg: *bindgroup_t) void = {
    @debug_assert(bg);
    wgpu.sg.stat(wgpu.bindings.num_discard_bindgroup, 1);
    if (bg.slot.state == SG_RESOURCESTATE_VALID) {
        if (bg.bindgroup) {
            wgpuBindGroupRelease(bg.bindgroup);
            bg.bindgroup = 0;
        }
        reset_bindgroup_to_alloc_state(bg);
        @debug_assert(bg.slot.state == SG_RESOURCESTATE_ALLOC);
    }
    if (bg.slot.state == SG_RESOURCESTATE_ALLOC) {
        dealloc_bindgroup(bg);
        @debug_assert(bg.slot.state == SG_RESOURCESTATE_INITIAL);
    }
}

fn discard_all_bindgroups(wgpu: *Impl) void = {
    bindgroups_pool_t* p = &wgpu.bindgroups_pool;
    for (int i = 0; i < p.pool.size; i++) {
        sg_resource_state state = p.bindgroups[i].slot.state;
        if ((state == SG_RESOURCESTATE_VALID) || (state == SG_RESOURCESTATE_FAILED)) {
            discard_bindgroup(&p.bindgroups[i]);
        }
    }
}

_SG_PANIC :: _SG_ERROR;

fn bindgroups_cache_init(wgpu: *Impl, desc: *SgDesc) void #once = {
    num := desc.wgpu_bindgroups_cache_size;
    @if(num <= 1) _SG_PANIC("WGPU_BINDGROUPSCACHE_SIZE_GREATER_ONE");
    @if(!Sg'ispow2(num)) _SG_PANIC("WGPU_BINDGROUPSCACHE_SIZE_POW2");
    num: i64 = intcast desc.wgpu_bindgroups_cache_size;
    wgpu.bindgroups_cache = (
        index_mask = num - 1,
        items = desc.allocator.alloc_zeroed(bindgroup_handle_t, num),
    );
}

fn bindgroups_cache_set(wgpu: *Impl, hash: uint64_t, bg_id: uint32_t) void = {
    uint32_t index := hash & wgpu.bindgroups_cache.index_mask;
    wgpu.bindgroups_cache.items[index].id = bg_id;
}

fn bindgroups_cache_get(wgpu: *Impl, hash: uint64_t) uint32_t = {
    uint32_t index := hash & wgpu.bindgroups_cache.index_mask;
    return wgpu.bindgroups_cache.items[index].id;
}

// called from wgpu resource destroy functions to also invalidate any
// bindgroups cache slot and bindgroup referencing that resource
fn bindgroups_cache_invalidate(wgpu: *Impl, type: bindgroups_cache_item_type_t, id: uint32_t) void = {
    const uint64_t key_mask = 0x0000FFFFFFFFFFFF;
    const uint64_t key_item = bindgroups_cache_item(type, 0, id) & key_mask;
    @debug_assert(wgpu.bindgroups_cache.items);
    for (uint32_t cache_item_idx = 0; cache_item_idx < wgpu.bindgroups_cache.num; cache_item_idx++) {
        const uint32_t bg_id = wgpu.bindgroups_cache.items[cache_item_idx].id;
        if (bg_id != SG_INVALID_ID) {
            bindgroup_t* bg = lookup_bindgroup(bg_id);
            @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_VALID));
            // check if resource is in bindgroup, if yes discard bindgroup and invalidate cache slot
            bool invalidate_cache_item = false;
            for (int key_item_idx = 0; key_item_idx < BINDGROUPSCACHEKEY_NUM_ITEMS; key_item_idx++) {
                if ((bg.key.items[key_item_idx] & key_mask) == key_item) {
                    invalidate_cache_item = true;
                    break;
                }
            }
            if (invalidate_cache_item) {
                discard_bindgroup(bg); bg = 0;
                bindgroups_cache_set(cache_item_idx, SG_INVALID_ID);
                wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_invalidates, 1);
            }
        }
    }
}

fn bindings_cache_clear(wgpu: *Impl) void = {
    wgpu.bindings_cache = zeroed @type wgpu.bindings_cache;
}

fn bindings_cache_vb_dirty(wgpu: *Impl, index: i64, vb: *Sg.Buffer.T, offset: uint64_t) bool = {
    @debug_assert((index >= 0) && (index < SG_MAX_VERTEXBUFFER_BINDSLOTS));
    if (vb) {
        return (wgpu.bindings_cache.vbs[index].buffer.id != vb.slot.id)
            || (wgpu.bindings_cache.vbs[index].offset != offset);
    } else {
        return wgpu.bindings_cache.vbs[index].buffer.id != SG_INVALID_ID;
    }
}

fn bindings_cache_vb_update(wgpu: *Impl, index: i64, vb: *Sg.Buffer.T, offset: u64) void = {
    @debug_assert((index >= 0) && (index < SG_MAX_VERTEXBUFFER_BINDSLOTS));
    if (vb) {
        wgpu.bindings_cache.vbs[index].buffer.id = vb.slot.id;
        wgpu.bindings_cache.vbs[index].offset = offset;
    } else {
        wgpu.bindings_cache.vbs[index].buffer.id = SG_INVALID_ID;
        wgpu.bindings_cache.vbs[index].offset = 0;
    }
}

fn bindings_cache_ib_dirty(wgpu: *Impl, ib: *Sg.Buffer.T, offset: u64) bool = {
    if (ib) {
        return (wgpu.bindings_cache.ib.buffer.id != ib.slot.id)
            || (wgpu.bindings_cache.ib.offset != offset);
    } else {
        return wgpu.bindings_cache.ib.buffer.id != SG_INVALID_ID;
    }
}

fn bindings_cache_ib_update(wgpu: *Impl, ib: *Sg.Buffer.T, offset: u64) void = {
    if (ib) {
        wgpu.bindings_cache.ib.buffer.id = ib.slot.id;
        wgpu.bindings_cache.ib.offset = offset;
    } else {
        wgpu.bindings_cache.ib.buffer.id = SG_INVALID_ID;
        wgpu.bindings_cache.ib.offset = 0;
    }
}

fn bindings_cache_bg_dirty(wgpu: *Impl, bg: ?*bindgroup_t) bool = {
    wgpu.bindings_cache.bg.id != if(bg, fn(bg) => bg.slot.id, => 0)
}

fn bindings_cache_bg_update(wgpu: *Impl, bg: ?*bindgroup_t) void = {
    ::if_opt(*bindgroup_t, u32);
    wgpu.bindings_cache.bg.id = if(bg, fn(bg) => bg.slot.id, => 0);
}

fn set_img_smp_sbuf_bindgroup(wgpu: *Impl, bg: ?*bindgroup_t) void = {
    if !wgpu.bindings_cache_bg_dirty(bg) {
        wgpu.sg.stat(.wgpu_bindings_num_skip_redundant_bindgroup, 1);
        return();
    };
    // else:
    
    wgpu.bindings_cache_bg_update(bg);
    wgpu.sg.stat(.wgpu_bindings_num_set_bindgroup, 1);
    if (wgpu.sg.cur_pass.is_compute) {
        cpass_enc := wgpu.cpass_enc.unwrap();
        group := wgpu.empty_bind_group;
        if bg { bg |
            @debug_assert(bg.slot.state == .VALID && !bg.bindgroup._.is_null());
            group = bg.bindgroup;
        };
        cpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, group, 0, zeroed(*u32));
    } else {
        rpass_enc := wgpu.rpass_enc.unwrap();
        group := wgpu.empty_bind_group;
        if bg { bg |
            @debug_assert(bg.slot.state == .VALID && !bg.bindgroup._.is_null());
            group = bg.bindgroup;
        };
        rpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, group, 0, zeroed(*u32));
    }
}

fn apply_bindgroup(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    if (!wgpu.sg.desc.wgpu_disable_bindgroups_cache) {
        bg: ?*bindgroup_t = .None;
        bindgroups_cache_key_t key;
        init_bindgroups_cache_key(&key, bnd);
        uint32_t bg_id = bindgroups_cache_get(key.hash);
        if (bg_id != SG_INVALID_ID) {
            // potential cache hit
            bg = lookup_bindgroup(bg_id);
            @debug_assert(bg && (bg.slot.state == SG_RESOURCESTATE_VALID));
            if (!compare_bindgroups_cache_key(&key, &bg.key)) {
                // cache collision, need to delete cached bindgroup
                wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_collisions, 1);
                discard_bindgroup(bg);
                bindgroups_cache_set(key.hash, SG_INVALID_ID);
                bg = 0;
            } else {
                wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_hits, 1);
            }
        } else {
            wgpu.sg.stat(wgpu.bindings.num_bindgroup_cache_misses, 1);
        }
        if (bg == 0) {
            // either no cache entry yet, or cache collision, create new bindgroup and store in cache
            bg = create_bindgroup(bnd);
            bindgroups_cache_set(key.hash, bg.slot.id);
        }
        if (bg && bg.slot.state == SG_RESOURCESTATE_VALID) {
            set_img_smp_sbuf_bindgroup(bg);
        } else {
            return false;
        }
    } else {
        // bindgroups cache disabled, create and destroy bindgroup on the fly (expensive!)
        bindgroup_t* bg = create_bindgroup(bnd);
        if (bg) {
            if (bg.slot.state == SG_RESOURCESTATE_VALID) {
                set_img_smp_sbuf_bindgroup(bg);
            }
            discard_bindgroup(bg);
        } else {
            return false;
        }
    };
    true
}

fn apply_index_buffer(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(wgpu.rpass_enc);
    const _sg_buffer_t* ib = bnd.ib;
    uint64_t offset = (uint64_t)bnd.ib_offset;
    if (bindings_cache_ib_dirty(ib, offset)) {
        bindings_cache_ib_update(ib, offset);
        if (ib) {
            const WGPU.IndexFormat format = indexformat(bnd.pip.cmn.index_type);
            const uint64_t buf_size = (uint64_t)ib.cmn.size;
            @debug_assert(buf_size > offset);
            const uint64_t max_bytes = buf_size - offset;
            setIndexBuffer(wgpu.rpass_enc, ib.wgpu.buf, format, offset, max_bytes);
        /* FIXME: the else-pass should actually set a null index buffer, but that doesn't seem to work yet
        } else {
            setIndexBuffer(wgpu.rpass_enc, 0, WGPU.IndexFormat_Undefined, 0, 0);
        */
        }
        wgpu.sg.stat(wgpu.bindings.num_set_index_buffer, 1);
    } else {
        wgpu.sg.stat(wgpu.bindings.num_skip_redundant_index_buffer, 1);
    };
    true
}

fn apply_vertex_buffers(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(wgpu.rpass_enc);
    for (uint32_t slot = 0; slot < SG_MAX_VERTEXBUFFER_BINDSLOTS; slot++) {
        const _sg_buffer_t* vb = bnd.vbs[slot];
        const uint64_t offset = (uint64_t)bnd.vb_offsets[slot];
        if (bindings_cache_vb_dirty(slot, vb, offset)) {
            bindings_cache_vb_update(slot, vb, offset);
            if (vb) {
                const uint64_t buf_size = (uint64_t)vb.cmn.size;
                @debug_assert(buf_size > offset);
                const uint64_t max_bytes = buf_size - offset;
                setVertexBuffer(wgpu.rpass_enc, slot, vb.wgpu.buf, offset, max_bytes);
            /* FIXME: the else-pass should actually set a null vertex buffer, but that doesn't seem to work yet
            } else {
                setVertexBuffer(wgpu.rpass_enc, slot, 0, 0, 0);
            */
            }
            wgpu.sg.stat(wgpu.bindings.num_set_vertex_buffer, 1);
        } else {
            wgpu.sg.stat(wgpu.bindings.num_skip_redundant_vertex_buffer, 1);
        }
    };
    true
}

fn setup_backend(wgpu: *Impl, desc: *SgDesc) void = {
    @debug_assert(!desc.environment.wgpu.device.is_null());
    @debug_assert(desc.uniform_buffer_size > 0);
    wgpu.sg.backend = .WGPU;
    wgpu.valid = true;
    wgpu.dev = bit_cast_unchecked(rawptr, WGPU.Device, desc.environment.wgpu.device);
    wgpu.queue = getQueue(wgpu.dev);
    @debug_assert(!wgpu.queue._.is_null());
    wgpu.rpass_enc = .None;
    wgpu.cpass_enc = .None;

    wgpu.init_caps();
    wgpu.uniform_buffer_init(desc);
    init_pool(wgpu.bindgroups_pool&, desc.allocator, intcast desc.wgpu_bindgroups_cache_size, size_of bindgroup_t);
    wgpu.bindgroups_cache_init(desc);
    wgpu.bindings_cache_clear();

    // create an empty bind group
    empty_bgl := createBindGroupLayout(wgpu.dev, @ref WGPU.BindGroupLayoutDescriptor.zeroed());
    @debug_assert(!empty_bgl._.is_null());
    wgpu.empty_bind_group = createBindGroup(wgpu.dev, @ref @as(WGPU.BindGroupDescriptor) (
        layout = empty_bgl,
    ));
    @debug_assert(!wgpu.empty_bind_group._.is_null());
    release(empty_bgl);

    // create initial per-frame command encoder
    wgpu.cmd_enc = createCommandEncoder(wgpu.dev, @ref WGPU.CommandEncoderDescriptor.zeroed());
    @debug_assert(!wgpu.cmd_enc._.is_null());
}

fn discard_backend(wgpu: *Impl) void = {
    @debug_assert(wgpu.valid);
    @debug_assert(wgpu.cmd_enc);
    wgpu.valid = false;
    discard_all_bindgroups();
    wgpu.sg.desc.allocator.dealloc(bindgroup_handle_t, wgpu.bindgroups_cache.items);
    bindgroups_pool_discard();
    uniform_buffer_discard();
    wgpuBindGroupRelease(wgpu.empty_bind_group); wgpu.empty_bind_group = 0;
    wgpuCommandEncoderRelease(wgpu.cmd_enc); wgpu.cmd_enc = 0;
    wgpuQueueRelease(wgpu.queue); wgpu.queue = 0;
    
    wgpu = zeroed Impl;
}

fn reset_state_cache(wgpu: *Impl) void = {
    bindings_cache_clear();
}

fn create(wgpu: *Impl, buf: *Sg.Buffer.T, desc: *Sg.Buffer.Desc) SgResourceState #once = {
    @debug_assert(buf.cmn.size > 0);
    injected := !desc.wgpu_buffer.is_null();
    if (injected) {
        buf.mtl.buf = bit_cast_unchecked(rawptr, WGPU.Buffer, desc.wgpu_buffer);
        addRef(buf.mtl.buf);
    } else {
        // buffer mapping size must be multiple of 4, so round up buffer size (only a problem
        // with index buffers containing odd number of indices)
        wgpu_buf_size := Sg'roundup(buf.cmn.size, 4);
        ::enum(@type buf.cmn.usage);
        map_at_creation := buf.cmn.usage == .IMMUTABLE && !desc.data.ptr.is_null();
        buf.mtl.buf = createBuffer(wgpu.dev, @ref @as(WGPU.BufferDescriptor) (
            usage = to_wgpu(buf.cmn.type, buf.cmn.usage),
            size = wgpu_buf_size.zext().bitcast(),
            mappedAtCreation = to_wgpu map_at_creation,
            label = str(desc.label),
        ));
        if buf.mtl.buf._.is_null() {
            _SG_ERROR("WGPU_CREATE_BUFFER_FAILED");
            return(.FAILED);
        }
        // NOTE: assume that WebGPU creates zero-initialized buffers
        if map_at_creation {
            @debug_assert(!desc.data.ptr.is_null() && (desc.data.len > 0));
            @debug_assert(desc.data.len <= intcast buf.cmn.size);
            // FIXME: inefficient on WASM
            ptr := getMappedRange(buf.mtl.buf, 0, intcast wgpu_buf_size);
            @debug_assert(!ptr.is_null());
            ptr.slice(desc.data.len).copy_from(desc.data);
            unmap(buf.mtl.buf);
        }
    };
    .VALID
}

fn discard_buffer(wgpu: *Impl, buf: *Sg.Buffer.T) void = {
    @debug_assert(!buf.is_null());
    if (buf.cmn.type == .STORAGEBUFFER) {
        bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_STORAGEBUFFER, buf.slot.id);
    }
    if (buf.wgpu.buf) {
        wgpuBufferRelease(buf.wgpu.buf);
    }
}

fn copy_buffer_data(wgpu: *Impl, buf: *Sg.Buffer.T, offset: u64, data: []u8) void = {
    @debug_assert((offset + data.size) <= (size_t)buf.cmn.size);
    @debug_assert(!data.ptr.is_null() && data.size > 0);
    
    // WebGPU's write-buffer requires the size to be a multiple of four, so we may need to split the copy
    // operation into two writeBuffer calls
    uint64_t clamped_size = data.size & ~3UL;
    uint64_t extra_size = data.size & 3UL;
    @debug_assert(extra_size < 4);
    wgpuQueueWriteBuffer(wgpu.queue, buf.wgpu.buf, offset, data.ptr, clamped_size);
    if (extra_size > 0) {
        const uint64_t extra_src_offset = clamped_size;
        const uint64_t extra_dst_offset = offset + clamped_size;
        uint8_t extra_data[4] = { 0 };
        uint8_t* extra_src_ptr = ((uint8_t*)data.ptr) + extra_src_offset;
        for (size_t i = 0; i < extra_size; i++) {
            extra_data[i] = extra_src_ptr[i];
        }
        wgpuQueueWriteBuffer(wgpu.queue, buf.wgpu.buf, extra_dst_offset, extra_src_ptr, 4);
    }
}

fn copy_image_data(wgpu: *Impl, img: *Sg.Image.T, wgpu_tex: *WGPU.Texture, data: *SgImageData) void = {
    wgpu_copy_tex := zeroed WGPU.ImageCopyTexture;
    wgpu_copy_tex.texture = wgpu_tex;
    wgpu_copy_tex.aspect = .All;
    num_faces := @if(img.cmn.type == .CUBE, 6, 1);
    range(0, num_faces) { face_index |
        range(0, img.cmn.num_mipmaps) { mip_index |
            wgpu_copy_tex.mipLevel = (uint32_t)mip_index;
            wgpu_copy_tex.origin.z = (uint32_t)face_index;
            int mip_width = _sg_miplevel_dim(img.cmn.width, mip_index);
            int mip_height = _sg_miplevel_dim(img.cmn.height, mip_index);
            wgpu_layout: WGPU.TextureDataLayout = (
                offset = 0,
                bytesPerRow = (uint32_t)sg_row_pitch(img.cmn.pixel_format, mip_width, 1),
                rowsPerImage = (uint32_t)_sg_num_rows(img.cmn.pixel_format, mip_height),
            );
            if (_sg_is_compressed_pixel_format(img.cmn.pixel_format)) {
                mip_width = _sg_roundup(mip_width, 4);
                mip_height = _sg_roundup(mip_height, 4);
            }
            wgpu_extent: WGPU.Extent3D = (
                width = (uint32_t)mip_width,
                height = (uint32_t)mip_height,
                depthOrArrayLayers = (uint32_t) @match(img.cmn.type) {
                    fn CUBE() => 1;
                    fn _3D() => _sg_miplevel_dim(img.cmn.num_slices, mip_index);
                    @default => img.cmn.num_slices;
                },
            );
            mip_data := data.subimage&[face_index]&[mip_index];
            wgpuQueueWriteTexture(wgpu.queue, wgpu_copy_tex&, mip_data.ptr, mip_data.len, wgpu_layout&, wgpu_extent&);
        }
    }
}

fn create(wgpu: *Impl, img: *Sg.Image.T, desc: *Sg.Image.Desc) SgResourceState ={
    @debug_assert(img && desc);
    const bool injected = (0 != desc.wgpu_texture);
    if (injected) {
        img.wgpu.tex = bit_cast_unchecked(rawptr, WGPU.Texture, desc.wgpu_texture);
        wgpuTextureAddRef(img.wgpu.tex);
        img.wgpu.view = bit_cast_unchecked(rawptr, WGPU.TextureView, desc.wgpu_texture_view);
        if (img.wgpu.view) {
            wgpuTextureViewAddRef(img.wgpu.view);
        }
    } else {
        wgpu_tex_desc: WGPU.TextureDescriptor = (
            label = str(desc.label),
            usage = {
                WGPU.TextureUsage_TextureBinding|WGPU.TextureUsage_CopyDst;
                if (desc.render_target) {
                    wgpu_tex_desc.usage |= WGPU.TextureUsage_RenderAttachment;
                }
            }
            dimension = texture_dimension(img.cmn.type),
            size = (
                width = (uint32_t) img.cmn.width, 
                height = (uint32_t) img.cmn.height,
                depthOrArrayLayers = @if(desc.type == .CUBE, 6, (uint32_t) img.cmn.num_slices),
            ),
            format = textureformat(img.cmn.pixel_format),
            mipLevelCount = (uint32_t) img.cmn.num_mipmaps,
            sampleCount = (uint32_t) img.cmn.sample_count,
        );
        img.wgpu.tex = wgpuDeviceCreateTexture(wgpu.dev, wgpu_tex_desc&);
        if (0 == img.wgpu.tex) {
            _SG_ERROR(WGPU._CREATE_TEXTURE_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
        if ((img.cmn.usage == SG_USAGE_IMMUTABLE) && !img.cmn.render_target) {
            copy_image_data(img, img.wgpu.tex, &desc.data);
        }
        wgpu_texview_desc: WGPU.TextureViewDescriptor = (
            label = stringview(desc.label),
            dimension = texture_view_dimension(img.cmn.type),
            mipLevelCount = (uint32_t)img.cmn.num_mipmaps;
            arrayLayerCount = @match(img.cmn.type) {
                fn CUBE() => 6;
                fn ARRAY() => (uint32_t)img.cmn.num_slices;
                @default => 1;
            },
            aspect = if (_sg_is_depth_or_depth_stencil_format(img.cmn.pixel_format)) {
                .DepthOnly;
            } else {
               .All
            },
        );
        img.wgpu.view = wgpuTextureCreateView(img.wgpu.tex, wgpu_texview_desc&);
        if (0 == img.wgpu.view) {
            _SG_ERROR(WGPU._CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    };
    .VALID
}

fn discard_image(wgpu: *Impl, img: *Sg.Image.T) void = {
    @debug_assert(img);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_IMAGE, img.slot.id);
    if (img.wgpu.view) {
        wgpuTextureViewRelease(img.wgpu.view);
        img.wgpu.view = 0;
    }
    if (img.wgpu.tex) {
        wgpuTextureRelease(img.wgpu.tex);
        img.wgpu.tex = 0;
    }
}

fn create(wgpu: *Impl, smp: *Sg.Sampler.T, desc: *Sg.Sampler.Desc) SgResourceState = {
    @debug_assert(smp && desc);
    @debug_assert(wgpu.dev);
    const bool injected = (0 != desc.wgpu_sampler);
    if (injected) {
        smp.wgpu.smp = (WGPU.Sampler) desc.wgpu_sampler;
        wgpuSamplerAddRef(smp.wgpu.smp);
    } else {
        wgpu_desc: WGPU.SamplerDescriptor = (
            label = str(desc.label);
            addressModeU = to_wgpu(desc.wrap_u),
            addressModeV = to_wgpu(desc.wrap_v),
            addressModeW = to_wgpu(desc.wrap_w),
            magFilter = to_wgpu(desc.mag_filter),
            minFilter = to_wgpu(desc.min_filter),
            mipmapFilter = to_wgpu(desc.mipmap_filter),
            lodMinClamp = desc.min_lod,
            lodMaxClamp = desc.max_lod,
            maxAnisotropy = (uint16_t)desc.max_anisotropy,
            compare = {
                it := to_wgpu(desc.compare);
                @if(it == .Never, .Undefined, it)
            },
        );
        smp.wgpu.smp = wgpuDeviceCreateSampler(wgpu.dev, wgpu_desc&);
        if (0 == smp.wgpu.smp) {
            _SG_ERROR(WGPU._CREATE_SAMPLER_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    };
    .VALID
}

fn discard_sampler(wgpu: *Impl, smp: *Sg.Sampler.T) void = {
    @debug_assert(smp);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_SAMPLER, smp.slot.id);
    if (smp.wgpu.smp) {
        wgpuSamplerRelease(smp.wgpu.smp);
        smp.wgpu.smp = 0;
    }
}

fn create_shader_func(wgpu: *Impl, func: *SgShaderFunction, label: CStr) shader_func_t = {
    @debug_assert(!func.entry.is_null());

    wgpu_shdmod_wgsl_desc: WGPU.ShaderSourceWGSL = (
        chain = (sType = .ShaderSourceWGSL),
        code = str(func.source),
    );

    wgpu_shdmod_desc: WGPU.ShaderModuleDescriptor = (
        nextInChain = wgpu_shdmod_wgsl_desc.chain&,
        label = str(label),
    );

    // NOTE: if compilation fails we won't actually find out in this call since
    // it always returns a valid module handle, and the GetCompilationInfo() call
    // is asynchronous
    res: shader_func_t = (
        module = createShaderModule(wgpu.dev, wgpu_shdmod_desc&),
        entry = func.entry.str().shallow_copy(wgpu.sg.desc.allocator), // :LEAK
    );
    if res.module._.is_null() {
        _SG_ERROR("WGPU_CREATE_SHADER_MODULE_FAILED");
    };
    res
}

fn discard_shader_func(wgpu: *Impl, func: *shader_func_t) void = {
    if !func.module._.is_null() {
        release(func.module);
        func.module = zeroed @type func.module;
    }
}

dynoffset_mapping_t :: @struct(sokol_slot: u8, wgpu_slot: u8);

// TODO: this does not spark joy. it's a paste from the metal one. 
//       make the field names the same and then use our surprise tool (generics). 
// NOTE: this is an out-of-range check for WGSL bindslots that's also active in release mode
fn ensure_wgsl_bindslot_ranges(wgpu: *Impl, desc: *Sg.Shader.Desc) bool = {
    X :: fn(count: i64, $get: @Fn(i: i64) u8, limit: u8, error) => 
        range(0, count) { i |
            if get(i) >= limit {
                _SG_ERROR(error);
                return false;
            };
        };
    X(Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS, fn(i) => desc.uniform_blocks&[i].wgsl_group0_binding_n, MAX_UB_BINDGROUP_BIND_SLOTS, "WGPU_UNIFORMBLOCK_WGSL_GROUP0_BINDING_OUT_OF_RANGE");
    X(Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS, fn(i) => desc.storage_buffers&[i].wgsl_group1_binding_n, MAX_IMG_SMP_SBUF_BIND_SLOTS, "WGPU_STORAGEBUFFER_WGSL_GROUP1_BINDING_OUT_OF_RANGE");
    X(Sg.SG_MAX_IMAGE_BINDSLOTS, fn(i) => desc.images&[i].wgsl_group1_binding_n, MAX_IMG_SMP_SBUF_BIND_SLOTS, "WGPU_IMAGE_WGSL_GROUP1_BINDING_OUT_OF_RANGE");
    X(Sg.SG_MAX_SAMPLER_BINDSLOTS, fn(i) => desc.samplers&[i].wgsl_group1_binding_n, MAX_IMG_SMP_SBUF_BIND_SLOTS, "WGPU_SAMPLER_WGSL_GROUP1_BINDING_OUT_OF_RANGE");
    true
}

fn create(wgpu: *Impl, shd: *Sg.Shader.T, desc: *Sg.Shader.Desc) SgResourceState #once = {
    // do a release-mode bounds-check on wgsl bindslots, even though out-of-range
    // bindslots can't cause out-of-bounds accesses in the wgpu backend, this
    // is done to be consistent with the other backends
    if(!wgpu.ensure_wgsl_bindslot_ranges(desc), => return(.FAILED));

    // build shader modules
    shd_valid := true;
    
    X :: fn(func, dest) => if !func.source.is_null() {
        dest[] = wgpu.create_shader_func(func, desc.label);
        shd_valid = shd_valid && !dest[].module._.is_null();
    };
    
    // TODO: these could be EnumMap(SgShaderStage, _)
    X(desc.vertex_func&, shd.mtl.vertex_func&);
    X(desc.fragment_func&, shd.mtl.fragment_func&);
    X(desc.compute_func&, shd.mtl.compute_func&);

    if (!shd_valid) {
        wgpu.discard_shader_func(shd.mtl.vertex_func&);
        wgpu.discard_shader_func(shd.mtl.fragment_func&);
        wgpu.discard_shader_func(shd.mtl.compute_func&);
        return(.FAILED);
    };

    // create bind group layout and bind group for uniform blocks
    // NOTE also need to create a mapping of sokol ub bind slots to array indices
    // for the dynamic offsets array in the setBindGroup call
    @debug_assert(MAX_UB_BINDGROUP_ENTRIES <= MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    bgl_entries := zeroed Array(WGPU.BindGroupLayoutEntry, MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    bg_entries  := zeroed Array(WGPU.BindGroupEntry, MAX_IMG_SMP_SBUF_BINDGROUP_ENTRIES);
    dynoffset_map := zeroed Array(dynoffset_mapping_t, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    bgl_index := 0;
    rangec(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { i, $continue |
        if(shd.cmn.uniform_blocks&[i].stage == .NONE, => continue());
        shd.mtl.ub_grp0_bnd_n&[i] = desc.uniform_blocks&[i].wgsl_group0_binding_n;
        bgl_entry := bgl_entries&[bgl_index]&;
        bgl_entry[] = (
            binding = zext shd.mtl.ub_grp0_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.uniform_blocks&[i].stage),
            buffer = (
                type = .Uniform,
                hasDynamicOffset = .True,
            ),
        );
        bg_entries&[bgl_index] = (
            binding = bgl_entry.binding,
            buffer = wgpu.uniform.buf,
            size = MAX_UNIFORM_UPDATE_SIZE,
        );
        dynoffset_map&[i] = (sokol_slot = trunc i, wgpu_slot = trunc bgl_entry.binding);
        bgl_index += 1;
    }
    
    bgl_desc: WGPU.BindGroupLayoutDescriptor = (entryCount = bgl_index, entries = bgl_entries&.as_ptr());
    shd.mtl.bgl_ub = createBindGroupLayout(wgpu.dev, bgl_desc&);
    @debug_assert(!shd.mtl.bgl_ub._.is_null());
    shd.mtl.bg_ub = createBindGroup(wgpu.dev, @ref @as(WGPU.BindGroupDescriptor) (
        layout = shd.mtl.bgl_ub,
        entryCount = bgl_index,
        entries = bg_entries&.as_ptr(),
    ));
    @debug_assert(!shd.mtl.bg_ub._.is_null());

    // sort the dynoffset_map by wgpu bindings, this is because the
    // dynamic offsets of the WebGPU setBindGroup call must be in
    // 'binding order', not 'bindgroup entry order'
    // TODO: did i flip it?
    sort :: import("@/lib/sort.fr")'quicksort(dynoffset_mapping_t, fn(a, b) => a.wgpu_slot < b.wgpu_slot);
    sort(dynoffset_map&.items().slice(0, bgl_index));

    shd.mtl.ub_num_dynoffsets = trunc bgl_index;
    range(0, bgl_index) { i |
        sokol_slot := dynoffset_map&[i].sokol_slot;
        shd.mtl.ub_dynoffsets&[zext sokol_slot] = trunc i;
    }

    // create bind group layout for images, samplers and storage buffers
    bgl_entries&.items().set_zeroed();
    bgl_index = 0;
    rangec(0, Sg.SG_MAX_IMAGE_BINDSLOTS) { i, $continue |
        if(shd.cmn.images&[i].stage == .NONE, => continue());
        
        msaa := shd.cmn.images&[i].multisampled;
        shd.mtl.img_grp1_bnd_n&[i] = desc.images&[i].wgsl_group1_binding_n;
        bgl_entries&[bgl_index] = (
            binding = zext shd.mtl.img_grp1_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.images&[i].stage),
            texture = (
                viewDimension = to_wgpu(shd.cmn.images&[i].image_type),
                sampleType = to_wgpu(shd.cmn.images&[i].sample_type, msaa),
                multisampled = to_wgpu msaa,
            )
        );
        bgl_index += 1;
    }
    rangec(0, Sg.SG_MAX_SAMPLER_BINDSLOTS) { i, $continue |
        if(shd.cmn.samplers&[i].stage == .NONE, => continue());
        
        shd.mtl.smp_grp1_bnd_n&[i] = desc.samplers&[i].wgsl_group1_binding_n;
        bgl_entries&[bgl_index] = (
            binding = zext shd.mtl.smp_grp1_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.samplers&[i].stage),
            sampler = (
                type = to_wgpu(shd.cmn.samplers&[i].sampler_type),
            ),
        );
        bgl_index += 1;
    }
    rangec(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i, $continue |
        if(shd.cmn.storage_buffers&[i].stage == .NONE, => continue());
        
        shd.mtl.sbuf_grp1_bnd_n&[i] = desc.storage_buffers&[i].wgsl_group1_binding_n;
        bgl_entries&[bgl_index] = (
            binding = zext shd.mtl.sbuf_grp1_bnd_n&[i],
            visibility = to_wgpu(shd.cmn.storage_buffers&[i].stage),
            buffer = (
                type = @if(shd.cmn.storage_buffers&[i].readonly, .ReadOnlyStorage, .Storage),
            ),
        );
        bgl_index += 1;
    }
    bgl_desc: WGPU.BindGroupLayoutDescriptor = (entryCount = bgl_index, entries = bgl_entries&.as_ptr());
    shd.mtl.bgl_img_smp_sbuf = createBindGroupLayout(wgpu.dev, bgl_desc&);
    if shd.mtl.bgl_img_smp_sbuf._.is_null() {
        _SG_ERROR("WGPU_SHADER_CREATE_BINDGROUP_LAYOUT_FAILED");
        return(.FAILED);
    };
    .VALID
}

fn discard_shader(wgpu: *Impl, shd: *Sg.Shader.T) void = {
    @debug_assert(shd);
    discard_shader_func(shd.mtl.vertex_func&);
    discard_shader_func(shd.mtl.fragment_func&);
    discard_shader_func(shd.mtl.compute_func&);
    if (shd.mtl.bgl_ub) {
        wgpuBindGroupLayoutRelease(shd.mtl.bgl_ub);
        shd.mtl.bgl_ub = 0;
    }
    if (shd.mtl.bg_ub) {
        wgpuBindGroupRelease(shd.mtl.bg_ub);
        shd.mtl.bg_ub = 0;
    }
    if (shd.mtl.bgl_img_smp_sbuf) {
        wgpuBindGroupLayoutRelease(shd.mtl.bgl_img_smp_sbuf);
        shd.mtl.bgl_img_smp_sbuf = 0;
    }
}

fn to_wgpu(it: SgColor) WGPU.Color = (
    r = cast it.r,
    g = cast it.g,
    b = cast it.b,
    a = cast it.a,
);

fn create(wgpu: *Impl, pip: *Sg.Pipeline.T, shd: *Sg.Shader.T, desc: *Sg.Pipeline.Desc) SgResourceState #once = {
    @debug_assert(desc.shader.id == shd.slot.id);
    @debug_assert(!shd.mtl.bgl_ub._.is_null());
    @debug_assert(!shd.mtl.bgl_img_smp_sbuf._.is_null());
    pip.cmn.shader_t = shd;

    pip.mtl.blend_color = to_wgpu desc.blend_color;

    // - @group(0) for uniform blocks
    // - @group(1) for all image, sampler and storagebuffer resources
    wgpu_bgl := zeroed Array(WGPU.BindGroupLayout, NUM_BINDGROUPS);
    wgpu_bgl&[UB_BINDGROUP_INDEX] = shd.mtl.bgl_ub;
    wgpu_bgl&[IMG_SMP_SBUF_BINDGROUP_INDEX] = shd.mtl.bgl_img_smp_sbuf;
    wgpu_pl_desc: WGPU.PipelineLayoutDescriptor = (
        bindGroupLayoutCount = NUM_BINDGROUPS,
        bindGroupLayouts = wgpu_bgl&.as_ptr(),
    );
    wgpu_pip_layout := createPipelineLayout(wgpu.dev, wgpu_pl_desc&);
    if wgpu_pip_layout._.is_null() {
        _SG_ERROR("WGPU_CREATE_PIPELINE_LAYOUT_FAILED");
        return(.FAILED);
    }

    if pip.cmn.compute {
        pip.mtl.cpip = createComputePipeline(wgpu.dev, @ref @as(WGPU.ComputePipelineDescriptor) (
            label = str(desc.label),
            layout = wgpu_pip_layout,
            compute = (
                module = shd.mtl.compute_func.module,
                entryPoint = shd.mtl.compute_func.entry,
            ),
        ));
        release(wgpu_pip_layout);
        if pip.mtl.cpip._.is_null() {
            _SG_ERROR("WGPU_CREATE_COMPUTE_PIPELINE_FAILED");
            return(.FAILED);
        }
    } else {
        wgpu_vb_layouts := zeroed Array(WGPU.VertexBufferLayout, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS);
        wgpu_vtx_attrs := zeroed Array(Array(WGPU.VertexAttribute, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS), Sg.SG_MAX_VERTEX_ATTRIBUTES);
        wgpu_vb_num := 0;
        rangeb(0, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS) { vb_idx, $break |
            vbl_state := desc.layout.buffers&[vb_idx]&;
            if(vbl_state.stride == 0, => break());
            wgpu_vb_layouts&[vb_idx] = (
                arrayStride = bitcast(zext vbl_state.stride),
                stepMode = to_wgpu(vbl_state.step_func),
                attributes = wgpu_vtx_attrs&[vb_idx]&.as_ptr(),
                attributeCount = 0,  // set by loop below
            );
            wgpu_vb_num += 1;
        }
        rangeb(0, Sg.SG_MAX_VERTEX_ATTRIBUTES) { va_idx, $break |
            va_state := desc.layout.attrs&[va_idx]&;
            ::enum(@type va_state.format);
            if(va_state.format == .INVALID, => break());
            vb_idx := va_state.buffer_index;
            @debug_assert(pip.cmn.vertex_buffer_layout_active&[zext vb_idx]);
            layout := wgpu_vb_layouts&[zext vb_idx]&;
            wgpu_vtx_attrs&[zext vb_idx]&[layout.attributeCount] = (
                format = to_wgpu va_state.format,
                offset = bitcast(zext va_state.offset),
                shaderLocation = trunc va_idx,
            );
            layout.attributeCount += 1;
        }

        ::if(*WGPU.FragmentState);
        ::if(*WGPU.DepthStencilState);
        pip.mtl.rpip = createRenderPipeline(wgpu.dev, @ref @as(WGPU.RenderPipelineDescriptor) (
            label = str(desc.label),
            layout = wgpu_pip_layout,
            vertex = (
                module = shd.mtl.vertex_func.module,
                entryPoint = shd.mtl.vertex_func.entry,
                bufferCount = wgpu_vb_num,
                buffers = wgpu_vb_layouts&.as_ptr(),
            ),
            primitive = (
                topology = to_wgpu(desc.primitive_type),
                stripIndexFormat = to_wgpu(desc.primitive_type, desc.index_type),
                frontFace = to_wgpu(desc.face_winding),
                cullMode = to_wgpu(desc.cull_mode),
            ),
            depthStencil = if desc.depth.pixel_format == .NONE {
                WGPU.DepthStencilState.ptr_from_int(0)
            } else {
                wgpu_ds_state: WGPU.DepthStencilState = (
                    format = to_wgpu(desc.depth.pixel_format),
                    depthWriteEnabled = to_wgpu(desc.depth.write_enabled),
                    depthCompare = to_wgpu(desc.depth.compare),
                    stencilFront = to_wgpu desc.stencil.front&,
                    stencilBack = to_wgpu desc.stencil.back&,
                    stencilReadMask = zext desc.stencil.read_mask,
                    stencilWriteMask = zext desc.stencil.write_mask,
                    depthBias = desc.depth.bias.cast().int().intcast(),
                    depthBiasSlopeScale = desc.depth.bias_slope_scale,
                    depthBiasClamp = desc.depth.bias_clamp,
                );
                wgpu_ds_state&
            },
            multisample = (
                count = bitcast desc.sample_count,
                mask = 0xFFFFFFFF,
                alphaToCoverageEnabled = to_wgpu desc.alpha_to_coverage_enabled,
            ),
            fragment = if desc.color_count == 0 {
                WGPU.FragmentState.ptr_from_int(0)
            } else {
                wgpu_ctgt_state := zeroed Array(WGPU.ColorTargetState, Sg.SG_MAX_COLOR_ATTACHMENTS);
                wgpu_blend_state := @uninitialized Array(WGPU.BlendState, Sg.SG_MAX_COLOR_ATTACHMENTS);
                @debug_assert(desc.color_count < Sg.SG_MAX_COLOR_ATTACHMENTS);
                range(0, intcast desc.color_count) { i |
                    dest := wgpu_ctgt_state&[i]&;
                    dest.format = to_wgpu desc.colors&[i].pixel_format;
                    dest.writeMask = to_wgpu(desc.colors&[i].write_mask);
                    it := desc.colors&[i].blend&;
                    if it.enabled {
                        dest.blend = wgpu_blend_state&[i]&;
                        dest.blend[] = (
                            color = (
                                operation = to_wgpu it.op_rgb,
                                srcFactor = to_wgpu it.src_factor_rgb,
                                dstFactor = to_wgpu it.dst_factor_rgb,
                            ),
                            alpha = (
                                operation = to_wgpu it.op_alpha,
                                srcFactor = to_wgpu it.src_factor_alpha,
                                dstFactor = to_wgpu it.dst_factor_alpha,
                            ),
                        );
                    }
                };
                @ref @as(WGPU.FragmentState) (
                    module = shd.mtl.fragment_func.module,
                    entryPoint = shd.mtl.fragment_func.entry,
                    targetCount = intcast desc.color_count,
                    targets = wgpu_ctgt_state&.as_ptr(), 
                )
            },
        ));
        release(wgpu_pip_layout);
        if pip.mtl.rpip._.is_null() {
            _SG_ERROR("WGPU_CREATE_RENDER_PIPELINE_FAILED");
            return(.FAILED);
        }
    };
    .VALID
}

// SAFETY: don't use this in a loop (the pointer will NOT stay live across iterations)
// TODO: I think my policy is that stack frame is valid for the whole function regardless 
//       of if the block ends. are we sure that's what i want to commit to? 
//       tho since that's not true for loops it's kinda incoherent to make it true for 
//       other blocks i guess? :Compiler
// 
// TODO: give macros access to the inferred result type so you don't have to always use @as
fn ref(value: FatExpr) FatExpr #macro = @{
    value := @[value];
    value&
};

fn to_wgpu(it: *SgStencilFaceState) WGPU.StencilFaceState = (
    compare = to_wgpu it.compare,
    failOp = to_wgpu it.fail_op,
    depthFailOp = to_wgpu it.depth_fail_op,
    passOp = to_wgpu it.pass_op,
);

fn discard_pipeline(wgpu: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip);
    bindgroups_cache_invalidate(BINDGROUPSCACHEITEMTYPE_PIPELINE, pip.slot.id);
    if wgpu.cur_pipeline { cur |
        if cur.identical(pip) {
            wgpu.cur_pipeline = .None;
            wgpu.cur_pipeline_id.id = 0;
        }
    }
    if (pip.mtl.rpip) {
        release(pip.mtl.rpip);
        pip.mtl.rpip = 0;
    }
    if (pip.mtl.cpip) {
        release(pip.mtl.cpip);
        pip.mtl.cpip = 0;
    }
}

fn create(wgpu: *Impl, atts: *Sg.Attachments.T, color_images: **Sg.Image.T, resolve_images: **Sg.Image.T, ds_img: *Sg.Image.T, desc: *Sg.Attachments.Desc) SgResourceState = {
    @debug_assert(atts && desc);
    @debug_assert(color_images && resolve_images);

    // copy image pointers and create renderable wgpu texture views
    for (int i = 0; i < atts.cmn.num_colors; i++) {
        const sg_attachment_desc* color_desc = &desc.colors[i];
        _SOKOL_UNUSED(color_desc);
        @debug_assert(color_desc.image.id != SG_INVALID_ID);
        @debug_assert(0 == atts.wgpu.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        @debug_assert(color_images[i].wgpu.tex);
        atts.wgpu.colors[i].image = color_images[i];

        wgpu_color_view_desc := color_desc.to_wgpu();
        atts.wgpu.colors[i].view = wgpuTextureCreateView(color_images[i].wgpu.tex, &wgpu_color_view_desc);
        if (0 == atts.wgpu.colors[i].view) {
            _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }

        const sg_attachment_desc* resolve_desc = &desc.resolves[i];
        if (resolve_desc.image.id != SG_INVALID_ID) {
            @debug_assert(0 == atts.wgpu.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            @debug_assert(resolve_images[i].wgpu.tex);
            atts.wgpu.resolves[i].image = resolve_images[i];

            wgpu_resolve_view_desc := resolve_desc.to_wgpu();
            atts.wgpu.resolves[i].view = wgpuTextureCreateView(resolve_images[i].wgpu.tex, &wgpu_resolve_view_desc);
            if (0 == atts.wgpu.resolves[i].view) {
                _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
        }
    }
    @debug_assert(0 == atts.wgpu.depth_stencil.image);
    const sg_attachment_desc* ds_desc = &desc.depth_stencil;
    if (ds_desc.image.id != SG_INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        @debug_assert(ds_img.wgpu.tex);
        atts.wgpu.depth_stencil.image = ds_img;
        wgpu_ds_view_desc: WGPU.TextureViewDescriptor = (
            baseMipLevel = (uint32_t) ds_desc.mip_level,
            mipLevelCount = 1,
            baseArrayLayer = (uint32_t) ds_desc.slice,
            arrayLayerCount = 1,
        );
        atts.wgpu.depth_stencil.view = wgpuTextureCreateView(ds_img.wgpu.tex, wgpu_ds_view_desc&);
        if (0 == atts.wgpu.depth_stencil.view) {
            _SG_ERROR(WGPU._ATTACHMENTS_CREATE_TEXTURE_VIEW_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    }
    return SG_RESOURCESTATE_VALID;
}

fn to_wgpu(it: *SgAttachmentDesc) WGPU.TextureViewDescriptor = (
    baseMipLevel = bitcast it.mip_level,
    mipLevelCount = 1,
    baseArrayLayer = bitcast it.slice,
    arrayLayerCount = 1,
);

fn discard_attachments(wgpu: *Impl, atts: *Sg.Attachments.T) void = {
    @debug_assert(atts);
    for (int i = 0; i < atts.cmn.num_colors; i++) {
        if (atts.wgpu.colors[i].view) {
            wgpuTextureViewRelease(atts.wgpu.colors[i].view);
            atts.wgpu.colors[i].view = 0;
        }
        if (atts.wgpu.resolves[i].view) {
            wgpuTextureViewRelease(atts.wgpu.resolves[i].view);
            atts.wgpu.resolves[i].view = 0;
        }
    }
    if (atts.wgpu.depth_stencil.view) {
        wgpuTextureViewRelease(atts.wgpu.depth_stencil.view);
        atts.wgpu.depth_stencil.view = 0;
    }
}

fn init_color_att(action: *SgColorAttachmentAction, color_view: WGPU.TextureView, resolve_view: WGPU.TextureView) WGPU.RenderPassColorAttachment = (
    view = color_view,
    resolveTarget = resolve_view,
    loadOp = to_wgpu(color_view, action.load_action),
    storeOp = to_wgpu(color_view, action.store_action),
    clearValue = to_wgpu action.clear_value,
);

fn init_ds_att(action: *SgPassAction, fmt: SgPixelFormat, view: WGPU.TextureView) WGPU.RenderPassDepthStencilAttachment = {
    wgpu_att := zeroed WGPU.RenderPassDepthStencilAttachment;
    wgpu_att.view = view;
    wgpu_att.depthLoadOp = to_wgpu(view, action.depth.load_action);
    wgpu_att.depthStoreOp = to_wgpu(view, action.depth.store_action);
    wgpu_att.depthClearValue = action.depth.clear_value;
    wgpu_att.depthReadOnly = to_wgpu false;
    if is_depth_stencil_format(fmt) {
        wgpu_att.stencilLoadOp = to_wgpu(view, action.stencil.load_action);
        wgpu_att.stencilStoreOp = to_wgpu(view, action.stencil.store_action);
    } else {
        wgpu_att.stencilLoadOp = .Undefined;
        wgpu_att.stencilStoreOp = .Undefined;
    }
    wgpu_att.stencilClearValue = zext action.stencil.clear_value;
    wgpu_att.stencilReadOnly = to_wgpu false;
    wgpu_att
}

fn begin_compute_pass(wgpu: *Impl, pass: *SgPass) void = {
    wgpu_pass_desc: WGPU.ComputePassDescriptor = (label = str(pass.label));
    cpass_enc := beginComputePass(wgpu.cmd_enc, wgpu_pass_desc&);
    @debug_assert(!cpass_enc._.is_null());
    wgpu.cpass_enc = (Some = cpass_enc);
    // clear initial bindings
    cpass_enc.setBindGroup(UB_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    cpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    wgpu.sg.stat(.wgpu_bindings_num_set_bindgroup, 1);
}

fn begin_render_pass(wgpu: *Impl, pass: *SgPass) void = {
    atts := wgpu.sg.cur_pass.atts;
    swapchain := pass.swapchain&;
    action := pass.action&;

    wgpu_pass_desc := zeroed WGPU.RenderPassDescriptor;
    wgpu_color_att := zeroed Array(WGPU.RenderPassColorAttachment, Sg.SG_MAX_COLOR_ATTACHMENTS);
    wgpu_pass_desc.label = str(pass.label);
    if !atts.is_null() {
        @debug_assert(atts.slot.state == .VALID);
        range(0, intcast atts.cmn.num_colors) { i | 
            wgpu_color_att&[i] = init_color_att(action.colors&[i]&, atts.mtl.colors&[i].view, atts.mtl.resolves&[i].view);
        }
        wgpu_pass_desc.colorAttachmentCount = intcast atts.cmn.num_colors;
        wgpu_pass_desc.colorAttachments = wgpu_color_att&.as_ptr();
        if !atts.mtl.depth_stencil.image.is_null() {
            wgpu_ds_att := init_ds_att(action, atts.mtl.depth_stencil.image.cmn.pixel_format, atts.mtl.depth_stencil.view);
            wgpu_pass_desc.depthStencilAttachment = wgpu_ds_att&;
        }
    } else {
        wgpu_color_view := bit_cast_unchecked(rawptr, WGPU.TextureView, swapchain.wgpu.render_view);
        wgpu_resolve_view := bit_cast_unchecked(rawptr, WGPU.TextureView, swapchain.wgpu.resolve_view);
        wgpu_depth_stencil_view := bit_cast_unchecked(rawptr, WGPU.TextureView, swapchain.wgpu.depth_stencil_view);
        wgpu_color_att&[0] = init_color_att(action.colors&[0]&, wgpu_color_view, wgpu_resolve_view);
        wgpu_pass_desc.colorAttachmentCount = 1;
        wgpu_pass_desc.colorAttachments = wgpu_color_att&.as_ptr();
        if !wgpu_depth_stencil_view._.is_null() {
            wgpu_ds_att := init_ds_att(action, swapchain.depth_format, wgpu_depth_stencil_view);
            wgpu_pass_desc.depthStencilAttachment = wgpu_ds_att&;
        }
    }
    rpass_enc := beginRenderPass(wgpu.cmd_enc, wgpu_pass_desc&);
    @debug_assert(!rpass_enc._.is_null());
    wgpu.rpass_enc = (Some = rpass_enc);

    rpass_enc.setBindGroup(UB_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    rpass_enc.setBindGroup(IMG_SMP_SBUF_BINDGROUP_INDEX, wgpu.empty_bind_group, 0, zeroed(*u32));
    wgpu.sg.stat(.wgpu_bindings_num_set_bindgroup, 1);
}

fn begin_pass(wgpu: *Impl, pass: *SgPass) void #once = {
    // TODO
    //@debug_assert(wgpu.dev);
    //@debug_assert(wgpu.cmd_enc);
    @debug_assert(wgpu.rpass_enc.is_none() && wgpu.cpass_enc.is_none());

    wgpu.cur_pipeline = .None;
    wgpu.cur_pipeline_id.id = SG_INVALID_ID;
    wgpu.bindings_cache_clear();

    if pass.compute {
        wgpu.begin_compute_pass(pass);
    } else {
        wgpu.begin_render_pass(pass);
    }
}

fn end_pass(wgpu: *Impl) void = {
    if wgpu.rpass_enc { rpass_enc |
        end(rpass_enc);
        release(rpass_enc);
        wgpu.rpass_enc = .None;
    }
    if wgpu.cpass_enc { cpass_enc |
        end(cpass_enc);
        release(cpass_enc);
        wgpu.cpass_enc = .None;
    }
}

fn commit(wgpu: *Impl) void = {
    @debug_assert(!wgpu.cmd_enc._.is_null());

    wgpu.uniform_buffer_on_commit();

    cmd_buf_desc := zeroed WGPU.CommandBufferDescriptor;
    wgpu_cmd_buf := finish(wgpu.cmd_enc, cmd_buf_desc&);
    @debug_assert(!wgpu_cmd_buf._.is_null());
    release(wgpu.cmd_enc);
    wgpu.cmd_enc = zeroed @type wgpu.cmd_enc;

    submit(wgpu.queue, 1, wgpu_cmd_buf&);
    release(wgpu_cmd_buf);

    // create a new render-command-encoder for next frame
    cmd_enc_desc := zeroed WGPU.CommandEncoderDescriptor;
    wgpu.cmd_enc = createCommandEncoder(wgpu.dev, cmd_enc_desc&);
}

fn apply_viewport(wgpu: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(wgpu.rpass_enc);
    // FIXME FIXME FIXME: CLIPPING THE VIEWPORT HERE IS WRONG!!!
    // (but currently required because WebGPU insists that the viewport rectangle must be
    // fully contained inside the framebuffer, but this doesn't make any sense, and also
    // isn't required by the backend APIs)
    const _sg_recti_t clip = _sg_clipi(x, y, w, h, wgpu.sg.cur_pass.width, wgpu.sg.cur_pass.height);
    float xf = (float) clip.x;
    float yf = (float) (origin_top_left ? clip.y : (wgpu.sg.cur_pass.height - (clip.y + clip.h)));
    float wf = (float) clip.w;
    float hf = (float) clip.h;
    setViewport(wgpu.rpass_enc, xf, yf, wf, hf, 0.0f, 1.0f);
}

fn apply_scissor_rect(wgpu: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(wgpu.rpass_enc);
    const _sg_recti_t clip = _sg_clipi(x, y, w, h, wgpu.sg.cur_pass.width, wgpu.sg.cur_pass.height);
    uint32_t sx = (uint32_t) clip.x;
    uint32_t sy = (uint32_t) (origin_top_left ? clip.y : (wgpu.sg.cur_pass.height - (clip.y + clip.h)));
    uint32_t sw = (uint32_t) clip.w;
    uint32_t sh = (uint32_t) clip.h;
    setScissorRect(wgpu.rpass_enc, sx, sy, sw, sh);
}

fn set_ub_bindgroup(wgpu: *Impl, shd: *Sg.Shader.T) void = {
    // NOTE: dynamic offsets must be in binding order, not in BindGroupEntry order
    @debug_assert(shd.mtl.ub_num_dynoffsets < Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    dyn_offsets := zeroed Array(u32, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    rangec(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { i, $continue |
        if(shd.cmn.uniform_blocks&[i].stage == .NONE, => continue());
        dynoffset_index := shd.mtl.ub_dynoffsets&[i];
        @debug_assert(dynoffset_index < shd.mtl.ub_num_dynoffsets);
        dyn_offsets&[zext dynoffset_index] = wgpu.uniform.bind_offsets&[i];
    }
    a, b, c, d := (@as(u32) UB_BINDGROUP_INDEX, shd.mtl.bg_ub, @as(i64) zext shd.mtl.ub_num_dynoffsets, dyn_offsets&.as_ptr());
    if wgpu.sg.cur_pass.is_compute {
        enc := wgpu.cpass_enc.unwrap();
        enc.setBindGroup(a, b, c, d);
    } else {
        enc := wgpu.rpass_enc.unwrap();
        enc.setBindGroup(a, b, c, d);
    }
}

fn apply_pipeline(wgpu: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip.cmn.shader_t.slot.id == pip.cmn.shader.id);
    wgpu.cur_pipeline = (Some = pip);
    wgpu.cur_pipeline_id.id = pip.slot.id;
    @debug_assert(pip.cmn.compute == wgpu.sg.cur_pass.is_compute);
    if pip.cmn.compute {
        @debug_assert(!pip.mtl.cpip._.is_null());
        cpass_enc := wgpu.cpass_enc.unwrap();
        setPipeline(cpass_enc, pip.mtl.cpip);
    } else {
        @debug_assert(!pip.mtl.rpip._.is_null());
        rpass_enc := wgpu.rpass_enc.unwrap();
        wgpu.use_indexed_draw = pip.cmn.index_type != .NONE;
        setPipeline(rpass_enc, pip.mtl.rpip);
        setBlendConstant(rpass_enc, pip.mtl.blend_color&);
        setStencilReference(rpass_enc, zext pip.cmn.stencil.ref);
    }
    // bind groups must be set because pipelines without uniform blocks or resource bindings
    // will still create 'empty' BindGroupLayouts
    wgpu.set_ub_bindgroup(pip.cmn.shader_t);
    wgpu.set_img_smp_sbuf_bindgroup(.None); // this will set the 'empty bind group'
}

fn apply_bindings(wgpu: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(bnd);
    @debug_assert(bnd.pip.shader && (bnd.pip.cmn.shader_id.id == bnd.pip.shader.slot.id));
    bool retval = true;
    if (!wgpu.sg.cur_pass.is_compute) {
        retval &= apply_index_buffer(bnd);
        retval &= apply_vertex_buffers(bnd);
    }
    retval &= apply_bindgroup(bnd);
    retval
}

fn apply_uniforms(wgpu: *Impl, ub_slot: i32, data: []u8) void #once = {
    alignment: i64 = wgpu.limits.minUniformBufferOffsetAlignment.zext();
    @debug_assert(!wgpu.uniform.staging.ptr.is_null(), "uninit");
    @debug_assert(wgpu.uniform.offset + data.len <= wgpu.uniform.staging.len, "you can't write more bytes than the whole buffer in one commit()");
    @debug_assert((wgpu.uniform.offset.bit_and(alignment - 1)) == 0);
    pip := wgpu.cur_pipeline.unwrap();
    @debug_assert(pip.slot.id == wgpu.cur_pipeline_id.id);
    shd := pip.cmn.shader_t;
    @debug_assert(shd.slot.id == pip.cmn.shader.id);
    @debug_assert(data.len == zext shd.cmn.uniform_blocks&[zext ub_slot].size);
    @debug_assert(data.len <= MAX_UNIFORM_UPDATE_SIZE);

    wgpu.sg.stat(.wgpu_uniforms_num_set_bindgroup, 1);
    wgpu.uniform.staging.subslice(wgpu.uniform.offset, data.len).copy_from(data);
    wgpu.uniform.bind_offsets&[zext ub_slot] = trunc wgpu.uniform.offset;
    new_off := wgpu.uniform.offset + data.len;
    wgpu.uniform.offset = Sg'roundup(new_off, alignment);

    wgpu.set_ub_bindgroup(shd);
}

fn draw(wgpu: *Impl, base_element: i32, num_elements: i32, num_instances: i32) void = {
    rpass_enc := wgpu.rpass_enc.unwrap();
    pip := wgpu.cur_pipeline.unwrap();
    @debug_assert(pip.slot.id == wgpu.cur_pipeline_id.id);
    ::enum(@type pip.cmn.index_type);
    if pip.cmn.index_type != .NONE {
        drawIndexed(rpass_enc, bitcast num_elements, bitcast num_instances, bitcast base_element, 0, 0);
    } else {
        draw(rpass_enc, bitcast num_elements, bitcast num_instances, bitcast base_element, 0);
    }
}

fn dispatch(wgpu: *Impl, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void = {
    @debug_assert(!wgpu.cpass_enc.is_null());
    wgpuComputePassEncoderDispatchWorkgroups(wgpu.cpass_enc, num_groups_x, num_groups_y, num_groups_z);
}

fn update_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, data: []u8) void =
    wgpu.copy_buffer_data(buf, 0, data);

fn append_buffer(wgpu: *Impl, buf: *Sg.Buffer.T, data: []u8, _new_frame: bool) void = 
    copy_buffer_data(buf, zext buf.cmn.append_pos, data);

fn update_image(wgpu: *Impl, img: *Sg.Image.T, data: *SgImageData) void #once = 
    copy_image_data(img, img.wgpu.tex, data);

