// simple 3D API wrapper
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.
// 
// Changes from Sokol:
// - no dependency on an objective-c compiler
// - removed ios, opengl, and macos <13 support (temporarily?)
// 
// A sad amount of this task is just mapping our enum values to the metal 
// ones. I've manually transcribed the numbers so you can cross compile 
// without finding a copy of the Metal.framework headers. If I ever do more
// serious bindings to metal, this should be updated to use that instead. 
// 

ASSERT_NOT_IOS :: fn() => ();
ASSERT_NOT_OLD :: fn() => ();

_SG_MTL_UB_ALIGN :: { ASSERT_NOT_IOS(); 256 };

ReleaseItem :: @struct {
    frame_index: u32;   // frame index at which it is safe to release this resource
    slot_index: i32;
};

IdPool :: @struct {
    pool: id("NSMutableArray");
    num_slots: i32;
    free_queue_top: i32;
    free_queue: []i32;
    release_queue_front: i32;
    release_queue_back: i32;
    release_queue: []ReleaseItem;
};

Buffer :: @struct {
    buf: Array(i32, SG_NUM_INFLIGHT_FRAMES);  // index into _sg_mtl_pool
};

Image :: @struct {
    tex: Array(i32, SG_NUM_INFLIGHT_FRAMES);
};

Sampler :: @struct {
    sampler_state: i32;
};

_sg_mtl_shader_func_t :: @struct {
    mtl_lib: i32;
    mtl_func: i32;
};

Shader :: @struct {
    vertex_func: _sg_mtl_shader_func_t;
    fragment_func: _sg_mtl_shader_func_t;
    compute_func: _sg_mtl_shader_func_t;
    threads_per_threadgroup: MTLSize;
    ub_buffer_n: Array(u8, SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    img_texture_n: Array(u8, SG_MAX_IMAGE_BINDSLOTS);
    smp_sampler_n: Array(u8, SG_MAX_SAMPLER_BINDSLOTS);
    sbuf_buffer_n: Array(u8, SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Pipeline :: @struct {
    prim_type: MTLPrimitiveType;
    index_size: i32;
    index_type: MTLIndexType;
    cull_mode: MTLCullMode;
    winding: MTLWinding;
    stencil_ref: u32;
    threads_per_threadgroup: MTLSize;
    cps: i32;    // MTLComputePipelineState
    rps: i32;    // MTLRenderPipelineState
    dss: i32;    // MTLDepthStencilState
};

Attachment :: @struct {
    image: *Sg.Image.T;
};

Attachments :: @struct {
    colors: Array(Attachment, SG_MAX_COLOR_ATTACHMENTS);
    resolves: Array(Attachment, SG_MAX_COLOR_ATTACHMENTS);
    depth_stencil: Attachment;
};

// resource binding state cache
_SG_MTL_MAX_STAGE_UB_BINDINGS :: SG_MAX_UNIFORMBLOCK_BINDSLOTS;
_SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS :: _SG_MTL_MAX_STAGE_UB_BINDINGS + SG_MAX_STORAGEBUFFER_BINDSLOTS;
_SG_MTL_MAX_STAGE_BUFFER_BINDINGS :: _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS + SG_MAX_VERTEXBUFFER_BINDSLOTS;
_SG_MTL_MAX_STAGE_IMAGE_BINDINGS :: SG_MAX_IMAGE_BINDSLOTS;
_SG_MTL_MAX_STAGE_SAMPLER_BINDINGS :: SG_MAX_SAMPLER_BINDSLOTS;

StateCache :: @struct {
    cur_pipeline: *Sg.Pipeline.T;
    cur_pipeline_id: Sg.Pipeline;
    cur_indexbuffer: *Sg.Buffer.T;
    cur_indexbuffer_id: Sg.Buffer;
    cur_indexbuffer_offset: i32;
    cur_vs_buffer_offsets: Array(int, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    cur_vs_buffer_ids: Array(Sg.Buffer, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    cur_fs_buffer_ids: Array(Sg.Buffer, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    cur_cs_buffer_ids: Array(Sg.Buffer, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    cur_vs_image_ids: Array(Sg.Image, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
    cur_fs_image_ids: Array(Sg.Image, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
    cur_cs_image_ids: Array(Sg.Image, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
    cur_vs_sampler_ids: Array(Sg.Sampler, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
    cur_fs_sampler_ids: Array(Sg.Sampler, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
    cur_cs_sampler_ids: Array(Sg.Sampler, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
};

Impl :: @struct {
    sg: *Sg.Self;
    valid: bool;
    use_shared_storage_mode: bool;
    cur_frame_rotate_index: u32;
    ub_size: i32;
    cur_ub_offset: i32;
    cur_ub_base_ptr: *u8;
    state_cache: StateCache;
    idpool: IdPool;
    sem: id("OS_dispatch_semaphore");
    device: id("MTLDevice");
    cmd_queue: id("MTLCommandQueue");
    cmd_buffer: id("MTLCommandBuffer");
    render_cmd_encoder: id("MTLRenderCommandEncoder");
    compute_cmd_encoder: id("MTLComputeCommandEncoder");
    cur_drawable: id("CAMetalDrawable");
    uniform_buffers: Array(id("MTLBuffer"), SG_NUM_INFLIGHT_FRAMES);
};

fn to_mtl(it: SgLoadAction) MTLLoadAction = @map_enum(it) 
    (CLEAR = 2, LOAD = 1, DONTCARE = 0);

fn to_mtl(it: SgStoreAction, resolve: bool) MTLStoreAction = {
    @debug_assert(@is(it, .STORE, .DONTCARE));
    @if(it == .STORE, @if(resolve, 3, 1), @if(resolve, 2, 0))
}

fn to_mtl(sg: *Sg.Self) MTLResourceOptions = {
    ASSERT_NOT_IOS();
    @if(mtl.use_shared_storage_mode, /*Shared*/ 0, /*Managed*/ 1.shift_left(4))
}

fn to_mtl(it: SgUsage) MTLResourceOptions = {
    @debug_assert(@is(it, .IMMUTABLE, .DYNAMIC, .STREAM));
    x := _sg_mtl_resource_options_storage_mode_managed_or_shared();
    if it != .IMMUTABLE {
        X |= MTLResourceCPUCacheModeWriteCombined;
    };
    x
}

fn to_mtl(it: SgVertexStep) MTLVertexStepFunction = @map_enum(it)
    (PER_VERTEX = 1, PER_INSTANCE = 2);

fn to_mtl(it: SgVertexFormat) MTLVertexFormat = @map_enum(it) (
    Float = 28, Float2 = 29, Float3 = 30, Float4 = 31,
    Int = 32, Int2 = 33, Int3 = 34, Int4 = 35,
    UInt = 36, UInt2 = 37, UInt3 = 38, UInt4 = 39,
    Byte4 = 6, Byte4N = 12, UByte4 = 3, UByte4N = 9, 
    Short2 = 16, Short2N = 22, UShort2 = 13, UShort2N = 19,
    Short4 = 18, Short4N = 24, UShort4 = 15, UShort4N = 21,
    UINT10_N2 = 41, Half2 = 25, Half4 = 27,
);

fn to_mtl(it: SgPrimitiveType) MTLPrimitiveType = @map_enum(it) 
    (POINTS = 0, LINES = 1, LINE_STRIP = 2, TRIANGLES = 3, TRIANGLE_STRIP = 3);

fn to_mtl(it: SgPixelFormat) MTLPixelFormat = {
    ASSERT_NOT_IOS();
    @enum_map(it) (
        R8 = 10, R8SN = 12, R8UI = 13, R8SI = 14,
        R16 = 20, R16SN = 22, R16UI = 23, R16SI = 24, R16F = 25,
        RG8 = 30, RG8SN = 32, RG8UI = 33, RG8SI = 34,
        R32UI = 53, R32SI = 54, R32F = 55, 
        RG16 = 60, RG16SN = 62, RG16UI  = 63, RG16SI = 64, RG16F = 65, 
        RGBA8 = 70, SRGB8A8 = 71, RGBA8SN = 72, RGBA8UI = 73, RGBA8SI = 74, 
        BGRA8 = 80, RGB10A2 = 90, RG11B10F = 92, RGB9E5 = 93, 
        RG32UI = 103, RG32SI = 104, RG32F = 105, 
        RGBA16 = 110, RGBA16SN = 112, RGBA16UI = 113, RGBA16SI = 114, RGBA16F = 115,
        RGBA32UI = 123, RGBA32SI = 124, RGBA32F = 125, 
        DEPTH = 252, DEPTH_STENCIL = 260,
        BC1_RGBA = 130, BC2_RGBA = 132, BC3_RGBA = 134, BC3_SRGBA = 135,
        BC4_R = 140, BC4_RSN = 141, BC5_RG = 142, BC5_RGSN = 143,
        BC6H_RGBF = 150, BC6H_RGBUF = 151, BC7_RGBA = 152, BC7_SRGBA = 153, 
    )
}

fn to_mtl(m: SgColorMask) MTLColorWriteMask = {
    MTLColorWriteMask mtl_mask = MTLColorWriteMaskNone;
    if (m & SG_COLORMASK_R) {
        mtl_mask |= MTLColorWriteMaskRed;
    }
    if (m & SG_COLORMASK_G) {
        mtl_mask |= MTLColorWriteMaskGreen;
    }
    if (m & SG_COLORMASK_B) {
        mtl_mask |= MTLColorWriteMaskBlue;
    }
    if (m & SG_COLORMASK_A) {
        mtl_mask |= MTLColorWriteMaskAlpha;
    }
    return mtl_mask;
}

fn to_mtl(it: SgBlendOp) MTLBlendOperation = @map_enum(it) 
    (ADD = 0, SUBTRACT = 1, REVERSE_SUBTRACT = 2, MIN = 3, MAX = 4);

fn to_mtl(it: SgBlendFactor) MTLBlendFactor = @map_enum(it) (
    ZERO = 0, ONE = 1, 
    SRC_COLOR = 2, ONE_MINUS_SRC_COLOR = 3,
    ALPHA     = 4, ONE_MINUS_SRC_ALPHA = 5, 
    DST_COLOR = 6, ONE_MINUS_DST_COLOR = 7, 
    DST_ALPHA = 8, ONE_MINUS_DST_ALPHA = 9,
    SRC_ALPHA_SATURATED = 10, 
    BLEND_COLOR = 11, ONE_MINUS_BLEND_COLOR = 12,
    BLEND_ALPHA = 13, ONE_MINUS_BLEND_ALPHA = 14,
);

fn to_mtl(it: SgCompareFunc) MTLCompareFunction = @map_enum(it) 
    (NEVER = 0, LESS = 1, EQUAL = 2, LESS_EQUAL = 3, GREATER = 4, NOT_EQUAL = 5, GREATER_EQUAL = 6, ALWAYS = 7);

fn to_mtl(it: SgStencilOp) MTLStencilOperation = @map_enum(it)
    (KEEP = 0, ZERO = 1, REPLACE = 2, INCR_CLAMP = 3, DECR_CLAMP = 4, INVERT = 5, INCR_WRAP = 6, DECR_WRAP = 7);

fn to_mtl(it: SgCullMode) MTLCullMode = @map_enum(it) 
    (NONE = MTLCullModeNone, FRONT = MTLCullModeFront, BACK = MTLCullModeBack);

fn to_mtl(it: SgFaceWinding) MTLWinding = @map_enum(it) 
    (CW = MTLWindingClockwise, CCW = MTLWindingCounterClockwise);

fn to_mtl(it: SgIndexType) MTLIndexType = @map_enum(it) 
    (UINT16 = MTLIndexTypeUInt16, UINT32 = MTLIndexTypeUInt32);

fn int to_mtl(it: SgIndexType) i32 = @map_enum(it) 
    (None = 0, UINT16 = 2, UINT32 = 4);

fn to_mtl(it: SgImageType) MTLTextureType = @map_enum(it)
    (_2D = 2, CUBE = 5, _3D = 7, ARRAY = 3);

fn to_mtl(mtl: *Impl, w: SgWrap) MTLSamplerAddressMode = {
    ASSERT_NOT_OLD();
    b := mtl.sg.features.image_clamp_to_border;
    @map_enum(w) (REPEAT = 2, CLAMP_TO_EDGE = 0, CLAMP_TO_BORDER = @if(b, 5, 0), MIRRORED_REPEAT = 3)
}

fn to_mtl(it: SgBorderColor) MTLSamplerBorderColor = {
    ASSERT_NOT_OLD();
    @map_enum(it) (TRANSPARENT_BLACK = 0, OPAQUE_BLACK = 1, OPAQUE_WHITE = 2)
}

fn mtl_minmag(it: SgFilter) MTLSamplerMinMagFilter = @enum_map(it)
    (NEAREST = 0, LINEAR = 1);

fn mtl_mip(f: SgFilter) MTLSamplerMipFilter = @enum_map(it) 
    (NEAREST = 1, LINEAR = 2);

fn _sg_mtl_vertexbuffer_bindslot(sokol_bindslot: i64) i64 =
    sokol_bindslot + _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS;

//-- a pool for all Metal resource objects, with deferred release queue ---------
fn init_pool(mtl: *Impl, desc: *SgDesc) void = {
    p := mtl.idpool&;
    p.num_slots = 2 *
        (
            2 * desc.buffer_pool_size +
            4 * desc.image_pool_size +
            1 * desc.sampler_pool_size +
            4 * desc.shader_pool_size +
            2 * desc.pipeline_pool_size +
            desc.attachments_pool_size +
            128
        );
    p.pool = [NSMutableArray arrayWithCapacity:(NSUInteger)p.num_slots];
    _SG_OBJC_RETAIN(p.pool);
    NSNull* null = [NSNull null];
    range(0, p.num_slots) { _ |
        @objc p.pool.addObject(null);
    }
    @debug_assert([p.pool count] == (NSUInteger)p.num_slots);
    // a queue of currently free slot indices
    p.free_queue_top = 0;
    a := mtl.sg.allocator;
    // pool slot 0 is reserved!
    p.free_queue = a.alloc_init(i32, p.num_slots, fn(i) => p.num_slots - 1 - i);
    // a circular queue which holds release items (frame index when a resource is to be released, and the resource's pool index
    p.release_queue_front = 0;
    p.release_queue_back = 0;
    
    // filled with _SG_MTL_INVALID_SLOT_INDEX
    p.release_queue = a.alloc_zeroed(ReleaseItem, p.num_slots);
}

fn destroy_pool(mtl: *Impl) void = {
    p := mtl.idpool&;
    _sg_free(p.release_queue);  p.release_queue = 0;
    _sg_free(p.free_queue);     p.free_queue = 0;
    release(p.pool);
}

// get a new free resource pool slot
fn _sg_mtl_alloc_pool_slot(mtl: *Impl) i32 = {
    p := mtl.idpool&;
    @debug_assert(p.free_queue_top > 0);
    const int slot_index = p.free_queue[--p.free_queue_top];
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
    slot_index
}

// put a free resource pool slot back into the free-queue
fn free_pool_slot(mtl: *Impl, slot_index: i32) void = {
    p := mtl.idpool&;
    @debug_assert(p.free_queue_top < p.num_slots);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
    p.free_queue[p.free_queue_top++] = slot_index;
}

// add an MTLResource to the pool, return pool index or 0 if input was 'nil'
fn add_resource(mtl: *Impl, res: ObjCId) i32 = {
    if(res.is_nil(), => return(_SG_MTL_INVALID_SLOT_INDEX));
    _sg_stats_add(metal.idpool.num_added, 1);
    slot_index := _sg_mtl_alloc_pool_slot();
    // NOTE: the NSMutableArray will take ownership of its items
    @debug_assert([NSNull null] == mtl.idpool.pool[(NSUInteger)slot_index]);
    mtl.idpool.pool[(NSUInteger)slot_index] = res;
    slot_index
}

/*  mark an MTLResource for release, this will put the resource into the
    deferred-release queue, and the resource will then be released N frames later. */
fn release_resource(mtl: *Impl, frame_index: u32, slot_index: i32) void = {
    if slot_index == _SG_MTL_INVALID_SLOT_INDEX {
        /* this means that a nil value was provided to mtl.add_resource */
        return();
    }
    p := mtl.idpool&;
    _sg_stats_add(metal.idpool.num_released, 1);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
    @debug_assert([NSNull null] != p.pool[(NSUInteger)slot_index]);
    int release_index = p.release_queue_front++;
    if (p.release_queue_front >= p.num_slots) {
        // wrap-around
        p.release_queue_front = 0;
    }
    // release queue full?
    @debug_assert(p.release_queue_front != p.release_queue_back);
    @debug_assert(0 == p.release_queue[release_index].frame_index);
    const uint32_t safe_to_release_frame_index = frame_index + SG_NUM_INFLIGHT_FRAMES + 1;
    p.release_queue[release_index].frame_index = safe_to_release_frame_index;
    p.release_queue[release_index].slot_index = slot_index;
}

// run garbage-collection pass on all resources in the release-queue
fn garbage_collect(mtl: *Impl, frame_index: u32) void = {
    p := mtl.idpool&;
    while => p.release_queue_back != p.release_queue_front {
        if (frame_index < p.release_queue[p.release_queue_back].frame_index) {
            // don't need to check further, release-items past this are too young
            return();
        }
        _sg_stats_add(metal.idpool.num_garbage_collected, 1);
        // safe to release this resource
        const int slot_index = p.release_queue[p.release_queue_back].slot_index;
        @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
        // note: the NSMutableArray takes ownership of its items, assigning an NSNull object will
        // release the object, no matter if using ARC or not
        @debug_assert(p.pool[(NSUInteger)slot_index] != [NSNull null]);
        p.pool[(NSUInteger)slot_index] = [NSNull null];
        // put the now free pool index back on the free queue
        mtl.free_pool_slot(slot_index);
        // reset the release queue slot and advance the back index
        p.release_queue[p.release_queue_back].frame_index = 0;
        p.release_queue[p.release_queue_back].slot_index = _SG_MTL_INVALID_SLOT_INDEX;
        p.release_queue_back++;
        if (p.release_queue_back >= p.num_slots) {
            // wrap-around
            p.release_queue_back = 0;
        }
    }
}

fn get_id(mtl: *Impl, slot_index: i32) ObjCId = 
    @objc mtl.idpool.pool.objectAtIndex(@as(i64) slot_index.intcast());

fn clear_state_cache(mtl: *Impl) void = {
    mtl.state_cache = zeroed @type mtl.state_cache;
}

// https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf
fn init_caps(mtl: *Impl) void = {
    ASSERT_NOT_IOS();
    mtl.sg.backend = SG_BACKEND_METAL_MACOS;
    mtl.sg.features.origin_top_left = true;
    mtl.sg.features.mrt_independent_blend_state = true;
    mtl.sg.features.mrt_independent_write_mask = true;
    mtl.sg.features.compute = true;
    mtl.sg.features.msaa_image_bindings = true;

    mtl.sg.features.image_clamp_to_border = false;
    ASSERT_NOT_OLD();
    mtl.sg.features.image_clamp_to_border = [mtl.device supportsFamily:MTLGPUFamilyApple7]
                                            || [mtl.device supportsFamily:MTLGPUFamilyMac2];
    if (!mtl.sg.features.image_clamp_to_border) {
        mtl.sg.features.image_clamp_to_border = [mtl.device supportsFamily:MTLGPUFamilyMetal3];
    }

    ASSERT_NOT_IOS();
    mtl.sg.limits.max_image_size_2d = 16 * 1024;
    mtl.sg.limits.max_image_size_cube = 16 * 1024;
    mtl.sg.limits.max_image_size_3d = 2 * 1024;
    mtl.sg.limits.max_image_size_array = 16 * 1024;
    mtl.sg.limits.max_image_array_layers = 2 * 1024;
    mtl.sg.limits.max_vertex_attrs = SG_MAX_VERTEX_ATTRIBUTES;

    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_R8]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_R8SN]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_R8UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_R8SI]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_R16]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_R16SN]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_R16UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_R16SI]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_R16F]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG8]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG8SN]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RG8UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RG8SI]);
    _sg_pixelformat_sr(&mtl.sg.formats[SG_PIXELFORMAT_R32UI]);
    _sg_pixelformat_sr(&mtl.sg.formats[SG_PIXELFORMAT_R32SI]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_R32F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG16]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG16SN]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RG16UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RG16SI]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG16F]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGBA8]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_SRGB8A8]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGBA8SN]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RGBA8UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RGBA8SI]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_BGRA8]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGB10A2]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG11B10F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_RGB9E5]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RG32UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RG32SI]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RG32F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGBA16]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGBA16SN]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RGBA16UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RGBA16SI]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGBA16F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RGBA32UI]);
    _sg_pixelformat_srm(&mtl.sg.formats[SG_PIXELFORMAT_RGBA32SI]);
    _sg_pixelformat_all(&mtl.sg.formats[SG_PIXELFORMAT_RGBA32F]);
    _sg_pixelformat_srmd(&mtl.sg.formats[SG_PIXELFORMAT_DEPTH]);
    _sg_pixelformat_srmd(&mtl.sg.formats[SG_PIXELFORMAT_DEPTH_STENCIL]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC1_RGBA]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC2_RGBA]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC3_RGBA]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC3_SRGBA]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC4_R]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC4_RSN]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC5_RG]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC5_RGSN]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC6H_RGBF]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC6H_RGBUF]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC7_RGBA]);
    _sg_pixelformat_sf(&mtl.sg.formats[SG_PIXELFORMAT_BC7_SRGBA]);
    ASSERT_NOT_IOS();
}

//-- main Metal backend state and functions ------------------------------------
fn setup_backend(mtl: *Impl, desc: *sg_desc) void = {
    // assume already zero-initialized
    @debug_assert(desc);
    @debug_assert(desc.environment.metal.device);
    @debug_assert(desc.uniform_buffer_size > 0);
    _sg_mtl_init_pool(desc);
    _sg_mtl_clear_state_cache();
    mtl.valid = true;
    mtl.ub_size = desc.uniform_buffer_size;
    mtl.sem = dispatch_semaphore_create(SG_NUM_INFLIGHT_FRAMES);
    mtl.device = (__bridge id<MTLDevice>) desc.environment.metal.device;
    mtl.cmd_queue = [mtl.device newCommandQueue];

    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        mtl.uniform_buffers[i] = [mtl.device
            newBufferWithLength:(NSUInteger)mtl.ub_size
            options:MTLResourceCPUCacheModeWriteCombined|MTLResourceStorageModeShared
        ];
        if SOKOL_DEBUG {
            mtl.uniform_buffers[i].label = [NSString stringWithFormat:@"sg-uniform-buffer.%d", i];
        }
    }

    if desc.mtl_force_managed_storage_mode {
        mtl.use_shared_storage_mode = false;
    } else {
        ASSERT_NOT_OLD();
        // on Intel Macs, always use managed resources even though the
        // device says it supports unified memory (because of texture restrictions)
        is_apple_gpu := [mtl.device supportsFamily:MTLGPUFamilyApple1];
        mtl.use_shared_storage_mode = is_apple_gpu;
    };
    init_caps();
}

fn discard_backend(mtl: *Impl) void = {
    @debug_assert(mtl.valid);
    // wait for the last frame to finish
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_wait(mtl.sem, DISPATCH_TIME_FOREVER);
    }
    // semaphore must be "relinquished" before destruction
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_signal(mtl.sem);
    }
    mtl.garbage_collect(mtl.sg.frame_index + SG_NUM_INFLIGHT_FRAMES + 2);
    destroy_pool();
    mtl.valid = false;

    release(mtl.sem);
    release(mtl.device);
    release(mtl.cmd_queue);
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        release(mtl.uniform_buffers[i]);
    }
    // NOTE: MTLCommandBuffer, MTLRenderCommandEncoder and MTLComputeCommandEncoder are auto-released
    mtl.cmd_buffer = zeroed ObjCId;
    mtl.render_cmd_encoder = zeroed ObjCId;
    mtl.compute_cmd_encoder = zeroed ObjCId;
}

fn reset_state_cache(mtl: *Impl) void = {
    _sg_mtl_clear_state_cache();
}

fn create_buffer(mtl: *Impl, buf: *Sg.Buffer.T, desc: *Sg.Buffer.Desc) sg_resource_state = {
    @debug_assert(buf && desc);
    @debug_assert(buf.cmn.size > 0);
    injected := (0 != desc.mtl_buffers[0]);
    mtl_options := to_mtl(buf.cmn.usage);
    range(0, buf.cmn.num_slots) { slot | 
        id<MTLBuffer> mtl_buf;
        if (injected) {
            @debug_assert(desc.mtl_buffers[slot]);
            mtl_buf = (__bridge id<MTLBuffer>) desc.mtl_buffers[slot];
        } else {
            if (desc.data.ptr) {
                @debug_assert(desc.data.size > 0);
                mtl_buf = [mtl.device newBufferWithBytes:desc.data.ptr length:(NSUInteger)buf.cmn.size options:mtl_options];
            } else {
                // this is guaranteed to zero-initialize the buffer
                mtl_buf = [mtl.device newBufferWithLength:(NSUInteger)buf.cmn.size options:mtl_options];
            }
            if (nil == mtl_buf) {
                _SG_ERROR(METAL_CREATE_BUFFER_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
        }
        if SOKOL_DEBUG && desc.label {
            mtl_buf.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        buf.mtl.buf[slot] = mtl.add_resource(mtl_buf);
        release(mtl_buf);
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard_buffer(mtl: *Impl, buf: *Sg.Buffer.T) void = {
    @debug_assert(buf);
    range(0, buf.cmn.num_slots) { slot | 
        // it's valid to call release resource with '0'
        _sg_mtl_release_resource(mtl.sg.frame_index, buf.mtl.buf[slot]);
    }
}

fn copy_image_data(mtl: *Impl, img: *Sg.Image.T, mtl_tex: id("MTLTexture"), data: *sg_image_data) void = {
    num_faces := if(img.cmn.type == SG_IMAGETYPE_CUBE, => 6, => 1);
    num_slices := if(img.cmn.type == SG_IMAGETYPE_ARRAY, => img.cmn.num_slices, => 1);
    range(0, num_faces) { face_index |
        range(0, img.cmn.num_mipmaps) { mip_index |
            @debug_assert(data.subimage[face_index][mip_index].ptr);
            @debug_assert(data.subimage[face_index][mip_index].size > 0);
            const uint8_t* data_ptr = (const uint8_t*)data.subimage[face_index][mip_index].ptr;
            const int mip_width = _sg_miplevel_dim(img.cmn.width, mip_index);
            const int mip_height = _sg_miplevel_dim(img.cmn.height, mip_index);
            int bytes_per_row = _sg_row_pitch(img.cmn.pixel_format, mip_width, 1);
            int bytes_per_slice = _sg_surface_pitch(img.cmn.pixel_format, mip_width, mip_height, 1);
            /* bytesPerImage special case: https://developer.apple.com/documentation/metal/mtltexture/1515679-replaceregion

                "Supply a nonzero value only when you copy data to a MTLTextureType3D type texture"
            */
            MTLOrigin :: @struct(x: u64, y: u64, z: u64);
            MTLRegion :: @struct(origin := zeroed MTLOrigin, size: MTLSize);

            mip_depth, bytes_per_image := if (img.cmn.type == SG_IMAGETYPE_3D) {
                // FIXME: apparently the minimal bytes_per_image size for 3D texture is 4 KByte... somehow need to handle this
                (_sg_miplevel_dim(img.cmn.num_slices, mip_index), bytes_per_slice)
            } else {
                (1, 0)
            };
            region: MTLRegion = (size = (width = (NSUInteger)mip_width, height = (NSUInteger)mip_height, depth = (NSUInteger)mip_depth);

            range(0, num_slices) { slice_index | 
                mtl_slice_index := @if(img.cmn.type == SG_IMAGETYPE_CUBE, face_index, slice_index);
                slice_offset := slice_index * bytes_per_slice;
                @debug_assert((slice_offset + bytes_per_slice) <= (int)data.subimage[face_index][mip_index].size);
                [mtl_tex replaceRegion:region
                    mipmapLevel:(NSUInteger)mip_index
                    slice:(NSUInteger)mtl_slice_index
                    withBytes:data_ptr + slice_offset
                    bytesPerRow:(NSUInteger)bytes_per_row
                    bytesPerImage:(NSUInteger)bytes_per_image];
            }
        }
    }
}

// initialize MTLTextureDescriptor with common attributes
fn _sg_mtl_init_texdesc_common(it: id("MTLTextureDescriptor"), img: *Sg.Image.T) bool = {
    @objc it.textureType = _sg_mtl_texture_type(img.cmn.type);
    @objc it.pixelFormat = _sg_mtl_pixel_format(img.cmn.pixel_format);
    if (0 == @objc @as(i64) it.pixelFormat()) {
        _SG_ERROR(METAL_TEXTURE_FORMAT_NOT_SUPPORTED);
        return false;
    }
    @objc it.width = (NSUInteger)img.cmn.width;
    @objc it.height = (NSUInteger)img.cmn.height;
    if (SG_IMAGETYPE_3D == img.cmn.type) {
        @objc it.depth = (NSUInteger)img.cmn.num_slices;
    } else {
        @objc it.depth = 1;
    }
    it.mipmapLevelCount = (NSUInteger)img.cmn.num_mipmaps;
    if (SG_IMAGETYPE_ARRAY == img.cmn.type) {
        @objc it.arrayLength = (NSUInteger)img.cmn.num_slices;
    } else {
        @objc it.arrayLength = 1;
    }
    @objc it.usage = MTLTextureUsageShaderRead;
    MTLResourceOptions res_options = 0;
    if (img.cmn.usage != SG_USAGE_IMMUTABLE) {
        res_options |= MTLResourceCPUCacheModeWriteCombined;
    }
    res_options |= _sg_mtl_resource_options_storage_mode_managed_or_shared();
    @objc it.resourceOptions = res_options;
    return true;
}

// initialize MTLTextureDescriptor with rendertarget attributes
fn init_texdesc_rt(mtl_desc: id("MTLTextureDescriptor"), img: *Sg.Image.T) {
    @debug_assert(img.cmn.render_target);
    @objc mtl_desc.usage = MTLTextureUsageShaderRead | MTLTextureUsageRenderTarget;
    @objc mtl_desc.resourceOptions = MTLResourceStorageModePrivate;
}

// initialize MTLTextureDescriptor with MSAA attributes
fn init_texdesc_rt_msaa(it: id("MTLTextureDescriptor"), img: *Sg.Image.T) {
    @debug_assert(img.cmn.sample_count > 1);
    @objc it.usage = MTLTextureUsageShaderRead | MTLTextureUsageRenderTarget;
    @objc it.resourceOptions = MTLResourceStorageModePrivate;
    @objc it.textureType = MTLTextureType2DMultisample;
    @objc it.sampleCount = (NSUInteger)img.cmn.sample_count;
}

fn create_image(mtl: *Impl, img: *Sg.Image.T, desc: *Sg.Image.Desc) sg_resource_state = {
    @debug_assert(img && desc);
    injected := (0 != desc.mtl_textures[0]);

    // first initialize all Metal resource pool slots to 'empty'
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        img.mtl.tex[i] = mtl.add_resource(nil);
    }

    // initialize a Metal texture descriptor
    mtl_desc := [[MTLTextureDescriptor alloc] init];
    if (!_sg_mtl_init_texdesc_common(mtl_desc, img)) {
        release(mtl_desc);
        return SG_RESOURCESTATE_FAILED;
    }
    if (img.cmn.render_target) {
        if (img.cmn.sample_count > 1) {
            init_texdesc_rt_msaa(mtl_desc, img);
        } else {
            init_texdesc_rt(mtl_desc, img);
        }
    }
    range(0, img.cmn.num_slots) { slot |
        id<MTLTexture> mtl_tex;
        if (injected) {
            @debug_assert(desc.mtl_textures[slot]);
            mtl_tex = (__bridge id<MTLTexture>) desc.mtl_textures[slot];
        } else {
            mtl_tex = [mtl.device newTextureWithDescriptor:mtl_desc];
            if (nil == mtl_tex) {
                release(mtl_desc);
                _SG_ERROR(METAL_CREATE_TEXTURE_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
            if ((img.cmn.usage == SG_USAGE_IMMUTABLE) && !img.cmn.render_target) {
                _sg_mtl_copy_image_data(img, mtl_tex, &desc.data);
            }
        }
        if SOKOL_DEBUG && desc.label {
            mtl_tex.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        img.mtl.tex[slot] = mtl.add_resource(mtl_tex);
        release(mtl_tex);
    }
    release(mtl_desc);
    return SG_RESOURCESTATE_VALID;
}

fn discard(mtl: *Impl, img: *Sg.Image.T) void = {
    @debug_assert(img);
    // it's valid to call release resource with a 'null resource'
    range(0, img.cmn.num_slots) { slot |
        _sg_mtl_release_resource(mtl.sg.frame_index, img.mtl.tex[slot]);
    }
}

fn create(mtl: *Impl, smp: *Sg.Sampler.T, desc: *Sg.Sampler.Desc) sg_resource_state = {
    @debug_assert(smp && desc);
    id<MTLSamplerState> mtl_smp;
    const bool injected = (0 != desc.mtl_sampler);
    if (injected) {
        @debug_assert(desc.mtl_sampler);
        mtl_smp = (__bridge id<MTLSamplerState>) desc.mtl_sampler;
    } else {
        it := [[MTLSamplerDescriptor alloc] init];
        @objc it.setSAddressMode(to_mtl(desc.wrap_u));
        @objc it.setTAddressMode(to_mtl(desc.wrap_v));
        @objc it.setRAddressMode(to_mtl(desc.wrap_w));
        if (mtl.sg.features.image_clamp_to_border) {
            ASSERT_NOT_OLD();
            @objc it.borderColor  = _sg_mtl_border_color(desc.border_color);
        }
        @objc it.minFilter = _sg_mtl_minmag_filter(desc.min_filter);
        @objc it.magFilter = _sg_mtl_minmag_filter(desc.mag_filter);
        @objc it.mipFilter = _sg_mtl_mipmap_filter(desc.mipmap_filter);
        @objc it.lodMinClamp = desc.min_lod;
        @objc it.lodMaxClamp = desc.max_lod;
        // FIXME: lodAverage?
        @objc it.maxAnisotropy = desc.max_anisotropy;
        @objc it.normalizedCoordinates = YES;
        @objc it.compareFunction = _sg_mtl_compare_func(desc.compare);
        if SOKOL_DEBUG && desc.label {
            @objc it.label = [NSString stringWithUTF8String:desc.label];
        }
        mtl_smp = [mtl.device newSamplerStateWithDescriptor:it];
        @objc it.release();
        if (nil == mtl_smp) {
            _SG_ERROR(METAL_CREATE_SAMPLER_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    }
    smp.mtl.sampler_state = mtl.add_resource(mtl_smp);
    release(mtl_smp);
    return SG_RESOURCESTATE_VALID;
}

fn discard(mtl: *Impl, smp: *Sg.Sampler.T) void = {
    @debug_assert(smp);
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(mtl.sg.frame_index, smp.mtl.sampler_state);
}

fn id(classname: Str) Type = ObjCId;

fn create_shader_func(mtl: *Impl, func: *sg_shader_function, label: CStr, label_ext: CStr, res: *_sg_mtl_shader_func_t) bool = {
    @debug_assert(res.mtl_lib == _SG_MTL_INVALID_SLOT_INDEX);
    @debug_assert(res.mtl_func == _SG_MTL_INVALID_SLOT_INDEX);
    err := zeroed id("NSError");
    mtl_lib: id("MTLLibrary") = if !func.bytecode.ptr.is_null() {
        @debug_assert(func.bytecode.size > 0);
        // TODO: where to #import from?
        fn dispatch_data_create(buffer: rawptr, size: i64, queue: i64, destructor: i64) id("NSData") #import("CoreFoundation");
    	// DISPATCH_DATA_DESTRUCTOR_DEFAULT is 0
        lib_data := dispatch_data_create(func.bytecode.ptr, func.bytecode.size, 0, 0);
        lib := @objc mtl.device.newLibraryWithData(lib_data, error = err&);
        release(lib_data);
        lib
    } else {
        src := @objc c.NSString.stringWithUTF8String(func.source);
        @objc mtl.device.newLibraryWithSource(src, options = zeroed(ObjCId), error = err&)
    };
    if !err.is_nil() {
        _SG_ERROR(METAL_SHADER_CREATION_FAILED);
        _SG_LOGMSG(METAL_SHADER_COMPILATION_OUTPUT, [err.localizedDescription UTF8String]);
     }
    if(mtl_lib.is_nil(), => return(false));
    if SOKOL_DEBUG && label {
        @debug_assert(label_ext);
        mtl_lib.label = [NSString stringWithFormat:@"%s.%s", label, label_ext];
    }
    @debug_assert(func.entry);
    mtl_func := [mtl_lib newFunctionWithName:[NSString stringWithUTF8String:func.entry]];
    if (mtl_func == nil) {
        _SG_ERROR(METAL_SHADER_ENTRY_NOT_FOUND);
        release(mtl_lib);
        return false;
    }
    res.mtl_lib = mtl.add_resource(mtl_lib);
    res.mtl_func = mtl.add_resource(mtl_func);
    release(mtl_lib);
    release(mtl_func);
    return true;
}

fn release(self: ObjCId) void = @objc self.release();

fn discard_shader_func(mtl: *Impl, func: *_sg_mtl_shader_func_t) void = {
    // it is valid to call _sg_mtl_release_resource with a 'null resource'
    _sg_mtl_release_resource(mtl.sg.frame_index, func.mtl_func);
    _sg_mtl_release_resource(mtl.sg.frame_index, func.mtl_lib);
}

// NOTE: this is an out-of-range check for MSL bindslots that's also active in release mode
fn ensure_msl_bindslot_ranges(mtl: *Impl, desc: *Sg.Shader.Desc) bool = {
    @debug_assert(desc);
    X :: fn(count: i64, $get: @Fn(i: i64) i64, limit: i64, error) => 
        range(0, count) { i |
            if get(i) >= limit {
                _SG_ERROR(error);
                return false;
            };
        };
    X(SG_MAX_UNIFORMBLOCK_BINDSLOTS, fn(i) => desc.uniform_blocks[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_BINDINGS, METAL_UNIFORMBLOCK_MSL_BUFFER_SLOT_OUT_OF_RANGE);
    X(SG_MAX_STORAGEBUFFER_BINDSLOTS, fn(i) => desc.storage_buffers[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS, METAL_STORAGEBUFFER_MSL_BUFFER_SLOT_OUT_OF_RANGE);
    X(SG_MAX_IMAGE_BINDSLOTS, fn(i) => desc.images[i].msl_buffer_n, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS, METAL_IMAGE_MSL_TEXTURE_SLOT_OUT_OF_RANGE);
    X(SG_MAX_SAMPLER_BINDSLOTS, fn(i) => desc.samplers[i].msl_sampler_n, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS, METAL_SAMPLER_MSL_SAMPLER_SLOT_OUT_OF_RANGE);
    true
}

MTLSize :: @struct(width: u64, height: u64, depth: u64);
fn create_shader(mtl: *Impl, shd: *Sg.Shader.T, desc: *Sg.Shader.Desc) sg_resource_state = {
    @debug_assert(shd && desc);

    // do a MSL bindslot range check also in release mode, and if that fails,
    // also fail shader creation
    if (!_sg_mtl_ensure_msl_bindslot_ranges(desc)) {
        return SG_RESOURCESTATE_FAILED;
    }
    
    shd.mtl.threads_per_threadgroup = (
        width = (NSUInteger)desc.mtl_threads_per_threadgroup.x,
        height = (NSUInteger)desc.mtl_threads_per_threadgroup.y,
        depth = (NSUInteger)desc.mtl_threads_per_threadgroup.z,
    );

    // copy resource bindslot mappings
    for (size_t i = 0; i < SG_MAX_UNIFORMBLOCK_BINDSLOTS; i++) {
        shd.mtl.ub_buffer_n[i] = desc.uniform_blocks[i].msl_buffer_n;
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        shd.mtl.sbuf_buffer_n[i] = desc.storage_buffers[i].msl_buffer_n;
    }
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        shd.mtl.img_texture_n[i] = desc.images[i].msl_texture_n;
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        shd.mtl.smp_sampler_n[i] = desc.samplers[i].msl_sampler_n;
    }

    // create metal library and function objects
    bool shd_valid = true;
    if (desc.vertex_func.source || desc.vertex_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.vertex_func, desc.label, "vs", &shd.mtl.vertex_func);
    }
    if (desc.fragment_func.source || desc.fragment_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.fragment_func, desc.label, "fs", &shd.mtl.fragment_func);
    }
    if (desc.compute_func.source || desc.compute_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.compute_func, desc.label, "cs", &shd.mtl.compute_func);
    }
    if (!shd_valid) {
        mtl.discard_shader_func(&shd.mtl.vertex_func);
        mtl.discard_shader_func(&shd.mtl.fragment_func);
        mtl.discard_shader_func(&shd.mtl.compute_func);
    }
    return shd_valid ? SG_RESOURCESTATE_VALID : SG_RESOURCESTATE_FAILED;
}

fn discard(mtl: *Impl, shd: *Sg.Shader.T) void = {
    mtl.discard_shader_func(&shd.mtl.vertex_func);
    mtl.discard_shader_func(&shd.mtl.fragment_func);
    mtl.discard_shader_func(&shd.mtl.compute_func);
}

MTLMutabilityImmutable :: 2;

fn create_pipeline(mtl: *Impl, pip: *Sg.Pipeline.T, shd: *Sg.Shader.T, desc: *Sg.Pipeline.Desc) sg_resource_state = {
    @debug_assert(pip && shd && desc);
    @debug_assert(desc.shader.id == shd.slot.id);

    pip.shader = shd;

    if (pip.cmn.is_compute) {
        NSError* err = NULL;
        MTLComputePipelineDescriptor* cp_desc = [[MTLComputePipelineDescriptor alloc] init];
        cp_desc.computeFunction = mtl.get_id(shd.mtl.compute_func.mtl_func);
        cp_desc.threadGroupSizeIsMultipleOfThreadExecutionWidth = true;
        for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
            const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
            @debug_assert((stage != SG_SHADERSTAGE_VERTEX) && (stage != SG_SHADERSTAGE_FRAGMENT));
            if ((stage == SG_SHADERSTAGE_COMPUTE) && shd.cmn.storage_buffers[i].readonly) {
                const NSUInteger mtl_slot = shd.mtl.sbuf_buffer_n[i];
                cp_desc.buffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        if SOKOL_DEBUG && desc.label {
            cp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        id<MTLComputePipelineState> mtl_cps = [mtl.device
            newComputePipelineStateWithDescriptor:cp_desc
            options:MTLPipelineOptionNone
            reflection:nil
            error:&err];
        release(cp_desc);
        if (nil == mtl_cps) {
            @debug_assert(err);
            _SG_ERROR(METAL_CREATE_CPS_FAILED);
            _SG_LOGMSG(METAL_CREATE_CPS_OUTPUT, [err.localizedDescription UTF8String]);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.cps = mtl.add_resource(mtl_cps);
        release(mtl_cps);
        pip.mtl.threads_per_threadgroup = shd.mtl.threads_per_threadgroup;
    } else {
        prim_type := desc.primitive_type;
        pip.mtl.prim_type = to_mtl(prim_type);
        pip.mtl.index_size = to_mtl(pip.cmn.index_type);
        if (SG_INDEXTYPE_NONE != pip.cmn.index_type) {
            pip.mtl.index_type = to_mtl(pip.cmn.index_type);
        }
        pip.mtl.cull_mode = to_mtl(desc.cull_mode);
        pip.mtl.winding = to_mtl(desc.face_winding);
        pip.mtl.stencil_ref = desc.stencil.ref;

        // create vertex-descriptor
        MTLVertexDescriptor* vtx_desc = [MTLVertexDescriptor vertexDescriptor];
        for (NSUInteger attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
            const sg_vertex_attr_state* a_state = &desc.layout.attrs[attr_index];
            if (a_state.format == SG_VERTEXFORMAT_INVALID) {
                break;
            }
            @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
            @debug_assert(pip.cmn.vertex_buffer_layout_active[a_state.buffer_index]);
            vtx_desc.attributes[attr_index].format = _sg_mtl_vertex_format(a_state.format);
            vtx_desc.attributes[attr_index].offset = (NSUInteger)a_state.offset;
            vtx_desc.attributes[attr_index].bufferIndex = _sg_mtl_vertexbuffer_bindslot((size_t)a_state.buffer_index);
        }
        for (NSUInteger layout_index = 0; layout_index < SG_MAX_VERTEXBUFFER_BINDSLOTS; layout_index++) {
            if (pip.cmn.vertex_buffer_layout_active[layout_index]) {
                const sg_vertex_buffer_layout_state* l_state = &desc.layout.buffers[layout_index];
                const NSUInteger mtl_vb_slot = _sg_mtl_vertexbuffer_bindslot(layout_index);
                @debug_assert(l_state.stride > 0);
                vtx_desc.layouts[mtl_vb_slot].stride = (NSUInteger)l_state.stride;
                vtx_desc.layouts[mtl_vb_slot].stepFunction = _sg_mtl_step_function(l_state.step_func);
                vtx_desc.layouts[mtl_vb_slot].stepRate = (NSUInteger)l_state.step_rate;
                if (SG_VERTEXSTEP_PER_INSTANCE == l_state.step_func) {
                    // NOTE: not actually used in _sg_mtl_draw()
                    pip.cmn.use_instanced_draw = true;
                }
            }
        }

        // render-pipeline descriptor
        rp_desc := [[MTLRenderPipelineDescriptor alloc] init];
        @objc rp_desc.vertexDescriptor = vtx_desc;
        @debug_assert(shd.mtl.vertex_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        @objc rp_desc.vertexFunction = mtl.get_id(shd.mtl.vertex_func.mtl_func);
        @debug_assert(shd.mtl.fragment_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        @objc rp_desc.fragmentFunction = mtl.get_id(shd.mtl.fragment_func.mtl_func);
        @objc rp_desc.rasterSampleCount = (NSUInteger)desc.sample_count;
        @objc rp_desc.alphaToCoverageEnabled = desc.alpha_to_coverage_enabled;
        @objc rp_desc.alphaToOneEnabled = NO;
        @objc rp_desc.rasterizationEnabled = YES;
        @objc rp_desc.depthAttachmentPixelFormat = _sg_mtl_pixel_format(desc.depth.pixel_format);
        if (desc.depth.pixel_format == SG_PIXELFORMAT_DEPTH_STENCIL) {
            @objc rp_desc.stencilAttachmentPixelFormat = _sg_mtl_pixel_format(desc.depth.pixel_format);
        }
        at := @objc rp_desc.colorAttachments();
        for (NSUInteger i = 0; i < (NSUInteger)desc.color_count; i++) {
            @debug_assert(i < SG_MAX_COLOR_ATTACHMENTS);
            const sg_color_target_state* cs = &desc.colors[i];
            it := @objc at.objectAtIndex(i);
            @objc it.pixelFormat = _sg_mtl_pixel_format(cs.pixel_format);
            @objc it.writeMask = _sg_mtl_color_write_mask(cs.write_mask);
            @objc it.blendingEnabled = cs.blend.enabled;
            @objc it.alphaBlendOperation = _sg_mtl_blend_op(cs.blend.op_alpha);
            @objc it.rgbBlendOperation = _sg_mtl_blend_op(cs.blend.op_rgb);
            @objc it.destinationAlphaBlendFactor = _sg_mtl_blend_factor(cs.blend.dst_factor_alpha);
            @objc it.destinationRGBBlendFactor = _sg_mtl_blend_factor(cs.blend.dst_factor_rgb);
            @objc it.sourceAlphaBlendFactor = _sg_mtl_blend_factor(cs.blend.src_factor_alpha);
            @objc it.sourceRGBBlendFactor = _sg_mtl_blend_factor(cs.blend.src_factor_rgb);
        }
        // set buffer mutability for all read-only buffers (vertex buffers and read-only storage buffers)
        for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
            if (pip.cmn.vertex_buffer_layout_active[i]) {
                const NSUInteger mtl_slot = _sg_mtl_vertexbuffer_bindslot(i);
                rp_desc.vertexBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
            const NSUInteger mtl_slot = shd.mtl.sbuf_buffer_n[i];
            const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
            @debug_assert(stage != SG_SHADERSTAGE_COMPUTE);
            if (stage == SG_SHADERSTAGE_VERTEX) {
                @debug_assert(shd.cmn.storage_buffers[i].readonly);
                rp_desc.vertexBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
                @debug_assert(shd.cmn.storage_buffers[i].readonly);
                rp_desc.fragmentBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        if SOKOL_DEBUG && desc.label {
            rp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        NSError* err = NULL;
        id<MTLRenderPipelineState> mtl_rps = [mtl.device newRenderPipelineStateWithDescriptor:rp_desc error:&err];
        release(rp_desc);
        if (nil == mtl_rps) {
            @debug_assert(err);
            _SG_ERROR(METAL_CREATE_RPS_FAILED);
            _SG_LOGMSG(METAL_CREATE_RPS_OUTPUT, [err.localizedDescription UTF8String]);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.rps = mtl.add_resource(mtl_rps);
        release(mtl_rps);

        // depth-stencil-state
        ds_desc := [[MTLDepthStencilDescriptor alloc] init];
        @objc ds_desc.setDepthCompareFunction(_sg_mtl_compare_func(desc.depth.compare));
        @objc ds_desc.setDepthWriteEnabled(desc.depth.write_enabled);
        if (desc.stencil.enabled) {
            const sg_stencil_face_state* sb = &desc.stencil.back;
            it := [[MTLStencilDescriptor alloc] init];
            @objc ds_desc.setBackFaceStencil(it);
            @objc it.setStencilFailureOperation(_sg_mtl_stencil_op(sb.fail_op));
            @objc it.setDepthFailureOperation(_sg_mtl_stencil_op(sb.depth_fail_op));
            @objc it.setDepthStencilPassOperation(_sg_mtl_stencil_op(sb.pass_op));
            @objc it.setStencilCompareFunction(_sg_mtl_compare_func(sb.compare));
            @objc it.setReadMask(desc.stencil.read_mask);
            @objc it.setWriteMask(desc.stencil.write_mask);
            const sg_stencil_face_state* sf = &desc.stencil.front;
            it := [[MTLStencilDescriptor alloc] init];
            @objc ds_desc.setFrontFaceStencil(it);
            @objc it.setStencilFailureOperation(_sg_mtl_stencil_op(sf.fail_op));
            @objc it.setDepthFailureOperation(_sg_mtl_stencil_op(sf.depth_fail_op));
            @objc it.setDepthStencilPassOperation(_sg_mtl_stencil_op(sf.pass_op));
            @objc it.setStencilCompareFunction(_sg_mtl_compare_func(sf.compare));
            @objc it.setReadMask(desc.stencil.read_mask);
            @objc it.setWriteMask(desc.stencil.write_mask);
        }
        if SOKOL_DEBUG && desc.label {
            ds_desc.label = [NSString stringWithFormat:@"%s.dss", desc.label];
        }
        id<MTLDepthStencilState> mtl_dss = [mtl.device newDepthStencilStateWithDescriptor:ds_desc];
        release(ds_desc);
        if (nil == mtl_dss) {
            _SG_ERROR(METAL_CREATE_DSS_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.dss = mtl.add_resource(mtl_dss);
        release(mtl_dss);
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard(mtl: *Impl, pip: *Pipeline.T) void = {
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.cps);
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.rps);
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.dss);
}

fn create_attachments(mtl: *Impl, atts: *Sg.Attachments.T, color_images: **Sg.Image.T, resolve_images: **Sg.Image.T, ds_img: *Sg.Image.T, desc: *Sg.Attachments.Desc) sg_resource_state = {
    @debug_assert(atts && desc);
    @debug_assert(color_images && resolve_images);

    // copy image pointers
    range(0, atts.cmn.num_colors) { i |
        color_desc := &desc.colors&.index(i);
        @debug_assert(color_desc.image.id != SG_INVALID_ID);
        @debug_assert(0 == atts.mtl.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        atts.mtl.colors[i].image = color_images[i];

        resolve_desc := &desc.resolves&.index(i);
        if (resolve_desc.image.id != SG_INVALID_ID) {
            @debug_assert(0 == atts.mtl.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            atts.mtl.resolves[i].image = resolve_images[i];
        }
    }
    @debug_assert(0 == atts.mtl.depth_stencil.image);
    ds_desc := desc.depth_stencil&;
    if (ds_desc.image.id != SG_INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        atts.mtl.depth_stencil.image = ds_img;
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard_attachments(_: *Impl, _: *Sg.Attachments.T) void = ();

fn attachments_color_image(_: *Impl, atts: *Sg.Attachments.T, index: i32) *Sg.Image.T = {
    // NOTE: may return null
    @debug_assert(atts && (index >= 0) && (index < SG_MAX_COLOR_ATTACHMENTS));
    atts.mtl.colors[index].image
}

fn attachments_resolve_image(_: *Impl, atts: *Sg.Attachments.T, index: i32) *Sg.Image.T = {
    // NOTE: may return null
    atts.mtl.resolves[@as(i64) index].image
}

fn attachments_ds_image(_: *Impl, atts: *Sg.Attachments.T) *Sg.Image.T = {
    // NOTE: may return null
    atts.mtl.depth_stencil.image
}

fn bind_uniform_buffers(mtl: *Impl) void = {
    // In the Metal backend, uniform buffer bindings happen once in sg_begin_pass() and
    // remain valid for the entire pass. Only binding offsets will be updated
    // in sg_apply_uniforms()
    if (mtl.sg.cur_pass.is_compute) {
        @debug_assert(nil != mtl.compute_cmd_encoder);
        for (size_t slot = 0; slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS; slot++) {
            [mtl.compute_cmd_encoder
                setBuffer:mtl.uniform_buffers[mtl.cur_frame_rotate_index]
                offset:0
                atIndex:slot];
        }
    } else {
        @debug_assert(nil != mtl.render_cmd_encoder);
        for (size_t slot = 0; slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS; slot++) {
            [mtl.render_cmd_encoder
                setVertexBuffer:mtl.uniform_buffers[mtl.cur_frame_rotate_index]
                offset:0
                atIndex:slot];
            [mtl.render_cmd_encoder
                setFragmentBuffer:mtl.uniform_buffers[mtl.cur_frame_rotate_index]
                offset:0
                atIndex:slot];
        }
    }
}

fn begin_compute_pass(mtl: *Impl, pass: *sg_pass) void = {
    @debug_assert(!mtl.cmd_buffer.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());

    // NOTE: we actually want computeCommandEncoderWithDispatchType:MTLDispatchTypeConcurrent, but
    // that requires bumping the macOS base version to 10.14
    mtl.compute_cmd_encoder = @objc mtl.cmd_buffer.computeCommandEncoder();
    if mtl.compute_cmd_encoder.is_nil() {
        mtl.sg.cur_pass.valid = false;
        return();
    }

    if SOKOL_DEBUG && pass.label {
        mtl.compute_cmd_encoder.label = @objc c.NSString.stringWithUTF8String(pass.label);
    }
}

fn begin_render_pass(mtl: *Impl, pass: *sg_pass) void = {
    @debug_assert(pass);
    @debug_assert(nil != mtl.cmd_buffer);
    @debug_assert(nil == mtl.render_cmd_encoder);
    @debug_assert(nil == mtl.compute_cmd_encoder);

    const _sg_attachments_t* atts = mtl.sg.cur_pass.atts;
    const sg_swapchain* swapchain = &pass.swapchain;
    const sg_pass_action* action = &pass.action;

    MTLRenderPassDescriptor* pass_desc = [MTLRenderPassDescriptor renderPassDescriptor];
    @debug_assert(pass_desc);
    if (atts) {
        // setup pass descriptor for offscreen rendering
        @debug_assert(atts.slot.state == SG_RESOURCESTATE_VALID);
        at := @objc pass_desc.colorAttachments();
        for (NSUInteger i = 0; i < (NSUInteger)atts.cmn.num_colors; i++) {
            ca := @objc at.objectAtIndex(i);
            cmn_color_att := &atts.cmn.colors[i];
            mtl_color_att := &atts.mtl.colors[i];
            color_att_img := mtl_color_att.image;
            cmn_resolve_att := &atts.cmn.resolves[i];
            mtl_resolve_att := &atts.mtl.resolves[i];
            resolve_att_img := mtl_resolve_att.image;
            @debug_assert(color_att_img.slot.state == SG_RESOURCESTATE_VALID);
            @debug_assert(color_att_img.slot.id == cmn_color_att.image_id.id);
            @debug_assert(color_att_img.mtl.tex[color_att_img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
            @objc ca.loadAction = _sg_mtl_load_action(action.colors[i].load_action);
            @objc ca.storeAction = _sg_mtl_store_action(action.colors[i].store_action, resolve_att_img != 0);
            c := action.colors[i].clear_value;
            @objc ca.clearColor = MTLClearColorMake(c.r, c.g, c.b, c.a);
            @objc ca.texture = mtl.get_id(color_att_img.mtl.tex[color_att_img.cmn.active_slot]);
            @objc ca.level = (NSUInteger)cmn_color_att.mip_level;
            switch (color_att_img.cmn.type) {
                case SG_IMAGETYPE_CUBE:
                case SG_IMAGETYPE_ARRAY:
                    @objc ca.slice = (NSUInteger)cmn_color_att.slice;
                    break;
                case SG_IMAGETYPE_3D:
                    @objc ca.depthPlane = (NSUInteger)cmn_color_att.slice;
                    break;
                default: break;
            }
            if (resolve_att_img) {
                @debug_assert(resolve_att_img.slot.state == SG_RESOURCESTATE_VALID);
                @debug_assert(resolve_att_img.slot.id == cmn_resolve_att.image_id.id);
                @debug_assert(resolve_att_img.mtl.tex[resolve_att_img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
                @objc ca.resolveTexture = mtl.get_id(resolve_att_img.mtl.tex[resolve_att_img.cmn.active_slot]);
                @objc ca.resolveLevel = (NSUInteger)cmn_resolve_att.mip_level;
                switch (resolve_att_img.cmn.type) {
                    case SG_IMAGETYPE_CUBE:
                    case SG_IMAGETYPE_ARRAY:
                        @objc ca.resolveSlice = (NSUInteger)cmn_resolve_att.slice;
                        break;
                    case SG_IMAGETYPE_3D:
                        @objc ca.resolveDepthPlane = (NSUInteger)cmn_resolve_att.slice;
                        break;
                    default: break;
                }
            }
        }
        const _sg_image_t* ds_att_img = atts.mtl.depth_stencil.image;
        if (0 != ds_att_img) {
            @debug_assert(ds_att_img.slot.state == SG_RESOURCESTATE_VALID);
            @debug_assert(ds_att_img.slot.id == atts.cmn.depth_stencil.image_id.id);
            @debug_assert(ds_att_img.mtl.tex[ds_att_img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
            d := @objc pass_desc.depthAttachment();
            @objc d.texture = mtl.get_id(ds_att_img.mtl.tex[ds_att_img.cmn.active_slot]);
            @objc d.loadAction = _sg_mtl_load_action(action.depth.load_action);
            @objc d.storeAction = _sg_mtl_store_action(action.depth.store_action, false);
            @objc d.clearDepth = action.depth.clear_value;
            const _sg_attachment_common_t* cmn_ds_att = &atts.cmn.depth_stencil;
            switch (ds_att_img.cmn.type) {
                case SG_IMAGETYPE_CUBE:
                case SG_IMAGETYPE_ARRAY:
                    @objc d.slice = (NSUInteger)cmn_ds_att.slice;
                    break;
                case SG_IMAGETYPE_3D:
                    @objc d.resolveDepthPlane = (NSUInteger)cmn_ds_att.slice;
                    break;
                default: break;
            }
            if (_sg_is_depth_stencil_format(ds_att_img.cmn.pixel_format)) {
                sa := @objc pass_desc.stencilAttachment();
                @objc sa.texture = mtl.get_id(ds_att_img.mtl.tex[ds_att_img.cmn.active_slot]);
                @objc sa.loadAction = _sg_mtl_load_action(action.stencil.load_action);
                @objc sa.storeAction = _sg_mtl_store_action(action.depth.store_action, false);
                @objc sa.clearStencil = action.stencil.clear_value;
                t := ds_att_img.cmn.type;
                if @is(t, .CUBE, .ARRAY) {
                    @objc sa.slice = (NSUInteger)cmn_ds_att.slice;
                }
                if @is(t, ._3D) {
                    @objc sa.resolveDepthPlane = (NSUInteger)cmn_ds_att.slice;
                }
            }
        }
    } else {
        // setup pass descriptor for swapchain rendering
        //
        // NOTE: at least in macOS Sonoma this no longer seems to be the case, the
        // current drawable is also valid in a minimized window
        // ===
        // an MTKView current_drawable will not be valid if window is minimized, don't do any rendering in this case
        if (0 == swapchain.metal.current_drawable) {
            mtl.sg.cur_pass.valid = false;
            return;
        }
        // pin the swapchain resources into memory so that they outlive their command buffer
        // (this is necessary because the command buffer doesn't retain references)
        int pass_desc_ref = mtl.add_resource(pass_desc);
        _sg_mtl_release_resource(mtl.sg.frame_index, pass_desc_ref);

        mtl.cur_drawable = (__bridge id<CAMetalDrawable>) swapchain.metal.current_drawable;
        ca := @objc (@objc pass_desc.colorAttachments()).objectAtIndex(0);
        if (swapchain.sample_count > 1) {
            // multi-sampling: render into msaa texture, resolve into drawable texture
            id<MTLTexture> msaa_tex = (__bridge id<MTLTexture>) swapchain.metal.msaa_color_texture;
            @debug_assert(msaa_tex != nil);
            @objc ca.texture = msaa_tex;
            @objc ca.resolveTexture = mtl.cur_drawable.texture;
            @objc ca.storeAction = MTLStoreActionMultisampleResolve;
        } else {
            // non-msaa: render into current_drawable
            @objc ca.texture = mtl.cur_drawable.texture;
            @objc ca.storeAction = MTLStoreActionStore;
        }
        @objc ca.loadAction = _sg_mtl_load_action(action.colors[0].load_action);
        c := action.colors[0].clear_value;
        @objc ca.clearColor = MTLClearColorMake(c.r, c.g, c.b, c.a);

        // optional depth-stencil texture
        if (swapchain.metal.depth_stencil_texture) {
            id<MTLTexture> ds_tex = (__bridge id<MTLTexture>) swapchain.metal.depth_stencil_texture;
            @debug_assert(ds_tex != nil);
            d := @objc pass_desc.depthAttachment();
            @objc d.texture = ds_tex;
            @objc d.storeAction = MTLStoreActionDontCare;
            @objc d.loadAction = _sg_mtl_load_action(action.depth.load_action);
            @objc d.clearDepth = action.depth.clear_value;
            if (_sg_is_depth_stencil_format(swapchain.depth_format)) {
                sa := @objc pass_desc.stencilAttachment();
                @objc sa.texture = ds_tex;
                @objc sa.storeAction = MTLStoreActionDontCare;
                @objc sa.loadAction = _sg_mtl_load_action(action.stencil.load_action);
                @objc sa.clearStencil = action.stencil.clear_value;
            }
        }
    }

    // NOTE: at least in macOS Sonoma, the following is no longer the case, a valid
    // render command encoder is also returned in a minimized window
    // ===
    // create a render command encoder, this might return nil if window is minimized
    mtl.render_cmd_encoder = @objc mtl.cmd_buffer.renderCommandEncoderWithDescriptor(pass_desc);
    if mtl.render_cmd_encoder.is_nil() {
        mtl.sg.cur_pass.valid = false;
        return();
    }

    if SOKOL_DEBUG && pass.label {
        mtl.render_cmd_encoder.label = [NSString stringWithUTF8String:pass.label];
    }
}

fn begin_pass(mtl: *Impl, pass: *sg_pass) void = {
    @debug_assert(pass);
    @debug_assert(mtl.cmd_queue);
    @debug_assert(nil == mtl.compute_cmd_encoder);
    @debug_assert(nil == mtl.render_cmd_encoder);
    @debug_assert(nil == mtl.cur_drawable);
    _sg_mtl_clear_state_cache();

    // if this is the first pass in the frame, create one command buffer and blit-cmd-encoder for the entire frame
    if (nil == mtl.cmd_buffer) {
        // block until the oldest frame in flight has finished
        dispatch_semaphore_wait(mtl.sem, DISPATCH_TIME_FOREVER);
        mtl.cmd_buffer = if mtl.sg.desc.mtl_use_command_buffer_with_retained_references) {
            @objc mtl.cmd_queue.commandBuffer()
        } else {
            @objc mtl.cmd_queue.commandBufferWithUnretainedReferences();
        };
        @objc mtl.cmd_buffer.enqueue();
        [mtl.cmd_buffer addCompletedHandler:^(id<MTLCommandBuffer> cmd_buf) {
            // NOTE: this code is called on a different thread!
            _ := cmd_buf;
            dispatch_semaphore_signal(mtl.sem);
        }];
    }

    // if this is first pass in frame, get uniform buffer base pointer
    if (0 == mtl.cur_ub_base_ptr) {
        mtl.cur_ub_base_ptr = (uint8_t*)[mtl.uniform_buffers[mtl.cur_frame_rotate_index] contents];
    }

    if (pass.compute) {
        mtl.begin_compute_pass(pass);
    } else {
        mtl.begin_render_pass(pass);
    }

    // bind uniform buffers, those bindings remain valid for the entire pass
    if mtl.sg.cur_pass.valid {
        _sg_mtl_bind_uniform_buffers();
    }
}

fn end_pass(mtl: *Impl) void = {
    if !mtl.render_cmd_encoder.is_nil() {
        @objc mtl.render_cmd_encoder.endEncoding();
        // NOTE: MTLRenderCommandEncoder is autoreleased
        mtl.render_cmd_encoder = zeroed ObjCId;
    }
    if !mtl.compute_cmd_encoder.is_nil() {
        @objc mtl.compute_cmd_encoder.endEncoding();
        // NOTE: MTLComputeCommandEncoder is autoreleased
        mtl.compute_cmd_encoder = zeroed ObjCId;

        ASSERT_NOT_IOS();
        // synchronize any managed buffers written by the GPU
        if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
            if (mtl.sg.compute.readwrite_sbufs.cur > 0) {
                blit_cmd_encoder := @objc mtl.cmd_buffer.blitCommandEncoder();
                for (uint32_t i = 0; i < mtl.sg.compute.readwrite_sbufs.cur; i++) {
                for mtl.sg.compute.readwrite_sbufs { it |
                    if mtl.sg.lookup(it) { sbuf |
                        [blit_cmd_encoder synchronizeResource:mtl.get_id(sbuf.mtl.buf[sbuf.cmn.active_slot])];
                    }
                }
                @objc blit_cmd_encoder.endEncoding();
            }
        }
    }
    // if this is a swapchain pass, present the drawable
    if !mtl.cur_drawable.is_nil() {
        @objc mtl.cmd_buffer.presentDrawable(mtl.cur_drawable);
        mtl.cur_drawable = zeroed ObjCId;
    }
}

fn commit(mtl: *Impl) void = {
    @debug_assert(nil == mtl.render_cmd_encoder);
    @debug_assert(nil == mtl.compute_cmd_encoder);
    @debug_assert(nil != mtl.cmd_buffer);

    // commit the frame's command buffer
    @objc mtl.cmd_buffer.commit();

    // garbage-collect resources pending for release
    mtl.garbage_collect(mtl.mtl.sg.frame_index);

    // rotate uniform buffer slot
    if (++mtl.cur_frame_rotate_index >= SG_NUM_INFLIGHT_FRAMES) {
        mtl.cur_frame_rotate_index = 0;
    }
    mtl.cur_ub_offset = 0;
    mtl.cur_ub_base_ptr = 0;
    // NOTE: MTLCommandBuffer is autoreleased
    mtl.cmd_buffer = nil;
}

fn apply_viewport(mtl: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(nil != mtl.render_cmd_encoder);
    @debug_assert(mtl.sg.cur_pass.height > 0);
    MTLViewport :: @struct(originX: f64, originY: f64, width: f64, height: f64, znear: f64, zfar: f64);
    vp: MTLViewport = (
        originX = (double) x,
        originY = (double) (origin_top_left ? y : (mtl.sg.cur_pass.height - (y + h))),
        width   = (double) w,
        height  = (double) h,
        znear   = 0.0,
        zfar    = 1.0,
    );
    @objc mtl.render_cmd_encoder.setViewport(vp);
}

fn apply_scissor_rect(mtl: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(nil != mtl.render_cmd_encoder);
    @debug_assert(mtl.sg.cur_pass.width > 0);
    @debug_assert(mtl.sg.cur_pass.height > 0);
    // clip against framebuffer rect
    clip: _sg_recti_t = clipi(x, y, w, h, mtl.sg.cur_pass.width, mtl.sg.cur_pass.height);
    MTLScissorRect :: @struct(x: u64, y: u64, width: u64, height: u64);
    r: MTLScissorRect = (
        x = (NSUInteger)clip.x,
        y = (NSUInteger) (origin_top_left ? clip.y : (mtl.sg.cur_pass.height - (clip.y + clip.h))),
        width = (NSUInteger)clip.w,
        height = (NSUInteger)clip.h,
    );
    [mtl.render_cmd_encoder setScissorRect:r];
}

fn apply_pipeline(mtl: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip);
    @debug_assert(pip.shader && (pip.cmn.shader_id.id == pip.shader.slot.id));
    if (mtl.state_cache.cur_pipeline_id.id != pip.slot.id) {
        mtl.state_cache.cur_pipeline = pip;
        mtl.state_cache.cur_pipeline_id.id = pip.slot.id;
        if (pip.cmn.is_compute) {
            @debug_assert(mtl.sg.cur_pass.is_compute);
            @debug_assert(nil != mtl.compute_cmd_encoder);
            @debug_assert(pip.mtl.cps != _SG_MTL_INVALID_SLOT_INDEX);
            [mtl.compute_cmd_encoder setComputePipelineState:mtl.get_id(pip.mtl.cps)];
        } else {
            @debug_assert(!mtl.sg.cur_pass.is_compute);
            @debug_assert(nil != mtl.render_cmd_encoder);
            sg_color c = pip.cmn.blend_color;
            [mtl.render_cmd_encoder setBlendColorRed:c.r green:c.g blue:c.b alpha:c.a];
            _sg_stats_add(metal.pipeline.num_set_blend_color, 1);
            [mtl.render_cmd_encoder setCullMode:pip.mtl.cull_mode];
            _sg_stats_add(metal.pipeline.num_set_cull_mode, 1);
            [mtl.render_cmd_encoder setFrontFacingWinding:pip.mtl.winding];
            _sg_stats_add(metal.pipeline.num_set_front_facing_winding, 1);
            [mtl.render_cmd_encoder setStencilReferenceValue:pip.mtl.stencil_ref];
            _sg_stats_add(metal.pipeline.num_set_stencil_reference_value, 1);
            [mtl.render_cmd_encoder setDepthBias:pip.cmn.depth.bias slopeScale:pip.cmn.depth.bias_slope_scale clamp:pip.cmn.depth.bias_clamp];
            _sg_stats_add(metal.pipeline.num_set_depth_bias, 1);
            @debug_assert(pip.mtl.rps != _SG_MTL_INVALID_SLOT_INDEX);
            [mtl.render_cmd_encoder setRenderPipelineState:mtl.get_id(pip.mtl.rps)];
            _sg_stats_add(metal.pipeline.num_set_render_pipeline_state, 1);
            @debug_assert(pip.mtl.dss != _SG_MTL_INVALID_SLOT_INDEX);
            [mtl.render_cmd_encoder setDepthStencilState:mtl.get_id(pip.mtl.dss)];
            _sg_stats_add(metal.pipeline.num_set_depth_stencil_state, 1);
        }
    }
}

fn apply_bindings(mtl: *Impl, bnd: *_sg_bindings_t) bool = {
    @debug_assert(bnd);
    @debug_assert(bnd.pip);
    @debug_assert(bnd.pip && bnd.pip.shader);
    @debug_assert(bnd.pip.shader.slot.id == bnd.pip.cmn.shader_id.id);
    shd, sc := (bnd.pip.shader, mtl.state_cache&);

    // don't set vertex- and index-buffers in compute passes
    if (!mtl.sg.cur_pass.is_compute) {
        @debug_assert(nil != mtl.render_cmd_encoder);
        // store index buffer binding, this will be needed later in sg_draw()
        sc.cur_indexbuffer = bnd.ib;
        sc.cur_indexbuffer_offset = bnd.ib_offset;
        sc.cur_indexbuffer_id.id = if (bnd.ib) {
            @debug_assert(bnd.pip.cmn.index_type != SG_INDEXTYPE_NONE);
            bnd.ib.slot.id
        } else {
            @debug_assert(bnd.pip.cmn.index_type == SG_INDEXTYPE_NONE);
            SG_INVALID_ID
        };
        // apply vertex buffers
        range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            continue :: local_return;
            const _sg_buffer_t* vb = bnd.vbs[i];
            if(vb == 0, => continue());
            mtl_slot := _sg_mtl_vertexbuffer_bindslot(i);
            @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
            vb_offset := bnd.vb_offsets[i];
            if ((sc.cur_vs_buffer_ids[mtl_slot].id != vb.slot.id) ||
                (sc.cur_vs_buffer_offsets[mtl_slot] != vb_offset))
            {
                sc.cur_vs_buffer_offsets[mtl_slot] = vb_offset;
                if (sc.cur_vs_buffer_ids[mtl_slot].id != vb.slot.id) {
                    // vertex buffer has changed
                    sc.cur_vs_buffer_ids[mtl_slot].id = vb.slot.id;
                    @debug_assert(vb.mtl.buf[vb.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
                    [mtl.render_cmd_encoder setVertexBuffer:mtl.get_id(vb.mtl.buf[vb.cmn.active_slot])
                        offset:(NSUInteger)vb_offset
                        atIndex:mtl_slot];
                } else {
                    // only vertex buffer offset has changed
                    [mtl.render_cmd_encoder setVertexBufferOffset:(NSUInteger)vb_offset atIndex:mtl_slot];
                }
                _sg_stats_add(metal.bindings.num_set_vertex_buffer, 1);
            }
        }
    }

    // apply image bindings
    range(0, SG_MAX_IMAGE_BINDSLOTS) { i |
        continue :: local_return;
        img := bnd.imgs[i];
        if (img == 0, => continue());
        @debug_assert(img.mtl.tex[img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        stage := shd.cmn.images[i].stage;
        @debug_assert((stage == SG_SHADERSTAGE_VERTEX) || (stage == SG_SHADERSTAGE_FRAGMENT) || (stage == SG_SHADERSTAGE_COMPUTE));
        mtl_slot := shd.mtl.img_texture_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
        
        enc, ids, stats := @match(stage) {
            fn VERTEX()   => (mtl.render_cmd_encoder, sc.cur_vs_image_ids&, metal.bindings.num_set_vertex_texture&);
            fn FRAGMENT() => (mtl.render_cmd_encoder, sc.cur_fs_image_ids&, metal.bindings.num_set_fragment_texture&);
            fn COMPUTE()  => (mtl.compute_cmd_encoder, sc.cur_cs_image_ids&, metal.bindings.num_set_compute_texture&);
        };
        @debug_assert(nil != enc);
        if ids[mtl_slot].id != img.slot.id {
            ids[mtl_slot].id = img.slot.id;
            id := mtl.get_id(img.mtl.tex[img.cmn.active_slot]);
            @match(stage) {
                fn VERTEX()   => @objc enc.setVertexTexture(id, atIndex = mtl_slot);
                fn FRAGMENT() => @objc enc.setFragmentTexture(id, atIndex = mtl_slot);
                fn COMPUTE()  => @objc enc.setTexture(id, atIndex = mtl_slot);
            };
            stats[] += 1;
        }
    }

    // apply sampler bindings
    range(0, SG_MAX_SAMPLER_BINDSLOTS) { i |
        continue :: local_return;
        const smp: *Sg.Sampler.T = bnd.smps[i];
        if (smp == 0, => continue());
        @debug_assert(smp.mtl.sampler_state != _SG_MTL_INVALID_SLOT_INDEX);
        const sg_shader_stage stage = shd.cmn.samplers[i].stage;
        @debug_assert((stage == SG_SHADERSTAGE_VERTEX) || (stage == SG_SHADERSTAGE_FRAGMENT) || (stage == SG_SHADERSTAGE_COMPUTE));
        const NSUInteger mtl_slot = shd.mtl.smp_sampler_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
        if (stage == SG_SHADERSTAGE_VERTEX) {
            @debug_assert(nil != mtl.render_cmd_encoder);
            if (sc.cur_vs_sampler_ids[mtl_slot].id != smp.slot.id) {
                sc.cur_vs_sampler_ids[mtl_slot].id = smp.slot.id;
                [mtl.render_cmd_encoder setVertexSamplerState:mtl.get_id(smp.mtl.sampler_state) atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_vertex_sampler_state, 1);
            }
        } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
            @debug_assert(nil != mtl.render_cmd_encoder);
            if (sc.cur_fs_sampler_ids[mtl_slot].id != smp.slot.id) {
                sc.cur_fs_sampler_ids[mtl_slot].id = smp.slot.id;
                [mtl.render_cmd_encoder setFragmentSamplerState:mtl.get_id(smp.mtl.sampler_state) atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_fragment_sampler_state, 1);
            }
        } else if (stage == SG_SHADERSTAGE_COMPUTE) {
            @debug_assert(nil != mtl.compute_cmd_encoder);
            if (sc.cur_cs_sampler_ids[mtl_slot].id != smp.slot.id) {
                sc.cur_cs_sampler_ids[mtl_slot].id = smp.slot.id;
                [mtl.compute_cmd_encoder setSamplerState:mtl.get_id(smp.mtl.sampler_state) atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_compute_sampler_state, 1);
            }
        }
    }

    // apply storage buffer bindings
    range(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
        const _sg_buffer_t* sbuf = bnd.sbufs[i];
        if (sbuf == 0) {
            continue;
        }
        @debug_assert(sbuf.mtl.buf[sbuf.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
        @debug_assert((stage == SG_SHADERSTAGE_VERTEX) || (stage == SG_SHADERSTAGE_FRAGMENT) || (stage == SG_SHADERSTAGE_COMPUTE));
        const NSUInteger mtl_slot = shd.mtl.sbuf_buffer_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS);
        if (stage == SG_SHADERSTAGE_VERTEX) {
            @debug_assert(nil != mtl.render_cmd_encoder);
            if (sc.cur_vs_buffer_ids[mtl_slot].id != sbuf.slot.id) {
                sc.cur_vs_buffer_ids[mtl_slot].id = sbuf.slot.id;
                [mtl.render_cmd_encoder setVertexBuffer:mtl.get_id(sbuf.mtl.buf[sbuf.cmn.active_slot]) offset:0 atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_vertex_buffer, 1);
            }
        } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
            @debug_assert(nil != mtl.render_cmd_encoder);
            if (sc.cur_fs_buffer_ids[mtl_slot].id != sbuf.slot.id) {
                sc.cur_fs_buffer_ids[mtl_slot].id = sbuf.slot.id;
                [mtl.render_cmd_encoder setFragmentBuffer:mtl.get_id(sbuf.mtl.buf[sbuf.cmn.active_slot]) offset:0 atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_fragment_buffer, 1);
            }
        } else if (stage == SG_SHADERSTAGE_COMPUTE) {
            @debug_assert(nil != mtl.compute_cmd_encoder);
            if (sc.cur_cs_buffer_ids[mtl_slot].id != sbuf.slot.id) {
                sc.cur_cs_buffer_ids[mtl_slot].id = sbuf.slot.id;
                [mtl.compute_cmd_encoder setBuffer:mtl.get_id(sbuf.mtl.buf[sbuf.cmn.active_slot]) offset:0 atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_compute_buffer, 1);
            }
        }
    }
    return true;
}

fn apply_uniforms(mtl: *Impl, ub_slot: i32, data: []u8) void = {
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert(((size_t)mtl.cur_ub_offset + data.size) <= (size_t)mtl.ub_size);
    @debug_assert((mtl.cur_ub_offset & (_SG_MTL_UB_ALIGN-1)) == 0);
    pip := mtl.state_cache.cur_pipeline;
    @debug_assert(pip && pip.shader);
    @debug_assert(pip.slot.id == mtl.state_cache.cur_pipeline_id.id);
    shd := pip.shader;
    @debug_assert(shd.slot.id == pip.cmn.shader_id.id);
    @debug_assert(data.size == shd.cmn.uniform_blocks[ub_slot].size);

    stage := shd.cmn.uniform_blocks[ub_slot].stage;
    mtl_slot := shd.mtl.ub_buffer_n[ub_slot];

    // copy to global uniform buffer, record offset into cmd encoder, and advance offset
    uint8_t* dst = &mtl.cur_ub_base_ptr[mtl.cur_ub_offset];
    memcpy(dst, data.ptr, data.size);
    if (stage == SG_SHADERSTAGE_VERTEX) {
        @debug_assert(nil != mtl.render_cmd_encoder);
        [mtl.render_cmd_encoder setVertexBufferOffset:(NSUInteger)mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_vertex_buffer_offset, 1);
    } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
        @debug_assert(nil != mtl.render_cmd_encoder);
        [mtl.render_cmd_encoder setFragmentBufferOffset:(NSUInteger)mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_fragment_buffer_offset, 1);
    } else if (stage == SG_SHADERSTAGE_COMPUTE) {
        @debug_assert(nil != mtl.compute_cmd_encoder);
        [mtl.compute_cmd_encoder setBufferOffset:(NSUInteger)mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_compute_buffer_offset, 1);
    } else {
        SOKOL_UNREACHABLE;
    }
    mtl.cur_ub_offset = _sg_roundup(mtl.cur_ub_offset + (int)data.size, _SG_MTL_UB_ALIGN);
}

fn draw(mtl: *Impl, base_element: i32, num_elements: i32, num_instances: i32) void = {
    sc := mtl.state_cache&;
    @debug_assert(nil != mtl.render_cmd_encoder);
    @debug_assert(sc.cur_pipeline && (sc.cur_pipeline.slot.id == sc.cur_pipeline_id.id));
    if (SG_INDEXTYPE_NONE != sc.cur_pipeline.cmn.index_type) {
        // indexed rendering
        @debug_assert(sc.cur_indexbuffer && (sc.cur_indexbuffer.slot.id == sc.cur_indexbuffer_id.id));
        const _sg_buffer_t* ib = sc.cur_indexbuffer;
        @debug_assert(ib.mtl.buf[ib.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        const NSUInteger index_buffer_offset = (NSUInteger) (sc.cur_indexbuffer_offset + base_element * sc.cur_pipeline.mtl.index_size);
        [mtl.render_cmd_encoder drawIndexedPrimitives:sc.cur_pipeline.mtl.prim_type
            indexCount:(NSUInteger)num_elements
            indexType:sc.cur_pipeline.mtl.index_type
            indexBuffer:mtl.get_id(ib.mtl.buf[ib.cmn.active_slot])
            indexBufferOffset:index_buffer_offset
            instanceCount:(NSUInteger)num_instances];
    } else {
        // non-indexed rendering
        [mtl.render_cmd_encoder drawPrimitives:sc.cur_pipeline.mtl.prim_type
            vertexStart:(NSUInteger)base_element
            vertexCount:(NSUInteger)num_elements
            instanceCount:(NSUInteger)num_instances];
    }
}

fn dispatch(mtl: *Impl, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void = {
    @debug_assert(nil != mtl.compute_cmd_encoder);
    @debug_assert(mtl.state_cache.cur_pipeline && (mtl.state_cache.cur_pipeline.slot.id == mtl.state_cache.cur_pipeline_id.id));
    cur_pip := mtl.state_cache.cur_pipeline;
    thread_groups: MTLSize = (
        width = (NSUInteger)num_groups_x,
        height = (NSUInteger)num_groups_y,
        depth = (NSUInteger)num_groups_z,
    );
    threads_per_threadgroup := cur_pip.mtl.threads_per_threadgroup;
    [mtl.compute_cmd_encoder dispatchThreadgroups:thread_groups threadsPerThreadgroup:threads_per_threadgroup];
}

NSRange :: @struct(location: u64, length: u64);

fn inc_active_slot(it: ~T) void #where = {
    it.cmn.active_slot += 1;
    if (it.cmn.active_slot >= it.cmn.num_slots) {
        it.cmn.active_slot = 0;
    }
}

fn update_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8) void = {
    @debug_assert(buf && data && data.ptr && (data.size > 0));
    inc_active_slot(buf);
    __unsafe_unretained id<MTLBuffer> mtl_buf = mtl.get_id(buf.mtl.buf[buf.cmn.active_slot]);
    dst_ptr := @objc @as(rawptr) mtl_buf.contents();
    memcpy(dst_ptr, data.ptr, data.size);
    ASSERT_NOT_IOS();
    if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
        [mtl_buf didModifyRange:NSMakeRange(0, data.size)];
    }
}

fn append_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8, new_frame: bool) void = {
    @debug_assert(buf && data && data.ptr && (data.size > 0));
    if new_frame {
        inc_active_slot(buf);
    }
    __unsafe_unretained id<MTLBuffer> mtl_buf = mtl.get_id(buf.mtl.buf[buf.cmn.active_slot]);
    dst_ptr := @objc @as(*u8) mtl_buf.contents();
    dst_ptr += buf.cmn.append_pos;
    memcpy(dst_ptr, data.ptr, data.size);
    ASSERT_NOT_IOS();
    if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
        r: NSRange = (location = (NSUInteger)buf.cmn.append_pos, length = (NSUInteger)data.size);
        @objc mtl_buf.didModifyRange(r);
    }
}

fn update_image(mtl: *Impl, img: *Sg.Image.T, data: *sg_image_data) void = {
    @debug_assert(img && data);
    inc_active_slot(img);
    mtl_tex := mtl.get_id(img.mtl.tex[img.cmn.active_slot]);
    mtl.copy_image_data(img, mtl_tex, data);
}

fn push_debug_group(mtl: *Impl, name: CStr) void = {
    name := @objc NSString.stringWithUTF8String(name);
    @objc mtl.encoder().pushDebugGroup(name);
}

fn pop_debug_group(mtl: *Impl) void =
    @objc mtl.encoder().popDebugGroup(name);

fn encoder(mtl: *Impl) ObjCId = {
    enc := mtl.render_cmd_encoder;
    @if(enc.is_nil(), mtl.compute_cmd_encoder, enc)
}

//
// These accessors are useful when you want to mix and match 
// our rendering api with your own direct use of the Metal api. 
// If there's some specific Metal feature you need that we don't 
// provide a wrapper around, you can use these to just do it yourself.  
//

fn device(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.device)
}

fn render_command_encoder(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.render_cmd_encoder)
}

fn compute_command_encoder(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.compute_cmd_encoder)
}

fn query_buffer_info(mtl: *Impl, buf_id: Sg.Buffer) sg_mtl_buffer_info = {
    @debug_assert(mtl.sg.valid);
    res := zeroed sg_mtl_buffer_info;
    ASSERT_METAL();
    buf := mtl.sg.lookup(buf_id) || return(res);
    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        if (buf.mtl.buf[i] != 0) {
            res.buf[i] = (__bridge void*) mtl.get_id(buf.mtl.buf[i]);
        }
    }
    res.active_slot = buf.cmn.active_slot;
    res
}

fn query_image_info(mtl: *Impl, img_id: Sg.Image) sg_mtl_image_info = {
    @debug_assert(mtl.sg.valid);
    res := zeroed sg_mtl_image_info;
    ASSERT_METAL();
    img := mtl.sg.lookup(img_id) || return(res);
    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        if (img.mtl.tex[i] != 0) {
            res.tex[i] = (__bridge void*) mtl.get_id(img.mtl.tex[i]);
        }
    }
    res.active_slot = img.cmn.active_slot;
    res
}

fn query_sampler_info(mtl: *Impl, id: Sg.Sampler) sg_mtl_sampler_info = {
    @debug_assert(mtl.sg.valid);
    res := zereod sg_mtl_sampler_info;
    ASSERT_METAL();
    smp := mtl.sg.lookup(id) || return(res);
    if (smp.mtl.sampler_state != 0) {
        res.smp = (__bridge void*) mtl.get_id(smp.mtl.sampler_state);
    }
    res
}

fn query_shader_info(mtl: *Impl, shd_id: Sg.Shader) sg_mtl_shader_info = {
    @debug_assert(mtl.sg.valid);
    sg_mtl_shader_info res;
    _sg_clear(&res, sizeof(res));
    ASSERT_METAL();
    shd := _sg_lookup_shader(&mtl.sg.pools, shd_id.id) || return(res);
    const int vertex_lib  = shd.mtl.vertex_func.mtl_lib;
    const int vertex_func = shd.mtl.vertex_func.mtl_func;
    const int fragment_lib  = shd.mtl.fragment_func.mtl_lib;
    const int fragment_func = shd.mtl.fragment_func.mtl_func;
    if (vertex_lib != 0) {
        res.vertex_lib = (__bridge void*) mtl.get_id(vertex_lib);
    }
    if (fragment_lib != 0) {
        res.fragment_lib = (__bridge void*) mtl.get_id(fragment_lib);
    }
    if (vertex_func != 0) {
        res.vertex_func = (__bridge void*) mtl.get_id(vertex_func);
    }
    if (fragment_func != 0) {
        res.fragment_func = (__bridge void*) mtl.get_id(fragment_func);
    }
    res
}

fn query_pipeline_info(mtl: *Impl, pip_id: Sg.Pipeline) sg_mtl_pipeline_info = {
    @debug_assert(mtl.sg.valid);
    res := zeroed sg_mtl_pipeline_info;
    ASSERT_METAL();
    pip := mtl.sg.lookup(id) || return(res);
    if (pip.mtl.rps != 0) {
        res.rps = (__bridge void*) mtl.get_id(pip.mtl.rps);
    }
    if (pip.mtl.dss != 0) {
        res.dss = (__bridge void*) mtl.get_id(pip.mtl.dss);
    }
    res
}
