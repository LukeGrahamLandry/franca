// simple 3D API wrapper
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.
// 
// Changes from Sokol:
// - no dependency on an objective-c compiler
// - removed ios, opengl, and macos <13 support (temporarily?)
// 
// A sad amount of this task is just mapping our enum values to the metal 
// ones. I've manually transcribed the numbers so you can cross compile 
// without finding a copy of the Metal.framework headers. If I ever do more
// serious bindings to metal, this should be updated to use that instead. 
// 

ASSERT_NOT_IOS :: fn() => ();
ASSERT_NOT_OLD :: fn() => ();

_SG_MTL_UB_ALIGN :: { ASSERT_NOT_IOS(); 256 };
_SG_MTL_INVALID_SLOT_INDEX :: 0;

ReleaseItem :: @struct {
    frame_index: u32;   // frame index at which it is safe to release this resource
    slot_index: i32;
};

IdPool :: @struct {
    pool: id("NSMutableArray");
    num_slots: i64;
    free_queue_top: i64;
    free_queue: []i32;
    release_queue_front: i64;
    release_queue_back: i64;
    release_queue: []ReleaseItem;
};

Buffer :: @struct {
    buf: Array(Id("MTLBuffer"), Sg.SG_NUM_INFLIGHT_FRAMES);  // index into _sg_mtl_pool
};

Image :: @struct {
    tex: Array(Id("MTLTexture"), Sg.SG_NUM_INFLIGHT_FRAMES);
};

Sampler :: @struct {
    sampler_state: Id("MTLSamplerState");
};

_sg_mtl_shader_func_t :: @struct {
    mtl_lib: Id("MTLLibrary");
    mtl_func: Id("MTLFunction");
};

Shader :: @struct {
    vertex_func: _sg_mtl_shader_func_t;
    fragment_func: _sg_mtl_shader_func_t;
    compute_func: _sg_mtl_shader_func_t;
    threads_per_threadgroup: MTLSize;
    ub_buffer_n: Array(u8, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    img_texture_n: Array(u8, Sg.SG_MAX_IMAGE_BINDSLOTS);
    smp_sampler_n: Array(u8, Sg.SG_MAX_SAMPLER_BINDSLOTS);
    sbuf_buffer_n: Array(u8, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Pipeline :: @struct {
    prim_type: M("MTLPrimitiveType");
    index_size: i32;
    index_type: M("MTLIndexType");
    cull_mode: M("MTLCullMode");
    winding: M("MTLWinding");
    stencil_ref: u32;
    threads_per_threadgroup: MTLSize;
    cps: Id("MTLComputePipelineState");
    rps: Id("MTLRenderPipelineState");
    dss: Id("MTLDepthStencilState");
};

Attachment :: @struct {
    image: *Sg.Image.T;
};

Attachments :: @struct {
    colors: Array(Attachment, Sg.SG_MAX_COLOR_ATTACHMENTS);
    resolves: Array(Attachment, Sg.SG_MAX_COLOR_ATTACHMENTS);
    depth_stencil: Attachment;
};

// resource binding state cache
_SG_MTL_MAX_STAGE_UB_BINDINGS :: Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS;
_SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS :: _SG_MTL_MAX_STAGE_UB_BINDINGS + Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS;
_SG_MTL_MAX_STAGE_BUFFER_BINDINGS :: _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS + Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS;
_SG_MTL_MAX_STAGE_IMAGE_BINDINGS :: Sg.SG_MAX_IMAGE_BINDSLOTS;
_SG_MTL_MAX_STAGE_SAMPLER_BINDINGS :: Sg.SG_MAX_SAMPLER_BINDSLOTS;


StateCacheEntry :: fn($T, $N) => EnumMap(SgShaderStage, Array(T, N));

StateCache :: @struct {
    cur_pipeline: *Sg.Pipeline.T;
    cur_pipeline_id: Sg.Pipeline;
    cur_indexbuffer: *Sg.Buffer.T;
    cur_indexbuffer_id: Sg.Buffer;
    cur_indexbuffer_offset: i32;
    cur_vs_buffer_offsets: Array(i32, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    buffer: StateCacheEntry(Sg.Buffer, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    image: StateCacheEntry(Sg.Image, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
    sampler: StateCacheEntry(Sg.Sampler, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
};

Impl :: @struct {
    sg: *Sg.Self;
    valid: bool;
    use_shared_storage_mode: bool;
    cur_frame_rotate_index: i64;
    ub_size: i32;
    cur_ub_offset: i32;
    cur_ub_base_ptr: *u8;
    state_cache: StateCache;
    idpool: IdPool;
    sem: id("OS_dispatch_semaphore");
    device: id("MTLDevice");
    cmd_queue: id("MTLCommandQueue");
    cmd_buffer: id("MTLCommandBuffer");
    render_cmd_encoder: id("MTLRenderCommandEncoder");
    compute_cmd_encoder: id("MTLComputeCommandEncoder");
    cur_drawable: id("CAMetalDrawable");
    uniform_buffers: Array(id("MTLBuffer"), Sg.SG_NUM_INFLIGHT_FRAMES);
    pass_completed_sem: id("_NSConcreteGlobalBlock");
};

// TODO: collect all the strings passed to this at comptime and load 
//       all the classes at startup so you don't have to check every time 
//       and don't have to worry about races. 
// TODO: decide if i like this style or the one in app.fr more and switch one of them. 
fn cls($name: Str) ObjCClass = {
    cache :: @static(ObjCClass) zeroed(ObjCClass);
    if bit_cast_unchecked(ObjCClass, i64, cache[]) == 0 {
        cache[] = objc_getClass(::name.sym().c_str());
    }
    cache[]
}

fn to_mtl(it: SgLoadAction) M("MTLLoadAction") = @map_enum(it) 
    (CLEAR = 2, LOAD = 1, DONTCARE = 0);

fn to_mtl(it: SgStoreAction, resolve: bool) M("MTLStoreAction") = {
    ::enum(@type it);
    @debug_assert(@is(it, .STORE, .DONTCARE));
    @if(it == .STORE, @if(resolve, 3, 1), @if(resolve, 2, 0))
}

fn resource_options_storage_mode_managed_or_shared(mtl: *Impl) M("MTLResourceOptions") = {
    ASSERT_NOT_IOS();
    @if(mtl.use_shared_storage_mode, /*Shared*/ 0, /*Managed*/ 1.shift_left(4))
}

fn to_mtl(mtl: *Impl, it: SgUsage) M("MTLResourceOptions") = {
    @debug_assert(@is(it, .IMMUTABLE, .DYNAMIC, .STREAM));
    x := mtl.resource_options_storage_mode_managed_or_shared();
    if it != .IMMUTABLE {
        X |= MTLResourceCPUCacheModeWriteCombined;
    };
    x
}

fn to_mtl(it: SgVertexStep) M("MTLVertexStepFunction") = @map_enum(it)
    (PER_VERTEX = 1, PER_INSTANCE = 2);

fn to_mtl(it: SgVertexFormat) M("MTLVertexFormat") = @map_enum(it) (
    Float = 28, Float2 = 29, Float3 = 30, Float4 = 31,
    Int = 32, Int2 = 33, Int3 = 34, Int4 = 35,
    UInt = 36, UInt2 = 37, UInt3 = 38, UInt4 = 39,
    Byte4 = 6, Byte4N = 12, UByte4 = 3, UByte4N = 9, 
    Short2 = 16, Short2N = 22, UShort2 = 13, UShort2N = 19,
    Short4 = 18, Short4N = 24, UShort4 = 15, UShort4N = 21,
    UINT10_N2 = 41, Half2 = 25, Half4 = 27,
);

fn to_mtl(it: SgPrimitiveType) M("MTLPrimitiveType") = @map_enum(it) 
    (POINTS = 0, LINES = 1, LINE_STRIP = 2, TRIANGLES = 3, TRIANGLE_STRIP = 3);

fn to_mtl(it: SgPixelFormat) M("MTLPixelFormat") = {
    ASSERT_NOT_IOS();
    @enum_map(it) (
        R8 = 10, R8SN = 12, R8UI = 13, R8SI = 14,
        R16 = 20, R16SN = 22, R16UI = 23, R16SI = 24, R16F = 25,
        RG8 = 30, RG8SN = 32, RG8UI = 33, RG8SI = 34,
        R32UI = 53, R32SI = 54, R32F = 55, 
        RG16 = 60, RG16SN = 62, RG16UI  = 63, RG16SI = 64, RG16F = 65, 
        RGBA8 = 70, SRGB8A8 = 71, RGBA8SN = 72, RGBA8UI = 73, RGBA8SI = 74, 
        BGRA8 = 80, RGB10A2 = 90, RG11B10F = 92, RGB9E5 = 93, 
        RG32UI = 103, RG32SI = 104, RG32F = 105, 
        RGBA16 = 110, RGBA16SN = 112, RGBA16UI = 113, RGBA16SI = 114, RGBA16F = 115,
        RGBA32UI = 123, RGBA32SI = 124, RGBA32F = 125, 
        DEPTH = 252, DEPTH_STENCIL = 260,
        BC1_RGBA = 130, BC2_RGBA = 132, BC3_RGBA = 134, BC3_SRGBA = 135,
        BC4_R = 140, BC4_RSN = 141, BC5_RG = 142, BC5_RGSN = 143,
        BC6H_RGBF = 150, BC6H_RGBUF = 151, BC7_RGBA = 152, BC7_SRGBA = 153, 
    )
}

fn to_mtl(m: SgColorMask) M("MTLColorWriteMask") = {
    MTLColorWriteMask mtl_mask = MTLColorWriteMaskNone;
    if (m & SG_COLORMASK_R) {
        mtl_mask |= MTLColorWriteMaskRed;
    }
    if (m & SG_COLORMASK_G) {
        mtl_mask |= MTLColorWriteMaskGreen;
    }
    if (m & SG_COLORMASK_B) {
        mtl_mask |= MTLColorWriteMaskBlue;
    }
    if (m & SG_COLORMASK_A) {
        mtl_mask |= MTLColorWriteMaskAlpha;
    }
    return mtl_mask;
}

fn to_mtl(it: SgBlendOp) M("MTLBlendOperation") = @map_enum(it) 
    (ADD = 0, SUBTRACT = 1, REVERSE_SUBTRACT = 2, MIN = 3, MAX = 4);

fn to_mtl(it: SgBlendFactor) M("MTLBlendFactor") = @map_enum(it) (
    ZERO = 0, ONE = 1, 
    SRC_COLOR = 2, ONE_MINUS_SRC_COLOR = 3,
    ALPHA     = 4, ONE_MINUS_SRC_ALPHA = 5, 
    DST_COLOR = 6, ONE_MINUS_DST_COLOR = 7, 
    DST_ALPHA = 8, ONE_MINUS_DST_ALPHA = 9,
    SRC_ALPHA_SATURATED = 10, 
    BLEND_COLOR = 11, ONE_MINUS_BLEND_COLOR = 12,
    BLEND_ALPHA = 13, ONE_MINUS_BLEND_ALPHA = 14,
);

fn to_mtl(it: SgCompareFunc) M("MTLCompareFunction") = @map_enum(it) 
    (NEVER = 0, LESS = 1, EQUAL = 2, LESS_EQUAL = 3, GREATER = 4, NOT_EQUAL = 5, GREATER_EQUAL = 6, ALWAYS = 7);

fn to_mtl(it: SgStencilOp) M("MTLStencilOperation") = @map_enum(it)
    (KEEP = 0, ZERO = 1, REPLACE = 2, INCR_CLAMP = 3, DECR_CLAMP = 4, INVERT = 5, INCR_WRAP = 6, DECR_WRAP = 7);

fn to_mtl(it: SgCullMode) M("MTLCullMode") = @map_enum(it) 
    (NONE = MTLCullModeNone, FRONT = MTLCullModeFront, BACK = MTLCullModeBack);

fn to_mtl(it: SgFaceWinding) M("MTLWinding") = @map_enum(it) 
    (CW = MTLWindingClockwise, CCW = MTLWindingCounterClockwise);

fn to_mtl(it: SgIndexType) M("MTLIndexType") = @map_enum(it) 
    (UINT16 = MTLIndexTypeUInt16, UINT32 = MTLIndexTypeUInt32);

fn to_mtl(it: SgIndexType) i32 = @map_enum(it) 
    (None = 0, UINT16 = 2, UINT32 = 4);

fn to_mtl(it: SgImageType) M("MTLTextureType") = @map_enum(it)
    (_2D = 2, CUBE = 5, _3D = 7, ARRAY = 3);

fn to_mtl(mtl: *Impl, w: SgWrap) M("MTLSamplerAddressMode") = {
    ASSERT_NOT_OLD();
    b := mtl.sg.features.image_clamp_to_border;
    @map_enum(w) (REPEAT = 2, CLAMP_TO_EDGE = 0, CLAMP_TO_BORDER = @if(b, 5, 0), MIRRORED_REPEAT = 3)
}

fn to_mtl(it: SgBorderColor) M("MTLSamplerBorderColor") = {
    ASSERT_NOT_OLD();
    @map_enum(it) (TRANSPARENT_BLACK = 0, OPAQUE_BLACK = 1, OPAQUE_WHITE = 2)
}

fn mtl_minmag(it: SgFilter) M("MTLSamplerMinMagFilter") = @enum_map(it)
    (NEAREST = 0, LINEAR = 1);

fn mtl_mip(f: SgFilter) M("MTLSamplerMipFilter") = @enum_map(it) 
    (NEAREST = 1, LINEAR = 2);

fn _sg_mtl_vertexbuffer_bindslot(sokol_bindslot: i64) i64 =
    sokol_bindslot + _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS;

//-- a pool for all Metal resource objects, with deferred release queue ---------
fn init_pool(mtl: *Impl, desc: *SgDesc) void = {
    p := mtl.idpool&;
    p.num_slots = 2 *
        (
            2 * desc.buffer_pool_size +
            4 * desc.image_pool_size +
            1 * desc.sampler_pool_size +
            4 * desc.shader_pool_size +
            2 * desc.pipeline_pool_size +
            desc.attachments_pool_size +
            128
        ).intcast();
    p.pool = @objc cls("NSMutableArray").arrayWithCapacity(@as(i64) p.num_slots);
    @objc p.pool.retain();
    null := @objc cls("NSNull").null();
    range(0, p.num_slots) { _ |
        @objc p.pool.addObject(null);
    }
    @debug_assert((@objc @as(i64) p.pool.count()) == @as(i64) p.num_slots);
    // a queue of currently free slot indices
    p.free_queue_top = 0;
    a := mtl.sg.desc.allocator;
    // pool slot 0 is reserved!
    p.free_queue = a.alloc_init(i32, p.num_slots, fn(i) => intcast p.num_slots - 1 - i);
    // a circular queue which holds release items (frame index when a resource is to be released, and the resource's pool index
    p.release_queue_front = 0;
    p.release_queue_back = 0;
    
    // filled with _SG_MTL_INVALID_SLOT_INDEX
    p.release_queue = a.alloc_zeroed(ReleaseItem, p.num_slots);
}

fn destroy_pool(mtl: *Impl) void = {
    p := mtl.idpool&;
    _sg_free(p.release_queue);  p.release_queue = 0;
    _sg_free(p.free_queue);     p.free_queue = 0;
    release(p.pool);
}

// get a new free resource pool slot
fn alloc_pool_slot(mtl: *Impl) i32 = {
    p := mtl.idpool&;
    @debug_assert(p.free_queue_top > 0);
    p.free_queue_top -= 1;
    slot_index := p.free_queue[p.free_queue_top];
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
    slot_index
}

// put a free resource pool slot back into the free-queue
fn free_pool_slot(mtl: *Impl, slot_index: i32) void = {
    p := mtl.idpool&;
    @debug_assert(p.free_queue_top < p.num_slots);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
    p.free_queue[p.free_queue_top] = slot_index;
    p.free_queue_top += 1;
}

// add an MTLResource to the pool, return pool index or 0 if input was 'nil'
fn add_resource(mtl: *Impl, res: id("T")) Id("T") = {
    if(res.is_nil(), => return(_SG_MTL_INVALID_SLOT_INDEX));
    mtl.sg.stat(.mtl_idpool_num_added, 1);
    slot_index := mtl.alloc_pool_slot();
    // NOTE: the NSMutableArray will take ownership of its items
    //@debug_assert([NSNull null] == mtl.idpool.pool[@as(i64) slot_index]);  // TODO
    @objc mtl.idpool.pool.replaceObjectAtIndex(res, atIndex = @as(i64) slot_index.intcast());
    slot_index
}

/*  mark an MTLResource for release, this will put the resource into the
    deferred-release queue, and the resource will then be released N frames later. */
fn release_resource(mtl: *Impl, frame_index: u32, slot_index: Id("T")) void = {
    if slot_index == _SG_MTL_INVALID_SLOT_INDEX {
        /* this means that a nil value was provided to mtl.add_resource */
        return();
    }
    p := mtl.idpool&;
    mtl.sg.stat(.mtl_idpool_num_released, 1);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
    //@debug_assert([NSNull null] != p.pool[@as(i64) slot_index]);  // TODO
    release_index := p.release_queue_front;
    p.release_queue_front += 1;
    if (p.release_queue_front >= p.num_slots) {
        // wrap-around
        p.release_queue_front = 0;
    }
    // release queue full?
    @debug_assert(p.release_queue_front != p.release_queue_back);
    @debug_assert(p.release_queue[release_index].frame_index == 0);
    safe_to_release_frame_index := frame_index + Sg.SG_NUM_INFLIGHT_FRAMES + 1;
    p.release_queue[release_index] = (frame_index = safe_to_release_frame_index, slot_index = slot_index);
}

// NSNull: you're not allowed to put nil in an NSMutableArray (even tho everything's a pointer anyway?), 
// so they have a magic sentinal value to use instead of nil. nil != [NSNull null]. 

// run garbage-collection pass on all resources in the release-queue
fn garbage_collect(mtl: *Impl, frame_index: u32) void = {
    p := mtl.idpool&;
    while => p.release_queue_back != p.release_queue_front {
        if (frame_index < p.release_queue[p.release_queue_back].frame_index) {
            // don't need to check further, release-items past this are too young
            return();
        }
        mtl.sg.stat(.mtl_num_garbage_collected, 1);
        // safe to release this resource
        slot_index := p.release_queue[p.release_queue_back].slot_index;
        @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
        // note: the NSMutableArray takes ownership of its items, assigning an NSNull object will
        // release the object, no matter if using ARC or not
        // use 
        null := @objc cls("NSNull").null();
        i: i64 = slot_index.intcast();
        @debug_assert(bit_cast_unchecked(ObjCId, i64, null) != bit_cast_unchecked(ObjCId, i64, @objc p.pool.objectAtIndex(i)));
        @objc p.pool.replaceObjectAtIndex(null, atIndex = i);
        // put the now free pool index back on the free queue
        mtl.free_pool_slot(slot_index);
        // reset the release queue slot and advance the back index
        p.release_queue[p.release_queue_back].frame_index = 0;
        p.release_queue[p.release_queue_back].slot_index = _SG_MTL_INVALID_SLOT_INDEX;
        p.release_queue_back += 1;
        if (p.release_queue_back >= p.num_slots) {
            // wrap-around
            p.release_queue_back = 0;
        }
    }
}

fn get_id(mtl: *Impl, slot_index: Id("T")) id("T") = 
    @objc mtl.idpool.pool.objectAtIndex(@as(i64) slot_index.intcast());

fn clear_state_cache(mtl: *Impl) void = {
    mtl.state_cache = zeroed @type mtl.state_cache;
}

MTLGPUFamily :: @enum(u32) (Apple1 = 1001, Apple7 = 1007, Mac2 = 2002, Metal3 = 5001);

fn supports_family(mtl: *Impl, family: MTLGPUFamily) bool =
    @objc @as(bool) mtl.device.supportsFamily(family);

// https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf
fn init_caps(mtl: *Impl) void = {
    ASSERT_NOT_IOS();
    //mtl.sg.backend = SG_BACKEND_METAL_MACOS;
    
    ASSERT_NOT_OLD();
    mtl.sg.features = (
        origin_top_left = true,
        mrt_independent_blend_state = true,
        mrt_independent_write_mask = true,
        compute = true,
        msaa_image_bindings = true,
        image_clamp_to_border = mtl.supports_family(.Apple7) || mtl.supports_family(.Mac2) || mtl.supports_family(.Metal3),
    );

    ASSERT_NOT_IOS();
    mtl.sg.limits = (
        max_image_size_2d = 16 * 1024,
        max_image_size_cube = 16 * 1024,
        max_image_size_3d = 2 * 1024,
        max_image_size_array = 16 * 1024,
        max_image_array_layers = 2 * 1024,
        max_vertex_attrs = Sg.SG_MAX_VERTEX_ATTRIBUTES,
    );
    
    ASSERT_NOT_IOS();
    @fill_formats(mtl.sg.formats&) (
        sfbrm = (.R8, .R8SN, .R16, .R16SN, .R16F, .RG8, .RG8SN, .R32F, .RG16, 
            .RG16SN, .RG16F, .RGBA8, .SRGB8A8, .RGBA8SN, .BGRA8, .RGB10A2, .RG11B10F, 
            .RG32F, .RGBA16, .RGBA16SN, .RGBA16F, .RGBA32F),
        srm = (.R8UI, .R8SI, .R16UI, .R16SI, .RG8UI, .RG8SI, .RG16UI, .RG16SI, .RGBA8UI, 
            .RGBA8SI, .RG32UI, .RG32SI, .RGBA16UI, .RGBA16SI, .RGBA32UI, .RGBA32SI),
        sr = (.R32UI, .R32SI),
        sf = (.RGB9E5, .BC1_RGBA, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC4_R, .BC4_RSN, 
            .BC5_RG, .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA),
        srmd = (.DEPTH, .DEPTH_STENCIL),
    );
}

fn ns_str(s: CStr) id("NSString") = 
    @objc cls("NSString").stringWithUTF8String(s);

//-- main Metal backend state and functions ------------------------------------
fn setup_backend(mtl: *Impl, desc: *SgDesc) void = {
    // assume already zero-initialized
    @debug_assert(!desc.environment.metal.device.is_null());
    @debug_assert(desc.uniform_buffer_size > 0);
    mtl.init_pool(desc);
    mtl.clear_state_cache();  // ... not assuming already zero-initialized?
    mtl.valid = true;
    mtl.ub_size = desc.uniform_buffer_size;
    mtl.sem = dispatch_semaphore_create(Sg.SG_NUM_INFLIGHT_FRAMES);
    mtl.device = bit_cast_unchecked(rawptr, id("MTLDevice"), desc.environment.metal.device);
    mtl.cmd_queue = @objc mtl.device.newCommandQueue();

    range(0, Sg.SG_NUM_INFLIGHT_FRAMES) { i |
        o := 1; // MTLResourceCPUCacheModeWriteCombined|MTLResourceStorageModeShared;
        mtl.uniform_buffers&[i] = @objc mtl.device.newBufferWithLength(@as(i64) mtl.ub_size.intcast(), options = o);
        if Sg.SOKOL_DEBUG {
            // TODO: is this supposed to use the varargs abi? or can i just use a normal selector based on the named args? 
            //       either way i need to change @objc to allow this
            //@objc mtl.uniform_buffers&[i].setLabel(@objc cls("NSString").stringWithFormat(ns_str("sg-uniform-buffer.%d"), i));
        }
    }

    if desc.mtl_force_managed_storage_mode {
        mtl.use_shared_storage_mode = false;
    } else {
        ASSERT_NOT_OLD();
        // on Intel Macs, always use managed resources even though the
        // device says it supports unified memory (because of texture restrictions)
        is_apple_gpu := mtl.supports_family(.Apple1);  
        mtl.use_shared_storage_mode = is_apple_gpu;
    };
    mtl.init_caps();
}

fn discard_backend(mtl: *Impl) void = {
    @debug_assert(mtl.valid);
    // wait for the last frame to finish
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_wait(mtl.sem, DISPATCH_TIME_FOREVER);
    }
    // semaphore must be "relinquished" before destruction
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_signal(mtl.sem);
    }
    mtl.garbage_collect(mtl.sg.frame_index + SG_NUM_INFLIGHT_FRAMES + 2);
    destroy_pool();
    mtl.valid = false;

    release(mtl.sem);
    release(mtl.device);
    release(mtl.cmd_queue);
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        release(mtl.uniform_buffers[i]);
    }
    // NOTE: MTLCommandBuffer, MTLRenderCommandEncoder and MTLComputeCommandEncoder are auto-released
    mtl.cmd_buffer = zeroed ObjCId;
    mtl.render_cmd_encoder = zeroed ObjCId;
    mtl.compute_cmd_encoder = zeroed ObjCId;
}

fn reset_state_cache(mtl: *Impl) void = {
    _sg_mtl_clear_state_cache();
}

fn create_buffer(mtl: *Impl, buf: *Sg.Buffer.T, desc: *Sg.Buffer.Desc) sg_resource_state = {
    @debug_assert(buf && desc);
    @debug_assert(buf.cmn.size > 0);
    injected := (0 != desc.mtl_buffers[0]);
    mtl_options := to_mtl(buf.cmn.usage);
    range(0, buf.cmn.num_slots) { slot | 
        id<MTLBuffer> mtl_buf;
        if (injected) {
            @debug_assert(desc.mtl_buffers[slot]);
            mtl_buf = (__bridge id<MTLBuffer>) desc.mtl_buffers[slot];
        } else {
            if (desc.data.ptr) {
                @debug_assert(desc.data.size > 0);
                mtl_buf = [mtl.device newBufferWithBytes:desc.data.ptr length:@as(i64) buf.cmn.size options:mtl_options];
            } else {
                // this is guaranteed to zero-initialize the buffer
                mtl_buf = [mtl.device newBufferWithLength:@as(i64) buf.cmn.size options:mtl_options];
            }
            if (nil == mtl_buf) {
                _SG_ERROR(METAL_CREATE_BUFFER_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
        }
        if Sg.SOKOL_DEBUG && desc.label {
            mtl_buf.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        buf.mtl.buf[slot] = mtl.add_resource(mtl_buf);
        release(mtl_buf);
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard_buffer(mtl: *Impl, buf: *Sg.Buffer.T) void = {
    @debug_assert(buf);
    range(0, buf.cmn.num_slots) { slot | 
        // it's valid to call release resource with '0'
        mtl.release_resource(mtl.sg.frame_index, buf.mtl.buf[slot]);
    }
}

fn copy_image_data(mtl: *Impl, img: *Sg.Image.T, mtl_tex: id("MTLTexture"), data: *sg_image_data) void = {
    num_faces := if(img.cmn.type == SG_IMAGETYPE_CUBE, => 6, => 1);
    num_slices := if(img.cmn.type == SG_IMAGETYPE_ARRAY, => img.cmn.num_slices, => 1);
    range(0, num_faces) { face_index |
        range(0, img.cmn.num_mipmaps) { mip_index |
            @debug_assert(data.subimage[face_index][mip_index].ptr);
            @debug_assert(data.subimage[face_index][mip_index].size > 0);
            data_ptr := (const uint8_t*)data.subimage[face_index][mip_index].ptr;
            mip_width := _sg_miplevel_dim(img.cmn.width, mip_index);
            mip_height := _sg_miplevel_dim(img.cmn.height, mip_index);
            bytes_per_row := _sg_row_pitch(img.cmn.pixel_format, mip_width, 1);
            bytes_per_slice := _sg_surface_pitch(img.cmn.pixel_format, mip_width, mip_height, 1);
            /* bytesPerImage special case: https://developer.apple.com/documentation/metal/mtltexture/1515679-replaceregion

                "Supply a nonzero value only when you copy data to a MTLTextureType3D type texture"
            */
            MTLOrigin :: @struct(x: u64, y: u64, z: u64);
            MTLRegion :: @struct(origin := zeroed MTLOrigin, size: MTLSize);

            mip_depth, bytes_per_image := if (img.cmn.type == SG_IMAGETYPE_3D) {
                // FIXME: apparently the minimal bytes_per_image size for 3D texture is 4 KByte... somehow need to handle this
                (_sg_miplevel_dim(img.cmn.num_slices, mip_index), bytes_per_slice)
            } else {
                (1, 0)
            };
            region: MTLRegion = (size = (width = @as(i64) mip_width, height = @as(i64) mip_height, depth = @as(i64) mip_depth);

            range(0, num_slices) { slice_index | 
                mtl_slice_index := @if(img.cmn.type == SG_IMAGETYPE_CUBE, face_index, slice_index);
                slice_offset := slice_index * bytes_per_slice;
                @debug_assert((slice_offset + bytes_per_slice) <= (int)data.subimage[face_index][mip_index].size);
                [mtl_tex replaceRegion:region
                    mipmapLevel:@as(i64) mip_index
                    slice:@as(i64) mtl_slice_index
                    withBytes:data_ptr + slice_offset
                    bytesPerRow:@as(i64) bytes_per_row
                    bytesPerImage:@as(i64) bytes_per_image];
            }
        }
    }
}

// initialize MTLTextureDescriptor with common attributes
fn _sg_mtl_init_texdesc_common(it: id("MTLTextureDescriptor"), img: *Sg.Image.T) bool = {
    @objc it.textureType = _sg_mtl_texture_type(img.cmn.type);
    @objc it.pixelFormat = _sg_mtl_pixel_format(img.cmn.pixel_format);
    if (0 == @objc @as(i64) it.pixelFormat()) {
        _SG_ERROR(METAL_TEXTURE_FORMAT_NOT_SUPPORTED);
        return false;
    }
    @objc it.width = @as(i64) img.cmn.width;
    @objc it.height = @as(i64) img.cmn.height;
    if (SG_IMAGETYPE_3D == img.cmn.type) {
        @objc it.depth = @as(i64) img.cmn.num_slices;
    } else {
        @objc it.depth = 1;
    }
    it.mipmapLevelCount = @as(i64) img.cmn.num_mipmaps;
    if (SG_IMAGETYPE_ARRAY == img.cmn.type) {
        @objc it.arrayLength = @as(i64) img.cmn.num_slices;
    } else {
        @objc it.arrayLength = 1;
    }
    @objc it.usage = MTLTextureUsageShaderRead;
    MTLResourceOptions res_options = 0;
    if (img.cmn.usage != SG_USAGE_IMMUTABLE) {
        res_options |= MTLResourceCPUCacheModeWriteCombined;
    }
    res_options |= _sg_mtl_resource_options_storage_mode_managed_or_shared();
    @objc it.resourceOptions = res_options;
    return true;
}

// initialize MTLTextureDescriptor with rendertarget attributes
fn init_texdesc_rt(mtl_desc: id("MTLTextureDescriptor"), img: *Sg.Image.T) void = {
    @debug_assert(img.cmn.render_target);
    @objc mtl_desc.usage = bit_or(MTLTextureUsageShaderRead, MTLTextureUsageRenderTarget);
    @objc mtl_desc.resourceOptions = MTLResourceStorageModePrivate;
}

// initialize MTLTextureDescriptor with MSAA attributes
fn init_texdesc_rt_msaa(it: id("MTLTextureDescriptor"), img: *Sg.Image.T) void = {
    @debug_assert(img.cmn.sample_count > 1);
    @objc it.usage = bit_or(MTLTextureUsageShaderRead, MTLTextureUsageRenderTarget);
    @objc it.resourceOptions = MTLResourceStorageModePrivate;
    @objc it.textureType = MTLTextureType2DMultisample;
    @objc it.sampleCount = @as(i64) img.cmn.sample_count;
}

new :: fn($classname: Str) id(classname) #generic = 
    @objc (@objc cls(classname).alloc()).init();

fn create_image(mtl: *Impl, img: *Sg.Image.T, desc: *Sg.Image.Desc) sg_resource_state = {
    @debug_assert(img && desc);
    injected := (0 != desc.mtl_textures[0]);

    // first initialize all Metal resource pool slots to 'empty'
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        img.mtl.tex[i] = mtl.add_resource(nil);
    }

    // initialize a Metal texture descriptor
    mtl_desc := new "MTLTextureDescriptor";
    if (!_sg_mtl_init_texdesc_common(mtl_desc, img)) {
        release(mtl_desc);
        return SG_RESOURCESTATE_FAILED;
    }
    if (img.cmn.render_target) {
        if (img.cmn.sample_count > 1) {
            init_texdesc_rt_msaa(mtl_desc, img);
        } else {
            init_texdesc_rt(mtl_desc, img);
        }
    }
    range(0, img.cmn.num_slots) { slot |
        id<MTLTexture> mtl_tex;
        if (injected) {
            @debug_assert(desc.mtl_textures[slot]);
            mtl_tex = (__bridge id<MTLTexture>) desc.mtl_textures[slot];
        } else {
            mtl_tex = [mtl.device newTextureWithDescriptor:mtl_desc];
            if (nil == mtl_tex) {
                release(mtl_desc);
                _SG_ERROR(METAL_CREATE_TEXTURE_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
            if ((img.cmn.usage == SG_USAGE_IMMUTABLE) && !img.cmn.render_target) {
                _sg_mtl_copy_image_data(img, mtl_tex, &desc.data);
            }
        }
        if Sg.SOKOL_DEBUG && desc.label {
            mtl_tex.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        img.mtl.tex[slot] = mtl.add_resource(mtl_tex);
        release(mtl_tex);
    }
    release(mtl_desc);
    return SG_RESOURCESTATE_VALID;
}

fn discard(mtl: *Impl, img: *Sg.Image.T) void = {
    @debug_assert(img);
    // it's valid to call release resource with a 'null resource'
    range(0, img.cmn.num_slots) { slot |
        _sg_mtl_release_resource(mtl.sg.frame_index, img.mtl.tex[slot]);
    }
}

fn create(mtl: *Impl, smp: *Sg.Sampler.T, desc: *Sg.Sampler.Desc) sg_resource_state = {
    @debug_assert(smp && desc);
    id<MTLSamplerState> mtl_smp;
    injected := (0 != desc.mtl_sampler);
    if (injected) {
        @debug_assert(desc.mtl_sampler);
        mtl_smp = (__bridge id<MTLSamplerState>) desc.mtl_sampler;
    } else {
        it := new "MTLSamplerDescriptor";
        @objc it.setSAddressMode(to_mtl(desc.wrap_u));
        @objc it.setTAddressMode(to_mtl(desc.wrap_v));
        @objc it.setRAddressMode(to_mtl(desc.wrap_w));
        if (mtl.sg.features.image_clamp_to_border) {
            ASSERT_NOT_OLD();
            @objc it.borderColor  = _sg_mtl_border_color(desc.border_color);
        }
        @objc it.minFilter = _sg_mtl_minmag_filter(desc.min_filter);
        @objc it.magFilter = _sg_mtl_minmag_filter(desc.mag_filter);
        @objc it.mipFilter = _sg_mtl_mipmap_filter(desc.mipmap_filter);
        @objc it.lodMinClamp = desc.min_lod;
        @objc it.lodMaxClamp = desc.max_lod;
        // FIXME: lodAverage?
        @objc it.maxAnisotropy = desc.max_anisotropy;
        @objc it.normalizedCoordinates = YES;
        @objc it.compareFunction = _sg_mtl_compare_func(desc.compare);
        if Sg.SOKOL_DEBUG && desc.label {
            @objc it.label = [NSString stringWithUTF8String:desc.label];
        }
        mtl_smp = @objc mtl.device.newSamplerStateWithDescriptor(it);
        @objc it.release();
        if (nil == mtl_smp) {
            _SG_ERROR(METAL_CREATE_SAMPLER_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    }
    smp.mtl.sampler_state = mtl.add_resource(mtl_smp);
    release(mtl_smp);
    return SG_RESOURCESTATE_VALID;
}

fn discard(mtl: *Impl, smp: *Sg.Sampler.T) void = {
    @debug_assert(smp);
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(mtl.sg.frame_index, smp.mtl.sampler_state);
}


// TODO: why doesn't #include_std work here? 
// TODO: i don't think i deduplicate accross import and #include_std so this will be the wrong type
#use("@/lib/sys/objective_c.fr");
fn id(classname: Str) Type = ObjCId;
fn Id(classname: Str) Type = i32;  // an index into the idpool
fn M(enumname: Str) Type = u32;  // a metal api enum

fn create_shader_func(mtl: *Impl, func: *sg_shader_function, label: CStr, label_ext: CStr, res: *_sg_mtl_shader_func_t) bool = {
    @debug_assert(res.mtl_lib == _SG_MTL_INVALID_SLOT_INDEX);
    @debug_assert(res.mtl_func == _SG_MTL_INVALID_SLOT_INDEX);
    err := zeroed id("NSError");
    mtl_lib: id("MTLLibrary") = if !func.bytecode.ptr.is_null() {
        @debug_assert(func.bytecode.size > 0);
        // TODO: where to #import from?
        fn dispatch_data_create(buffer: rawptr, size: i64, queue: i64, destructor: i64) id("NSData") #import("CoreFoundation");
    	// DISPATCH_DATA_DESTRUCTOR_DEFAULT is 0
        lib_data := dispatch_data_create(func.bytecode.ptr, func.bytecode.size, 0, 0);
        lib := @objc mtl.device.newLibraryWithData(lib_data, error = err&);
        release(lib_data);
        lib
    } else {
        src := @objc cls("NSString").stringWithUTF8String(func.source);
        @objc mtl.device.newLibraryWithSource(src, options = zeroed(ObjCId), error = err&)
    };
    if !err.is_nil() {
        _SG_ERROR(METAL_SHADER_CREATION_FAILED);
        _SG_LOGMSG(METAL_SHADER_COMPILATION_OUTPUT, [err.localizedDescription UTF8String]);
     }
    if(mtl_lib.is_nil(), => return(false));
    if Sg.SOKOL_DEBUG && label {
        @debug_assert(label_ext);
        mtl_lib.label = [NSString stringWithFormat:@"%s.%s", label, label_ext];
    }
    @debug_assert(func.entry);
    mtl_func := [mtl_lib newFunctionWithName:[NSString stringWithUTF8String:func.entry]];
    if (mtl_func == nil) {
        _SG_ERROR(METAL_SHADER_ENTRY_NOT_FOUND);
        release(mtl_lib);
        return false;
    }
    res.mtl_lib = mtl.add_resource(mtl_lib);
    res.mtl_func = mtl.add_resource(mtl_func);
    release(mtl_lib);
    release(mtl_func);
    true
}

fn release(self: ObjCId) void = @objc self.release();

fn discard(mtl: *Impl, func: _sg_mtl_shader_func_t) void = {
    // it is valid to call _sg_mtl_release_resource with a 'null resource'
    mtl.release_resource(mtl.sg.frame_index, func.mtl_func);
    mtl.release_resource(mtl.sg.frame_index, func.mtl_lib);
}

// NOTE: this is an out-of-range check for MSL bindslots that's also active in release mode
fn ensure_msl_bindslot_ranges(mtl: *Impl, desc: *Sg.Shader.Desc) bool = {
    @debug_assert(desc);
    X :: fn(count: i64, $get: @Fn(i: i64) i64, limit: i64, error) => 
        range(0, count) { i |
            if get(i) >= limit {
                _SG_ERROR(error);
                return false;
            };
        };
    X(SG_MAX_UNIFORMBLOCK_BINDSLOTS, fn(i) => desc.uniform_blocks[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_BINDINGS, METAL_UNIFORMBLOCK_MSL_BUFFER_SLOT_OUT_OF_RANGE);
    X(SG_MAX_STORAGEBUFFER_BINDSLOTS, fn(i) => desc.storage_buffers[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS, METAL_STORAGEBUFFER_MSL_BUFFER_SLOT_OUT_OF_RANGE);
    X(SG_MAX_IMAGE_BINDSLOTS, fn(i) => desc.images[i].msl_buffer_n, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS, METAL_IMAGE_MSL_TEXTURE_SLOT_OUT_OF_RANGE);
    X(SG_MAX_SAMPLER_BINDSLOTS, fn(i) => desc.samplers[i].msl_sampler_n, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS, METAL_SAMPLER_MSL_SAMPLER_SLOT_OUT_OF_RANGE);
    true
}

MTLSize :: @struct(width: u64, height: u64, depth: u64);

fn create_shader(mtl: *Impl, shd: *Sg.Shader.T, desc: *Sg.Shader.Desc) sg_resource_state = {
    @debug_assert(shd && desc);

    // do a MSL bindslot range check also in release mode, and if that fails,
    // also fail shader creation
    if (!_sg_mtl_ensure_msl_bindslot_ranges(desc)) {
        return SG_RESOURCESTATE_FAILED;
    }
    
    shd.mtl.threads_per_threadgroup = (
        width = @as(i64) desc.mtl_threads_per_threadgroup.x,
        height = @as(i64) desc.mtl_threads_per_threadgroup.y,
        depth = @as(i64) desc.mtl_threads_per_threadgroup.z,
    );

    enumerate shd.mtl.ub_buffer_n& { i, it |
        it[] = desc.uniform_blocks&[i].msl_buffer_n;
    }
    enumerate shd.mtl.sbuf_buffer_n& { i, it |
        it[] = desc.storage_buffers&[i].msl_buffer_n;
    }
    enumerate shd.mtl.img_texture_n& { i, it |
        it[] = desc.images&[i].msl_texture_n;
    }
    enumerate shd.mtl.smp_sampler_n& { i, it |
        it[] = desc.samplers&[i].msl_sampler_n;
    }

    // create metal library and function objects
    bool shd_valid = true;
    if (desc.vertex_func.source || desc.vertex_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.vertex_func, desc.label, "vs", &shd.mtl.vertex_func);
    }
    if (desc.fragment_func.source || desc.fragment_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.fragment_func, desc.label, "fs", &shd.mtl.fragment_func);
    }
    if (desc.compute_func.source || desc.compute_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.compute_func, desc.label, "cs", &shd.mtl.compute_func);
    }
    if (!shd_valid) {
        mtl.discard(shd.mtl.vertex_func);
        mtl.discard(shd.mtl.fragment_func);
        mtl.discard(shd.mtl.compute_func);
    }
    return shd_valid ? SG_RESOURCESTATE_VALID : SG_RESOURCESTATE_FAILED;
}

fn discard(mtl: *Impl, shd: *Sg.Shader.T) void = {
    mtl.discard(shd.mtl.vertex_func);
    mtl.discard(shd.mtl.fragment_func);
    mtl.discard(shd.mtl.compute_func);
}

MTLMutabilityImmutable :: 2;

fn create_pipeline(mtl: *Impl, pip: *Sg.Pipeline.T, shd: *Sg.Shader.T, desc: *Sg.Pipeline.Desc) sg_resource_state = {
    @debug_assert(pip && shd && desc);
    @debug_assert(desc.shader.id == shd.slot.id);

    pip.shader = shd;

    if (pip.cmn.is_compute) {
        err := zeroed id("NSError");
        cp_desc := new "MTLComputePipelineDescriptor";
        cp_desc.computeFunction = mtl.get_id(shd.mtl.compute_func.mtl_func);
        cp_desc.threadGroupSizeIsMultipleOfThreadExecutionWidth = true;
        for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
            const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
            @debug_assert((stage != SG_SHADERSTAGE_VERTEX) && (stage != SG_SHADERSTAGE_FRAGMENT));
            if ((stage == SG_SHADERSTAGE_COMPUTE) && shd.cmn.storage_buffers[i].readonly) {
                mtl_slot := shd.mtl.sbuf_buffer_n[i];
                cp_desc.buffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        if Sg.SOKOL_DEBUG && desc.label {
            cp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        mtl_cps := [mtl.device
            newComputePipelineStateWithDescriptor:cp_desc
            options:MTLPipelineOptionNone
            reflection:nil
            error:&err];
        release(cp_desc);
        if mtl_cps.is_nil() {
            @debug_assert(err);
            _SG_ERROR(METAL_CREATE_CPS_FAILED);
            _SG_LOGMSG(METAL_CREATE_CPS_OUTPUT, [err.localizedDescription UTF8String]);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.cps = mtl.add_resource(mtl_cps);
        release(mtl_cps);
        pip.mtl.threads_per_threadgroup = shd.mtl.threads_per_threadgroup;
    } else {
        prim_type := desc.primitive_type;
        pip.mtl.prim_type = to_mtl(prim_type);
        pip.mtl.index_size = to_mtl(pip.cmn.index_type);
        if (SG_INDEXTYPE_NONE != pip.cmn.index_type) {
            pip.mtl.index_type = to_mtl(pip.cmn.index_type);
        }
        pip.mtl.cull_mode = to_mtl(desc.cull_mode);
        pip.mtl.winding = to_mtl(desc.face_winding);
        pip.mtl.stencil_ref = desc.stencil.ref;

        // create vertex-descriptor
        vtx_desc := @objc cls("MTLVertexDescriptor").vertexDescriptor();
        for (NSUInteger attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
            const sg_vertex_attr_state* a_state = &desc.layout.attrs[attr_index];
            if (a_state.format == SG_VERTEXFORMAT_INVALID) {
                break;
            }
            @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
            @debug_assert(pip.cmn.vertex_buffer_layout_active[a_state.buffer_index]);
            it := @objc (@objc vtx_desc.attributes()).objectAtIndex(attr_index);
            @objc it.setFormat(to_mtl(a_state.format));
            @objc it.setOffset(@as(i64) a_state.offset);
            @objc it.setBufferIndex(_sg_mtl_vertexbuffer_bindslot((size_t)a_state.buffer_index));
        }
        for (NSUInteger layout_index = 0; layout_index < SG_MAX_VERTEXBUFFER_BINDSLOTS; layout_index++) {
            if (pip.cmn.vertex_buffer_layout_active[layout_index]) {
                l_state := desc.layout.buffers&.index(layout_index);
                mtl_vb_slot := _sg_mtl_vertexbuffer_bindslot(layout_index);
                @debug_assert(l_state.stride > 0);
                it := @objc (@objc vtx_desc.layouts()).objectAtIndex(mtl_vb_slot);
                @objc it.stride = @as(i64) l_state.stride;
                @objc it.stepFunction = _sg_mtl_step_function(l_state.step_func);
                @objc it.stepRate = @as(i64) l_state.step_rate;
                if (SG_VERTEXSTEP_PER_INSTANCE == l_state.step_func) {
                    // NOTE: not actually used in _sg_mtl_draw()
                    pip.cmn.use_instanced_draw = true;
                }
            }
        }

        // render-pipeline descriptor
        rp_desc := new "MTLRenderPipelineDescriptor";
        @objc rp_desc.vertexDescriptor = vtx_desc;
        @debug_assert(shd.mtl.vertex_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        @objc rp_desc.vertexFunction = mtl.get_id(shd.mtl.vertex_func.mtl_func);
        @debug_assert(shd.mtl.fragment_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        @objc rp_desc.fragmentFunction = mtl.get_id(shd.mtl.fragment_func.mtl_func);
        @objc rp_desc.rasterSampleCount = @as(i64) desc.sample_count;
        @objc rp_desc.alphaToCoverageEnabled = desc.alpha_to_coverage_enabled;
        @objc rp_desc.alphaToOneEnabled = NO;
        @objc rp_desc.rasterizationEnabled = YES;
        @objc rp_desc.depthAttachmentPixelFormat = _sg_mtl_pixel_format(desc.depth.pixel_format);
        if (desc.depth.pixel_format == SG_PIXELFORMAT_DEPTH_STENCIL) {
            @objc rp_desc.stencilAttachmentPixelFormat = _sg_mtl_pixel_format(desc.depth.pixel_format);
        }
        at := @objc rp_desc.colorAttachments();
        for (NSUInteger i = 0; i < @as(i64) desc.color_count; i++) {
            @debug_assert(i < SG_MAX_COLOR_ATTACHMENTS);
            cs := desc.colors&[i]&;
            it := @objc at.objectAtIndex(i);
            @objc it.pixelFormat = _sg_mtl_pixel_format(cs.pixel_format);
            @objc it.writeMask = _sg_mtl_color_write_mask(cs.write_mask);
            @objc it.blendingEnabled = cs.blend.enabled;
            @objc it.alphaBlendOperation = _sg_mtl_blend_op(cs.blend.op_alpha);
            @objc it.rgbBlendOperation = _sg_mtl_blend_op(cs.blend.op_rgb);
            @objc it.destinationAlphaBlendFactor = _sg_mtl_blend_factor(cs.blend.dst_factor_alpha);
            @objc it.destinationRGBBlendFactor = _sg_mtl_blend_factor(cs.blend.dst_factor_rgb);
            @objc it.sourceAlphaBlendFactor = _sg_mtl_blend_factor(cs.blend.src_factor_alpha);
            @objc it.sourceRGBBlendFactor = _sg_mtl_blend_factor(cs.blend.src_factor_rgb);
        }
        // set buffer mutability for all read-only buffers (vertex buffers and read-only storage buffers)
        for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
            if (pip.cmn.vertex_buffer_layout_active[i]) {
                mtl_slot := _sg_mtl_vertexbuffer_bindslot(i);
                rp_desc.vertexBuffers[@as(i64) mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
            mtl_slot := shd.mtl.sbuf_buffer_n[i];
            stage := shd.cmn.storage_buffers[i].stage;
            @debug_assert(stage != SG_SHADERSTAGE_COMPUTE);
            if (stage == SG_SHADERSTAGE_VERTEX) {
                @debug_assert(shd.cmn.storage_buffers[i].readonly);
                rp_desc.vertexBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
                @debug_assert(shd.cmn.storage_buffers[i].readonly);
                rp_desc.fragmentBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        if Sg.SOKOL_DEBUG && desc.label {
            rp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        err := zeroed id("NSError");
        mtl_rps := @objc mtl.device.newRenderPipelineStateWithDescriptor(rp_desc, error = err&);
        release(rp_desc);
        if (nil == mtl_rps) {
            @debug_assert(err);
            _SG_ERROR(METAL_CREATE_RPS_FAILED);
            _SG_LOGMSG(METAL_CREATE_RPS_OUTPUT, [err.localizedDescription UTF8String]);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.rps = mtl.add_resource(mtl_rps);
        release(mtl_rps);

        // depth-stencil-state
        ds_desc := new "MTLDepthStencilDescriptor";
        @objc ds_desc.setDepthCompareFunction(_sg_mtl_compare_func(desc.depth.compare));
        @objc ds_desc.setDepthWriteEnabled(desc.depth.write_enabled);
        if (desc.stencil.enabled) {
            sb := desc.stencil.back&;
            it := new "MTLStencilDescriptor";
            @objc ds_desc.setBackFaceStencil(it);
            @objc it.setStencilFailureOperation(_sg_mtl_stencil_op(sb.fail_op));
            @objc it.setDepthFailureOperation(_sg_mtl_stencil_op(sb.depth_fail_op));
            @objc it.setDepthStencilPassOperation(_sg_mtl_stencil_op(sb.pass_op));
            @objc it.setStencilCompareFunction(_sg_mtl_compare_func(sb.compare));
            @objc it.setReadMask(desc.stencil.read_mask);
            @objc it.setWriteMask(desc.stencil.write_mask);
            const sg_stencil_face_state* sf = &desc.stencil.front;
            it := new "MTLStencilDescriptor";
            @objc ds_desc.setFrontFaceStencil(it);
            @objc it.setStencilFailureOperation(_sg_mtl_stencil_op(sf.fail_op));
            @objc it.setDepthFailureOperation(_sg_mtl_stencil_op(sf.depth_fail_op));
            @objc it.setDepthStencilPassOperation(_sg_mtl_stencil_op(sf.pass_op));
            @objc it.setStencilCompareFunction(_sg_mtl_compare_func(sf.compare));
            @objc it.setReadMask(desc.stencil.read_mask);
            @objc it.setWriteMask(desc.stencil.write_mask);
        }
        if Sg.SOKOL_DEBUG && desc.label {
            ds_desc.label = [NSString stringWithFormat:@"%s.dss", desc.label];
        }
        mtl_dss := mtl.device.newDepthStencilStateWithDescriptor(ds_desc);
        release(ds_desc);
        if (nil == mtl_dss) {
            _SG_ERROR(METAL_CREATE_DSS_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.dss = mtl.add_resource(mtl_dss);
        release(mtl_dss);
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard(mtl: *Impl, pip: *Pipeline.T) void = {
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.cps);
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.rps);
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.dss);
}

fn create_attachments(mtl: *Impl, atts: *Sg.Attachments.T, color_images: **Sg.Image.T, resolve_images: **Sg.Image.T, ds_img: *Sg.Image.T, desc: *Sg.Attachments.Desc) sg_resource_state = {
    // copy image pointers
    range(0, atts.cmn.num_colors) { i |
        color_desc := desc.colors&.index(i);
        @debug_assert(color_desc.image.id != SG_INVALID_ID);
        @debug_assert(0 == atts.mtl.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        atts.mtl.colors[i].image = color_images[i];

        resolve_desc := &desc.resolves&.index(i);
        if (resolve_desc.image.id != SG_INVALID_ID) {
            @debug_assert(0 == atts.mtl.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            atts.mtl.resolves[i].image = resolve_images[i];
        }
    }
    @debug_assert(0 == atts.mtl.depth_stencil.image);
    ds_desc := desc.depth_stencil&;
    if (ds_desc.image.id != SG_INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        atts.mtl.depth_stencil.image = ds_img;
    }
    return SG_RESOURCESTATE_VALID;
}

fn discard_attachments(_: *Impl, _: *Sg.Attachments.T) void = ();

fn attachments_color_image(_: *Impl, atts: *Sg.Attachments.T, index: i32) *Sg.Image.T = {
    // NOTE: may return null
    @debug_assert(atts && (index >= 0) && (index < SG_MAX_COLOR_ATTACHMENTS));
    atts.mtl.colors[index].image
}

fn attachments_resolve_image(_: *Impl, atts: *Sg.Attachments.T, index: i32) *Sg.Image.T = {
    // NOTE: may return null
    atts.mtl.resolves[@as(i64) index].image
}

fn attachments_ds_image(_: *Impl, atts: *Sg.Attachments.T) *Sg.Image.T = {
    // NOTE: may return null
    atts.mtl.depth_stencil.image
}

fn bind_uniform_buffers(mtl: *Impl) void = {
    // In the Metal backend, uniform buffer bindings happen once in sg_begin_pass() and
    // remain valid for the entire pass. Only binding offsets will be updated
    // in sg_apply_uniforms()
    if (mtl.sg.cur_pass.is_compute) {
        @debug_assert(!mtl.compute_cmd_encoder.is_nil());
        range(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { slot |
            b := mtl.uniform_buffers&[mtl.cur_frame_rotate_index];
            @objc mtl.compute_cmd_encoder.setBuffer(b, offset = 0, atIndex = slot);
        }
    } else {
        @debug_assert(!mtl.render_cmd_encoder.is_nil());
        range(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { slot |
            b := mtl.uniform_buffers&[mtl.cur_frame_rotate_index];
            @objc mtl.render_cmd_encoder.setBuffer(b, offset = 0, atIndex = slot);
            @objc mtl.render_cmd_encoder.setFragmentBuffer(b, offset = 0, atIndex = slot);
        }
    }
}

fn begin_compute_pass(mtl: *Impl, pass: *SgPass) void = {
    @debug_assert(!mtl.cmd_buffer.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());

    // NOTE: we actually want computeCommandEncoderWithDispatchType:MTLDispatchTypeConcurrent, but
    // that requires bumping the macOS base version to 10.14
    mtl.compute_cmd_encoder = @objc mtl.cmd_buffer.computeCommandEncoder();
    if mtl.compute_cmd_encoder.is_nil() {
        mtl.sg.cur_pass.valid = false;
        return();
    }

    if Sg.SOKOL_DEBUG && !pass.label.is_null() {
        @objc mtl.compute_cmd_encoder.setLabel(@objc cls("NSString").stringWithUTF8String(pass.label));
    }
}

fn begin_render_pass(mtl: *Impl, pass: *SgPass) void = {
    @debug_assert(!mtl.cmd_buffer.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());

    atts := mtl.sg.cur_pass.atts;
    swapchain := pass.swapchain&;
    action := pass.action&;
    MTLClearColor :: @struct(red: f64, green: f64, blue: f64, alpha: f64);
    setClearColor :: fn(it, c) => {
        c: MTLClearColor = (red = c.r.cast(), green = c.g.cast(), blue = c.b.cast(), alpha = c.a.cast());
        @objc it.setClearColor(c);
    } 
    
    pass_desc := @objc cls("MTLRenderPassDescriptor").renderPassDescriptor();
    if !atts.is_null() {
        // setup pass descriptor for offscreen rendering
        ::enum(@type atts.slot.state);
        @debug_assert(atts.slot.state == .VALID);
        at := @objc pass_desc.colorAttachments();
        
        check :: fn(att, other) => {
            @debug_assert(att.slot.state == .VALID);
            @debug_assert(att.slot.id == other.image.id);
            @debug_assert(att.mtl.tex&[att.cmn.active_slot.intcast()] != 0); // _SG_MTL_INVALID_SLOT_INDEX
        };
        
        slice_or_depth :: fn(self, img, att) => {
            if @is(img.cmn.type, .CUBE, .ARRAY) {
                @objc self.setResolveSlice(@as(i64) att.slice.intcast());
            }
            if img.cmn.type == ._3D {
                @objc self.setResolveDepthPlane(@as(i64) att.slice.intcast());
            }
        }
        
        setTexture :: fn(it, img) => @objc it.setTexture(mtl.get_id(img.mtl.tex&[img.cmn.active_slot.intcast()]));
        range(0, atts.cmn.num_colors.intcast()) { i |
            ca := @objc at.objectAtIndex(i);
            cmn_color_att := atts.cmn.colors&[i]&;
            color_att_img := atts.mtl.colors&[i].image;
            cmn_resolve_att := atts.cmn.resolves&[i]&;
            resolve_att_img := atts.mtl.resolves&[i].image;
            check(color_att_img, cmn_color_att);
            @objc ca.setLoadAction(to_mtl(action.colors&[i].load_action));
            ::ptr_utils(@type resolve_att_img[]);
            @objc ca.setStoreAction(to_mtl(action.colors&[i].store_action, !resolve_att_img.is_null()));
            c := action.colors&[i].clear_value;
            setClearColor(ca, action.colors&[i].clear_value);
            setTexture(ca, color_att_img);
            @objc ca.setLevel(@as(i64) cmn_color_att.mip_level.intcast());
            ::enum(@type color_att_img.cmn.type);
            if @is(color_att_img.cmn.type, .CUBE, .ARRAY) {
                @objc ca.setSlice(@as(i64) cmn_color_att.slice.intcast());
            }
            if color_att_img.cmn.type == ._3D {
                @objc ca.setDepthPlane(@as(i64) cmn_color_att.slice.intcast());
            }
            if !resolve_att_img.is_null() {
                check(resolve_att_img, cmn_resolve_att);
                @objc ca.setResolveTexture(mtl.get_id(resolve_att_img.mtl.tex&[resolve_att_img.cmn.active_slot.intcast()]));
                @objc ca.setResolveLevel(@as(i64) cmn_resolve_att.mip_level.intcast());
                slice_or_depth(ca, resolve_att_img, cmn_resolve_att);
            }
        }
        ds_att_img := atts.mtl.depth_stencil.image;
        if !ds_att_img.is_null() {
            check(ds_att_img, atts.cmn.depth_stencil);
            d := @objc pass_desc.depthAttachment();
            setTexture(d, ds_att_img);
            @objc d.setLoadAction(to_mtl(action.depth.load_action));
            @objc d.setStoreAction(to_mtl(action.depth.store_action, false));
            @objc d.setClearDepth(action.depth.clear_value);
            cmn_ds_att := atts.cmn.depth_stencil&;
            slice_or_depth(d, ds_att_img, cmn_ds_att);
            if is_depth_stencil_format(ds_att_img.cmn.pixel_format) {
                sa := @objc pass_desc.stencilAttachment();
                setTexture(sa, ds_att_img);
                @objc sa.setLoadAction(to_mtl(action.stencil.load_action));
                @objc sa.setStoreAction(to_mtl(action.depth.store_action, false));
                @objc sa.setClearStencil(action.stencil.clear_value);
                slice_or_depth(sa, ds_att_img, cmn_ds_att);
            }
        }
    } else {
        // setup pass descriptor for swapchain rendering
        //
        // NOTE: at least in macOS Sonoma this no longer seems to be the case, the
        // current drawable is also valid in a minimized window
        // ===
        // an MTKView current_drawable will not be valid if window is minimized, don't do any rendering in this case
        if swapchain.metal.current_drawable.is_null() {
            mtl.sg.cur_pass.valid = false;
            return;
        }
        // pin the swapchain resources into memory so that they outlive their command buffer
        // (this is necessary because the command buffer doesn't retain references)
        pass_desc_ref := mtl.add_resource(pass_desc);
        mtl.release_resource(mtl.sg.frame_index, pass_desc_ref);

        mtl.cur_drawable = bit_cast_unchecked(rawptr, id("CAMetalDrawable"), swapchain.metal.current_drawable);
        ca := @objc pass_desc.colorAttachments();
        ca := @objc ca.objectAtIndex(0);
        if (swapchain.sample_count > 1) {
            // multi-sampling: render into msaa texture, resolve into drawable texture
            msaa_tex := bit_cast_unchecked(rawptr, id("MTLTexture"), swapchain.metal.msaa_color_texture);
            @debug_assert(!msaa_tex.is_nil());
            @objc ca.setTexture(msaa_tex);
            @objc ca.setResolveTexture(@objc mtl.cur_drawable.texture());
            @objc ca.setStoreAction(2); // MTLStoreActionMultisampleResolve
        } else {
            // non-msaa: render into current_drawable
            @objc ca.setTexture(@objc mtl.cur_drawable.texture());
            @objc ca.setStoreAction(1); // MTLStoreActionStore
        }
        @objc ca.setLoadAction(to_mtl(action.colors&[0].load_action));
        setClearColor(ca, action.colors&[0].clear_value);

        // optional depth-stencil texture
        if !swapchain.metal.depth_stencil_texture.is_null() {
            ds_tex := bit_cast_unchecked(rawptr, id("MTLTexture"), swapchain.metal.depth_stencil_texture);
            d := @objc pass_desc.depthAttachment();
            @objc d.setTexture(ds_tex);
            @objc d.setStoreAction(0); // MTLStoreActionDontCare
            @objc d.setLoadAction(to_mtl(action.depth.load_action));
            @objc d.setClearDepth(action.depth.clear_value);
            if is_depth_stencil_format(swapchain.depth_format) {
                sa := @objc pass_desc.stencilAttachment();
                @objc sa.setTexture(ds_tex);
                @objc sa.setStoreAction(0); // MTLStoreActionDontCare
                @objc sa.setLoadAction(to_mtl(action.stencil.load_action));
                @objc sa.setClearStencil(action.stencil.clear_value);
            }
        }
    }

    // NOTE: at least in macOS Sonoma, the following is no longer the case, a valid
    // render command encoder is also returned in a minimized window
    // ===
    // create a render command encoder, this might return nil if window is minimized
    mtl.render_cmd_encoder = @objc mtl.cmd_buffer.renderCommandEncoderWithDescriptor(pass_desc);
    if mtl.render_cmd_encoder.is_nil() {
        mtl.sg.cur_pass.valid = false;
        return();
    }

    if Sg.Sg.SOKOL_DEBUG && !pass.label.is_null() {
        @objc mtl.render_cmd_encoder.setLabel(@objc cls("NSString").stringWithUTF8String(pass.label));
    }
}

fn is_null(self: CStr) bool = self.ptr.is_null();

// TODO: where to #import this from?
DISPATCH_TIME_FOREVER :: bit_not(0);
fn dispatch_semaphore_wait(dsema: id("OS_dispatch_semaphore"), timeout: i64) i64 #import("CoreFoundation");
fn dispatch_semaphore_create(value: i64) id("OS_dispatch_semaphore") #import("CoreFoundation");
    
fn begin_pass(mtl: *Impl, pass: *SgPass) void = {
    @debug_assert(!mtl.cmd_queue.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());
    @debug_assert(mtl.cur_drawable.is_nil());
    mtl.clear_state_cache();

    // if this is the first pass in the frame, create one command buffer and blit-cmd-encoder for the entire frame
    if mtl.cmd_buffer.is_nil() {
        // block until the oldest frame in flight has finished
        dispatch_semaphore_wait(mtl.sem, DISPATCH_TIME_FOREVER);
        ::if(ObjCId);
        mtl.cmd_buffer = if mtl.sg.desc.mtl_use_command_buffer_with_retained_references {
            @objc mtl.cmd_queue.commandBuffer()
        } else {
            @objc mtl.cmd_queue.commandBufferWithUnretainedReferences()
        };
        @objc mtl.cmd_buffer.enqueue();
        todo();
        @objc mtl.cmd_buffer.addCompletedHandler(mtl.pass_completed_sem);
        // TODO: please_just_make_a_fucking_block__LEAK
        //[mtl.cmd_buffer addCompletedHandler:^(id<MTLCommandBuffer> cmd_buf) {
        //    // NOTE: this code is called on a different thread!
        //    _ := cmd_buf;
        //    dispatch_semaphore_signal(mtl.sem);
        //}];
    }

    // if this is first pass in frame, get uniform buffer base pointer
    if mtl.cur_ub_base_ptr.is_null() {
        mtl.cur_ub_base_ptr = @objc @as(*u8) mtl.uniform_buffers&[mtl.cur_frame_rotate_index].contents();
    }

    if (pass.compute) {
        mtl.begin_compute_pass(pass);
    } else {
        mtl.begin_render_pass(pass);
    }

    // bind uniform buffers, those bindings remain valid for the entire pass
    if mtl.sg.cur_pass.valid {
        mtl.bind_uniform_buffers();
    }
}

fn end_pass(mtl: *Impl) void = {
    if !mtl.render_cmd_encoder.is_nil() {
        @objc mtl.render_cmd_encoder.endEncoding();
        // NOTE: MTLRenderCommandEncoder is autoreleased
        mtl.render_cmd_encoder = zeroed ObjCId;
    }
    if !mtl.compute_cmd_encoder.is_nil() {
        @objc mtl.compute_cmd_encoder.endEncoding();
        // NOTE: MTLComputeCommandEncoder is autoreleased
        mtl.compute_cmd_encoder = zeroed ObjCId;

        ASSERT_NOT_IOS();
        // synchronize any managed buffers written by the GPU
        ASSERT_NOT_IOS();
        if !mtl.use_shared_storage_mode {
            if (mtl.sg.compute.readwrite_sbufs.len > 0) {
                blit_cmd_encoder := @objc mtl.cmd_buffer.blitCommandEncoder();
                for mtl.sg.compute.readwrite_sbufs { it |
                    if mtl.sg.lookup(it) { sbuf |
                        it := mtl.get_id(sbuf.mtl.buf&[@as(i64) sbuf.cmn.active_slot.intcast()]);
                        @objc blit_cmd_encoder.synchronizeResource(it);
                    }
                }
                @objc blit_cmd_encoder.endEncoding();
            }
        }
    }
    // if this is a swapchain pass, present the drawable
    if !mtl.cur_drawable.is_nil() {
        @objc mtl.cmd_buffer.presentDrawable(mtl.cur_drawable);
        mtl.cur_drawable = zeroed ObjCId;
    }
}

fn is_nil(self: ObjCId) bool = 
    bit_cast_unchecked(ObjCId, i64, self) == 0;

fn commit(mtl: *Impl) void = {
    @debug_assert(mtl.render_cmd_encoder.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(!mtl.cmd_buffer.is_nil());

    // commit the frame's command buffer
    @objc mtl.cmd_buffer.commit();

    // garbage-collect resources pending for release
    mtl.garbage_collect(mtl.sg.frame_index);

    // rotate uniform buffer slot
    mtl.cur_frame_rotate_index += 1;
    if mtl.cur_frame_rotate_index >= Sg.SG_NUM_INFLIGHT_FRAMES {
        mtl.cur_frame_rotate_index = 0;
    }
    mtl.cur_ub_offset = 0;
    mtl.cur_ub_base_ptr = u8.ptr_from_int(0);
    // NOTE: MTLCommandBuffer is autoreleased
    mtl.cmd_buffer = zeroed ObjCId;
}

fn apply_viewport(mtl: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(!mtl.render_cmd_encoder.is_nil() && mtl.sg.cur_pass.height > 0);
    MTLViewport :: @struct(originX: f64, originY: f64, width: f64, height: f64, znear: f64, zfar: f64);
    vp: MTLViewport = (
        originX = (double) x,
        originY = (double) (origin_top_left ? y : (mtl.sg.cur_pass.height - (y + h))),
        width   = (double) w,
        height  = (double) h,
        znear   = 0.0,
        zfar    = 1.0,
    );
    @objc mtl.render_cmd_encoder.setViewport(vp);
}

fn apply_scissor_rect(mtl: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(!mtl.render_cmd_encoder,is_nil() && mtl.sg.cur_pass.width > 0 && mtl.sg.cur_pass.height > 0);
    // clip against framebuffer rect
    clip: _sg_recti_t = clipi(x, y, w, h, mtl.sg.cur_pass.width, mtl.sg.cur_pass.height);
    MTLScissorRect :: @struct(x: u64, y: u64, width: u64, height: u64);
    r: MTLScissorRect = (
        x = @as(i64) clip.x,
        y = @as(i64)  (origin_top_left ? clip.y : (mtl.sg.cur_pass.height - (clip.y + clip.h))),
        width = @as(i64) clip.w,
        height = @as(i64) clip.h,
    );
    [mtl.render_cmd_encoder setScissorRect:r];
}

fn apply_pipeline(mtl: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(!pip.shader.is_null() && pip.cmn.shader_id.id == pip.shader.slot.id);
    if (mtl.state_cache.cur_pipeline_id.id != pip.slot.id) {
        mtl.state_cache.cur_pipeline = pip;
        mtl.state_cache.cur_pipeline_id.id = pip.slot.id;
        if (pip.cmn.is_compute) {
            @debug_assert(mtl.sg.cur_pass.is_compute);
            @debug_assert(nil != mtl.compute_cmd_encoder);
            @debug_assert(pip.mtl.cps != _SG_MTL_INVALID_SLOT_INDEX);
            [mtl.compute_cmd_encoder setComputePipelineState:mtl.get_id(pip.mtl.cps)];
        } else {
            @debug_assert(!mtl.sg.cur_pass.is_compute);
            @debug_assert(nil != mtl.render_cmd_encoder);
            sg_color c = pip.cmn.blend_color;
            [mtl.render_cmd_encoder setBlendColorRed:c.r green:c.g blue:c.b alpha:c.a];
            _sg_stats_add(metal.pipeline.num_set_blend_color, 1);
            [mtl.render_cmd_encoder setCullMode:pip.mtl.cull_mode];
            _sg_stats_add(metal.pipeline.num_set_cull_mode, 1);
            [mtl.render_cmd_encoder setFrontFacingWinding:pip.mtl.winding];
            _sg_stats_add(metal.pipeline.num_set_front_facing_winding, 1);
            [mtl.render_cmd_encoder setStencilReferenceValue:pip.mtl.stencil_ref];
            _sg_stats_add(metal.pipeline.num_set_stencil_reference_value, 1);
            [mtl.render_cmd_encoder setDepthBias:pip.cmn.depth.bias slopeScale:pip.cmn.depth.bias_slope_scale clamp:pip.cmn.depth.bias_clamp];
            _sg_stats_add(metal.pipeline.num_set_depth_bias, 1);
            @debug_assert(pip.mtl.rps != _SG_MTL_INVALID_SLOT_INDEX);
            [mtl.render_cmd_encoder setRenderPipelineState:mtl.get_id(pip.mtl.rps)];
            _sg_stats_add(metal.pipeline.num_set_render_pipeline_state, 1);
            @debug_assert(pip.mtl.dss != _SG_MTL_INVALID_SLOT_INDEX);
            [mtl.render_cmd_encoder setDepthStencilState:mtl.get_id(pip.mtl.dss)];
            _sg_stats_add(metal.pipeline.num_set_depth_stencil_state, 1);
        }
    }
}

fn apply_bindings(mtl: *Impl, bnd: *_sg_bindings_t) bool = {
    @debug_assert(bnd.pip);
    @debug_assert(bnd.pip && bnd.pip.shader);
    @debug_assert(bnd.pip.shader.slot.id == bnd.pip.cmn.shader_id.id);
    shd, sc := (bnd.pip.shader, mtl.state_cache&);

    // don't set vertex- and index-buffers in compute passes
    if (!mtl.sg.cur_pass.is_compute) {
        @debug_assert(nil != mtl.render_cmd_encoder);
        // store index buffer binding, this will be needed later in sg_draw()
        sc.cur_indexbuffer = bnd.ib;
        sc.cur_indexbuffer_offset = bnd.ib_offset;
        sc.cur_indexbuffer_id.id = if (bnd.ib) {
            @debug_assert(bnd.pip.cmn.index_type != SG_INDEXTYPE_NONE);
            bnd.ib.slot.id
        } else {
            @debug_assert(bnd.pip.cmn.index_type == SG_INDEXTYPE_NONE);
            SG_INVALID_ID
        };
        // apply vertex buffers
        range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            continue :: local_return;
            vb := bnd.vbs[i];
            if(vb == 0, => continue());
            mtl_slot := _sg_mtl_vertexbuffer_bindslot(i);
            @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
            vb_offset := bnd.vb_offsets[i];
            cur := sc.buffer&.index(.Vertex).index(mtl_slot);
            if ((cur.id != vb.slot.id) ||
                (sc.cur_vs_buffer_offsets[mtl_slot] != vb_offset))
            {
                sc.cur_vs_buffer_offsets[mtl_slot] = vb_offset;
                if (cur.id != vb.slot.id) {
                    // vertex buffer has changed
                    cur.id = vb.slot.id;
                    @debug_assert(vb.mtl.buf[vb.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
                    [mtl.render_cmd_encoder setVertexBuffer:mtl.get_id(vb.mtl.buf[vb.cmn.active_slot])
                        offset:@as(i64) vb_offset
                        atIndex:mtl_slot];
                } else {
                    // only vertex buffer offset has changed
                    [mtl.render_cmd_encoder setVertexBufferOffset:@as(i64) vb_offset atIndex:mtl_slot];
                }
                _sg_stats_add(metal.bindings.num_set_vertex_buffer, 1);
            }
        }
    }
    
    apply :: fn(stage, entry, res, mtl_slot, $v, $f, $c) => {
        ids := entry.index(stage);
        enc := @if(stage == .COMPUTE, mtl.compute_cmd_encoder, mtl.render_cmd_encoder);
        @debug_assert(nil != enc);
        if ids[mtl_slot].id != res.slot.id {
            ids[mtl_slot].id = res.slot.id;
            s := @match(stage) {
                fn VERTEX()   => v(enc, mtl_slot);
                fn FRAGMENT() => f(enc, mtl_slot);
                fn COMPUTE()  => c(enc, mtl_slot);
            };
            sg.stat(s, 1);
        }
    }
    
    // apply image bindings
    range(0, SG_MAX_IMAGE_BINDSLOTS) { i |
        continue :: local_return;
        img := bnd.imgs[i];
        if (img == 0, => continue());
        @debug_assert(img.mtl.tex[img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        stage := shd.cmn.images[i].stage;
        mtl_slot := shd.mtl.img_texture_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
        
        id := mtl.get_id(img.mtl.tex[img.cmn.active_slot]);  // todo: rather not hoist
        apply(stage, sc.image&, img, mtl_slot) { e, s |
            @objc e.setVertexTexture(id, atIndex = s);
            .mtl_bindings_num_set_vertex_texture
        } fragment { e, s |
            @objc e.setFragmentTexture(id, atIndex = s);
            .mtl_bindings_num_set_fragment_texture
        } compute { e, s |
            @objc e.setTexture(id, atIndex = s);
            .mtl_bindings_num_set_compute_texture
        };
    }

    // apply sampler bindings
    range(0, SG_MAX_SAMPLER_BINDSLOTS) { i |
        continue :: local_return;
        smp := bnd.smps[i];
        if (smp == 0, => continue());
        @debug_assert(smp.mtl.sampler_state != _SG_MTL_INVALID_SLOT_INDEX);
        stage := shd.cmn.samplers[i].stage;
        mtl_slot := shd.mtl.smp_sampler_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
        id := mtl.get_id(smp.mtl.sampler_state);  // todo: rather not hoist
        apply(stage, sc.sampler&, smp, mtl_slot) { e, s |
            @objc e.setVertexSamplerState(id, atIndex = s);
            .mtl_bindings_num_set_vertex_sampler_state
        } fragment { e, s |
            @objc e.setFragmentSamplerState(id, atIndex = s);
            .mtl_bindings_num_set_fragmnet_sampler_state
        } compute { e, s |
            @objc e.setSamplerState(id, atIndex = s);
            .mtl_bindings_num_set_compute_sampler_state
        };
    }

    // apply storage buffer bindings
    range(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
        sbuf := bnd.sbufs[i];
        if(sbuf == 0, => continue());
        @debug_assert(sbuf.mtl.buf[sbuf.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        stage := shd.cmn.storage_buffers[i].stage;
        mtl_slot := shd.mtl.sbuf_buffer_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS);
        id := mtl.get_id(sbuf.mtl.buf[sbuf.cmn.active_slot]);
        apply(stage, sc.buffer&, sbuf, mtl_slot) { e, s |
            @objc e.setVertexBuffer(id, offset = 0, atIndex = s);
            .mtl_bindings_num_set_vertex_buffer
        } fragment { e, s |
            @objc e.setFragmentBuffer(id, offset = 0, atIndex = s);
            .mtl_bindings_num_set_fragment_buffer
        } compute { e, s |
            @objc e.setBuffer(id, offset = 0, atIndex = s);
            .mtl_bindings_num_set_compute_buffer
        };
    }
    return true;
}

fn apply_uniforms(mtl: *Impl, ub_slot: i32, data: []u8) void = {
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert(((size_t)mtl.cur_ub_offset + data.size) <= (size_t)mtl.ub_size);
    @debug_assert((mtl.cur_ub_offset & (_SG_MTL_UB_ALIGN-1)) == 0);
    pip := mtl.state_cache.cur_pipeline;
    @debug_assert(pip && pip.shader);
    @debug_assert(pip.slot.id == mtl.state_cache.cur_pipeline_id.id);
    shd := pip.shader;
    @debug_assert(shd.slot.id == pip.cmn.shader_id.id);
    @debug_assert(data.size == shd.cmn.uniform_blocks[ub_slot].size);

    stage := shd.cmn.uniform_blocks[ub_slot].stage;
    mtl_slot := shd.mtl.ub_buffer_n[ub_slot];

    // copy to global uniform buffer, record offset into cmd encoder, and advance offset
    uint8_t* dst = &mtl.cur_ub_base_ptr[mtl.cur_ub_offset];
    memcpy(dst, data.ptr, data.size);
    if (stage == SG_SHADERSTAGE_VERTEX) {
        @debug_assert(nil != mtl.render_cmd_encoder);
        [mtl.render_cmd_encoder setVertexBufferOffset:@as(i64) mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_vertex_buffer_offset, 1);
    } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
        @debug_assert(nil != mtl.render_cmd_encoder);
        [mtl.render_cmd_encoder setFragmentBufferOffset:@as(i64) mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_fragment_buffer_offset, 1);
    } else if (stage == SG_SHADERSTAGE_COMPUTE) {
        @debug_assert(nil != mtl.compute_cmd_encoder);
        [mtl.compute_cmd_encoder setBufferOffset:@as(i64) mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_compute_buffer_offset, 1);
    } else {
        SOKOL_UNREACHABLE;
    }
    mtl.cur_ub_offset = _sg_roundup(mtl.cur_ub_offset + (int)data.size, _SG_MTL_UB_ALIGN);
}

fn draw(mtl: *Impl, base_element: i32, num_elements: i32, num_instances: i32) void = {
    sc := mtl.state_cache&;
    @debug_assert(nil != mtl.render_cmd_encoder);
    @debug_assert(sc.cur_pipeline && (sc.cur_pipeline.slot.id == sc.cur_pipeline_id.id));
    if (SG_INDEXTYPE_NONE != sc.cur_pipeline.cmn.index_type) {
        // indexed rendering
        @debug_assert(sc.cur_indexbuffer && (sc.cur_indexbuffer.slot.id == sc.cur_indexbuffer_id.id));
        const _sg_buffer_t* ib = sc.cur_indexbuffer;
        @debug_assert(ib.mtl.buf[ib.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        const NSUInteger index_buffer_offset = @as(i64)  (sc.cur_indexbuffer_offset + base_element * sc.cur_pipeline.mtl.index_size);
        [mtl.render_cmd_encoder drawIndexedPrimitives:sc.cur_pipeline.mtl.prim_type
            indexCount:@as(i64) num_elements
            indexType:sc.cur_pipeline.mtl.index_type
            indexBuffer:mtl.get_id(ib.mtl.buf[ib.cmn.active_slot])
            indexBufferOffset:index_buffer_offset
            instanceCount:@as(i64) num_instances];
    } else {
        // non-indexed rendering
        [mtl.render_cmd_encoder drawPrimitives:sc.cur_pipeline.mtl.prim_type
            vertexStart:@as(i64) base_element
            vertexCount:@as(i64) num_elements
            instanceCount:@as(i64) num_instances];
    }
}

fn dispatch(mtl: *Impl, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void = {
    @debug_assert(nil != mtl.compute_cmd_encoder);
    @debug_assert(mtl.state_cache.cur_pipeline && (mtl.state_cache.cur_pipeline.slot.id == mtl.state_cache.cur_pipeline_id.id));
    cur_pip := mtl.state_cache.cur_pipeline;
    thread_groups: MTLSize = (
        width = @as(i64) num_groups_x,
        height = @as(i64) num_groups_y,
        depth = @as(i64) num_groups_z,
    );
    threads_per_threadgroup := cur_pip.mtl.threads_per_threadgroup;
    [mtl.compute_cmd_encoder dispatchThreadgroups:thread_groups threadsPerThreadgroup:threads_per_threadgroup];
}

NSRange :: @struct(location: u64, length: u64);

fn inc_active_slot(it: ~T) void #where = {
    it.cmn.active_slot += 1;
    if (it.cmn.active_slot >= it.cmn.num_slots) {
        it.cmn.active_slot = 0;
    }
}

fn update_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8) void = {
    @debug_assert(buf && data && data.ptr && (data.size > 0));
    inc_active_slot(buf);
    __unsafe_unretained id<MTLBuffer> mtl_buf = mtl.get_id(buf.mtl.buf[buf.cmn.active_slot]);
    dst_ptr := @objc @as(rawptr) mtl_buf.contents();
    memcpy(dst_ptr, data.ptr, data.size);
    ASSERT_NOT_IOS();
    if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
        [mtl_buf didModifyRange:NSMakeRange(0, data.size)];
    }
}

fn append_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8, new_frame: bool) void = {
    @debug_assert(buf && data && data.ptr && (data.size > 0));
    if new_frame {
        inc_active_slot(buf);
    }
    __unsafe_unretained id<MTLBuffer> mtl_buf = mtl.get_id(buf.mtl.buf[buf.cmn.active_slot]);
    dst_ptr := @objc @as(*u8) mtl_buf.contents();
    dst_ptr += buf.cmn.append_pos;
    memcpy(dst_ptr, data.ptr, data.size);
    ASSERT_NOT_IOS();
    if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
        r: NSRange = (location = @as(i64) buf.cmn.append_pos, length = @as(i64) data.size);
        @objc mtl_buf.didModifyRange(r);
    }
}

fn update_image(mtl: *Impl, img: *Sg.Image.T, data: *sg_image_data) void = {
    @debug_assert(img && data);
    inc_active_slot(img);
    mtl_tex := mtl.get_id(img.mtl.tex[img.cmn.active_slot]);
    mtl.copy_image_data(img, mtl_tex, data);
}

fn push_debug_group(mtl: *Impl, name: CStr) void = {
    name := @objc NSString.stringWithUTF8String(name);
    @objc mtl.encoder().pushDebugGroup(name);
}

fn pop_debug_group(mtl: *Impl) void =
    @objc mtl.encoder().popDebugGroup(name);

fn encoder(mtl: *Impl) ObjCId = {
    enc := mtl.render_cmd_encoder;
    @if(enc.is_nil(), mtl.compute_cmd_encoder, enc)
}

//
// These accessors are useful when you want to mix and match 
// our rendering api with your own direct use of the Metal api. 
// If there's some specific Metal feature you need that we don't 
// provide a wrapper around, you can use these to just do it yourself.  
//

fn device(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.device)
}

fn render_command_encoder(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.render_cmd_encoder)
}

fn compute_command_encoder(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.compute_cmd_encoder)
}

fn query_buffer_info(mtl: *Impl, buf_id: Sg.Buffer) sg_mtl_buffer_info = {
    @debug_assert(mtl.sg.valid);
    res := zeroed sg_mtl_buffer_info;
    ASSERT_METAL();
    buf := mtl.sg.lookup(buf_id) || return(res);
    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        if (buf.mtl.buf[i] != 0) {
            res.buf[i] = (__bridge void*) mtl.get_id(buf.mtl.buf[i]);
        }
    }
    res.active_slot = buf.cmn.active_slot;
    res
}

fn query_image_info(mtl: *Impl, img_id: Sg.Image) sg_mtl_image_info = {
    @debug_assert(mtl.sg.valid);
    res := zeroed sg_mtl_image_info;
    ASSERT_METAL();
    img := mtl.sg.lookup(img_id) || return(res);
    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        if (img.mtl.tex[i] != 0) {
            res.tex[i] = (__bridge void*) mtl.get_id(img.mtl.tex[i]);
        }
    }
    res.active_slot = img.cmn.active_slot;
    res
}

fn query_sampler_info(mtl: *Impl, id: Sg.Sampler) sg_mtl_sampler_info = {
    @debug_assert(mtl.sg.valid);
    res := zereod sg_mtl_sampler_info;
    ASSERT_METAL();
    smp := mtl.sg.lookup(id) || return(res);
    if (smp.mtl.sampler_state != 0) {
        res.smp = (__bridge void*) mtl.get_id(smp.mtl.sampler_state);
    }
    res
}

fn query_shader_info(mtl: *Impl, shd_id: Sg.Shader) sg_mtl_shader_info = {
    @debug_assert(mtl.sg.valid);
    sg_mtl_shader_info res;
    _sg_clear(&res, sizeof(res));
    ASSERT_METAL();
    shd := _sg_lookup_shader(&mtl.sg.pools, shd_id.id) || return(res);
    const int vertex_lib  = shd.mtl.vertex_func.mtl_lib;
    const int vertex_func = shd.mtl.vertex_func.mtl_func;
    const int fragment_lib  = shd.mtl.fragment_func.mtl_lib;
    const int fragment_func = shd.mtl.fragment_func.mtl_func;
    if (vertex_lib != 0) {
        res.vertex_lib = (__bridge void*) mtl.get_id(vertex_lib);
    }
    if (fragment_lib != 0) {
        res.fragment_lib = (__bridge void*) mtl.get_id(fragment_lib);
    }
    if (vertex_func != 0) {
        res.vertex_func = (__bridge void*) mtl.get_id(vertex_func);
    }
    if (fragment_func != 0) {
        res.fragment_func = (__bridge void*) mtl.get_id(fragment_func);
    }
    res
}

fn query_pipeline_info(mtl: *Impl, pip_id: Sg.Pipeline) sg_mtl_pipeline_info = {
    @debug_assert(mtl.sg.valid);
    res := zeroed sg_mtl_pipeline_info;
    ASSERT_METAL();
    pip := mtl.sg.lookup(id) || return(res);
    if (pip.mtl.rps != 0) {
        res.rps = (__bridge void*) mtl.get_id(pip.mtl.rps);
    }
    if (pip.mtl.dss != 0) {
        res.dss = (__bridge void*) mtl.get_id(pip.mtl.dss);
    }
    res
}
