// Metal 3D API wrapper
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.
// 
// A sad amount of this task is just mapping our enum values to the metal 
// ones. I've manually transcribed the numbers so you can cross compile 
// without finding a copy of the Metal.framework headers. If I ever do more
// serious bindings to metal, this should be updated to use that instead. 
// 

ASSERT_NOT_IOS :: fn() => ();
ASSERT_NOT_OLD :: fn() => ();

_SG_MTL_UB_ALIGN :: { ASSERT_NOT_IOS(); 256 };
_SG_MTL_INVALID_SLOT_INDEX :: 0;

ReleaseItem :: @struct {
    frame_index: u32;   // frame index at which it is safe to release this resource
    slot_index: i32;
};

IdPool :: @struct {
    pool: id("NSMutableArray");
    num_slots: i64;
    free_queue_top: i64;
    free_queue: []i32;
    release_queue_front: i64;
    release_queue_back: i64;
    release_queue: []ReleaseItem;
};

Buffer :: @struct {
    buf: Array(Id("MTLBuffer"), Sg.SG_NUM_INFLIGHT_FRAMES);  // index into _sg_mtl_pool
};

Image :: @struct {
    tex: Array(Id("MTLTexture"), Sg.SG_NUM_INFLIGHT_FRAMES);
};

Sampler :: @struct {
    sampler_state: Id("MTLSamplerState");
};

_sg_mtl_shader_func_t :: @struct {
    mtl_lib: Id("MTLLibrary");
    mtl_func: Id("MTLFunction");
};

Shader :: @struct {
    vertex_func: _sg_mtl_shader_func_t;
    fragment_func: _sg_mtl_shader_func_t;
    compute_func: _sg_mtl_shader_func_t;
    threads_per_threadgroup: MTLSize;
    ub_buffer_n: Array(u8, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS);
    img_texture_n: Array(u8, Sg.SG_MAX_IMAGE_BINDSLOTS);
    smp_sampler_n: Array(u8, Sg.SG_MAX_SAMPLER_BINDSLOTS);
    sbuf_buffer_n: Array(u8, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS);
};

Pipeline :: @struct {
    prim_type: M("MTLPrimitiveType");
    index_size: i32;
    index_type: M("MTLIndexType");
    cull_mode: M("MTLCullMode");
    winding: M("MTLWinding");
    stencil_ref: u32;
    threads_per_threadgroup: MTLSize;
    cps: Id("MTLComputePipelineState");
    rps: Id("MTLRenderPipelineState");
    dss: Id("MTLDepthStencilState");
};

Attachments :: Sg'AttachmentsImpl(Impl, @struct {
    image: *Sg.Image.T;
});

// resource binding state cache
_SG_MTL_MAX_STAGE_UB_BINDINGS :: Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS;
_SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS :: _SG_MTL_MAX_STAGE_UB_BINDINGS + Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS;
_SG_MTL_MAX_STAGE_BUFFER_BINDINGS :: _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS + Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS;
_SG_MTL_MAX_STAGE_IMAGE_BINDINGS :: Sg.SG_MAX_IMAGE_BINDSLOTS;
_SG_MTL_MAX_STAGE_SAMPLER_BINDINGS :: Sg.SG_MAX_SAMPLER_BINDSLOTS;


StateCacheEntry :: fn($T, $N) => EnumMap(SgShaderStage, Array(T, N));

StateCache :: @struct {
    cur_pipeline: *Sg.Pipeline.T;
    cur_pipeline_id: Sg.Pipeline;
    cur_indexbuffer: *Sg.Buffer.T;
    cur_indexbuffer_id: Sg.Buffer;
    cur_indexbuffer_offset: i32;
    cur_vs_buffer_offsets: Array(i32, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    buffer: StateCacheEntry(Sg.Buffer, _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
    image: StateCacheEntry(Sg.Image, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
    sampler: StateCacheEntry(Sg.Sampler, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
};

Impl :: @struct {
    sg: *Sg.Self;
    valid: bool;
    use_shared_storage_mode: bool;
    cur_frame_rotate_index: i64;
    ub_size: i32;
    cur_ub_offset: i32;
    cur_ub_base_ptr: *u8;
    state_cache: StateCache;
    idpool: IdPool;
    sem: id("OS_dispatch_semaphore");
    device: id("MTLDevice");
    cmd_queue: id("MTLCommandQueue");
    cmd_buffer: id("MTLCommandBuffer");
    render_cmd_encoder: id("MTLRenderCommandEncoder");
    compute_cmd_encoder: id("MTLComputeCommandEncoder");
    cur_drawable: id("CAMetalDrawable");
    uniform_buffers: Array(id("MTLBuffer"), Sg.SG_NUM_INFLIGHT_FRAMES);
    pass_completed_sem: id("_NSConcreteGlobalBlock");
};

fn to_mtl(it: SgLoadAction) M("MTLLoadAction") = @map_enum(it) 
    (CLEAR = 2, LOAD = 1, DONTCARE = 0);

fn to_mtl(it: SgStoreAction, resolve: bool) M("MTLStoreAction") = {
    ::enum(@type it);
    @debug_assert(@is(it, .STORE, .DONTCARE));
    @if(it == .STORE, @if(resolve, 3, 1), @if(resolve, 2, 0))
}

fn resource_options_storage_mode_managed_or_shared(mtl: *Impl) M("MTLResourceOptions") = {
    ASSERT_NOT_IOS();
    @if(mtl.use_shared_storage_mode, /*Shared*/ 0, /*Managed*/ 1.shift_left(4))
}

fn to_mtl(mtl: *Impl, it: SgUsage) M("MTLResourceOptions") = {
    ::enum(@type it);
    @debug_assert(@is(it, .IMMUTABLE, .DYNAMIC, .STREAM));
    x := mtl.resource_options_storage_mode_managed_or_shared();
    if it != .IMMUTABLE {
        x = bit_or(x, 1);  // MTLResourceCPUCacheModeWriteCombined;
    };
    x
}

fn to_mtl(it: SgVertexStep) M("MTLVertexStepFunction") = @map_enum(it)
    (PER_VERTEX = 1, PER_INSTANCE = 2);

fn to_mtl(it: SgVertexFormat) M("MTLVertexFormat") = @map_enum(it) (
    FLOAT = 28, FLOAT2 = 29, FLOAT3 = 30, FLOAT4 = 31,
    INT = 32, INT2 = 33, INT3 = 34, INT4 = 35,
    UINT = 36, UINT2 = 37, UINT3 = 38, UINT4 = 39,
    BYTE4 = 6, BYTE4N = 12, UBYTE4 = 3, UBYTE4N = 9, 
    SHORT2 = 16, SHORT2N = 22, USHORT2 = 13, USHORT2N = 19,
    SHORT4 = 18, SHORT4N = 24, USHORT4 = 15, USHORT4N = 21,
    UINT10_N2 = 41, HALF2 = 25, HALF4 = 27,
);

fn to_mtl(it: SgPrimitiveType) M("MTLPrimitiveType") = @map_enum(it) 
    (POINTS = 0, LINES = 1, LINE_STRIP = 2, TRIANGLES = 3, TRIANGLE_STRIP = 3);

fn to_mtl(it: SgPixelFormat) M("MTLPixelFormat") = {
    ASSERT_NOT_IOS();
    @map_enum(it) (
        R8 = 10, R8SN = 12, R8UI = 13, R8SI = 14,
        R16 = 20, R16SN = 22, R16UI = 23, R16SI = 24, R16F = 25,
        RG8 = 30, RG8SN = 32, RG8UI = 33, RG8SI = 34,
        R32UI = 53, R32SI = 54, R32F = 55, 
        RG16 = 60, RG16SN = 62, RG16UI  = 63, RG16SI = 64, RG16F = 65, 
        RGBA8 = 70, SRGB8A8 = 71, RGBA8SN = 72, RGBA8UI = 73, RGBA8SI = 74, 
        BGRA8 = 80, RGB10A2 = 90, RG11B10F = 92, RGB9E5 = 93, 
        RG32UI = 103, RG32SI = 104, RG32F = 105, 
        RGBA16 = 110, RGBA16SN = 112, RGBA16UI = 113, RGBA16SI = 114, RGBA16F = 115,
        RGBA32UI = 123, RGBA32SI = 124, RGBA32F = 125, 
        DEPTH = 252, DEPTH_STENCIL = 260,
        BC1_RGBA = 130, BC2_RGBA = 132, BC3_RGBA = 134, BC3_SRGBA = 135,
        BC4_R = 140, BC4_RSN = 141, BC5_RG = 142, BC5_RGSN = 143,
        BC6H_RGBF = 150, BC6H_RGBUF = 151, BC7_RGBA = 152, BC7_SRGBA = 153, 
    )
}

fn to_mtl(m: SgColorMask) M("MTLColorWriteMask") = {
    bit :: fn(it: SgColorMask, n: i64) => 
        bit_and(@as(u32) m, @as(u32) it).ne(0).int().trunc().shift_left(n);
    bit(.R, 3).bit_or(bit(.G, 2)).bit_or(bit(.B, 1)).bit_or(bit(.A, 0))
}

fn to_mtl(it: SgBlendOp) M("MTLBlendOperation") = @map_enum(it) 
    (ADD = 0, SUBTRACT = 1, REVERSE_SUBTRACT = 2, MIN = 3, MAX = 4);

fn to_mtl(it: SgBlendFactor) M("MTLBlendFactor") = @map_enum(it) (
    ZERO = 0, ONE = 1, 
    SRC_COLOR = 2, ONE_MINUS_SRC_COLOR = 3,
    SRC_ALPHA = 4, ONE_MINUS_SRC_ALPHA = 5, 
    DST_COLOR = 6, ONE_MINUS_DST_COLOR = 7, 
    DST_ALPHA = 8, ONE_MINUS_DST_ALPHA = 9,
    SRC_ALPHA_SATURATED = 10, 
    BLEND_COLOR = 11, ONE_MINUS_BLEND_COLOR = 12,
    BLEND_ALPHA = 13, ONE_MINUS_BLEND_ALPHA = 14,
);

fn to_mtl(it: SgCompareFunc) M("MTLCompareFunction") = @map_enum(it) 
    (NEVER = 0, LESS = 1, EQUAL = 2, LESS_EQUAL = 3, GREATER = 4, NOT_EQUAL = 5, GREATER_EQUAL = 6, ALWAYS = 7);

fn to_mtl(it: SgStencilOp) M("MTLStencilOperation") = @map_enum(it)
    (KEEP = 0, ZERO = 1, REPLACE = 2, INCR_CLAMP = 3, DECR_CLAMP = 4, INVERT = 5, INCR_WRAP = 6, DECR_WRAP = 7);

fn to_mtl(it: SgCullMode) M("MTLCullMode") = @map_enum(it) 
    (NONE = 0, FRONT = 1, BACK = 2);

fn to_mtl(it: SgFaceWinding) M("MTLWinding") = @map_enum(it) 
    (CW = 0, CCW = 1);

fn to_mtl(it: SgIndexType) M("MTLIndexType") = @map_enum(it) 
    (UINT16 = 0, UINT32 = 1);

fn to_mtl(it: SgIndexType) i32 = @map_enum(it) 
    (NONE = 0, UINT16 = 2, UINT32 = 4);

fn to_mtl(it: SgImageType) M("MTLTextureType") = @map_enum(it)
    (_2D = 2, CUBE = 5, _3D = 7, ARRAY = 3);

fn to_mtl(mtl: *Impl, w: SgWrap) M("MTLSamplerAddressMode") = {
    ASSERT_NOT_OLD();
    b := mtl.sg.features.image_clamp_to_border;
    @map_enum(w) (REPEAT = 2, CLAMP_TO_EDGE = 0, CLAMP_TO_BORDER = @if(b, 5, 0), MIRRORED_REPEAT = 3)
}

fn to_mtl(it: SgBorderColor) M("MTLSamplerBorderColor") = {
    ASSERT_NOT_OLD();
    @map_enum(it) (TRANSPARENT_BLACK = 0, OPAQUE_BLACK = 1, OPAQUE_WHITE = 2)
}

fn mtl_minmag(it: SgFilter) M("MTLSamplerMinMagFilter") = @map_enum(it)
    (NEAREST = 0, LINEAR = 1);

fn mtl_mipmap(it: SgFilter) M("MTLSamplerMipFilter") = @map_enum(it) 
    (NEAREST = 1, LINEAR = 2);

fn _sg_mtl_vertexbuffer_bindslot(sokol_bindslot: i64) i64 =
    sokol_bindslot + _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS;

//-- a pool for all Metal resource objects, with deferred release queue ---------
fn init_pool(mtl: *Impl, desc: *SgDesc) void = {
    p := mtl.idpool&;
    p.num_slots = 2 *
        (
            2 * desc.buffer_pool_size +
            4 * desc.image_pool_size +
            1 * desc.sampler_pool_size +
            4 * desc.shader_pool_size +
            2 * desc.pipeline_pool_size +
            desc.attachments_pool_size +
            128
        ).intcast();
    p.pool = @objc cls("NSMutableArray").arrayWithCapacity(@as(i64) p.num_slots);
    @objc p.pool.retain();
    null := @objc cls("NSNull").null();
    range(0, p.num_slots) { _ |
        @objc p.pool.addObject(null);
    }
    @debug_assert((@objc @as(i64) p.pool.count()) == @as(i64) p.num_slots);
    a := mtl.sg.desc.allocator;
    // a queue of currently free slot indices
    // pool slot 0 is reserved!
    p.free_queue = a.alloc_init(i32, p.num_slots, fn(i) => intcast p.num_slots - 1 - i);
    p.free_queue_top = p.num_slots - 1;
    
    // a circular queue which holds release items (frame index when a resource is to be released, and the resource's pool index
    p.release_queue_front = 0;
    p.release_queue_back = 0;
    
    // filled with _SG_MTL_INVALID_SLOT_INDEX
    p.release_queue = a.alloc_zeroed(ReleaseItem, p.num_slots);
}

fn destroy_pool(mtl: *Impl) void = {
    p := mtl.idpool&;
    _sg_free(p.release_queue);  p.release_queue = 0;
    _sg_free(p.free_queue);     p.free_queue = 0;
    release(p.pool);
}

// get a new free resource pool slot
fn alloc_pool_slot(mtl: *Impl) i32 = {
    p := mtl.idpool&;
    @debug_assert(p.free_queue_top > 0, "free_queue empty");
    p.free_queue_top -= 1;
    slot_index := p.free_queue[p.free_queue_top];
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
    slot_index
}

// put a free resource pool slot back into the free-queue
fn free_pool_slot(mtl: *Impl, slot_index: i32) void = {
    p := mtl.idpool&;
    @debug_assert(p.free_queue_top < p.num_slots);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
    p.free_queue[p.free_queue_top] = slot_index;
    p.free_queue_top += 1;
}

// add an MTLResource to the pool, return pool index or 0 if input was 'nil'
fn add_resource(mtl: *Impl, res: id("T")) Id("T") = {
    if(res.is_nil(), => return(_SG_MTL_INVALID_SLOT_INDEX));
    mtl.sg.stat(.mtl_idpool_num_added, 1);
    slot_index := mtl.alloc_pool_slot();
    // NOTE: the NSMutableArray will take ownership of its items
    //@debug_assert([NSNull null] == mtl.idpool.pool[@as(i64) slot_index]);  // TODO
    @objc mtl.idpool.pool.replaceObjectAtIndex(@as(i64) slot_index.intcast(), withObject = res);
    slot_index
}

/*  mark an MTLResource for release, this will put the resource into the
    deferred-release queue, and the resource will then be released N frames later. */
fn release_resource(mtl: *Impl, frame_index: u32, slot_index: Id("T")) void = {
    if slot_index == _SG_MTL_INVALID_SLOT_INDEX {
        /* this means that a nil value was provided to mtl.add_resource */
        return();
    }
    p := mtl.idpool&;
    mtl.sg.stat(.mtl_idpool_num_released, 1);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
    //@debug_assert([NSNull null] != p.pool[@as(i64) slot_index]);  // TODO
    release_index := p.release_queue_front;
    p.release_queue_front += 1;
    if (p.release_queue_front >= p.num_slots) {
        // wrap-around
        p.release_queue_front = 0;
    }
    // release queue full?
    @debug_assert(p.release_queue_front != p.release_queue_back);
    @debug_assert(p.release_queue[release_index].frame_index == 0);
    safe_to_release_frame_index := frame_index + Sg.SG_NUM_INFLIGHT_FRAMES + 1;
    p.release_queue[release_index] = (frame_index = safe_to_release_frame_index, slot_index = slot_index);
}

// NSNull: you're not allowed to put nil in an NSMutableArray (even tho everything's a pointer anyway?), 
// so they have a magic sentinal value to use instead of nil. nil != [NSNull null]. 

// run garbage-collection pass on all resources in the release-queue
fn garbage_collect(mtl: *Impl, frame_index: u32) void = {
    p := mtl.idpool&;
    while => p.release_queue_back != p.release_queue_front {
        if (frame_index < p.release_queue[p.release_queue_back].frame_index) {
            // don't need to check further, release-items past this are too young
            return();
        }
        mtl.sg.stat(.mtl_num_garbage_collected, 1);
        // safe to release this resource
        slot_index := p.release_queue[p.release_queue_back].slot_index;
        @debug_assert((slot_index > 0) && (slot_index < p.num_slots.intcast()));
        // note: the NSMutableArray takes ownership of its items, assigning an NSNull object will
        // release the object, no matter if using ARC or not
        // use 
        null := @objc cls("NSNull").null();
        i: i64 = slot_index.intcast();
        @debug_assert(bit_cast_unchecked(ObjCId, i64, null) != bit_cast_unchecked(ObjCId, i64, @objc p.pool.objectAtIndex(i)));
        @objc p.pool.replaceObjectAtIndex(i, withObject = null);
        // put the now free pool index back on the free queue
        mtl.free_pool_slot(slot_index);
        // reset the release queue slot and advance the back index
        p.release_queue[p.release_queue_back].frame_index = 0;
        p.release_queue[p.release_queue_back].slot_index = _SG_MTL_INVALID_SLOT_INDEX;
        p.release_queue_back += 1;
        if (p.release_queue_back >= p.num_slots) {
            // wrap-around
            p.release_queue_back = 0;
        }
    }
}

fn get_id(mtl: *Impl, slot_index: Id("T")) id("T") = 
    @objc mtl.idpool.pool.objectAtIndexedSubscript(@as(i64) slot_index.intcast());

fn clear_state_cache(mtl: *Impl) void = {
    mtl.state_cache = zeroed @type mtl.state_cache;
}

MTLGPUFamily :: @enum(u32) (Apple1 = 1001, Apple7 = 1007, Mac2 = 2002, Metal3 = 5001);

fn supports_family(mtl: *Impl, family: MTLGPUFamily) bool =
    @objc @as(bool) mtl.device.supportsFamily(family);

// https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf
fn init_caps(mtl: *Impl) void = {
    ASSERT_NOT_IOS();
    //mtl.sg.backend = SG_BACKEND_METAL_MACOS;
    
    ASSERT_NOT_OLD();
    mtl.sg.features = (
        origin_top_left = true,
        mrt_independent_blend_state = true,
        mrt_independent_write_mask = true,
        compute = true,
        msaa_image_bindings = true,
        image_clamp_to_border = mtl.supports_family(.Apple7) || mtl.supports_family(.Mac2) || mtl.supports_family(.Metal3),
    );

    ASSERT_NOT_IOS();
    mtl.sg.limits = (
        max_image_size_2d = 16 * 1024,
        max_image_size_cube = 16 * 1024,
        max_image_size_3d = 2 * 1024,
        max_image_size_array = 16 * 1024,
        max_image_array_layers = 2 * 1024,
        max_vertex_attrs = Sg.SG_MAX_VERTEX_ATTRIBUTES,
    );
    
    ASSERT_NOT_IOS();
    @fill_formats(mtl.sg.formats&) (
        sfbrm = (.R8, .R8SN, .R16, .R16SN, .R16F, .RG8, .RG8SN, .R32F, .RG16, 
            .RG16SN, .RG16F, .RGBA8, .SRGB8A8, .RGBA8SN, .BGRA8, .RGB10A2, .RG11B10F, 
            .RG32F, .RGBA16, .RGBA16SN, .RGBA16F, .RGBA32F),
        srm = (.R8UI, .R8SI, .R16UI, .R16SI, .RG8UI, .RG8SI, .RG16UI, .RG16SI, .RGBA8UI, 
            .RGBA8SI, .RG32UI, .RG32SI, .RGBA16UI, .RGBA16SI, .RGBA32UI, .RGBA32SI),
        sr = (.R32UI, .R32SI),
        sf = (.RGB9E5, .BC1_RGBA, .BC2_RGBA, .BC3_RGBA, .BC3_SRGBA, .BC4_R, .BC4_RSN, 
            .BC5_RG, .BC5_RGSN, .BC6H_RGBF, .BC6H_RGBUF, .BC7_RGBA, .BC7_SRGBA),
        srmd = (.DEPTH, .DEPTH_STENCIL),
    );
}

fn ns_str(s: CStr) id("NSString") = 
    @objc cls("NSString").stringWithUTF8String(s);

//-- main Metal backend state and functions ------------------------------------
fn setup_backend(mtl: *Impl, desc: *SgDesc) void = {
    // assume already zero-initialized
    @debug_assert(!desc.environment.metal.device.is_null());
    @debug_assert(desc.uniform_buffer_size > 0);
    mtl.init_pool(desc);
    mtl.clear_state_cache();  // ... not assuming already zero-initialized?
    mtl.valid = true;
    mtl.ub_size = desc.uniform_buffer_size;
    mtl.sem = dispatch_semaphore_create(Sg.SG_NUM_INFLIGHT_FRAMES);
    mtl.device = bit_cast_unchecked(rawptr, id("MTLDevice"), desc.environment.metal.device);
    mtl.cmd_queue = @objc mtl.device.newCommandQueue();
    
    {
        aBlock := new_global_block(mtl.sg.desc.allocator, Impl.raw_from_ptr(mtl), @as(rawptr) pass_completed_handler);
        mtl.pass_completed_sem = bit_cast_unchecked(rawptr, ObjCId, aBlock);
    };

    range(0, Sg.SG_NUM_INFLIGHT_FRAMES) { i |
        o := 1; // MTLResourceCPUCacheModeWriteCombined|MTLResourceStorageModeShared;
        mtl.uniform_buffers&[i] = @objc mtl.device.newBufferWithLength(@as(i64) mtl.ub_size.intcast(), options = o);
        if Sg.SOKOL_DEBUG {
            // TODO: is this supposed to use the varargs abi? or can i just use a normal selector based on the named args? 
            //       either way i need to change @objc to allow this :MsgSendVarArgs
            //@objc mtl.uniform_buffers&[i].setLabel(@objc cls("NSString").stringWithFormat(ns_str("sg-uniform-buffer.%d"), i));
        }
    }

    if desc.mtl_force_managed_storage_mode {
        mtl.use_shared_storage_mode = false;
    } else {
        ASSERT_NOT_OLD();
        // on Intel Macs, always use managed resources even though the
        // device says it supports unified memory (because of texture restrictions)
        is_apple_gpu := mtl.supports_family(.Apple1);  
        mtl.use_shared_storage_mode = is_apple_gpu;
    };
    mtl.init_caps();
}

fn discard_backend(mtl: *Impl) void = {
    @debug_assert(mtl.valid);
    // wait for the last frame to finish
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_wait(mtl.sem, DISPATCH_TIME_FOREVER);
    }
    // semaphore must be "relinquished" before destruction
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_signal(mtl.sem);
    }
    mtl.garbage_collect(mtl.sg.frame_index + SG_NUM_INFLIGHT_FRAMES + 2);
    destroy_pool();
    mtl.valid = false;

    release(mtl.sem);
    release(mtl.device);
    release(mtl.cmd_queue);
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        release(mtl.uniform_buffers[i]);
    }
    // NOTE: MTLCommandBuffer, MTLRenderCommandEncoder and MTLComputeCommandEncoder are auto-released
    mtl.cmd_buffer = zeroed ObjCId;
    mtl.render_cmd_encoder = zeroed ObjCId;
    mtl.compute_cmd_encoder = zeroed ObjCId;
}

fn reset_state_cache(mtl: *Impl) void = {
    _sg_mtl_clear_state_cache();
}

fn create(mtl: *Impl, buf: *Sg.Buffer.T, desc: *Sg.Buffer.Desc) SgResourceState = {
    @debug_assert(buf.cmn.size > 0);
    injected := !desc.mtl_buffers&[0].is_null();
    mtl_options := mtl.to_mtl(buf.cmn.usage);
    range(0, intcast buf.cmn.num_slots) { slot | 
        mtl_buf := if injected {
            bit_cast_unchecked(rawptr, id"MTLBuffer", desc.mtl_buffers&[slot])
        } else {
            if !desc.data.ptr.is_null() {
                @debug_assert(desc.data.len > 0);
                @objc mtl.device.newBufferWithBytes(desc.data.ptr, length = @as(i64) intcast buf.cmn.size, options = mtl_options)
            } else {
                @debug_assert(desc.data.len == 0);
                // this is guaranteed to zero-initialize the buffer
                @objc mtl.device.newBufferWithLength(@as(i64) intcast buf.cmn.size, options = mtl_options)
            }
        };
        if mtl_buf.is_nil() {
            @debug_assert(!injected, "don't give us null pointers");
            _SG_ERROR("METAL_CREATE_BUFFER_FAILED");
            return(.FAILED);
        }
        if Sg.SOKOL_DEBUG && !desc.label.is_null() { // :MsgSendVarArgs
            //mtl_buf.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        buf.mtl.buf&[slot] = mtl.add_resource(mtl_buf);
        release(mtl_buf);
    };
    .VALID
}

fn discard_buffer(mtl: *Impl, buf: *Sg.Buffer.T) void = {
    @debug_assert(buf);
    range(0, buf.cmn.num_slots) { slot | 
        // it's valid to call release resource with '0'
        mtl.release_resource(mtl.sg.frame_index, buf.mtl.buf[slot]);
    }
}

fn copy_image_data(mtl: *Impl, img: *Sg.Image.T, mtl_tex: id("MTLTexture"), data: *SgImageData) void = {
    num_faces := if(img.cmn.type == .CUBE, => 6, => 1);
    num_slices := if(img.cmn.type == .ARRAY, => img.cmn.num_slices, => 1);
    range(0, num_faces) { face_index |
        range(0, intcast img.cmn.num_mipmaps) { mip_index |
            buf := data.subimage&[face_index]&[mip_index];
            @debug_assert(!buf.ptr.is_null() && buf.len > 0, "empty buffer at face=%, mip=%", face_index, mip_index);
            mip_width := Sg'miplevel_dim(img.cmn.width, intcast mip_index);
            mip_height := Sg'miplevel_dim(img.cmn.height, intcast mip_index);
            bytes_per_row := intcast row_pitch(img.cmn.pixel_format, mip_width, 1);
            bytes_per_slice := intcast surface_pitch(img.cmn.pixel_format, mip_width, mip_height, 1);
            /* bytesPerImage special case: https://developer.apple.com/documentation/metal/mtltexture/1515679-replaceregion

                "Supply a nonzero value only when you copy data to a MTLTextureType3D type texture"
            */
            MTLOrigin :: @struct(x: i64, y: i64, z: i64);
            MTLRegion :: @struct(origin := zeroed MTLOrigin, size: MTLSize);

            ::if(Ty(i64, i64));
            mip_depth, bytes_per_image := if img.cmn.type == ._3D {
                // FIXME: apparently the minimal bytes_per_image size for 3D texture is 4 KByte... somehow need to handle this
                (intcast Sg'miplevel_dim(img.cmn.num_slices, intcast mip_index), bytes_per_slice)
            } else {
                (1, 0)
            };
            region: MTLRegion = (size = (width = intcast mip_width, height = intcast mip_height, depth = mip_depth));

            range(0, intcast num_slices) { slice_index | 
                mtl_slice_index := @if(img.cmn.type == .CUBE, face_index, slice_index);
                slice_offset := slice_index * bytes_per_slice;
                @debug_assert((slice_offset + bytes_per_slice) <= buf.len);
                @objc mtl_tex.replaceRegion(region,
                    mipmapLevel = @as(i64) mip_index,
                    slice = @as(i64) mtl_slice_index,
                    withBytes = buf.ptr.offset(slice_offset),
                    bytesPerRow = bytes_per_row,
                    bytesPerImage = bytes_per_image,
                );
            }
        }
    }
}

// i happen to know the abi for u64 is the same as i32, mov(Kw) zeroes the high bits

// initialize MTLTextureDescriptor with common attributes
fn _sg_mtl_init_texdesc_common(mtl: *Impl, it: id("MTLTextureDescriptor"), img: *Sg.Image.T) bool = {
    @objc_set(it) (
        textureType = to_mtl(img.cmn.type),
        pixelFormat = to_mtl(img.cmn.pixel_format),
        width = img.cmn.width,
        height = img.cmn.height,
        depth = @if(img.cmn.type == ._3D, img.cmn.num_slices, 1),
        mipmapLevelCount = img.cmn.num_mipmaps,
        arrayLength = @if(img.cmn.type == .ARRAY, img.cmn.num_slices, 1),
        usage = MTLTextureUsageShaderRead,
        resourceOptions = @as(M"MTLResourceOptions")
            int(img.cmn.usage != .IMMUTABLE).trunc() // MTLResourceCPUCacheModeWriteCombined is 1
            .bit_or(mtl.resource_options_storage_mode_managed_or_shared()),
    );
    if (0 == @objc @as(i64) it.pixelFormat()) {
        _SG_ERROR("METAL_TEXTURE_FORMAT_NOT_SUPPORTED");
        return false;
    }
     true
}

MTLResourceStorageModePrivate :: 2.shift_left(4);
MTLTextureUsageShaderRead :: 1;
MTLTextureUsageRenderTarget :: 4;

// initialize MTLTextureDescriptor with rendertarget attributes
fn init_texdesc_rt(mtl_desc: id("MTLTextureDescriptor"), img: *Sg.Image.T) void = {
    @debug_assert(img.cmn.render_target);
    @objc_set(mtl_desc) (
        usage = bit_or(MTLTextureUsageShaderRead, MTLTextureUsageRenderTarget),
        resourceOptions = MTLResourceStorageModePrivate,
    );
}

// initialize MTLTextureDescriptor with MSAA attributes
fn init_texdesc_rt_msaa(it: id("MTLTextureDescriptor"), img: *Sg.Image.T) void = {
    @debug_assert(img.cmn.sample_count > 1);
    @objc_set(it) (
        usage = bit_or(MTLTextureUsageShaderRead, MTLTextureUsageRenderTarget),
        resourceOptions = MTLResourceStorageModePrivate,
        textureType = @as(M"MTLTextureType") 4, // 2DMultisample
        sampleCount = @as(i64) intcast img.cmn.sample_count,
    );
}

fn create(mtl: *Impl, img: *Sg.Image.T, desc: *Sg.Image.Desc) SgResourceState = {
    injected := !desc.mtl_textures&[0].is_null();

    // first initialize all Metal resource pool slots to 'empty'
    range(0, Sg.SG_NUM_INFLIGHT_FRAMES) { i |
        img.mtl.tex&[i] = mtl.add_resource(zeroed id("MTLTexture"));
    }

    // initialize a Metal texture descriptor
    mtl_desc := new "MTLTextureDescriptor";
    if (!mtl._sg_mtl_init_texdesc_common(mtl_desc, img)) {
        release(mtl_desc);
        return(.FAILED);
    }
    if (img.cmn.render_target) {
        if (img.cmn.sample_count > 1) {
            init_texdesc_rt_msaa(mtl_desc, img);
        } else {
            init_texdesc_rt(mtl_desc, img);
        }
    }
    range(0, intcast img.cmn.num_slots) { slot |
        mtl_tex := if (injected) {
            @debug_assert(!desc.mtl_textures&[slot].is_null());
            bit_cast_unchecked(rawptr, id("MTLTexture"), desc.mtl_textures&[slot])
        } else {
            mtl_tex := @objc mtl.device.newTextureWithDescriptor(mtl_desc);
            if mtl_tex.is_nil() {
                release(mtl_desc);
                _SG_ERROR("METAL_CREATE_TEXTURE_FAILED");
                return(.FAILED);
            }
            if img.cmn.usage == .IMMUTABLE && !img.cmn.render_target {
                mtl.copy_image_data(img, mtl_tex, desc.data&);
            };
            mtl_tex
        }
        if Sg.SOKOL_DEBUG && !desc.label.is_null() { // :MsgSendVarArgs
            //mtl_tex.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        img.mtl.tex&[slot] = mtl.add_resource(mtl_tex);
        release(mtl_tex);
    };
    release(mtl_desc);
    .VALID
}

// TODO: for all the :MsgSendVarArgs labels, 
//       now that the default value is empty string, check for that instead

fn discard(mtl: *Impl, img: *Sg.Image.T) void = {
    @debug_assert(img);
    // it's valid to call release resource with a 'null resource'
    range(0, img.cmn.num_slots) { slot |
        _sg_mtl_release_resource(mtl.sg.frame_index, img.mtl.tex[slot]);
    }
}

fn create(mtl: *Impl, smp: *Sg.Sampler.T, desc: *Sg.Sampler.Desc) SgResourceState = {
    injected := !desc.mtl_sampler.is_null();
    mtl_smp := if (injected) {
        bit_cast_unchecked(rawptr, id("MTLSamplerState"), desc.mtl_sampler)
    } else {
        it := new "MTLSamplerDescriptor";
        if (mtl.sg.features.image_clamp_to_border) {
            ASSERT_NOT_OLD();
            @objc it.setBorderColor(to_mtl(desc.border_color));
        }
        @objc_set(it) (
            sAddressMode = mtl.to_mtl(desc.wrap_u),
            tAddressMode = mtl.to_mtl(desc.wrap_v),
            rAddressMode = mtl.to_mtl(desc.wrap_w),
            minFilter = mtl_minmag desc.min_filter,
            magFilter = mtl_minmag desc.mag_filter,
            mipFilter = mtl_mipmap desc.mipmap_filter,
            lodMinClamp = desc.min_lod,
            lodMaxClamp = desc.max_lod,
            // FIXME: lodAverage?
            maxAnisotropy = desc.max_anisotropy,
            normalizedCoordinates = true,
            compareFunction = to_mtl desc.compare,
        );
        if Sg.SOKOL_DEBUG && !desc.label.is_null() {
            @objc it.setLabel(@objc cls("NSString").stringWithUTF8String(desc.label));
        }
        mtl_smp := @objc mtl.device.newSamplerStateWithDescriptor(it);
        @objc it.release();
        if mtl_smp.is_nil() {
            _SG_ERROR("METAL_CREATE_SAMPLER_FAILED");
            return(.FAILED);
        }
        mtl_smp
    };
    smp.mtl.sampler_state = mtl.add_resource(mtl_smp);
    release(mtl_smp);
    .VALID
}

fn discard(mtl: *Impl, smp: *Sg.Sampler.T) void = {
    // it's valid to call release resource with a 'null resource'
    mtl.release_resource(mtl.sg.frame_index, smp.mtl.sampler_state);
}

// TODO: why doesn't #include_std work here? 
// TODO: i don't think i deduplicate accross import and #include_std so this will be the wrong type
#use("@/lib/sys/objective_c.fr");
fn id(classname: Str) Type = ObjCId;
fn Id(classname: Str) Type = i32;  // an index into the idpool

// a metal api enum. just using this as a comment in case i end up doing more serious bindings. 
fn M(enumname: Str) Type = u32;

fn create_shader_func(mtl: *Impl, func: *SgShaderFunction, label: CStr, label_ext: CStr, res: *_sg_mtl_shader_func_t) bool = {
    @debug_assert(res.mtl_lib == _SG_MTL_INVALID_SLOT_INDEX);
    @debug_assert(res.mtl_func == _SG_MTL_INVALID_SLOT_INDEX);
    err := zeroed id("NSError");
    mtl_lib: id("MTLLibrary") = if !func.bytecode.ptr.is_null() {
        @debug_assert(func.bytecode.len > 0);
        // TODO: where to #import from?
        fn dispatch_data_create(buffer: *u8, size: i64, queue: i64, destructor: i64) id("NSData") #import("libc");
    	// DISPATCH_DATA_DESTRUCTOR_DEFAULT is 0
        lib_data := dispatch_data_create(func.bytecode.ptr, func.bytecode.len, 0, 0);
        lib := @objc mtl.device.newLibraryWithData(lib_data, error = err&);
        release(lib_data);
        lib
    } else {
        src := @objc cls("NSString").stringWithUTF8String(func.source);
        @objc mtl.device.newLibraryWithSource(src, options = zeroed(ObjCId), error = err&)
    };
    if !err.is_nil() {
        _SG_ERROR("METAL_SHADER_CREATION_FAILED");
        _SG_LOGMSG("METAL_SHADER_COMPILATION_OUTPUT", @objc @as(CStr) UTF8String(@objc err.localizedDescription()));
     }
    if(mtl_lib.is_nil(), => return(false));
    if Sg.SOKOL_DEBUG && !label.is_null() {  // :MsgSendVarArgs
        //@debug_assert(label_ext);
        //mtl_lib.label = [NSString stringWithFormat:@"%s.%s", label, label_ext];
    }
    @debug_assert(!func.entry.is_null());
    name := @objc cls("NSString").stringWithUTF8String(func.entry);
    mtl_func := @objc mtl_lib.newFunctionWithName(name);
    if mtl_func.is_nil() {
        _SG_ERROR("METAL_SHADER_ENTRY_NOT_FOUND");
        release(mtl_lib);
        return false;
    }
    res.mtl_lib = mtl.add_resource(mtl_lib);
    res.mtl_func = mtl.add_resource(mtl_func);
    release(mtl_lib);
    release(mtl_func);
    true
}

fn release(self: ObjCId) void = 
    @objc @as(void) self.release();

fn discard(mtl: *Impl, func: _sg_mtl_shader_func_t) void = {
    // it is valid to call _sg_mtl_release_resource with a 'null resource'
    mtl.release_resource(mtl.sg.frame_index, func.mtl_func);
    mtl.release_resource(mtl.sg.frame_index, func.mtl_lib);
}

// NOTE: this is an out-of-range check for MSL bindslots that's also active in release mode
fn ensure_msl_bindslot_ranges(mtl: *Impl, desc: *Sg.Shader.Desc) bool = {
    X :: fn(count: i64, $get: @Fn(i: i64) u8, limit: u8, error) => 
        range(0, count) { i |
            if get(i) >= limit {
                _SG_ERROR(error);
                return false;
            };
        };
    X(Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS, fn(i) => desc.uniform_blocks&[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_BINDINGS, "METAL_UNIFORMBLOCK_MSL_BUFFER_SLOT_OUT_OF_RANGE");
    X(Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS, fn(i) => desc.storage_buffers&[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS, "METAL_STORAGEBUFFER_MSL_BUFFER_SLOT_OUT_OF_RANGE");
    X(Sg.SG_MAX_IMAGE_BINDSLOTS, fn(i) => desc.images&[i].msl_texture_n, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS, "METAL_IMAGE_MSL_TEXTURE_SLOT_OUT_OF_RANGE");
    X(Sg.SG_MAX_SAMPLER_BINDSLOTS, fn(i) => desc.samplers&[i].msl_sampler_n, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS, "METAL_SAMPLER_MSL_SAMPLER_SLOT_OUT_OF_RANGE");
    true
}

MTLSize :: @struct(width: i64, height: i64, depth: i64);

fn create(mtl: *Impl, shd: *Sg.Shader.T, desc: *Sg.Shader.Desc) SgResourceState = {
    // do a MSL bindslot range check also in release mode, and if that fails,
    // also fail shader creation
    if !mtl.ensure_msl_bindslot_ranges(desc) {
        return(.FAILED);
    }
    
    shd.mtl.threads_per_threadgroup = (
        width = intcast desc.mtl_threads_per_threadgroup.x,
        height = intcast desc.mtl_threads_per_threadgroup.y,
        depth = intcast desc.mtl_threads_per_threadgroup.z,
    );

    enumerate shd.mtl.ub_buffer_n& { i, it |
        it[] = desc.uniform_blocks&[i].msl_buffer_n;
    }
    enumerate shd.mtl.sbuf_buffer_n& { i, it |
        it[] = desc.storage_buffers&[i].msl_buffer_n;
    }
    enumerate shd.mtl.img_texture_n& { i, it |
        it[] = desc.images&[i].msl_texture_n;
    }
    enumerate shd.mtl.smp_sampler_n& { i, it |
        it[] = desc.samplers&[i].msl_sampler_n;
    }

    
    has :: fn(func) => !func.bytecode.ptr.is_null() || !func.source.is_null();
    // create metal library and function objects
    shd_valid := true
        && (!has(desc.vertex_func&)   || mtl.create_shader_func(desc.vertex_func&, desc.label, "vs", shd.mtl.vertex_func&))
        && (!has(desc.fragment_func&) || mtl.create_shader_func(desc.fragment_func&, desc.label, "fs", shd.mtl.fragment_func&))
        && (!has(desc.compute_func&)  || mtl.create_shader_func(desc.compute_func&, desc.label, "cs", shd.mtl.compute_func&))
    ;
    if !shd_valid {
        mtl.discard(shd.mtl.vertex_func);
        mtl.discard(shd.mtl.fragment_func);
        mtl.discard(shd.mtl.compute_func);
    }
    @if(shd_valid, .VALID, .FAILED)
}

fn discard(mtl: *Impl, shd: *Sg.Shader.T) void = {
    mtl.discard(shd.mtl.vertex_func);
    mtl.discard(shd.mtl.fragment_func);
    mtl.discard(shd.mtl.compute_func);
}

MTLMutabilityImmutable :: 2;

fn create(mtl: *Impl, pip: *Sg.Pipeline.T, shd: *Sg.Shader.T, desc: *Sg.Pipeline.Desc) SgResourceState = {
    @debug_assert(desc.shader.id == shd.slot.id);

    pip.cmn.shader_t = shd;
    
    fn set_immutable(self: ObjCId, mtl_slot: ~T) void #where = {
        // TODO: I don't super understand why you can't nest these. @objc #macro bug? 
        self := @objc self.objectAtIndexedSubscript(mtl_slot);
        @objc self.setMutability(MTLMutabilityImmutable);
    }

    if pip.cmn.compute {
        err := zeroed id("NSError");
        cp_desc := new "MTLComputePipelineDescriptor";
        @objc_set(cp_desc) (
            computeFunction = mtl.get_id(shd.mtl.compute_func.mtl_func),
            threadGroupSizeIsMultipleOfThreadExecutionWidth = true,
        );
        range(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
            stage := shd.cmn.storage_buffers&[i].stage;
            @debug_assert(!@is(stage, .VERTEX, .FRAGMENT));
            if stage == .COMPUTE && shd.cmn.storage_buffers&[i].readonly {
                mtl_slot := shd.mtl.sbuf_buffer_n&[i];
                set_immutable(@objc cp_desc.buffers(), mtl_slot);
            }
        }
        if Sg.SOKOL_DEBUG && !desc.label.is_null() {
            // :MsgSendVarArgs
            //cp_desc.label = @objc cls("NSString").stringWithFormat(@"%s", desc.label);
        }
        mtl_cps := @objc mtl.device.newComputePipelineStateWithDescriptor(cp_desc, 
            options = @as(M"MTLPipelineOption") 0, 
            reflection = zeroed ObjCId, 
            error = err&,
        );
        release(cp_desc);
        if mtl_cps.is_nil() {
            @debug_assert(!err.is_nil());
            _SG_ERROR("METAL_CREATE_CPS_FAILED");
            _SG_LOGMSG("METAL_CREATE_CPS_FAILED", @objc @as(CStr) (@objc err.localizedDescription()).UTF8String());
            return(.FAILED);
        }
        pip.mtl.cps = mtl.add_resource(mtl_cps);
        release(mtl_cps);
        pip.mtl.threads_per_threadgroup = shd.mtl.threads_per_threadgroup;
    } else {
        prim_type := desc.primitive_type;
        pip.mtl.prim_type = to_mtl(prim_type);
        pip.mtl.index_size = to_mtl(pip.cmn.index_type);
        if pip.cmn.index_type != .NONE {
            pip.mtl.index_type = to_mtl(pip.cmn.index_type);
        }
        pip.mtl.cull_mode = to_mtl(desc.cull_mode);
        pip.mtl.winding = to_mtl(desc.face_winding);
        pip.mtl.stencil_ref = zext desc.stencil.ref;

        // create vertex-descriptor
        vtx_desc := @objc cls("MTLVertexDescriptor").vertexDescriptor();
        
        rangeb(0, Sg.SG_MAX_VERTEX_ATTRIBUTES) { attr_index, $break |
            a_state := desc.layout.attrs&[attr_index]&;
            ::enum(@type a_state.format);
            if a_state.format == .INVALID {
                break();
            }
            @debug_assert(a_state.buffer_index < Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS);
            @debug_assert(pip.cmn.vertex_buffer_layout_active&[intcast a_state.buffer_index]);
            xx := @objc vtx_desc.attributes();
            it := @objc xx.objectAtIndexedSubscript(attr_index);
            @objc it.setFormat(to_mtl(a_state.format));
            @objc it.setOffset(@as(i64) intcast a_state.offset);
            @objc it.setBufferIndex(_sg_mtl_vertexbuffer_bindslot(intcast a_state.buffer_index));
        }
        range(0, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS) { layout_index | 
            if pip.cmn.vertex_buffer_layout_active&[layout_index] {
                l_state := desc.layout.buffers&.index(layout_index);
                mtl_vb_slot := _sg_mtl_vertexbuffer_bindslot(layout_index);
                @debug_assert(l_state.stride > 0);
                xx := @objc vtx_desc.layouts();
                it := @objc xx.objectAtIndexedSubscript(mtl_vb_slot);
                @objc_set(it) (
                    stride = @as(i64) intcast l_state.stride,
                    stepFunction = to_mtl(l_state.step_func),
                    stepRate = @as(i64) intcast l_state.step_rate,
                );
                ::enum(@type l_state.step_func);
                if l_state.step_func == .PER_INSTANCE {
                    // NOTE: not actually used in _sg_mtl_draw()
                    pip.cmn.use_instanced_draw = true;
                }
            }
        }

        rp_desc := new "MTLRenderPipelineDescriptor";
        @debug_assert(shd.mtl.vertex_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        @debug_assert(shd.mtl.fragment_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        @objc_set(rp_desc) (
            vertexDescriptor = vtx_desc,
            vertexFunction = mtl.get_id(shd.mtl.vertex_func.mtl_func),
            fragmentFunction = mtl.get_id(shd.mtl.fragment_func.mtl_func),
            rasterSampleCount = @as(i64) intcast desc.sample_count,
            alphaToCoverageEnabled = desc.alpha_to_coverage_enabled,
            alphaToOneEnabled = false,
            rasterizationEnabled = true,
            depthAttachmentPixelFormat = to_mtl(desc.depth.pixel_format),
        );
        if desc.depth.pixel_format == .DEPTH_STENCIL {
            @objc rp_desc.setStencilAttachmentPixelFormat(to_mtl(desc.depth.pixel_format));
        }
        at := @objc rp_desc.colorAttachments();
        range(0, intcast desc.color_count) { i |
            @debug_assert(i < Sg.SG_MAX_COLOR_ATTACHMENTS);
            cs := desc.colors&[i]&;
            it := @objc at.objectAtIndexedSubscript(i);
            @debug_assert(!it.is_nil());
            @objc_set(it) (
                pixelFormat = to_mtl(cs.pixel_format),
                writeMask = to_mtl(cs.write_mask),
                blendingEnabled = cs.blend.enabled,
                alphaBlendOperation = to_mtl(cs.blend.op_alpha),
                rgbBlendOperation = to_mtl(cs.blend.op_rgb),
                destinationAlphaBlendFactor = to_mtl(cs.blend.dst_factor_alpha),
                destinationRGBBlendFactor = to_mtl(cs.blend.dst_factor_rgb),
                sourceAlphaBlendFactor = to_mtl(cs.blend.src_factor_alpha),
                sourceRGBBlendFactor = to_mtl(cs.blend.src_factor_rgb),
            );
        }
        // set buffer mutability for all read-only buffers (vertex buffers and read-only storage buffers)
        range(0, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            if (pip.cmn.vertex_buffer_layout_active&[i]) {
                mtl_slot := _sg_mtl_vertexbuffer_bindslot(i);
                set_immutable(@objc rp_desc.vertexBuffers(), mtl_slot);
            }
        }
        range(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
            mtl_slot := shd.mtl.sbuf_buffer_n&[i];
            stage := shd.cmn.storage_buffers&[i].stage;
            @debug_assert(stage == .NONE || shd.cmn.storage_buffers&[i].readonly);
            @match(stage) {
                fn VERTEX()   => set_immutable(@objc rp_desc.vertexBuffers(), mtl_slot);
                fn FRAGMENT() => set_immutable(@objc rp_desc.fragmentBuffers(), mtl_slot);
                fn COMPUTE()  => @debug_assert(false, "compute buffer in non-compute pipeline");
                fn NONE()     => ();
            };
        }
        if Sg.SOKOL_DEBUG && !desc.label.is_null() {  // :MsgSendVarArgs
            //rp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        err := zeroed id("NSError");
        mtl_rps := @objc mtl.device.newRenderPipelineStateWithDescriptor(rp_desc, error = err&);
        release(rp_desc);
        if mtl_rps.is_nil() {
            @debug_assert(!err.is_nil());
            _SG_ERROR("METAL_CREATE_RPS_FAILED");
            _SG_LOGMSG("METAL_CREATE_RPS_FAILED", @objc @as(CStr) (@objc err.localizedDescription()).UTF8String());
            return(.FAILED);
        }
        pip.mtl.rps = mtl.add_resource(mtl_rps);
        release(mtl_rps);

        // depth-stencil-state
        ds_desc := new "MTLDepthStencilDescriptor";
        @objc ds_desc.setDepthCompareFunction(to_mtl(desc.depth.compare));
        @objc ds_desc.setDepthWriteEnabled(desc.depth.write_enabled);
        if desc.stencil.enabled {
            stencil :: fn(d: *Sg.Pipeline.Desc, f: *SgStencilFaceState) id("MTLStencilDescriptor") = {
                it := new "MTLStencilDescriptor";
                @objc_set(it) (
                    stencilFailureOperation   = to_mtl f.fail_op,
                    depthFailureOperation     = to_mtl f.depth_fail_op,
                    depthStencilPassOperation = to_mtl f.pass_op,
                    stencilCompareFunction    = to_mtl f.compare,
                    readMask  = d.stencil.read_mask,
                    writeMask = d.stencil.write_mask,
                );
                it
            };
            
            @objc_set(ds_desc) (
                backFaceStencil  = stencil(desc, desc.stencil.back&),
                frontFaceStencil = stencil(desc, desc.stencil.front&),
            );
        }
        if Sg.SOKOL_DEBUG && !desc.label.is_null() { // :MsgSendVarArgs
            //ds_desc.label = [NSString stringWithFormat:@"%s.dss", desc.label];
        }
        mtl_dss := @objc mtl.device.newDepthStencilStateWithDescriptor(ds_desc);
        release(ds_desc);
        if mtl_dss.is_nil() {
            _SG_ERROR("METAL_CREATE_DSS_FAILED"); 
            return(.FAILED);
        }
        pip.mtl.dss = mtl.add_resource(mtl_dss);
        release(mtl_dss);
    };
    .VALID
}

fn discard(mtl: *Impl, pip: *Sg.Pipeline.T) void = {
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.cps);
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.rps);
    _sg_mtl_release_resource(mtl.sg.frame_index, pip.mtl.dss);
}

fn create(mtl: *Impl, atts: *Sg.Attachments.T, color_images: **Sg.Image.T, resolve_images: **Sg.Image.T, ds_img: *Sg.Image.T, desc: *Sg.Attachments.Desc) sg_resource_state = {
    // copy image pointers
    range(0, atts.cmn.num_colors) { i |
        color_desc := desc.colors&.index(i);
        @debug_assert(color_desc.image.id != SG_INVALID_ID);
        @debug_assert(0 == atts.mtl.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        atts.mtl.colors[i].image = color_images[i];

        resolve_desc := &desc.resolves&.index(i);
        if (resolve_desc.image.id != SG_INVALID_ID) {
            @debug_assert(0 == atts.mtl.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            atts.mtl.resolves[i].image = resolve_images[i];
        }
    }
    @debug_assert(0 == atts.mtl.depth_stencil.image);
    ds_desc := desc.depth_stencil&;
    if (ds_desc.image.id != SG_INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        atts.mtl.depth_stencil.image = ds_img;
    }
    return SG_RESOURCESTATE_VALID;
}

// TODO: make `_` not be a real identifier so you can ignore both of these with one. 
fn discard(_: *Impl, __: *Sg.Attachments.T) void = ();

fn bind_uniform_buffers(mtl: *Impl) void = {
    // In the Metal backend, uniform buffer bindings happen once in sg_begin_pass() and
    // remain valid for the entire pass. Only binding offsets will be updated
    // in sg_apply_uniforms()
    if (mtl.sg.cur_pass.is_compute) {
        @debug_assert(!mtl.compute_cmd_encoder.is_nil());
        range(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { slot |
            b := mtl.uniform_buffers&[mtl.cur_frame_rotate_index];
            @objc mtl.compute_cmd_encoder.setBuffer(b, offset = 0, atIndex = slot);
        }
    } else {
        @debug_assert(!mtl.render_cmd_encoder.is_nil());
        range(0, Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS) { slot |
            b := mtl.uniform_buffers&[mtl.cur_frame_rotate_index];
            @objc mtl.render_cmd_encoder.setVertexBuffer(b, offset = 0, atIndex = slot);
            @objc mtl.render_cmd_encoder.setFragmentBuffer(b, offset = 0, atIndex = slot);
        }
    }
}

fn begin_compute_pass(mtl: *Impl, pass: *SgPass) void = {
    @debug_assert(!mtl.cmd_buffer.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());

    // NOTE: we actually want computeCommandEncoderWithDispatchType:MTLDispatchTypeConcurrent, but
    // that requires bumping the macOS base version to 10.14
    mtl.compute_cmd_encoder = @objc mtl.cmd_buffer.computeCommandEncoder();
    if mtl.compute_cmd_encoder.is_nil() {
        mtl.sg.cur_pass.valid = false;
        return();
    }

    if Sg.SOKOL_DEBUG && !pass.label.is_null() {
        @objc mtl.compute_cmd_encoder.setLabel(@objc cls("NSString").stringWithUTF8String(pass.label));
    }
}

MTLClearColor :: @struct(red: f64, green: f64, blue: f64, alpha: f64);
fn to_mtl(c: SgColor) MTLClearColor =
    (red = c.r.cast(), green = c.g.cast(), blue = c.b.cast(), alpha = c.a.cast());

fn begin_render_pass(mtl: *Impl, pass: *SgPass) void = {
    @debug_assert(!mtl.cmd_buffer.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());

    pass_desc := @objc cls("MTLRenderPassDescriptor").renderPassDescriptor();
    action := pass.action&;
    atts := mtl.sg.cur_pass.atts;
    if !atts.is_null() { // TODO: UNTESTED
        // setup pass descriptor for offscreen rendering
        ::enum(@type atts.slot.state);
        @debug_assert(atts.slot.state == .VALID);
        at := @objc pass_desc.colorAttachments();
        
        check :: fn(att, other) => {
            @debug_assert(att.slot.state == .VALID);
            @debug_assert(att.slot.id == other.image.id);
            @debug_assert(att.mtl.tex&[att.cmn.active_slot.intcast()] != 0); // _SG_MTL_INVALID_SLOT_INDEX
        };
        
        slice_or_depth :: fn(self, img, att) => {
            if @is(img.cmn.type, .CUBE, .ARRAY) {
                @objc self.setResolveSlice(@as(i64) att.slice.intcast());
            }
            if img.cmn.type == ._3D {
                @objc self.setResolveDepthPlane(@as(i64) att.slice.intcast());
            }
        }
        
        get_texture :: fn(img) => mtl.get_id(img.mtl.tex&[img.cmn.active_slot.intcast()]);
        range(0, atts.cmn.num_colors.intcast()) { i |
            ca := @objc at.objectAtIndexedSubscript(i);
            cmn_color_att := atts.cmn.colors&[i]&;
            color_att_img := atts.mtl.colors&[i].image;
            resolve_att_img := atts.mtl.resolves&[i].image;
            check(color_att_img, cmn_color_att);
            ::ptr_utils(@type resolve_att_img[]);
            it := action.colors&.index(i);
            have_resolve := !resolve_att_img.is_null();
            @objc_set(ca) (
                load_action = to_mtl it.load_action,
                store_action = to_mtl(it.store_action, have_resolve),
                clearColor = to_mtl it.clear_value,
                texture = get_texture(color_att_img),
                level = @as(i64) cmn_color_att.mip_level.intcast(),
            );
            ::enum(@type color_att_img.cmn.type);
            if @is(color_att_img.cmn.type, .CUBE, .ARRAY) {
                @objc ca.setSlice(@as(i64) cmn_color_att.slice.intcast());
            }
            if color_att_img.cmn.type == ._3D {
                @objc ca.setDepthPlane(@as(i64) cmn_color_att.slice.intcast());
            }
            if have_resolve {
                cmn_resolve_att := atts.cmn.resolves&[i]&;
                check(resolve_att_img, cmn_resolve_att);
                @objc ca.setResolveTexture(mtl.get_id(resolve_att_img.mtl.tex&[resolve_att_img.cmn.active_slot.intcast()]));
                @objc ca.setResolveLevel(@as(i64) cmn_resolve_att.mip_level.intcast());
                slice_or_depth(ca, resolve_att_img, cmn_resolve_att);
            }
        }
        ds_att_img := atts.mtl.depth_stencil.image;
        if !ds_att_img.is_null() {
            check(ds_att_img, atts.cmn.depth_stencil);
            d := @objc pass_desc.depthAttachment();
            @objc_set(d) (
                texture = get_texture(ds_att_img),
                loadAction = to_mtl action.depth.load_action,
                storeAction = to_mtl(action.depth.store_action, false),
                clearDepth = action.depth.clear_value,
            );
            cmn_ds_att := atts.cmn.depth_stencil&;
            slice_or_depth(d, ds_att_img, cmn_ds_att);
            if is_depth_stencil_format(ds_att_img.cmn.pixel_format) {
                sa := @objc pass_desc.stencilAttachment();
                @objc_set(sa) (
                    texture = get_texture(ds_att_img),
                    loadAction = to_mtl action.stencil.load_action,
                    storeAction = to_mtl(action.depth.store_action, false),
                    clearStencil = action.stencil.clear_value,
                );
                slice_or_depth(sa, ds_att_img, cmn_ds_att);
            }
        }
    } else {
        swapchain := pass.swapchain&;
    
        // setup pass descriptor for swapchain rendering
        //
        // NOTE: at least in macOS Sonoma this no longer seems to be the case, the
        // current drawable is also valid in a minimized window
        // ===
        // an MTKView current_drawable will not be valid if window is minimized, don't do any rendering in this case
        if swapchain.metal.current_drawable.is_null() {
            mtl.sg.cur_pass.valid = false;
            return;
        }
        // pin the swapchain resources into memory so that they outlive their command buffer
        // (this is necessary because the command buffer doesn't retain references)
        pass_desc_ref := mtl.add_resource(pass_desc);
        mtl.release_resource(mtl.sg.frame_index, pass_desc_ref);

        mtl.cur_drawable = bit_cast_unchecked(rawptr, id("CAMetalDrawable"), swapchain.metal.current_drawable);
        ca := @objc pass_desc.colorAttachments();
        ca := @objc ca.objectAtIndexedSubscript(0);
        if (swapchain.sample_count > 1) {
            // multi-sampling: render into msaa texture, resolve into drawable texture
            msaa_tex := bit_cast_unchecked(rawptr, id("MTLTexture"), swapchain.metal.msaa_color_texture);
            @debug_assert(!msaa_tex.is_nil());
            @objc ca.setTexture(msaa_tex);
            @objc ca.setResolveTexture(@objc mtl.cur_drawable.texture());
            @objc ca.setStoreAction(2); // MTLStoreActionMultisampleResolve
        } else {
            // non-msaa: render into current_drawable
            @objc ca.setTexture(@objc mtl.cur_drawable.texture());
            @objc ca.setStoreAction(1); // MTLStoreActionStore
        }
        @objc ca.setLoadAction(to_mtl(action.colors&[0].load_action));
        @objc ca.setClearColor(action.colors&[0].clear_value.to_mtl());

        // optional depth-stencil texture
        if !swapchain.metal.depth_stencil_texture.is_null() {
            ds_tex := bit_cast_unchecked(rawptr, id("MTLTexture"), swapchain.metal.depth_stencil_texture);
            @debug_assert(!ds_tex.is_null());
            @objc_set(@objc pass_desc.depthAttachment()) (
                texture = ds_tex,
                storeAction = 0, // MTLStoreActionDontCare
                loadAction = to_mtl(action.depth.load_action),
                clearDepth = @as(f64) cast action.depth.clear_value,
            );
            if is_depth_stencil_format(swapchain.depth_format) {
                @objc_set(@objc pass_desc.stencilAttachment()) (
                    texture = ds_tex,
                    storeAction = 0, // MTLStoreActionDontCare
                    loadAction = to_mtl(action.stencil.load_action),
                    clearStencil = action.stencil.clear_value,
                );
            }
        }
    }

    // NOTE: at least in macOS Sonoma, the following is no longer the case, a valid
    // render command encoder is also returned in a minimized window
    // ===
    // create a render command encoder, this might return nil if window is minimized
    mtl.render_cmd_encoder = @objc mtl.cmd_buffer.renderCommandEncoderWithDescriptor(pass_desc);
    if mtl.render_cmd_encoder.is_nil() {
        mtl.sg.cur_pass.valid = false;
        return();
    }

    if Sg.Sg.SOKOL_DEBUG && !pass.label.is_null() {
        @objc mtl.render_cmd_encoder.setLabel(@objc cls("NSString").stringWithUTF8String(pass.label));
    }
}

fn is_null(self: CStr) bool = self.ptr.is_null();

// TODO: where to #import this from?
DISPATCH_TIME_FOREVER :: bit_not(0);
fn dispatch_semaphore_wait(dsema: id("OS_dispatch_semaphore"), timeout: i64) i64 #import("libc");
fn dispatch_semaphore_create(value: i64) id("OS_dispatch_semaphore") #import("libc");
fn dispatch_semaphore_signal(dsema: id("OS_dispatch_semaphore")) i64 #import("libc");

fn begin_pass(mtl: *Impl, pass: *SgPass) void = {
    @debug_assert(!mtl.cmd_queue.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(mtl.render_cmd_encoder.is_nil());
    @debug_assert(mtl.cur_drawable.is_nil());
    mtl.clear_state_cache();

    // if this is the first pass in the frame, create one command buffer and blit-cmd-encoder for the entire frame
    if mtl.cmd_buffer.is_nil() {
        // block until the oldest frame in flight has finished
        dispatch_semaphore_wait(mtl.sem, DISPATCH_TIME_FOREVER);
        ::if(ObjCId);
        mtl.cmd_buffer = if mtl.sg.desc.mtl_use_command_buffer_with_retained_references {
            @objc mtl.cmd_queue.commandBuffer()
        } else {
            @objc mtl.cmd_queue.commandBufferWithUnretainedReferences()
        };
        @objc mtl.cmd_buffer.enqueue();
        @objc mtl.cmd_buffer.addCompletedHandler(mtl.pass_completed_sem);
    }

    // if this is first pass in frame, get uniform buffer base pointer
    if mtl.cur_ub_base_ptr.is_null() {
        mtl.cur_ub_base_ptr = @objc @as(*u8) mtl.uniform_buffers&[mtl.cur_frame_rotate_index].contents();
    }

    if (pass.compute) {
        mtl.begin_compute_pass(pass);
    } else {
        mtl.begin_render_pass(pass);
    }

    // bind uniform buffers, those bindings remain valid for the entire pass
    if mtl.sg.cur_pass.valid {
        mtl.bind_uniform_buffers();
    }
}

pass_completed_handler :: fn(self: *Block_literal_1, _: id"MTLCommandBuffer") void = {
    // NOTE: this code is called on a different thread!
    mtl := Impl.ptr_from_raw(self.userdata);
    dispatch_semaphore_signal(mtl.sem);
};

fn end_pass(mtl: *Impl) void = {
    if !mtl.render_cmd_encoder.is_nil() {
        @objc mtl.render_cmd_encoder.endEncoding();
        // NOTE: MTLRenderCommandEncoder is autoreleased
        mtl.render_cmd_encoder = zeroed ObjCId;
    }
    if !mtl.compute_cmd_encoder.is_nil() {
        @objc mtl.compute_cmd_encoder.endEncoding();
        // NOTE: MTLComputeCommandEncoder is autoreleased
        mtl.compute_cmd_encoder = zeroed ObjCId;

        ASSERT_NOT_IOS();
        // synchronize any managed buffers written by the GPU
        ASSERT_NOT_IOS();
        if !mtl.use_shared_storage_mode {
            if (mtl.sg.compute.readwrite_sbufs.len > 0) {
                blit_cmd_encoder := @objc mtl.cmd_buffer.blitCommandEncoder();
                for mtl.sg.compute.readwrite_sbufs { it |
                    if mtl.sg.lookup(it) { sbuf |
                        it := mtl.get_id(sbuf.mtl.buf&[@as(i64) sbuf.cmn.active_slot.intcast()]);
                        @objc blit_cmd_encoder.synchronizeResource(it);
                    }
                }
                @objc blit_cmd_encoder.endEncoding();
            }
        }
    }
    // if this is a swapchain pass, present the drawable
    if !mtl.cur_drawable.is_nil() {
        @objc mtl.cmd_buffer.presentDrawable(mtl.cur_drawable);
        mtl.cur_drawable = zeroed ObjCId;
    }
}

fn is_nil(self: ObjCId) bool = 
    bit_cast_unchecked(ObjCId, i64, self) == 0;

fn commit(mtl: *Impl) void = {
    @debug_assert(mtl.render_cmd_encoder.is_nil());
    @debug_assert(mtl.compute_cmd_encoder.is_nil());
    @debug_assert(!mtl.cmd_buffer.is_nil());

    // commit the frame's command buffer
    @objc mtl.cmd_buffer.commit();

    // garbage-collect resources pending for release
    mtl.garbage_collect(mtl.sg.frame_index);

    // rotate uniform buffer slot
    mtl.cur_frame_rotate_index += 1;
    if mtl.cur_frame_rotate_index >= Sg.SG_NUM_INFLIGHT_FRAMES {
        mtl.cur_frame_rotate_index = 0;
    }
    mtl.cur_ub_offset = 0;
    mtl.cur_ub_base_ptr = u8.ptr_from_int(0);
    // NOTE: MTLCommandBuffer is autoreleased
    mtl.cmd_buffer = zeroed ObjCId;
}

fn apply_viewport(mtl: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(!mtl.render_cmd_encoder.is_nil() && mtl.sg.cur_pass.height > 0);
    MTLViewport :: @struct(originX: f64, originY: f64, width: f64, height: f64, znear: f64, zfar: f64);
    vp: MTLViewport = (
        originX = float intcast(x),
        originY = float intcast(@if(origin_top_left, y, (mtl.sg.cur_pass.height - (y + h)))),
        width   = float intcast(w),
        height  = float intcast(h),
        znear   = 0.0,
        zfar    = 1.0,
    );
    @objc mtl.render_cmd_encoder.setViewport(vp);
}

fn apply_scissor_rect(mtl: *Impl, x: i32, y: i32, w: i32, h: i32, origin_top_left: bool) void = {
    @debug_assert(!mtl.render_cmd_encoder.is_nil() && mtl.sg.cur_pass.width > 0 && mtl.sg.cur_pass.height > 0);
    // clip against framebuffer rect
    clip: Sg._sg_recti_t = clipi(x, y, w, h, mtl.sg.cur_pass.width, mtl.sg.cur_pass.height);
    MTLScissorRect :: @struct(x: i64, y: i64, width: i64, height: i64);
    r: MTLScissorRect = (
        x = intcast clip.x,
        y = intcast @if(origin_top_left, clip.y, mtl.sg.cur_pass.height - (clip.y + clip.h)),
        width = intcast clip.w,
        height = intcast clip.h,
    );
    @objc mtl.render_cmd_encoder.setScissorRect(r);
}

fn apply_pipeline(mtl: *Impl, pip: *Sg.Pipeline.T) void = {
    @debug_assert(pip.cmn.shader.id == pip.cmn.shader_t.slot.id);
    if (mtl.state_cache.cur_pipeline_id.id != pip.slot.id) {
        mtl.state_cache.cur_pipeline = pip;
        mtl.state_cache.cur_pipeline_id.id = pip.slot.id;
        @debug_assert_eq(pip.cmn.compute, mtl.sg.cur_pass.is_compute);
        if pip.cmn.compute {
            @debug_assert(!mtl.compute_cmd_encoder.is_nil());
            @debug_assert(pip.mtl.cps != _SG_MTL_INVALID_SLOT_INDEX);
            @objc mtl.compute_cmd_encoder.setComputePipelineState(mtl.get_id(pip.mtl.cps));
        } else {
            e := mtl.render_cmd_encoder;
            @debug_assert(!e.is_nil());
            c := pip.cmn.blend_color;
            mtl.sg.stat(.mtl_num_apply_render_pipeline, 1);
            @debug_assert(pip.mtl.rps != 0 && pip.mtl.dss != 0);
            
            @objc e.setBlendColorRed(c.r, green = c.g, blue = c.b, alpha = c.a);
            @objc e.setDepthBias(pip.cmn.depth.bias, slopeScale = pip.cmn.depth.bias_slope_scale, clamp = pip.cmn.depth.bias_clamp);
            @objc_set(e) (
                cullMode = pip.mtl.cull_mode,
                frontFacingWinding = pip.mtl.winding,
                stencilReferenceValue = pip.mtl.stencil_ref,
                renderPipelineState = mtl.get_id(pip.mtl.rps),
                depthStencilState = mtl.get_id(pip.mtl.dss),
            );
        }
    }
}

fn apply_bindings(mtl: *Impl, bnd: *Sg.ResolvedBindings) bool = {
    @debug_assert(bnd.pip.cmn.shader_t.slot.id == bnd.pip.cmn.shader.id);
    shd, sc := (bnd.pip.cmn.shader_t, mtl.state_cache&);

    // don't set vertex- and index-buffers in compute passes
    if (!mtl.sg.cur_pass.is_compute) {
        @debug_assert(!mtl.render_cmd_encoder.is_nil());
        // store index buffer binding, this will be needed later in sg_draw()
        sc.cur_indexbuffer = bnd.ib;
        sc.cur_indexbuffer_offset = bnd.ib_offset;
        ::ptr_utils(@type bnd.ib[]);
        ::if(u32);
        sc.cur_indexbuffer_id.id = if !bnd.ib.is_null() {
            @debug_assert(bnd.pip.cmn.index_type != .NONE);
            bnd.ib.slot.id
        } else {
            @debug_assert(bnd.pip.cmn.index_type == .NONE);
            Sg.SG_INVALID_ID
        };
        // apply vertex buffers
        range(0, Sg.SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            continue :: local_return;
            vb := bnd.vbs&[i];
            if(vb.is_null(), => continue());
            mtl_slot := _sg_mtl_vertexbuffer_bindslot(i);
            @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
            vb_offset := bnd.vb_offsets&[i];
            cur := sc.buffer&.index(.VERTEX).index(mtl_slot);
            sc_off := sc.cur_vs_buffer_offsets&.index(mtl_slot);
            if cur.id != vb.slot.id || sc_off[] != vb_offset {
                sc_off[] = vb_offset;
                if (cur.id != vb.slot.id) {
                    // vertex buffer has changed
                    cur.id = vb.slot.id;
                    @debug_assert(vb.mtl.buf&[intcast vb.cmn.active_slot] != 0);
                    id := mtl.get_id(vb.mtl.buf&[intcast vb.cmn.active_slot]);
                    @objc mtl.render_cmd_encoder.setVertexBuffer(id, offset = @as(i64) intcast vb_offset, atIndex = mtl_slot);
                } else {
                    // only vertex buffer offset has changed
                    @objc mtl.render_cmd_encoder.setVertexBufferOffset(@as(i64) intcast vb_offset, atIndex = mtl_slot);
                }
                mtl.sg.stat(.mtl_bindings_num_set_vertex_buffer, 1);
            }
        }
    }
    
    F :: @Fn(e: ObjCId, s: i32) Sg.Stat;
    apply :: fn(stage, entry, res, mtl_slot, $v: F, $f: F, $c: F) => {
        ids := entry.index(stage);
        enc := @if(stage == .COMPUTE, mtl.compute_cmd_encoder, mtl.render_cmd_encoder);
        @debug_assert(!enc.is_nil());
        if ids[zext mtl_slot].id != res.slot.id {
            ids[zext mtl_slot].id = res.slot.id;
            s := @match(stage) {
                fn NONE() => @panic("applied NONE stage");
                fn VERTEX()   => v(enc, mtl_slot);
                fn FRAGMENT() => f(enc, mtl_slot);
                fn COMPUTE()  => c(enc, mtl_slot);
            };
            mtl.sg.stat(s, 1);
        }
    }
    
    // apply image bindings
    range(0, Sg.SG_MAX_IMAGE_BINDSLOTS) { i |
        continue :: local_return;
        img := bnd.imgs&[i];
        ::ptr_utils(@type img[]);
        if(img.is_null(), => continue());
        @debug_assert(img.mtl.tex&[intcast img.cmn.active_slot] != 0);
        stage := shd.cmn.images&[i].stage;
        mtl_slot := shd.mtl.img_texture_n&[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
        
        id := mtl.get_id(img.mtl.tex&[intcast img.cmn.active_slot]);  // todo: rather not hoist
        apply(stage, sc.image&, img, mtl_slot) { e, s |
            @objc e.setVertexTexture(id, atIndex = s);
            .mtl_bindings_num_set_vertex_texture
        } fragment { e, s |
            @objc e.setFragmentTexture(id, atIndex = s);
            .mtl_bindings_num_set_fragment_texture
        } compute { e, s |
            @objc e.setTexture(id, atIndex = s);
            .mtl_bindings_num_set_compute_texture
        };
    }

    // apply sampler bindings
    range(0, Sg.SG_MAX_SAMPLER_BINDSLOTS) { i |
        continue :: local_return;
        smp := bnd.smps&[i];
        ::ptr_utils(@type smp[]);
        if(smp.is_null(), => continue());
        @debug_assert(smp.mtl.sampler_state != _SG_MTL_INVALID_SLOT_INDEX);
        stage := shd.cmn.samplers&[i].stage;
        mtl_slot := shd.mtl.smp_sampler_n&[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
        id := mtl.get_id(smp.mtl.sampler_state);  // todo: rather not hoist
        apply(stage, sc.sampler&, smp, mtl_slot) { e, s |
            @objc e.setVertexSamplerState(id, atIndex = s);
            .mtl_bindings_num_set_vertex_sampler_state
        } fragment { e, s |
            @objc e.setFragmentSamplerState(id, atIndex = s);
            .mtl_bindings_num_set_fragmnet_sampler_state
        } compute { e, s |
            @objc e.setSamplerState(id, atIndex = s);
            .mtl_bindings_num_set_compute_sampler_state
        };
    }

    // apply storage buffer bindings
    range(0, Sg.SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
        continue :: local_return;
        sbuf := bnd.sbufs&[i];
        if(sbuf.is_null(), => continue());
        @debug_assert(sbuf.mtl.buf&[intcast sbuf.cmn.active_slot] != 0);
        stage := shd.cmn.storage_buffers&[i].stage;
        mtl_slot := shd.mtl.sbuf_buffer_n&[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS);
        id := mtl.get_id(sbuf.mtl.buf&[intcast sbuf.cmn.active_slot]);
        apply(stage, sc.buffer&, sbuf, mtl_slot) { e, s |
            @objc e.setVertexBuffer(id, offset = 0, atIndex = s);
            .mtl_bindings_num_set_vertex_buffer
        } fragment { e, s |
            @objc e.setFragmentBuffer(id, offset = 0, atIndex = s);
            .mtl_bindings_num_set_fragment_buffer
        } compute { e, s |
            @objc e.setBuffer(id, offset = 0, atIndex = s);
            .mtl_bindings_num_set_compute_buffer
        };
    }
    return true;
}

fn apply_uniforms(mtl: *Impl, ub_slot: i32, data: []u8) void = {
    @debug_assert((ub_slot >= 0) && (ub_slot < Sg.SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert((mtl.cur_ub_offset + intcast data.len) <= mtl.ub_size);
    @debug_assert(mtl.cur_ub_offset.bit_and(_SG_MTL_UB_ALIGN - 1) == 0);
    pip := mtl.state_cache.cur_pipeline;
    @debug_assert(pip.slot.id == mtl.state_cache.cur_pipeline_id.id);
    shd := pip.cmn.shader_t;
    @debug_assert(shd.slot.id == pip.cmn.shader.id);
    @debug_assert(data.len == zext shd.cmn.uniform_blocks&[intcast ub_slot].size);

    stage := shd.cmn.uniform_blocks&[intcast ub_slot].stage;
    mtl_slot := shd.mtl.ub_buffer_n&[intcast ub_slot];

    // copy to global uniform buffer, record offset into cmd encoder, and advance offset
    dst := mtl.cur_ub_base_ptr.offset(intcast mtl.cur_ub_offset);
    dst.slice(data.len).copy_from(data);
    @match(stage) {
        fn VERTEX() => {
            @debug_assert(!mtl.render_cmd_encoder.is_null());
            @objc mtl.render_cmd_encoder.setVertexBufferOffset(mtl.cur_ub_offset, atIndex = mtl_slot);
            mtl.sg.stat(.mtl_uniforms_num_set_vertex_buffer_offset, 1);
        }
        fn FRAGMENT() => {
            @debug_assert(!mtl.render_cmd_encoder.is_null());
            @objc mtl.render_cmd_encoder.setFragmentBufferOffset(mtl.cur_ub_offset, atIndex = mtl_slot);
            mtl.sg.stat(.mtl_uniforms_num_set_fragment_buffer_offset, 1);
        }
        fn COMPUTE() => {
            @debug_assert(!mtl.compute_cmd_encoder.is_null());
            @objc mtl.compute_cmd_encoder.setBufferOffset(mtl.cur_ub_offset, atIndex = mtl_slot);
            mtl.sg.stat(.mtl_uniforms_num_set_compute_buffer_offset, 1);
        }
        @default => unreachable();
    }
    new := mtl.cur_ub_offset + intcast data.len;
    mtl.cur_ub_offset = Sg'roundup(new, _SG_MTL_UB_ALIGN);
}

fn draw(mtl: *Impl, base_element: i32, num_elements: i32, num_instances: i32) void = {
    sc := mtl.state_cache&;
    @debug_assert(!mtl.render_cmd_encoder.is_nil());
    @debug_assert(sc.cur_pipeline.slot.id == sc.cur_pipeline_id.id);
    prim := sc.cur_pipeline.mtl.prim_type;
    ::enum(SgIndexType);
    if sc.cur_pipeline.cmn.index_type != .NONE {
        // indexed rendering
        @debug_assert(sc.cur_indexbuffer.slot.id == sc.cur_indexbuffer_id.id);
        ib := sc.cur_indexbuffer;
        ib := ib.mtl.buf&[intcast ib.cmn.active_slot];
        @debug_assert(ib != 0);
        index_buffer_offset: i64 = intcast(sc.cur_indexbuffer_offset + base_element * sc.cur_pipeline.mtl.index_size);
        @objc mtl.render_cmd_encoder.drawIndexedPrimitives(prim, 
            indexCount = @as(i64) intcast num_elements,
            indexType = sc.cur_pipeline.mtl.index_type,
            indexBuffer = mtl.get_id(ib),
            indexBufferOffset = index_buffer_offset,
            instanceCount = @as(i64) intcast num_instances,
        );
    } else {
        // non-indexed rendering
        @objc mtl.render_cmd_encoder.drawPrimitives(prim,
            vertexStart   = @as(i64) intcast base_element,
            vertexCount   = @as(i64) intcast num_elements,
            instanceCount = @as(i64) intcast num_instances,
        );
    }
}

fn dispatch(mtl: *Impl, num_groups_x: i32, num_groups_y: i32, num_groups_z: i32) void = {
    @debug_assert(nil != mtl.compute_cmd_encoder);
    @debug_assert(mtl.state_cache.cur_pipeline && (mtl.state_cache.cur_pipeline.slot.id == mtl.state_cache.cur_pipeline_id.id));
    cur_pip := mtl.state_cache.cur_pipeline;
    thread_groups: MTLSize = (
        width  = @as(i64) num_groups_x,
        height = @as(i64) num_groups_y,
        depth  = @as(i64) num_groups_z,
    );
    threads_per_threadgroup := cur_pip.mtl.threads_per_threadgroup;
    [mtl.compute_cmd_encoder dispatchThreadgroups:thread_groups threadsPerThreadgroup:threads_per_threadgroup];
}

NSRange :: @struct(location: i64, length: i64);

fn inc_active_slot(it: ~T) void #where = {
    it.cmn.active_slot += 1;
    if it.cmn.active_slot >= it.cmn.num_slots {
        it.cmn.active_slot = 0;
    }
}

fn update_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8) void #once = 
    append_buffer(mtl, buf, data, true, 0);

fn append_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8, new_frame: bool) void #once = 
    append_buffer(mtl, buf, data, new_frame, buf.cmn.append_pos);

fn append_buffer(mtl: *Impl, buf: *Sg.Buffer.T, data: []u8, inc: bool, append_pos: i64) void = {
    @if(inc) inc_active_slot(buf);
    mtl_buf := mtl.get_id(buf.mtl.buf&[intcast buf.cmn.active_slot]);
    dest := @objc @as(*u8) mtl_buf.contents();
    dest.offset(append_pos).slice(data.len).copy_from(data);
    ASSERT_NOT_IOS();
    if !mtl.use_shared_storage_mode {
        r: NSRange = (location = append_pos, length = data.len);
        @objc mtl_buf.didModifyRange(r);
    }
}

fn update_image(mtl: *Impl, img: *Sg.Image.T, data: *SgImageData) void #once = {
    inc_active_slot(img);
    mtl_tex := mtl.get_id(img.mtl.tex[img.cmn.active_slot]);
    mtl.copy_image_data(img, mtl_tex, data);
}

fn push_debug_group(mtl: *Impl, name: CStr) void = {
    name := @objc cls("NSString").stringWithUTF8String(name);
    @objc mtl.encoder().pushDebugGroup(name);
}

fn pop_debug_group(mtl: *Impl) void =
    @objc @as(void) mtl.encoder().popDebugGroup();

fn encoder(mtl: *Impl) ObjCId = {
    enc := mtl.render_cmd_encoder;
    @if(enc.is_nil(), mtl.compute_cmd_encoder, enc)
}

//
// These accessors are useful when you want to mix and match 
// our rendering api with your own direct use of the Metal api. 
// If there's some specific Metal feature you need that we don't 
// provide a wrapper around, you can use these to just do it yourself.  
//

fn device(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.device)
}

fn render_command_encoder(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.render_cmd_encoder)
}

fn compute_command_encoder(mtl: *Impl) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, mtl.compute_cmd_encoder)
}
