// simple 3D API wrapper
// Adapted from sokol_gfx.h - https://github.com/floooh/sokol
// zlib/libpng license. Copyright (c) 2018 Andre Weissflog.
// 
// Changes from Sokol:
// - no dependency on an objective-c compiler
// - removed ios, opengl, and macos <13 support (temporarily?)
// 
// A sad amount of this task is just mapping our enum values to the metal 
// ones. I've manually transcribed the numbers so you can cross compile 
// without finding a copy of the Metal.framework headers. If I ever do more
// serious bindings to metal, this should be updated to use that instead. 
// 

ASSERT_NOT_IOS :: fn() => ();
ASSERT_NOT_OLD :: fn() => ();

_SG_MTL_UB_ALIGN :: { ASSERT_NOT_IOS(); 256 };

typedef struct {
    uint32_t frame_index;   // frame index at which it is safe to release this resource
    int slot_index;
} _sg_mtl_release_item_t;

typedef struct {
    NSMutableArray* pool;
    int num_slots;
    int free_queue_top;
    int* free_queue;
    int release_queue_front;
    int release_queue_back;
    _sg_mtl_release_item_t* release_queue;
} _sg_mtl_idpool_t;

typedef struct _sg_buffer_s {
    _sg_slot_t slot;
    _sg_buffer_common_t cmn;
    struct {
        int buf[SG_NUM_INFLIGHT_FRAMES];  // index into _sg_mtl_pool
    } mtl;
} _sg_mtl_buffer_t;
typedef _sg_mtl_buffer_t _sg_buffer_t;

typedef struct _sg_image_s {
    _sg_slot_t slot;
    _sg_image_common_t cmn;
    struct {
        int tex[SG_NUM_INFLIGHT_FRAMES];
    } mtl;
} _sg_mtl_image_t;
typedef _sg_mtl_image_t _sg_image_t;

typedef struct _sg_sampler_s {
    _sg_slot_t slot;
    _sg_sampler_common_t cmn;
    struct {
        int sampler_state;
    } mtl;
} _sg_mtl_sampler_t;
typedef _sg_mtl_sampler_t _sg_sampler_t;

typedef struct {
    int mtl_lib;
    int mtl_func;
} _sg_mtl_shader_func_t;

typedef struct _sg_shader_s {
    _sg_slot_t slot;
    _sg_shader_common_t cmn;
    struct {
        _sg_mtl_shader_func_t vertex_func;
        _sg_mtl_shader_func_t fragment_func;
        _sg_mtl_shader_func_t compute_func;
        MTLSize threads_per_threadgroup;
        uint8_t ub_buffer_n[SG_MAX_UNIFORMBLOCK_BINDSLOTS];
        uint8_t img_texture_n[SG_MAX_IMAGE_BINDSLOTS];
        uint8_t smp_sampler_n[SG_MAX_SAMPLER_BINDSLOTS];
        uint8_t sbuf_buffer_n[SG_MAX_STORAGEBUFFER_BINDSLOTS];
    } mtl;
} _sg_mtl_shader_t;
typedef _sg_mtl_shader_t _sg_shader_t;

typedef struct _sg_pipeline_s {
    _sg_slot_t slot;
    _sg_pipeline_common_t cmn;
    _sg_shader_t* shader;
    struct {
        MTLPrimitiveType prim_type;
        int index_size;
        MTLIndexType index_type;
        MTLCullMode cull_mode;
        MTLWinding winding;
        uint32_t stencil_ref;
        MTLSize threads_per_threadgroup;
        int cps;    // MTLComputePipelineState
        int rps;    // MTLRenderPipelineState
        int dss;    // MTLDepthStencilState
    } mtl;
} _sg_mtl_pipeline_t;
typedef _sg_mtl_pipeline_t _sg_pipeline_t;

typedef struct {
    _sg_image_t* image;
} _sg_mtl_attachment_t;

typedef struct _sg_attachments_s {
    _sg_slot_t slot;
    _sg_attachments_common_t cmn;
    struct {
        _sg_mtl_attachment_t colors[SG_MAX_COLOR_ATTACHMENTS];
        _sg_mtl_attachment_t resolves[SG_MAX_COLOR_ATTACHMENTS];
        _sg_mtl_attachment_t depth_stencil;
    } mtl;
} _sg_mtl_attachments_t;
typedef _sg_mtl_attachments_t _sg_attachments_t;

// resource binding state cache
#define _SG_MTL_MAX_STAGE_UB_BINDINGS (SG_MAX_UNIFORMBLOCK_BINDSLOTS)
#define _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS (_SG_MTL_MAX_STAGE_UB_BINDINGS + SG_MAX_STORAGEBUFFER_BINDSLOTS)
#define _SG_MTL_MAX_STAGE_BUFFER_BINDINGS (_SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS + SG_MAX_VERTEXBUFFER_BINDSLOTS)
#define _SG_MTL_MAX_STAGE_IMAGE_BINDINGS (SG_MAX_IMAGE_BINDSLOTS)
#define _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS (SG_MAX_SAMPLER_BINDSLOTS)
typedef struct {
    const _sg_pipeline_t* cur_pipeline;
    sg_pipeline cur_pipeline_id;
    const _sg_buffer_t* cur_indexbuffer;
    sg_buffer cur_indexbuffer_id;
    int cur_indexbuffer_offset;
    int cur_vs_buffer_offsets[_SG_MTL_MAX_STAGE_BUFFER_BINDINGS];
    sg_buffer cur_vs_buffer_ids[_SG_MTL_MAX_STAGE_BUFFER_BINDINGS];
    sg_buffer cur_fs_buffer_ids[_SG_MTL_MAX_STAGE_BUFFER_BINDINGS];
    sg_buffer cur_cs_buffer_ids[_SG_MTL_MAX_STAGE_BUFFER_BINDINGS];
    sg_image cur_vs_image_ids[_SG_MTL_MAX_STAGE_IMAGE_BINDINGS];
    sg_image cur_fs_image_ids[_SG_MTL_MAX_STAGE_IMAGE_BINDINGS];
    sg_image cur_cs_image_ids[_SG_MTL_MAX_STAGE_IMAGE_BINDINGS];
    sg_sampler cur_vs_sampler_ids[_SG_MTL_MAX_STAGE_SAMPLER_BINDINGS];
    sg_sampler cur_fs_sampler_ids[_SG_MTL_MAX_STAGE_SAMPLER_BINDINGS];
    sg_sampler cur_cs_sampler_ids[_SG_MTL_MAX_STAGE_SAMPLER_BINDINGS];
} _sg_mtl_state_cache_t;

typedef struct {
    bool valid;
    bool use_shared_storage_mode;
    uint32_t cur_frame_rotate_index;
    int ub_size;
    int cur_ub_offset;
    uint8_t* cur_ub_base_ptr;
    _sg_mtl_state_cache_t state_cache;
    _sg_mtl_idpool_t idpool;
    dispatch_semaphore_t sem;
    id<MTLDevice> device;
    id<MTLCommandQueue> cmd_queue;
    id<MTLCommandBuffer> cmd_buffer;
    id<MTLRenderCommandEncoder> render_cmd_encoder;
    id<MTLComputeCommandEncoder> compute_cmd_encoder;
    id<CAMetalDrawable> cur_drawable;
    id<MTLBuffer> uniform_buffers[SG_NUM_INFLIGHT_FRAMES];
} _sg_mtl_backend_t;

fn mtl(it: SgLoadAction) MTLLoadAction = @map_enum(it) 
    (CLEAR = 2, LOAD = 1, DONTCARE = 0);

fn mtl(it: SgStoreAction, resolve: bool) MTLStoreAction = {
    @debug_assert(@is(it, .STORE, .DONTCARE));
    @if(it == .STORE, @if(resolve, 3, 1), @if(resolve, 2, 0))
}

fn mtl(sg: *Sg.Self) MTLResourceOptions = {
    ASSERT_NOT_IOS();
    @if(sg.mtl.use_shared_storage_mode, /*Shared*/ 0, /*Managed*/ 1.shift_left(4))
}

fn mtl(sg_usage it) MTLResourceOptions = {
    @debug_assert(@is(it, .IMMUTABLE, .DYNAMIC, .STREAM));
    x := _sg_mtl_resource_options_storage_mode_managed_or_shared();
    if it != .IMMUTABLE {
        X |= MTLResourceCPUCacheModeWriteCombined;
    };
    x
}

fn mtl(sg_vertex_step it) MTLVertexStepFunction = @map_enum(it)
    (PER_VERTEX = 1, PER_INSTANCE = 2);

fn mtl(it: sg_vertex_format) MTLVertexFormat = @map_enum(it) (
    Float = 28, Float2 = 29, Float3 = 30, Float4 = 31,
    Int = 32, Int2 = 33, Int3 = 34, Int4 = 35,
    UInt = 36, UInt2 = 37, UInt3 = 38, UInt4 = 39,
    Byte4 = 6, Byte4N = 12, UByte4 = 3, UByte4N = 9, 
    Short2 = 16, Short2N = 22, UShort2 = 13, UShort2N = 19,
    Short4 = 18, Short4N = 24, UShort4 = 15, UShort4N = 21,
    UINT10_N2 = 41, Half2 = 25, Half4 = 27,
);

fn mtl(sg_primitive_type t) MTLPrimitiveType = @map_enum(t) 
    (POINTS = 0, LINES = 1, LINE_STRIP = 2, TRIANGLES = 3, TRIANGLE_STRIP = 3);

fn mtl(it: sg_pixel_format) MTLPixelFormat = {
    ASSERT_NOT_IOS();
    @enum_map(it) (
        R8 = 10, R8SN = 12, R8UI = 13, R8SI = 14,
        R16 = 20, R16SN = 22, R16UI = 23, R16SI = 24, R16F = 25,
        RG8 = 30, RG8SN = 32, RG8UI = 33, RG8SI = 34,
        R32UI = 53, R32SI = 54, R32F = 55, 
        RG16 = 60, RG16SN = 62, RG16UI  = 63, RG16SI = 64, RG16F = 65, 
        RGBA8 = 70, SRGB8A8 = 71, RGBA8SN = 72, RGBA8UI = 73, RGBA8SI = 74, 
        BGRA8 = 80, RGB10A2 = 90, RG11B10F = 92, RGB9E5 = 93, 
        RG32UI = 103, RG32SI = 104, RG32F = 105, 
        RGBA16 = 110, RGBA16SN = 112, RGBA16UI = 113, RGBA16SI = 114, RGBA16F = 115,
        RGBA32UI = 123, RGBA32SI = 124, RGBA32F = 125, 
        DEPTH = 252, DEPTH_STENCIL = 260,
        BC1_RGBA = 130, BC2_RGBA = 132, BC3_RGBA = 134, BC3_SRGBA = 135,
        BC4_R = 140, BC4_RSN = 141, BC5_RG = 142, BC5_RGSN = 143,
        BC6H_RGBF = 150, BC6H_RGBUF = 151, BC7_RGBA = 152, BC7_SRGBA = 153, 
    )
}

fn mtl(sg_color_mask m) MTLColorWriteMask = {
    MTLColorWriteMask mtl_mask = MTLColorWriteMaskNone;
    if (m & SG_COLORMASK_R) {
        mtl_mask |= MTLColorWriteMaskRed;
    }
    if (m & SG_COLORMASK_G) {
        mtl_mask |= MTLColorWriteMaskGreen;
    }
    if (m & SG_COLORMASK_B) {
        mtl_mask |= MTLColorWriteMaskBlue;
    }
    if (m & SG_COLORMASK_A) {
        mtl_mask |= MTLColorWriteMaskAlpha;
    }
    return mtl_mask;
}

fn mtl(sg_blend_op it) MTLBlendOperation = @map_enum(it) 
    (ADD = 0, SUBTRACT = 1, REVERSE_SUBTRACT = 2, MIN = 3, MAX = 4);

fn mtl(sg_blend_factor it) MTLBlendFactor = @map_enum(it) (
    ZERO = 0, ONE = 1, 
    SRC_COLOR = 2, ONE_MINUS_SRC_COLOR = 3,
    ALPHA     = 4, ONE_MINUS_SRC_ALPHA = 5, 
    DST_COLOR = 6, ONE_MINUS_DST_COLOR = 7, 
    DST_ALPHA = 8, ONE_MINUS_DST_ALPHA = 9,
    SRC_ALPHA_SATURATED = 10, 
    BLEND_COLOR = 11, ONE_MINUS_BLEND_COLOR = 12,
    BLEND_ALPHA = 13, ONE_MINUS_BLEND_ALPHA = 14,
);

fn mtl(sg_compare_func it) MTLCompareFunction = @map_enum(it) 
    (NEVER = 0, LESS = 1, EQUAL = 2, LESS_EQUAL = 3, GREATER = 4, NOT_EQUAL = 5, GREATER_EQUAL = 6, ALWAYS = 7);

fn mtl(sg_stencil_op it) MTLStencilOperation = @map_enum(it)
    (KEEP = 0, ZERO = 1, REPLACE = 2, INCR_CLAMP = 3, DECR_CLAMP = 4, INVERT = 5, INCR_WRAP = 6, DECR_WRAP = 7);

fn mtl(sg_cull_mode it) MTLCullMode = @map_enum(it) 
    (NONE = MTLCullModeNone, FRONT = MTLCullModeFront, BACK = MTLCullModeBack);

fn mtl(sg_face_winding it) MTLWinding = @map_enum(it) 
    (CW = MTLWindingClockwise, CCW = MTLWindingCounterClockwise);

fn mtl(sg_index_type it) MTLIndexType = @map_enum(it) 
    (UINT16 = MTLIndexTypeUInt16, UINT32 = MTLIndexTypeUInt32);

fn int mtl(sg_index_type it) i32 = @map_enum(it) 
    (None = 0, UINT16 = 2, UINT32 = 4);

fn mtl(sg_image_type t) MTLTextureType = @map_enum(it)
    (_2D = 2, CUBE = 5, _3D = 7, ARRAY = 3);

fn mtl(sg_wrap w) MTLSamplerAddressMode = {
    ASSERT_NOT_OLD();
    b := sg.features.image_clamp_to_border;
    @map_enum(w) (REPEAT = 2, CLAMP_TO_EDGE = 0, CLAMP_TO_BORDER = @if(b, 5, 0), MIRRORED_REPEAT = 3)
}

fn mtl(sg_border_color it) MTLSamplerBorderColor = {
    ASSERT_NOT_OLD();
    @map_enum(it) (TRANSPARENT_BLACK = 0, OPAQUE_BLACK = 1, OPAQUE_WHITE = 2)
}

fn mtl_minmag(sg_filter it) MTLSamplerMinMagFilter = @enum_map(it)
    (NEAREST = 0, LINEAR = 1);

fn mtl_mip(sg_filter f) MTLSamplerMipFilter = @enum_map(it) 
    (NEAREST = 1, LINEAR = 2);

fn _sg_mtl_vertexbuffer_bindslot(sokol_bindslot: i64) i64 =
    sokol_bindslot + _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS;

//-- a pool for all Metal resource objects, with deferred release queue ---------
fn _sg_mtl_init_pool(const sg_desc* desc) {
    p := sg.mtl.idpool&;
    p.num_slots = 2 *
        (
            2 * desc.buffer_pool_size +
            4 * desc.image_pool_size +
            1 * desc.sampler_pool_size +
            4 * desc.shader_pool_size +
            2 * desc.pipeline_pool_size +
            desc.attachments_pool_size +
            128
        );
    p.pool = [NSMutableArray arrayWithCapacity:(NSUInteger)p.num_slots];
    _SG_OBJC_RETAIN(p.pool);
    NSNull* null = [NSNull null];
    range(0, p.num_slots) { _ |
        @objc p.pool.addObject(null);
    }
    @debug_assert([p.pool count] == (NSUInteger)p.num_slots);
    // a queue of currently free slot indices
    p.free_queue_top = 0;
    p.free_queue = (int*)_sg_malloc_clear((size_t)p.num_slots * sizeof(int));
    // pool slot 0 is reserved!
    for (int i = p.num_slots-1; i >= 1; i--) {
        p.free_queue[p.free_queue_top++] = i;
    }
    // a circular queue which holds release items (frame index when a resource is to be released, and the resource's pool index
    p.release_queue_front = 0;
    p.release_queue_back = 0;
    p.release_queue = (_sg_mtl_release_item_t*)_sg_malloc_clear((size_t)p.num_slots * sizeof(_sg_mtl_release_item_t));
    for (int i = 0; i < p.num_slots; i++) {
        p.release_queue[i].frame_index = 0;
        p.release_queue[i].slot_index = _SG_MTL_INVALID_SLOT_INDEX;
    }
}

fn _sg_mtl_destroy_pool(sg: *Sg.Self) void{
    p := sg.mtl.idpool&;
    _sg_free(p.release_queue);  p.release_queue = 0;
    _sg_free(p.free_queue);     p.free_queue = 0;
    release(p.pool);
}

// get a new free resource pool slot
fn _sg_mtl_alloc_pool_slot(sg: *Sg.Self) i32 = {
    p := sg.mtl.idpool&;
    @debug_assert(p.free_queue_top > 0);
    const int slot_index = p.free_queue[--p.free_queue_top];
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
    slot_index
}

// put a free resource pool slot back into the free-queue
fn _sg_mtl_free_pool_slot(slot_index: i32) void = {
    p := sg.mtl.idpool&;
    @debug_assert(p.free_queue_top < p.num_slots);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
    p.free_queue[p.free_queue_top++] = slot_index;
}

// add an MTLResource to the pool, return pool index or 0 if input was 'nil'
fn _sg_mtl_add_resource(id res) i32 = {
    if(res.is_nil(), => return(_SG_MTL_INVALID_SLOT_INDEX));
    _sg_stats_add(metal.idpool.num_added, 1);
    slot_index := _sg_mtl_alloc_pool_slot();
    // NOTE: the NSMutableArray will take ownership of its items
    @debug_assert([NSNull null] == sg.mtl.idpool.pool[(NSUInteger)slot_index]);
    sg.mtl.idpool.pool[(NSUInteger)slot_index] = res;
    slot_index
}

/*  mark an MTLResource for release, this will put the resource into the
    deferred-release queue, and the resource will then be released N frames later. */
fn _sg_mtl_release_resource(uint32_t frame_index, int slot_index) {
    if slot_index == _SG_MTL_INVALID_SLOT_INDEX {
        /* this means that a nil value was provided to _sg_mtl_add_resource */
        return();
    }
    p := sg.mtl.idpool&;
    _sg_stats_add(metal.idpool.num_released, 1);
    @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
    @debug_assert([NSNull null] != p.pool[(NSUInteger)slot_index]);
    int release_index = p.release_queue_front++;
    if (p.release_queue_front >= p.num_slots) {
        // wrap-around
        p.release_queue_front = 0;
    }
    // release queue full?
    @debug_assert(p.release_queue_front != p.release_queue_back);
    @debug_assert(0 == p.release_queue[release_index].frame_index);
    const uint32_t safe_to_release_frame_index = frame_index + SG_NUM_INFLIGHT_FRAMES + 1;
    p.release_queue[release_index].frame_index = safe_to_release_frame_index;
    p.release_queue[release_index].slot_index = slot_index;
}

// run garbage-collection pass on all resources in the release-queue
fn _sg_mtl_garbage_collect(uint32_t frame_index) {
    p := sg.mtl.idpool&;
    while => p.release_queue_back != p.release_queue_front {
        if (frame_index < p.release_queue[p.release_queue_back].frame_index) {
            // don't need to check further, release-items past this are too young
            return();
        }
        _sg_stats_add(metal.idpool.num_garbage_collected, 1);
        // safe to release this resource
        const int slot_index = p.release_queue[p.release_queue_back].slot_index;
        @debug_assert((slot_index > 0) && (slot_index < p.num_slots));
        // note: the NSMutableArray takes ownership of its items, assigning an NSNull object will
        // release the object, no matter if using ARC or not
        @debug_assert(p.pool[(NSUInteger)slot_index] != [NSNull null]);
        p.pool[(NSUInteger)slot_index] = [NSNull null];
        // put the now free pool index back on the free queue
        _sg_mtl_free_pool_slot(slot_index);
        // reset the release queue slot and advance the back index
        p.release_queue[p.release_queue_back].frame_index = 0;
        p.release_queue[p.release_queue_back].slot_index = _SG_MTL_INVALID_SLOT_INDEX;
        p.release_queue_back++;
        if (p.release_queue_back >= p.num_slots) {
            // wrap-around
            p.release_queue_back = 0;
        }
    }
}

fn _sg_mtl_id(sg: *Sg.Self, slot_index: i32) ObjCId = 
    @objc sg.mtl.idpool.pool.objectAtIndex(@as(i64) slot_index.intcast());

fn _sg_mtl_clear_state_cache(void) {
    _sg_clear(&sg.mtl.state_cache, sizeof(sg.mtl.state_cache));
}

// https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf
fn _sg_mtl_init_caps(void) {
    ASSERT_NOT_IOS();
    sg.backend = SG_BACKEND_METAL_MACOS;
    sg.features.origin_top_left = true;
    sg.features.mrt_independent_blend_state = true;
    sg.features.mrt_independent_write_mask = true;
    sg.features.compute = true;
    sg.features.msaa_image_bindings = true;

    sg.features.image_clamp_to_border = false;
    ASSERT_NOT_OLD();
    sg.features.image_clamp_to_border = [sg.mtl.device supportsFamily:MTLGPUFamilyApple7]
                                            || [sg.mtl.device supportsFamily:MTLGPUFamilyMac2];
    if (!sg.features.image_clamp_to_border) {
        sg.features.image_clamp_to_border = [sg.mtl.device supportsFamily:MTLGPUFamilyMetal3];
    }

    ASSERT_NOT_IOS();
    sg.limits.max_image_size_2d = 16 * 1024;
    sg.limits.max_image_size_cube = 16 * 1024;
    sg.limits.max_image_size_3d = 2 * 1024;
    sg.limits.max_image_size_array = 16 * 1024;
    sg.limits.max_image_array_layers = 2 * 1024;
    sg.limits.max_vertex_attrs = SG_MAX_VERTEX_ATTRIBUTES;

    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_R8]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_R8SN]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_R8UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_R8SI]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_R16]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_R16SN]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_R16UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_R16SI]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_R16F]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG8]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG8SN]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RG8UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RG8SI]);
    _sg_pixelformat_sr(&sg.formats[SG_PIXELFORMAT_R32UI]);
    _sg_pixelformat_sr(&sg.formats[SG_PIXELFORMAT_R32SI]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_R32F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG16]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG16SN]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RG16UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RG16SI]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG16F]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGBA8]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_SRGB8A8]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGBA8SN]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RGBA8UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RGBA8SI]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_BGRA8]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGB10A2]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG11B10F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_RGB9E5]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RG32UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RG32SI]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RG32F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGBA16]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGBA16SN]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RGBA16UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RGBA16SI]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGBA16F]);
    ASSERT_NOT_IOS();
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RGBA32UI]);
    _sg_pixelformat_srm(&sg.formats[SG_PIXELFORMAT_RGBA32SI]);
    _sg_pixelformat_all(&sg.formats[SG_PIXELFORMAT_RGBA32F]);
    _sg_pixelformat_srmd(&sg.formats[SG_PIXELFORMAT_DEPTH]);
    _sg_pixelformat_srmd(&sg.formats[SG_PIXELFORMAT_DEPTH_STENCIL]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC1_RGBA]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC2_RGBA]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC3_RGBA]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC3_SRGBA]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC4_R]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC4_RSN]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC5_RG]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC5_RGSN]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC6H_RGBF]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC6H_RGBUF]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC7_RGBA]);
    _sg_pixelformat_sf(&sg.formats[SG_PIXELFORMAT_BC7_SRGBA]);
    ASSERT_NOT_IOS();
}

//-- main Metal backend state and functions ------------------------------------
fn _sg_mtl_setup_backend(const sg_desc* desc) {
    // assume already zero-initialized
    @debug_assert(desc);
    @debug_assert(desc.environment.metal.device);
    @debug_assert(desc.uniform_buffer_size > 0);
    _sg_mtl_init_pool(desc);
    _sg_mtl_clear_state_cache();
    sg.mtl.valid = true;
    sg.mtl.ub_size = desc.uniform_buffer_size;
    sg.mtl.sem = dispatch_semaphore_create(SG_NUM_INFLIGHT_FRAMES);
    sg.mtl.device = (__bridge id<MTLDevice>) desc.environment.metal.device;
    sg.mtl.cmd_queue = [sg.mtl.device newCommandQueue];

    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        sg.mtl.uniform_buffers[i] = [sg.mtl.device
            newBufferWithLength:(NSUInteger)sg.mtl.ub_size
            options:MTLResourceCPUCacheModeWriteCombined|MTLResourceStorageModeShared
        ];
        if SOKOL_DEBUG {
            sg.mtl.uniform_buffers[i].label = [NSString stringWithFormat:@"sg-uniform-buffer.%d", i];
        }
    }

    if desc.mtl_force_managed_storage_mode {
        sg.mtl.use_shared_storage_mode = false;
    } else {
        ASSERT_NOT_OLD();
        // on Intel Macs, always use managed resources even though the
        // device says it supports unified memory (because of texture restrictions)
        const bool is_apple_gpu = [sg.mtl.device supportsFamily:MTLGPUFamilyApple1];
        if (!is_apple_gpu) {
            sg.mtl.use_shared_storage_mode = false;
        } else {
            sg.mtl.use_shared_storage_mode = true;
        }
    };
    _sg_mtl_init_caps();
}

fn _sg_mtl_discard_backend(void) {
    @debug_assert(sg.mtl.valid);
    // wait for the last frame to finish
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_wait(sg.mtl.sem, DISPATCH_TIME_FOREVER);
    }
    // semaphore must be "relinquished" before destruction
    range(0, SG_NUM_INFLIGHT_FRAMES) { _ |
        dispatch_semaphore_signal(sg.mtl.sem);
    }
    _sg_mtl_garbage_collect(sg.frame_index + SG_NUM_INFLIGHT_FRAMES + 2);
    _sg_mtl_destroy_pool();
    sg.mtl.valid = false;

    release(sg.mtl.sem);
    release(sg.mtl.device);
    release(sg.mtl.cmd_queue);
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        release(sg.mtl.uniform_buffers[i]);
    }
    // NOTE: MTLCommandBuffer, MTLRenderCommandEncoder and MTLComputeCommandEncoder are auto-released
    sg.mtl.cmd_buffer = zeroed ObjCId;
    sg.mtl.render_cmd_encoder = zeroed ObjCId;
    sg.mtl.compute_cmd_encoder = zeroed ObjCId;
}

fn _sg_mtl_reset_state_cache(void) {
    _sg_mtl_clear_state_cache();
}

fn _sg_mtl_create_buffer(_sg_buffer_t* buf, const sg_buffer_desc* desc) sg_resource_state = {
    @debug_assert(buf && desc);
    @debug_assert(buf.cmn.size > 0);
    injected := (0 != desc.mtl_buffers[0]);
    MTLResourceOptions mtl_options = _sg_mtl_buffer_resource_options(buf.cmn.usage);
    range(0, buf.cmn.num_slots) { slot | 
        id<MTLBuffer> mtl_buf;
        if (injected) {
            @debug_assert(desc.mtl_buffers[slot]);
            mtl_buf = (__bridge id<MTLBuffer>) desc.mtl_buffers[slot];
        } else {
            if (desc.data.ptr) {
                @debug_assert(desc.data.size > 0);
                mtl_buf = [sg.mtl.device newBufferWithBytes:desc.data.ptr length:(NSUInteger)buf.cmn.size options:mtl_options];
            } else {
                // this is guaranteed to zero-initialize the buffer
                mtl_buf = [sg.mtl.device newBufferWithLength:(NSUInteger)buf.cmn.size options:mtl_options];
            }
            if (nil == mtl_buf) {
                _SG_ERROR(METAL_CREATE_BUFFER_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
        }
        if SOKOL_DEBUG && desc.label {
            mtl_buf.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        buf.mtl.buf[slot] = _sg_mtl_add_resource(mtl_buf);
        release(mtl_buf);
    }
    return SG_RESOURCESTATE_VALID;
}

fn _sg_mtl_discard_buffer(_sg_buffer_t* buf) {
    @debug_assert(buf);
    range(0, buf.cmn.num_slots) { slot | 
        // it's valid to call release resource with '0'
        _sg_mtl_release_resource(sg.frame_index, buf.mtl.buf[slot]);
    }
}

fn _sg_mtl_copy_image_data(const _sg_image_t* img, __unsafe_unretained id<MTLTexture> mtl_tex, const sg_image_data* data) {
    num_faces := if(img.cmn.type == SG_IMAGETYPE_CUBE, => 6, => 1);
    num_slices := if(img.cmn.type == SG_IMAGETYPE_ARRAY, => img.cmn.num_slices, => 1);
    for (int face_index = 0; face_index < num_faces; face_index++) {
    range(0, num_faces) { face_index |
        range(0, img.cmn.num_mipmaps) { mip_index |
            @debug_assert(data.subimage[face_index][mip_index].ptr);
            @debug_assert(data.subimage[face_index][mip_index].size > 0);
            const uint8_t* data_ptr = (const uint8_t*)data.subimage[face_index][mip_index].ptr;
            const int mip_width = _sg_miplevel_dim(img.cmn.width, mip_index);
            const int mip_height = _sg_miplevel_dim(img.cmn.height, mip_index);
            int bytes_per_row = _sg_row_pitch(img.cmn.pixel_format, mip_width, 1);
            int bytes_per_slice = _sg_surface_pitch(img.cmn.pixel_format, mip_width, mip_height, 1);
            /* bytesPerImage special case: https://developer.apple.com/documentation/metal/mtltexture/1515679-replaceregion

                "Supply a nonzero value only when you copy data to a MTLTextureType3D type texture"
            */
            MTLOrigin :: @struct(x: u64, y: u64, z: u64);
            MTLRegion :: @struct(origin := zeroed MTLOrigin, size: MTLSize);

            mip_depth, bytes_per_image := if (img.cmn.type == SG_IMAGETYPE_3D) {
                // FIXME: apparently the minimal bytes_per_image size for 3D texture is 4 KByte... somehow need to handle this
                (_sg_miplevel_dim(img.cmn.num_slices, mip_index), bytes_per_slice)
            } else {
                (1, 0)
            };
            region: MTLRegion = (size = (width = (NSUInteger)mip_width, height = (NSUInteger)mip_height, depth = (NSUInteger)mip_depth);

            range(0, num_slices) { slice_index | 
                mtl_slice_index := @if(img.cmn.type == SG_IMAGETYPE_CUBE, face_index, slice_index);
                slice_offset := slice_index * bytes_per_slice;
                @debug_assert((slice_offset + bytes_per_slice) <= (int)data.subimage[face_index][mip_index].size);
                [mtl_tex replaceRegion:region
                    mipmapLevel:(NSUInteger)mip_index
                    slice:(NSUInteger)mtl_slice_index
                    withBytes:data_ptr + slice_offset
                    bytesPerRow:(NSUInteger)bytes_per_row
                    bytesPerImage:(NSUInteger)bytes_per_image];
            }
        }
    }
}

// initialize MTLTextureDescriptor with common attributes
fn _sg_mtl_init_texdesc_common(mtl_texture_descriptor: ObjCId, _sg_image_t* img) bool = {
    it := mtl_texture_descriptor;
    @objc it.textureType = _sg_mtl_texture_type(img.cmn.type);
    @objc it.pixelFormat = _sg_mtl_pixel_format(img.cmn.pixel_format);
    if (0 == @objc @as(i64) it.pixelFormat()) {
        _SG_ERROR(METAL_TEXTURE_FORMAT_NOT_SUPPORTED);
        return false;
    }
    @objc it.width = (NSUInteger)img.cmn.width;
    @objc it.height = (NSUInteger)img.cmn.height;
    if (SG_IMAGETYPE_3D == img.cmn.type) {
        @objc it.depth = (NSUInteger)img.cmn.num_slices;
    } else {
        @objc it.depth = 1;
    }
    it.mipmapLevelCount = (NSUInteger)img.cmn.num_mipmaps;
    if (SG_IMAGETYPE_ARRAY == img.cmn.type) {
        @objc it.arrayLength = (NSUInteger)img.cmn.num_slices;
    } else {
        @objc it.arrayLength = 1;
    }
    @objc it.usage = MTLTextureUsageShaderRead;
    MTLResourceOptions res_options = 0;
    if (img.cmn.usage != SG_USAGE_IMMUTABLE) {
        res_options |= MTLResourceCPUCacheModeWriteCombined;
    }
    res_options |= _sg_mtl_resource_options_storage_mode_managed_or_shared();
    @objc it.resourceOptions = res_options;
    return true;
}

// initialize MTLTextureDescriptor with rendertarget attributes
fn _sg_mtl_init_texdesc_rt(MTLTextureDescriptor* mtl_desc, _sg_image_t* img) {
    @debug_assert(img.cmn.render_target);
    @objc mtl_desc.usage = MTLTextureUsageShaderRead | MTLTextureUsageRenderTarget;
    @objc mtl_desc.resourceOptions = MTLResourceStorageModePrivate;
}

// initialize MTLTextureDescriptor with MSAA attributes
fn _sg_mtl_init_texdesc_rt_msaa(mtl_texture_descriptor: ObjCId, _sg_image_t* img) {
    @debug_assert(img.cmn.sample_count > 1);
    it := mtl_texture_descriptor;
    @objc it.usage = MTLTextureUsageShaderRead | MTLTextureUsageRenderTarget;
    @objc it.resourceOptions = MTLResourceStorageModePrivate;
    @objc it.textureType = MTLTextureType2DMultisample;
    @objc it.sampleCount = (NSUInteger)img.cmn.sample_count;
}

fn sg_resource_state _sg_mtl_create_image(_sg_image_t* img, const sg_image_desc* desc) {
    @debug_assert(img && desc);
    injected := (0 != desc.mtl_textures[0]);

    // first initialize all Metal resource pool slots to 'empty'
    range(0, SG_NUM_INFLIGHT_FRAMES) { i |
        img.mtl.tex[i] = _sg_mtl_add_resource(nil);
    }

    // initialize a Metal texture descriptor
    mtl_desc := [[MTLTextureDescriptor alloc] init];
    if (!_sg_mtl_init_texdesc_common(mtl_desc, img)) {
        release(mtl_desc);
        return SG_RESOURCESTATE_FAILED;
    }
    if (img.cmn.render_target) {
        if (img.cmn.sample_count > 1) {
            _sg_mtl_init_texdesc_rt_msaa(mtl_desc, img);
        } else {
            _sg_mtl_init_texdesc_rt(mtl_desc, img);
        }
    }
    range(0, img.cmn.num_slots) { slot |
        id<MTLTexture> mtl_tex;
        if (injected) {
            @debug_assert(desc.mtl_textures[slot]);
            mtl_tex = (__bridge id<MTLTexture>) desc.mtl_textures[slot];
        } else {
            mtl_tex = [sg.mtl.device newTextureWithDescriptor:mtl_desc];
            if (nil == mtl_tex) {
                release(mtl_desc);
                _SG_ERROR(METAL_CREATE_TEXTURE_FAILED);
                return SG_RESOURCESTATE_FAILED;
            }
            if ((img.cmn.usage == SG_USAGE_IMMUTABLE) && !img.cmn.render_target) {
                _sg_mtl_copy_image_data(img, mtl_tex, &desc.data);
            }
        }
        if SOKOL_DEBUG && desc.label {
            mtl_tex.label = [NSString stringWithFormat:@"%s.%d", desc.label, slot];
        }
        img.mtl.tex[slot] = _sg_mtl_add_resource(mtl_tex);
        release(mtl_tex);
    }
    release(mtl_desc);
    return SG_RESOURCESTATE_VALID;
}

fn _sg_mtl_discard_image(_sg_image_t* img) {
    @debug_assert(img);
    // it's valid to call release resource with a 'null resource'
    range(0, img.cmn.num_slots) { slot |
        _sg_mtl_release_resource(sg.frame_index, img.mtl.tex[slot]);
    }
}

fn sg_resource_state _sg_mtl_create_sampler(_sg_sampler_t* smp, const sg_sampler_desc* desc) {
    @debug_assert(smp && desc);
    id<MTLSamplerState> mtl_smp;
    const bool injected = (0 != desc.mtl_sampler);
    if (injected) {
        @debug_assert(desc.mtl_sampler);
        mtl_smp = (__bridge id<MTLSamplerState>) desc.mtl_sampler;
    } else {
        it := [[MTLSamplerDescriptor alloc] init];
        @objc it.setSAddressMode(mtl(desc.wrap_u));
        @objc it.setTAddressMode(mtl(desc.wrap_v));
        @objc it.setRAddressMode(mtl(desc.wrap_w));
        if (sg.features.image_clamp_to_border) {
            ASSERT_NOT_OLD();
            @objc it.borderColor  = _sg_mtl_border_color(desc.border_color);
        }
        @objc it.minFilter = _sg_mtl_minmag_filter(desc.min_filter);
        @objc it.magFilter = _sg_mtl_minmag_filter(desc.mag_filter);
        @objc it.mipFilter = _sg_mtl_mipmap_filter(desc.mipmap_filter);
        @objc it.lodMinClamp = desc.min_lod;
        @objc it.lodMaxClamp = desc.max_lod;
        // FIXME: lodAverage?
        @objc it.maxAnisotropy = desc.max_anisotropy;
        @objc it.normalizedCoordinates = YES;
        @objc it.compareFunction = _sg_mtl_compare_func(desc.compare);
        if SOKOL_DEBUG && desc.label {
            @objc it.label = [NSString stringWithUTF8String:desc.label];
        }
        mtl_smp = [sg.mtl.device newSamplerStateWithDescriptor:it];
        @objc it.release();
        if (nil == mtl_smp) {
            _SG_ERROR(METAL_CREATE_SAMPLER_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
    }
    smp.mtl.sampler_state = _sg_mtl_add_resource(mtl_smp);
    release(mtl_smp);
    return SG_RESOURCESTATE_VALID;
}

fn _sg_mtl_discard_sampler(_sg_sampler_t* smp) {
    @debug_assert(smp);
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(sg.frame_index, smp.mtl.sampler_state);
}

fn id<MTLLibrary> _sg_mtl_compile_library(const char* src) {
    NSError* err = NULL;
    id<MTLLibrary> lib = [sg.mtl.device
        newLibraryWithSource:[NSString stringWithUTF8String:src]
        options:nil
        error:&err
    ];
    if (err) {
        _SG_ERROR(METAL_SHADER_COMPILATION_FAILED);
        _SG_LOGMSG(METAL_SHADER_COMPILATION_OUTPUT, [err.localizedDescription UTF8String]);
    }
    return lib;
}

fn id<MTLLibrary> _sg_mtl_library_from_bytecode(const void* ptr, size_t num_bytes) {
    NSError* err = NULL;
    dispatch_data_t lib_data = dispatch_data_create(ptr, num_bytes, NULL, DISPATCH_DATA_DESTRUCTOR_DEFAULT);
    id<MTLLibrary> lib = [sg.mtl.device newLibraryWithData:lib_data error:&err];
    if (err) {
        _SG_ERROR(METAL_SHADER_CREATION_FAILED);
        _SG_LOGMSG(METAL_SHADER_COMPILATION_OUTPUT, [err.localizedDescription UTF8String]);
    }
    release(lib_data);
    return lib;
}

fn bool _sg_mtl_create_shader_func(const sg_shader_function* func, const char* label, const char* label_ext, _sg_mtl_shader_func_t* res) {
    @debug_assert(res.mtl_lib == _SG_MTL_INVALID_SLOT_INDEX);
    @debug_assert(res.mtl_func == _SG_MTL_INVALID_SLOT_INDEX);
    id<MTLLibrary> mtl_lib = nil;
    mtl_lib = if (func.bytecode.ptr) {
        @debug_assert(func.bytecode.size > 0);
         _sg_mtl_library_from_bytecode(func.bytecode.ptr, func.bytecode.size)
    } else if (func.source) {
        _sg_mtl_compile_library(func.source)
    };
    if (mtl_lib == nil) {
        return false;
    }
    if SOKOL_DEBUG && label {
        @debug_assert(label_ext);
        mtl_lib.label = [NSString stringWithFormat:@"%s.%s", label, label_ext];
    }
    @debug_assert(func.entry);
    id<MTLFunction> mtl_func = [mtl_lib newFunctionWithName:[NSString stringWithUTF8String:func.entry]];
    if (mtl_func == nil) {
        _SG_ERROR(METAL_SHADER_ENTRY_NOT_FOUND);
        release(mtl_lib);
        return false;
    }
    res.mtl_lib = _sg_mtl_add_resource(mtl_lib);
    res.mtl_func = _sg_mtl_add_resource(mtl_func);
    release(mtl_lib);
    release(mtl_func);
    return true;
}

fn release(self: ObjCId) void = @objc self.release();

fn _sg_mtl_discard_shader_func(const _sg_mtl_shader_func_t* func) {
    // it is valid to call _sg_mtl_release_resource with a 'null resource'
    _sg_mtl_release_resource(sg.frame_index, func.mtl_func);
    _sg_mtl_release_resource(sg.frame_index, func.mtl_lib);
}

// NOTE: this is an out-of-range check for MSL bindslots that's also active in release mode
fn _sg_mtl_ensure_msl_bindslot_ranges(const sg_shader_desc* desc) bool = {
    @debug_assert(desc);
    X :: fn(count: i64, $get: @Fn(i: i64) i64, limit: i64, error) => 
        range(0, count) { i |
            if get(i) >= limit {
                _SG_ERROR(error);
                return false;
            };
        };
    X(SG_MAX_UNIFORMBLOCK_BINDSLOTS, fn(i) => desc.uniform_blocks[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_BINDINGS, METAL_UNIFORMBLOCK_MSL_BUFFER_SLOT_OUT_OF_RANGE);
    X(SG_MAX_STORAGEBUFFER_BINDSLOTS, fn(i) => desc.storage_buffers[i].msl_buffer_n, _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS, METAL_STORAGEBUFFER_MSL_BUFFER_SLOT_OUT_OF_RANGE);
    X(SG_MAX_IMAGE_BINDSLOTS, fn(i) => desc.images[i].msl_buffer_n, _SG_MTL_MAX_STAGE_IMAGE_BINDINGS, METAL_IMAGE_MSL_TEXTURE_SLOT_OUT_OF_RANGE);
    X(SG_MAX_SAMPLER_BINDSLOTS, fn(i) => desc.samplers[i].msl_sampler_n, _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS, METAL_SAMPLER_MSL_SAMPLER_SLOT_OUT_OF_RANGE);
    true
}

MTLSize :: @struct(width: u64, height: u64, depth: u64);
fn sg_resource_state _sg_mtl_create_shader(_sg_shader_t* shd, const sg_shader_desc* desc) {
    @debug_assert(shd && desc);

    // do a MSL bindslot range check also in release mode, and if that fails,
    // also fail shader creation
    if (!_sg_mtl_ensure_msl_bindslot_ranges(desc)) {
        return SG_RESOURCESTATE_FAILED;
    }
    
    shd.mtl.threads_per_threadgroup = (
        width = (NSUInteger)desc.mtl_threads_per_threadgroup.x,
        height = (NSUInteger)desc.mtl_threads_per_threadgroup.y,
        depth = (NSUInteger)desc.mtl_threads_per_threadgroup.z,
    );

    // copy resource bindslot mappings
    for (size_t i = 0; i < SG_MAX_UNIFORMBLOCK_BINDSLOTS; i++) {
        shd.mtl.ub_buffer_n[i] = desc.uniform_blocks[i].msl_buffer_n;
    }
    for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
        shd.mtl.sbuf_buffer_n[i] = desc.storage_buffers[i].msl_buffer_n;
    }
    for (size_t i = 0; i < SG_MAX_IMAGE_BINDSLOTS; i++) {
        shd.mtl.img_texture_n[i] = desc.images[i].msl_texture_n;
    }
    for (size_t i = 0; i < SG_MAX_SAMPLER_BINDSLOTS; i++) {
        shd.mtl.smp_sampler_n[i] = desc.samplers[i].msl_sampler_n;
    }

    // create metal library and function objects
    bool shd_valid = true;
    if (desc.vertex_func.source || desc.vertex_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.vertex_func, desc.label, "vs", &shd.mtl.vertex_func);
    }
    if (desc.fragment_func.source || desc.fragment_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.fragment_func, desc.label, "fs", &shd.mtl.fragment_func);
    }
    if (desc.compute_func.source || desc.compute_func.bytecode.ptr) {
        shd_valid &= _sg_mtl_create_shader_func(&desc.compute_func, desc.label, "cs", &shd.mtl.compute_func);
    }
    if (!shd_valid) {
        _sg_mtl_discard_shader_func(&shd.mtl.vertex_func);
        _sg_mtl_discard_shader_func(&shd.mtl.fragment_func);
        _sg_mtl_discard_shader_func(&shd.mtl.compute_func);
    }
    return shd_valid ? SG_RESOURCESTATE_VALID : SG_RESOURCESTATE_FAILED;
}

fn _sg_mtl_discard_shader(_sg_shader_t* shd) {
    @debug_assert(shd);
    _sg_mtl_discard_shader_func(&shd.mtl.vertex_func);
    _sg_mtl_discard_shader_func(&shd.mtl.fragment_func);
    _sg_mtl_discard_shader_func(&shd.mtl.compute_func);
}

MTLMutabilityImmutable :: 2;

fn sg_resource_state _sg_mtl_create_pipeline(_sg_pipeline_t* pip, _sg_shader_t* shd, const sg_pipeline_desc* desc) {
    @debug_assert(pip && shd && desc);
    @debug_assert(desc.shader.id == shd.slot.id);

    pip.shader = shd;

    if (pip.cmn.is_compute) {
        NSError* err = NULL;
        MTLComputePipelineDescriptor* cp_desc = [[MTLComputePipelineDescriptor alloc] init];
        cp_desc.computeFunction = _sg_mtl_id(shd.mtl.compute_func.mtl_func);
        cp_desc.threadGroupSizeIsMultipleOfThreadExecutionWidth = true;
        for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
            const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
            @debug_assert((stage != SG_SHADERSTAGE_VERTEX) && (stage != SG_SHADERSTAGE_FRAGMENT));
            if ((stage == SG_SHADERSTAGE_COMPUTE) && shd.cmn.storage_buffers[i].readonly) {
                const NSUInteger mtl_slot = shd.mtl.sbuf_buffer_n[i];
                cp_desc.buffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        if SOKOL_DEBUG && desc.label {
            cp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        id<MTLComputePipelineState> mtl_cps = [sg.mtl.device
            newComputePipelineStateWithDescriptor:cp_desc
            options:MTLPipelineOptionNone
            reflection:nil
            error:&err];
        release(cp_desc);
        if (nil == mtl_cps) {
            @debug_assert(err);
            _SG_ERROR(METAL_CREATE_CPS_FAILED);
            _SG_LOGMSG(METAL_CREATE_CPS_OUTPUT, [err.localizedDescription UTF8String]);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.cps = _sg_mtl_add_resource(mtl_cps);
        release(mtl_cps);
        pip.mtl.threads_per_threadgroup = shd.mtl.threads_per_threadgroup;
    } else {
        prim_type := desc.primitive_type;
        pip.mtl.prim_type = mtl(prim_type);
        pip.mtl.index_size = mtl(pip.cmn.index_type);
        if (SG_INDEXTYPE_NONE != pip.cmn.index_type) {
            pip.mtl.index_type = mtl(pip.cmn.index_type);
        }
        pip.mtl.cull_mode = mtl(desc.cull_mode);
        pip.mtl.winding = mtl(desc.face_winding);
        pip.mtl.stencil_ref = desc.stencil.ref;

        // create vertex-descriptor
        MTLVertexDescriptor* vtx_desc = [MTLVertexDescriptor vertexDescriptor];
        for (NSUInteger attr_index = 0; attr_index < SG_MAX_VERTEX_ATTRIBUTES; attr_index++) {
            const sg_vertex_attr_state* a_state = &desc.layout.attrs[attr_index];
            if (a_state.format == SG_VERTEXFORMAT_INVALID) {
                break;
            }
            @debug_assert(a_state.buffer_index < SG_MAX_VERTEXBUFFER_BINDSLOTS);
            @debug_assert(pip.cmn.vertex_buffer_layout_active[a_state.buffer_index]);
            vtx_desc.attributes[attr_index].format = _sg_mtl_vertex_format(a_state.format);
            vtx_desc.attributes[attr_index].offset = (NSUInteger)a_state.offset;
            vtx_desc.attributes[attr_index].bufferIndex = _sg_mtl_vertexbuffer_bindslot((size_t)a_state.buffer_index);
        }
        for (NSUInteger layout_index = 0; layout_index < SG_MAX_VERTEXBUFFER_BINDSLOTS; layout_index++) {
            if (pip.cmn.vertex_buffer_layout_active[layout_index]) {
                const sg_vertex_buffer_layout_state* l_state = &desc.layout.buffers[layout_index];
                const NSUInteger mtl_vb_slot = _sg_mtl_vertexbuffer_bindslot(layout_index);
                @debug_assert(l_state.stride > 0);
                vtx_desc.layouts[mtl_vb_slot].stride = (NSUInteger)l_state.stride;
                vtx_desc.layouts[mtl_vb_slot].stepFunction = _sg_mtl_step_function(l_state.step_func);
                vtx_desc.layouts[mtl_vb_slot].stepRate = (NSUInteger)l_state.step_rate;
                if (SG_VERTEXSTEP_PER_INSTANCE == l_state.step_func) {
                    // NOTE: not actually used in _sg_mtl_draw()
                    pip.cmn.use_instanced_draw = true;
                }
            }
        }

        // render-pipeline descriptor
        MTLRenderPipelineDescriptor* rp_desc = [[MTLRenderPipelineDescriptor alloc] init];
        rp_desc.vertexDescriptor = vtx_desc;
        @debug_assert(shd.mtl.vertex_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        rp_desc.vertexFunction = _sg_mtl_id(shd.mtl.vertex_func.mtl_func);
        @debug_assert(shd.mtl.fragment_func.mtl_func != _SG_MTL_INVALID_SLOT_INDEX);
        rp_desc.fragmentFunction = _sg_mtl_id(shd.mtl.fragment_func.mtl_func);
        rp_desc.rasterSampleCount = (NSUInteger)desc.sample_count;
        rp_desc.alphaToCoverageEnabled = desc.alpha_to_coverage_enabled;
        rp_desc.alphaToOneEnabled = NO;
        rp_desc.rasterizationEnabled = YES;
        rp_desc.depthAttachmentPixelFormat = _sg_mtl_pixel_format(desc.depth.pixel_format);
        if (desc.depth.pixel_format == SG_PIXELFORMAT_DEPTH_STENCIL) {
            rp_desc.stencilAttachmentPixelFormat = _sg_mtl_pixel_format(desc.depth.pixel_format);
        }
        at := @objc rp_desc.colorAttachments();
        for (NSUInteger i = 0; i < (NSUInteger)desc.color_count; i++) {
            @debug_assert(i < SG_MAX_COLOR_ATTACHMENTS);
            const sg_color_target_state* cs = &desc.colors[i];
            it := @objc at.objectAtIndex(i);
            @objc it.pixelFormat = _sg_mtl_pixel_format(cs.pixel_format);
            @objc it.writeMask = _sg_mtl_color_write_mask(cs.write_mask);
            @objc it.blendingEnabled = cs.blend.enabled;
            @objc it.alphaBlendOperation = _sg_mtl_blend_op(cs.blend.op_alpha);
            @objc it.rgbBlendOperation = _sg_mtl_blend_op(cs.blend.op_rgb);
            @objc it.destinationAlphaBlendFactor = _sg_mtl_blend_factor(cs.blend.dst_factor_alpha);
            @objc it.destinationRGBBlendFactor = _sg_mtl_blend_factor(cs.blend.dst_factor_rgb);
            @objc it.sourceAlphaBlendFactor = _sg_mtl_blend_factor(cs.blend.src_factor_alpha);
            @objc it.sourceRGBBlendFactor = _sg_mtl_blend_factor(cs.blend.src_factor_rgb);
        }
        // set buffer mutability for all read-only buffers (vertex buffers and read-only storage buffers)
        for (size_t i = 0; i < SG_MAX_VERTEXBUFFER_BINDSLOTS; i++) {
            if (pip.cmn.vertex_buffer_layout_active[i]) {
                const NSUInteger mtl_slot = _sg_mtl_vertexbuffer_bindslot(i);
                rp_desc.vertexBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        for (size_t i = 0; i < SG_MAX_STORAGEBUFFER_BINDSLOTS; i++) {
            const NSUInteger mtl_slot = shd.mtl.sbuf_buffer_n[i];
            const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
            @debug_assert(stage != SG_SHADERSTAGE_COMPUTE);
            if (stage == SG_SHADERSTAGE_VERTEX) {
                @debug_assert(shd.cmn.storage_buffers[i].readonly);
                rp_desc.vertexBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
                @debug_assert(shd.cmn.storage_buffers[i].readonly);
                rp_desc.fragmentBuffers[mtl_slot].mutability = MTLMutabilityImmutable;
            }
        }
        if SOKOL_DEBUG && desc.label {
            rp_desc.label = [NSString stringWithFormat:@"%s", desc.label];
        }
        NSError* err = NULL;
        id<MTLRenderPipelineState> mtl_rps = [sg.mtl.device newRenderPipelineStateWithDescriptor:rp_desc error:&err];
        release(rp_desc);
        if (nil == mtl_rps) {
            @debug_assert(err);
            _SG_ERROR(METAL_CREATE_RPS_FAILED);
            _SG_LOGMSG(METAL_CREATE_RPS_OUTPUT, [err.localizedDescription UTF8String]);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.rps = _sg_mtl_add_resource(mtl_rps);
        release(mtl_rps);

        // depth-stencil-state
        ds_desc := [[MTLDepthStencilDescriptor alloc] init];
        @objc ds_desc.setDepthCompareFunction(_sg_mtl_compare_func(desc.depth.compare));
        @objc ds_desc.setDepthWriteEnabled(desc.depth.write_enabled);
        if (desc.stencil.enabled) {
            const sg_stencil_face_state* sb = &desc.stencil.back;
            it := [[MTLStencilDescriptor alloc] init];
            @objc ds_desc.setBackFaceStencil(it);
            @objc it.setStencilFailureOperation(_sg_mtl_stencil_op(sb.fail_op));
            @objc it.setDepthFailureOperation(_sg_mtl_stencil_op(sb.depth_fail_op));
            @objc it.setDepthStencilPassOperation(_sg_mtl_stencil_op(sb.pass_op));
            @objc it.setStencilCompareFunction(_sg_mtl_compare_func(sb.compare));
            @objc it.setReadMask(desc.stencil.read_mask);
            @objc it.setWriteMask(desc.stencil.write_mask);
            const sg_stencil_face_state* sf = &desc.stencil.front;
            it := [[MTLStencilDescriptor alloc] init];
            @objc ds_desc.setFrontFaceStencil(it);
            @objc it.setStencilFailureOperation(_sg_mtl_stencil_op(sf.fail_op));
            @objc it.setDepthFailureOperation(_sg_mtl_stencil_op(sf.depth_fail_op));
            @objc it.setDepthStencilPassOperation(_sg_mtl_stencil_op(sf.pass_op));
            @objc it.setStencilCompareFunction(_sg_mtl_compare_func(sf.compare));
            @objc it.setReadMask(desc.stencil.read_mask);
            @objc it.setWriteMask(desc.stencil.write_mask);
        }
        if SOKOL_DEBUG && desc.label {
            ds_desc.label = [NSString stringWithFormat:@"%s.dss", desc.label];
        }
        id<MTLDepthStencilState> mtl_dss = [sg.mtl.device newDepthStencilStateWithDescriptor:ds_desc];
        release(ds_desc);
        if (nil == mtl_dss) {
            _SG_ERROR(METAL_CREATE_DSS_FAILED);
            return SG_RESOURCESTATE_FAILED;
        }
        pip.mtl.dss = _sg_mtl_add_resource(mtl_dss);
        release(mtl_dss);
    }
    return SG_RESOURCESTATE_VALID;
}

fn _sg_mtl_discard_pipeline(_sg_pipeline_t* pip) {
    @debug_assert(pip);
    // it's valid to call release resource with a 'null resource'
    _sg_mtl_release_resource(sg.frame_index, pip.mtl.cps);
    _sg_mtl_release_resource(sg.frame_index, pip.mtl.rps);
    _sg_mtl_release_resource(sg.frame_index, pip.mtl.dss);
}

fn sg_resource_state _sg_mtl_create_attachments(_sg_attachments_t* atts, _sg_image_t** color_images, _sg_image_t** resolve_images, _sg_image_t* ds_img, const sg_attachments_desc* desc) {
    @debug_assert(atts && desc);
    @debug_assert(color_images && resolve_images);

    // copy image pointers
    range(0, atts.cmn.num_colors) { i |
        color_desc := &desc.colors&.index(i);
        @debug_assert(color_desc.image.id != SG_INVALID_ID);
        @debug_assert(0 == atts.mtl.colors[i].image);
        @debug_assert(color_images[i] && (color_images[i].slot.id == color_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_color_format(color_images[i].cmn.pixel_format));
        atts.mtl.colors[i].image = color_images[i];

        resolve_desc := &desc.resolves&.index(i);
        if (resolve_desc.image.id != SG_INVALID_ID) {
            @debug_assert(0 == atts.mtl.resolves[i].image);
            @debug_assert(resolve_images[i] && (resolve_images[i].slot.id == resolve_desc.image.id));
            @debug_assert(color_images[i] && (color_images[i].cmn.pixel_format == resolve_images[i].cmn.pixel_format));
            atts.mtl.resolves[i].image = resolve_images[i];
        }
    }
    @debug_assert(0 == atts.mtl.depth_stencil.image);
    ds_desc := desc.depth_stencil&;
    if (ds_desc.image.id != SG_INVALID_ID) {
        @debug_assert(ds_img && (ds_img.slot.id == ds_desc.image.id));
        @debug_assert(_sg_is_valid_rendertarget_depth_format(ds_img.cmn.pixel_format));
        atts.mtl.depth_stencil.image = ds_img;
    }
    return SG_RESOURCESTATE_VALID;
}

fn _sg_mtl_discard_attachments(_sg_attachments_t* atts) {
    @debug_assert(atts);
    _ := (atts);
}

fn _sg_image_t* _sg_mtl_attachments_color_image(const _sg_attachments_t* atts, int index) {
    // NOTE: may return null
    @debug_assert(atts && (index >= 0) && (index < SG_MAX_COLOR_ATTACHMENTS));
    return atts.mtl.colors[index].image;
}

fn _sg_image_t* _sg_mtl_attachments_resolve_image(const _sg_attachments_t* atts, int index) {
    // NOTE: may return null
    @debug_assert(atts && (index >= 0) && (index < SG_MAX_COLOR_ATTACHMENTS));
    return atts.mtl.resolves[index].image;
}

fn _sg_image_t* _sg_mtl_attachments_ds_image(const _sg_attachments_t* atts) {
    // NOTE: may return null
    @debug_assert(atts);
    return atts.mtl.depth_stencil.image;
}

fn _sg_mtl_bind_uniform_buffers(void) {
    // In the Metal backend, uniform buffer bindings happen once in sg_begin_pass() and
    // remain valid for the entire pass. Only binding offsets will be updated
    // in sg_apply_uniforms()
    if (sg.cur_pass.is_compute) {
        @debug_assert(nil != sg.mtl.compute_cmd_encoder);
        for (size_t slot = 0; slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS; slot++) {
            [sg.mtl.compute_cmd_encoder
                setBuffer:sg.mtl.uniform_buffers[sg.mtl.cur_frame_rotate_index]
                offset:0
                atIndex:slot];
        }
    } else {
        @debug_assert(nil != sg.mtl.render_cmd_encoder);
        for (size_t slot = 0; slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS; slot++) {
            [sg.mtl.render_cmd_encoder
                setVertexBuffer:sg.mtl.uniform_buffers[sg.mtl.cur_frame_rotate_index]
                offset:0
                atIndex:slot];
            [sg.mtl.render_cmd_encoder
                setFragmentBuffer:sg.mtl.uniform_buffers[sg.mtl.cur_frame_rotate_index]
                offset:0
                atIndex:slot];
        }
    }
}

fn _sg_mtl_begin_compute_pass(const sg_pass* pass) {
    @debug_assert(pass); (void)pass;
    @debug_assert(nil != sg.mtl.cmd_buffer);
    @debug_assert(nil == sg.mtl.compute_cmd_encoder);
    @debug_assert(nil == sg.mtl.render_cmd_encoder);

    // NOTE: we actually want computeCommandEncoderWithDispatchType:MTLDispatchTypeConcurrent, but
    // that requires bumping the macOS base version to 10.14
    sg.mtl.compute_cmd_encoder = [sg.mtl.cmd_buffer computeCommandEncoder];
    if (nil == sg.mtl.compute_cmd_encoder) {
        sg.cur_pass.valid = false;
        return;
    }

    if SOKOL_DEBUG && pass.label {
        sg.mtl.compute_cmd_encoder.label = [NSString stringWithUTF8String:pass.label];
    }
}

fn _sg_mtl_begin_render_pass(const sg_pass* pass) {
    @debug_assert(pass);
    @debug_assert(nil != sg.mtl.cmd_buffer);
    @debug_assert(nil == sg.mtl.render_cmd_encoder);
    @debug_assert(nil == sg.mtl.compute_cmd_encoder);

    const _sg_attachments_t* atts = sg.cur_pass.atts;
    const sg_swapchain* swapchain = &pass.swapchain;
    const sg_pass_action* action = &pass.action;

    MTLRenderPassDescriptor* pass_desc = [MTLRenderPassDescriptor renderPassDescriptor];
    @debug_assert(pass_desc);
    if (atts) {
        // setup pass descriptor for offscreen rendering
        @debug_assert(atts.slot.state == SG_RESOURCESTATE_VALID);
        at := @objc pass_desc.colorAttachments();
        for (NSUInteger i = 0; i < (NSUInteger)atts.cmn.num_colors; i++) {
            ca := @objc at.objectAtIndex(i);
            const _sg_attachment_common_t* cmn_color_att = &atts.cmn.colors[i];
            const _sg_mtl_attachment_t* mtl_color_att = &atts.mtl.colors[i];
            const _sg_image_t* color_att_img = mtl_color_att.image;
            const _sg_attachment_common_t* cmn_resolve_att = &atts.cmn.resolves[i];
            const _sg_mtl_attachment_t* mtl_resolve_att = &atts.mtl.resolves[i];
            const _sg_image_t* resolve_att_img = mtl_resolve_att.image;
            @debug_assert(color_att_img.slot.state == SG_RESOURCESTATE_VALID);
            @debug_assert(color_att_img.slot.id == cmn_color_att.image_id.id);
            @debug_assert(color_att_img.mtl.tex[color_att_img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
            @objc ca.loadAction = _sg_mtl_load_action(action.colors[i].load_action);
            @objc ca.storeAction = _sg_mtl_store_action(action.colors[i].store_action, resolve_att_img != 0);
            sg_color c = action.colors[i].clear_value;
            @objc ca.clearColor = MTLClearColorMake(c.r, c.g, c.b, c.a);
            @objc ca.texture = _sg_mtl_id(color_att_img.mtl.tex[color_att_img.cmn.active_slot]);
            @objc ca.level = (NSUInteger)cmn_color_att.mip_level;
            switch (color_att_img.cmn.type) {
                case SG_IMAGETYPE_CUBE:
                case SG_IMAGETYPE_ARRAY:
                    @objc ca.slice = (NSUInteger)cmn_color_att.slice;
                    break;
                case SG_IMAGETYPE_3D:
                    @objc ca.depthPlane = (NSUInteger)cmn_color_att.slice;
                    break;
                default: break;
            }
            if (resolve_att_img) {
                @debug_assert(resolve_att_img.slot.state == SG_RESOURCESTATE_VALID);
                @debug_assert(resolve_att_img.slot.id == cmn_resolve_att.image_id.id);
                @debug_assert(resolve_att_img.mtl.tex[resolve_att_img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
                @objc ca.resolveTexture = _sg_mtl_id(resolve_att_img.mtl.tex[resolve_att_img.cmn.active_slot]);
                @objc ca.resolveLevel = (NSUInteger)cmn_resolve_att.mip_level;
                switch (resolve_att_img.cmn.type) {
                    case SG_IMAGETYPE_CUBE:
                    case SG_IMAGETYPE_ARRAY:
                        @objc ca.resolveSlice = (NSUInteger)cmn_resolve_att.slice;
                        break;
                    case SG_IMAGETYPE_3D:
                        @objc ca.resolveDepthPlane = (NSUInteger)cmn_resolve_att.slice;
                        break;
                    default: break;
                }
            }
        }
        const _sg_image_t* ds_att_img = atts.mtl.depth_stencil.image;
        if (0 != ds_att_img) {
            @debug_assert(ds_att_img.slot.state == SG_RESOURCESTATE_VALID);
            @debug_assert(ds_att_img.slot.id == atts.cmn.depth_stencil.image_id.id);
            @debug_assert(ds_att_img.mtl.tex[ds_att_img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
            d := @objc pass_desc.depthAttachment();
            @objc d.texture = _sg_mtl_id(ds_att_img.mtl.tex[ds_att_img.cmn.active_slot]);
            @objc d.loadAction = _sg_mtl_load_action(action.depth.load_action);
            @objc d.storeAction = _sg_mtl_store_action(action.depth.store_action, false);
            @objc d.clearDepth = action.depth.clear_value;
            const _sg_attachment_common_t* cmn_ds_att = &atts.cmn.depth_stencil;
            switch (ds_att_img.cmn.type) {
                case SG_IMAGETYPE_CUBE:
                case SG_IMAGETYPE_ARRAY:
                    @objc d.slice = (NSUInteger)cmn_ds_att.slice;
                    break;
                case SG_IMAGETYPE_3D:
                    @objc d.resolveDepthPlane = (NSUInteger)cmn_ds_att.slice;
                    break;
                default: break;
            }
            if (_sg_is_depth_stencil_format(ds_att_img.cmn.pixel_format)) {
                sa := @objc pass_desc.stencilAttachment();
                @objc sa.texture = _sg_mtl_id(ds_att_img.mtl.tex[ds_att_img.cmn.active_slot]);
                @objc sa.loadAction = _sg_mtl_load_action(action.stencil.load_action);
                @objc sa.storeAction = _sg_mtl_store_action(action.depth.store_action, false);
                @objc sa.clearStencil = action.stencil.clear_value;
                switch (ds_att_img.cmn.type) {
                    case SG_IMAGETYPE_CUBE:
                    case SG_IMAGETYPE_ARRAY:
                        @objc sa.slice = (NSUInteger)cmn_ds_att.slice;
                        break;
                    case SG_IMAGETYPE_3D:
                        @objc sa.resolveDepthPlane = (NSUInteger)cmn_ds_att.slice;
                        break;
                    default: break;
                }
            }
        }
    } else {
        // setup pass descriptor for swapchain rendering
        //
        // NOTE: at least in macOS Sonoma this no longer seems to be the case, the
        // current drawable is also valid in a minimized window
        // ===
        // an MTKView current_drawable will not be valid if window is minimized, don't do any rendering in this case
        if (0 == swapchain.metal.current_drawable) {
            sg.cur_pass.valid = false;
            return;
        }
        // pin the swapchain resources into memory so that they outlive their command buffer
        // (this is necessary because the command buffer doesn't retain references)
        int pass_desc_ref = _sg_mtl_add_resource(pass_desc);
        _sg_mtl_release_resource(sg.frame_index, pass_desc_ref);

        sg.mtl.cur_drawable = (__bridge id<CAMetalDrawable>) swapchain.metal.current_drawable;
        ca := pass_desc.colorAttachments&.index(0);
        if (swapchain.sample_count > 1) {
            // multi-sampling: render into msaa texture, resolve into drawable texture
            id<MTLTexture> msaa_tex = (__bridge id<MTLTexture>) swapchain.metal.msaa_color_texture;
            @debug_assert(msaa_tex != nil);
            @objc ca.texture = msaa_tex;
            @objc ca.resolveTexture = sg.mtl.cur_drawable.texture;
            @objc ca.storeAction = MTLStoreActionMultisampleResolve;
        } else {
            // non-msaa: render into current_drawable
            @objc ca.texture = sg.mtl.cur_drawable.texture;
            @objc ca.storeAction = MTLStoreActionStore;
        }
        @objc ca.loadAction = _sg_mtl_load_action(action.colors[0].load_action);
        const sg_color c = action.colors[0].clear_value;
        @objc ca.clearColor = MTLClearColorMake(c.r, c.g, c.b, c.a);

        // optional depth-stencil texture
        if (swapchain.metal.depth_stencil_texture) {
            id<MTLTexture> ds_tex = (__bridge id<MTLTexture>) swapchain.metal.depth_stencil_texture;
            @debug_assert(ds_tex != nil);
            d := @objc pass_desc.depthAttachment();
            @objc d.texture = ds_tex;
            @objc d.storeAction = MTLStoreActionDontCare;
            @objc d.loadAction = _sg_mtl_load_action(action.depth.load_action);
            @objc d.clearDepth = action.depth.clear_value;
            if (_sg_is_depth_stencil_format(swapchain.depth_format)) {
                sa := @objc pass_desc.stencilAttachment();
                @objc sa.texture = ds_tex;
                @objc sa.storeAction = MTLStoreActionDontCare;
                @objc sa.loadAction = _sg_mtl_load_action(action.stencil.load_action);
                @objc sa.clearStencil = action.stencil.clear_value;
            }
        }
    }

    // NOTE: at least in macOS Sonoma, the following is no longer the case, a valid
    // render command encoder is also returned in a minimized window
    // ===
    // create a render command encoder, this might return nil if window is minimized
    sg.mtl.render_cmd_encoder = [sg.mtl.cmd_buffer renderCommandEncoderWithDescriptor:pass_desc];
    if (nil == sg.mtl.render_cmd_encoder) {
        sg.cur_pass.valid = false;
        return;
    }

    if SOKOL_DEBUG && pass.label {
        sg.mtl.render_cmd_encoder.label = [NSString stringWithUTF8String:pass.label];
    }
}

fn _sg_mtl_begin_pass(const sg_pass* pass) {
    @debug_assert(pass);
    @debug_assert(sg.mtl.cmd_queue);
    @debug_assert(nil == sg.mtl.compute_cmd_encoder);
    @debug_assert(nil == sg.mtl.render_cmd_encoder);
    @debug_assert(nil == sg.mtl.cur_drawable);
    _sg_mtl_clear_state_cache();

    // if this is the first pass in the frame, create one command buffer and blit-cmd-encoder for the entire frame
    if (nil == sg.mtl.cmd_buffer) {
        // block until the oldest frame in flight has finished
        dispatch_semaphore_wait(sg.mtl.sem, DISPATCH_TIME_FOREVER);
        if (sg.desc.mtl_use_command_buffer_with_retained_references) {
            sg.mtl.cmd_buffer = [sg.mtl.cmd_queue commandBuffer];
        } else {
            sg.mtl.cmd_buffer = [sg.mtl.cmd_queue commandBufferWithUnretainedReferences];
        }
        [sg.mtl.cmd_buffer enqueue];
        [sg.mtl.cmd_buffer addCompletedHandler:^(id<MTLCommandBuffer> cmd_buf) {
            // NOTE: this code is called on a different thread!
            _ := cmd_buf;
            dispatch_semaphore_signal(sg.mtl.sem);
        }];
    }

    // if this is first pass in frame, get uniform buffer base pointer
    if (0 == sg.mtl.cur_ub_base_ptr) {
        sg.mtl.cur_ub_base_ptr = (uint8_t*)[sg.mtl.uniform_buffers[sg.mtl.cur_frame_rotate_index] contents];
    }

    if (pass.compute) {
        _sg_mtl_begin_compute_pass(pass);
    } else {
        _sg_mtl_begin_render_pass(pass);
    }

    // bind uniform buffers, those bindings remain valid for the entire pass
    if (sg.cur_pass.valid) {
        _sg_mtl_bind_uniform_buffers();
    }
}

fn _sg_mtl_end_pass(void) {
    if (nil != sg.mtl.render_cmd_encoder) {
        [sg.mtl.render_cmd_encoder endEncoding];
        // NOTE: MTLRenderCommandEncoder is autoreleased
        sg.mtl.render_cmd_encoder = nil;
    }
    if (nil != sg.mtl.compute_cmd_encoder) {
        [sg.mtl.compute_cmd_encoder endEncoding];
        // NOTE: MTLComputeCommandEncoder is autoreleased
        sg.mtl.compute_cmd_encoder = nil;

        ASSERT_NOT_IOS();
        // synchronize any managed buffers written by the GPU
        if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
            if (sg.compute.readwrite_sbufs.cur > 0) {
                id<MTLBlitCommandEncoder> blit_cmd_encoder = [sg.mtl.cmd_buffer blitCommandEncoder];
                for (uint32_t i = 0; i < sg.compute.readwrite_sbufs.cur; i++) {
                    _sg_buffer_t* sbuf = _sg_lookup_buffer(&sg.pools, sg.compute.readwrite_sbufs.items[i]);
                    if (sbuf) {
                        [blit_cmd_encoder synchronizeResource:_sg_mtl_id(sbuf.mtl.buf[sbuf.cmn.active_slot])];
                    }
                }
                [blit_cmd_encoder endEncoding];
            }
        }
    }
    // if this is a swapchain pass, present the drawable
    if (nil != sg.mtl.cur_drawable) {
        [sg.mtl.cmd_buffer presentDrawable:sg.mtl.cur_drawable];
        sg.mtl.cur_drawable = nil;
    }
}

fn _sg_mtl_commit(void) {
    @debug_assert(nil == sg.mtl.render_cmd_encoder);
    @debug_assert(nil == sg.mtl.compute_cmd_encoder);
    @debug_assert(nil != sg.mtl.cmd_buffer);

    // commit the frame's command buffer
    [sg.mtl.cmd_buffer commit];

    // garbage-collect resources pending for release
    _sg_mtl_garbage_collect(sg.frame_index);

    // rotate uniform buffer slot
    if (++sg.mtl.cur_frame_rotate_index >= SG_NUM_INFLIGHT_FRAMES) {
        sg.mtl.cur_frame_rotate_index = 0;
    }
    sg.mtl.cur_ub_offset = 0;
    sg.mtl.cur_ub_base_ptr = 0;
    // NOTE: MTLCommandBuffer is autoreleased
    sg.mtl.cmd_buffer = nil;
}

fn _sg_mtl_apply_viewport(int x, int y, int w, int h, bool origin_top_left) {
    @debug_assert(nil != sg.mtl.render_cmd_encoder);
    @debug_assert(sg.cur_pass.height > 0);
    MTLViewport :: @struct(originX: f64, originY: f64, width: f64, height: f64, znear: f64, zfar: f64);
    vp: MTLViewport = (
        originX = (double) x,
        originY = (double) (origin_top_left ? y : (sg.cur_pass.height - (y + h))),
        width   = (double) w,
        height  = (double) h,
        znear   = 0.0,
        zfar    = 1.0,
    );
    @objc sg.mtl.render_cmd_encoder.setViewport(vp);
}

fn _sg_mtl_apply_scissor_rect(int x, int y, int w, int h, bool origin_top_left) {
    @debug_assert(nil != sg.mtl.render_cmd_encoder);
    @debug_assert(sg.cur_pass.width > 0);
    @debug_assert(sg.cur_pass.height > 0);
    // clip against framebuffer rect
    const _sg_recti_t clip = _sg_clipi(x, y, w, h, sg.cur_pass.width, sg.cur_pass.height);
    MTLScissorRect :: @struct(x: u64, y: u64, width: u64, height: u64);
    r: MTLScissorRect = (
        x = (NSUInteger)clip.x,
        y = (NSUInteger) (origin_top_left ? clip.y : (sg.cur_pass.height - (clip.y + clip.h))),
        width = (NSUInteger)clip.w,
        height = (NSUInteger)clip.h,
    );
    [sg.mtl.render_cmd_encoder setScissorRect:r];
}

fn _sg_mtl_apply_pipeline(_sg_pipeline_t* pip) {
    @debug_assert(pip);
    @debug_assert(pip.shader && (pip.cmn.shader_id.id == pip.shader.slot.id));
    if (sg.mtl.state_cache.cur_pipeline_id.id != pip.slot.id) {
        sg.mtl.state_cache.cur_pipeline = pip;
        sg.mtl.state_cache.cur_pipeline_id.id = pip.slot.id;
        if (pip.cmn.is_compute) {
            @debug_assert(sg.cur_pass.is_compute);
            @debug_assert(nil != sg.mtl.compute_cmd_encoder);
            @debug_assert(pip.mtl.cps != _SG_MTL_INVALID_SLOT_INDEX);
            [sg.mtl.compute_cmd_encoder setComputePipelineState:_sg_mtl_id(pip.mtl.cps)];
        } else {
            @debug_assert(!sg.cur_pass.is_compute);
            @debug_assert(nil != sg.mtl.render_cmd_encoder);
            sg_color c = pip.cmn.blend_color;
            [sg.mtl.render_cmd_encoder setBlendColorRed:c.r green:c.g blue:c.b alpha:c.a];
            _sg_stats_add(metal.pipeline.num_set_blend_color, 1);
            [sg.mtl.render_cmd_encoder setCullMode:pip.mtl.cull_mode];
            _sg_stats_add(metal.pipeline.num_set_cull_mode, 1);
            [sg.mtl.render_cmd_encoder setFrontFacingWinding:pip.mtl.winding];
            _sg_stats_add(metal.pipeline.num_set_front_facing_winding, 1);
            [sg.mtl.render_cmd_encoder setStencilReferenceValue:pip.mtl.stencil_ref];
            _sg_stats_add(metal.pipeline.num_set_stencil_reference_value, 1);
            [sg.mtl.render_cmd_encoder setDepthBias:pip.cmn.depth.bias slopeScale:pip.cmn.depth.bias_slope_scale clamp:pip.cmn.depth.bias_clamp];
            _sg_stats_add(metal.pipeline.num_set_depth_bias, 1);
            @debug_assert(pip.mtl.rps != _SG_MTL_INVALID_SLOT_INDEX);
            [sg.mtl.render_cmd_encoder setRenderPipelineState:_sg_mtl_id(pip.mtl.rps)];
            _sg_stats_add(metal.pipeline.num_set_render_pipeline_state, 1);
            @debug_assert(pip.mtl.dss != _SG_MTL_INVALID_SLOT_INDEX);
            [sg.mtl.render_cmd_encoder setDepthStencilState:_sg_mtl_id(pip.mtl.dss)];
            _sg_stats_add(metal.pipeline.num_set_depth_stencil_state, 1);
        }
    }
}

fn _sg_mtl_apply_bindings(_sg_bindings_t* bnd) bool = {
    @debug_assert(bnd);
    @debug_assert(bnd.pip);
    @debug_assert(bnd.pip && bnd.pip.shader);
    @debug_assert(bnd.pip.shader.slot.id == bnd.pip.cmn.shader_id.id);
    shd, sc := (bnd.pip.shader, sg.mtl.state_cache&);

    // don't set vertex- and index-buffers in compute passes
    if (!sg.cur_pass.is_compute) {
        @debug_assert(nil != sg.mtl.render_cmd_encoder);
        // store index buffer binding, this will be needed later in sg_draw()
        sc.cur_indexbuffer = bnd.ib;
        sc.cur_indexbuffer_offset = bnd.ib_offset;
        sc.cur_indexbuffer_id.id = if (bnd.ib) {
            @debug_assert(bnd.pip.cmn.index_type != SG_INDEXTYPE_NONE);
            bnd.ib.slot.id
        } else {
            @debug_assert(bnd.pip.cmn.index_type == SG_INDEXTYPE_NONE);
            SG_INVALID_ID
        };
        // apply vertex buffers
        range(0, SG_MAX_VERTEXBUFFER_BINDSLOTS) { i |
            continue :: local_return;
            const _sg_buffer_t* vb = bnd.vbs[i];
            if(vb == 0, => continue());
            mtl_slot := _sg_mtl_vertexbuffer_bindslot(i);
            @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_BUFFER_BINDINGS);
            vb_offset := bnd.vb_offsets[i];
            if ((sc.cur_vs_buffer_ids[mtl_slot].id != vb.slot.id) ||
                (sc.cur_vs_buffer_offsets[mtl_slot] != vb_offset))
            {
                sc.cur_vs_buffer_offsets[mtl_slot] = vb_offset;
                if (sc.cur_vs_buffer_ids[mtl_slot].id != vb.slot.id) {
                    // vertex buffer has changed
                    sc.cur_vs_buffer_ids[mtl_slot].id = vb.slot.id;
                    @debug_assert(vb.mtl.buf[vb.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
                    [sg.mtl.render_cmd_encoder setVertexBuffer:_sg_mtl_id(vb.mtl.buf[vb.cmn.active_slot])
                        offset:(NSUInteger)vb_offset
                        atIndex:mtl_slot];
                } else {
                    // only vertex buffer offset has changed
                    [sg.mtl.render_cmd_encoder setVertexBufferOffset:(NSUInteger)vb_offset atIndex:mtl_slot];
                }
                _sg_stats_add(metal.bindings.num_set_vertex_buffer, 1);
            }
        }
    }

    // apply image bindings
    range(0, SG_MAX_IMAGE_BINDSLOTS) { i |
        continue :: local_return;
        img := bnd.imgs[i];
        if (img == 0, => continue());
        @debug_assert(img.mtl.tex[img.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        stage := shd.cmn.images[i].stage;
        @debug_assert((stage == SG_SHADERSTAGE_VERTEX) || (stage == SG_SHADERSTAGE_FRAGMENT) || (stage == SG_SHADERSTAGE_COMPUTE));
        mtl_slot := shd.mtl.img_texture_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_IMAGE_BINDINGS);
        
        enc, ids, stats := @match(stage) {
            fn VERTEX()   => (sg.mtl.render_cmd_encoder, sc.cur_vs_image_ids&, metal.bindings.num_set_vertex_texture&);
            fn FRAGMENT() => (sg.mtl.render_cmd_encoder, sc.cur_fs_image_ids&, metal.bindings.num_set_fragment_texture&);
            fn COMPUTE()  => (sg.mtl.compute_cmd_encoder, sc.cur_cs_image_ids&, metal.bindings.num_set_compute_texture&);
        };
        @debug_assert(nil != enc);
        if ids[mtl_slot].id != img.slot.id {
            ids[mtl_slot].id = img.slot.id;
            id := _sg_mtl_id(img.mtl.tex[img.cmn.active_slot]);
            @match(stage) {
                fn VERTEX()   => @objc enc.setVertexTexture(id, atIndex = mtl_slot);
                fn FRAGMENT() => @objc enc.setFragmentTexture(id, atIndex = mtl_slot);
                fn COMPUTE()  => @objc enc.setTexture(id, atIndex = mtl_slot);
            };
            stats[] += 1;
        }
    }

    // apply sampler bindings
    range(0, SG_MAX_SAMPLER_BINDSLOTS) { i |
        continue :: local_return;
        const _sg_sampler_t* smp = bnd.smps[i];
        if (smp == 0, => continue());
        @debug_assert(smp.mtl.sampler_state != _SG_MTL_INVALID_SLOT_INDEX);
        const sg_shader_stage stage = shd.cmn.samplers[i].stage;
        @debug_assert((stage == SG_SHADERSTAGE_VERTEX) || (stage == SG_SHADERSTAGE_FRAGMENT) || (stage == SG_SHADERSTAGE_COMPUTE));
        const NSUInteger mtl_slot = shd.mtl.smp_sampler_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_SAMPLER_BINDINGS);
        if (stage == SG_SHADERSTAGE_VERTEX) {
            @debug_assert(nil != sg.mtl.render_cmd_encoder);
            if (sc.cur_vs_sampler_ids[mtl_slot].id != smp.slot.id) {
                sc.cur_vs_sampler_ids[mtl_slot].id = smp.slot.id;
                [sg.mtl.render_cmd_encoder setVertexSamplerState:_sg_mtl_id(smp.mtl.sampler_state) atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_vertex_sampler_state, 1);
            }
        } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
            @debug_assert(nil != sg.mtl.render_cmd_encoder);
            if (sc.cur_fs_sampler_ids[mtl_slot].id != smp.slot.id) {
                sc.cur_fs_sampler_ids[mtl_slot].id = smp.slot.id;
                [sg.mtl.render_cmd_encoder setFragmentSamplerState:_sg_mtl_id(smp.mtl.sampler_state) atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_fragment_sampler_state, 1);
            }
        } else if (stage == SG_SHADERSTAGE_COMPUTE) {
            @debug_assert(nil != sg.mtl.compute_cmd_encoder);
            if (sc.cur_cs_sampler_ids[mtl_slot].id != smp.slot.id) {
                sc.cur_cs_sampler_ids[mtl_slot].id = smp.slot.id;
                [sg.mtl.compute_cmd_encoder setSamplerState:_sg_mtl_id(smp.mtl.sampler_state) atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_compute_sampler_state, 1);
            }
        }
    }

    // apply storage buffer bindings
    range(0, SG_MAX_STORAGEBUFFER_BINDSLOTS) { i |
        const _sg_buffer_t* sbuf = bnd.sbufs[i];
        if (sbuf == 0) {
            continue;
        }
        @debug_assert(sbuf.mtl.buf[sbuf.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        const sg_shader_stage stage = shd.cmn.storage_buffers[i].stage;
        @debug_assert((stage == SG_SHADERSTAGE_VERTEX) || (stage == SG_SHADERSTAGE_FRAGMENT) || (stage == SG_SHADERSTAGE_COMPUTE));
        const NSUInteger mtl_slot = shd.mtl.sbuf_buffer_n[i];
        @debug_assert(mtl_slot < _SG_MTL_MAX_STAGE_UB_SBUF_BINDINGS);
        if (stage == SG_SHADERSTAGE_VERTEX) {
            @debug_assert(nil != sg.mtl.render_cmd_encoder);
            if (sc.cur_vs_buffer_ids[mtl_slot].id != sbuf.slot.id) {
                sc.cur_vs_buffer_ids[mtl_slot].id = sbuf.slot.id;
                [sg.mtl.render_cmd_encoder setVertexBuffer:_sg_mtl_id(sbuf.mtl.buf[sbuf.cmn.active_slot]) offset:0 atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_vertex_buffer, 1);
            }
        } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
            @debug_assert(nil != sg.mtl.render_cmd_encoder);
            if (sc.cur_fs_buffer_ids[mtl_slot].id != sbuf.slot.id) {
                sc.cur_fs_buffer_ids[mtl_slot].id = sbuf.slot.id;
                [sg.mtl.render_cmd_encoder setFragmentBuffer:_sg_mtl_id(sbuf.mtl.buf[sbuf.cmn.active_slot]) offset:0 atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_fragment_buffer, 1);
            }
        } else if (stage == SG_SHADERSTAGE_COMPUTE) {
            @debug_assert(nil != sg.mtl.compute_cmd_encoder);
            if (sc.cur_cs_buffer_ids[mtl_slot].id != sbuf.slot.id) {
                sc.cur_cs_buffer_ids[mtl_slot].id = sbuf.slot.id;
                [sg.mtl.compute_cmd_encoder setBuffer:_sg_mtl_id(sbuf.mtl.buf[sbuf.cmn.active_slot]) offset:0 atIndex:mtl_slot];
                _sg_stats_add(metal.bindings.num_set_compute_buffer, 1);
            }
        }
    }
    return true;
}

fn _sg_mtl_apply_uniforms(int ub_slot, const sg_range* data) {
    @debug_assert((ub_slot >= 0) && (ub_slot < SG_MAX_UNIFORMBLOCK_BINDSLOTS));
    @debug_assert(((size_t)sg.mtl.cur_ub_offset + data.size) <= (size_t)sg.mtl.ub_size);
    @debug_assert((sg.mtl.cur_ub_offset & (_SG_MTL_UB_ALIGN-1)) == 0);
    pip := sg.mtl.state_cache.cur_pipeline;
    @debug_assert(pip && pip.shader);
    @debug_assert(pip.slot.id == sg.mtl.state_cache.cur_pipeline_id.id);
    shd := pip.shader;
    @debug_assert(shd.slot.id == pip.cmn.shader_id.id);
    @debug_assert(data.size == shd.cmn.uniform_blocks[ub_slot].size);

    stage := shd.cmn.uniform_blocks[ub_slot].stage;
    mtl_slot := shd.mtl.ub_buffer_n[ub_slot];

    // copy to global uniform buffer, record offset into cmd encoder, and advance offset
    uint8_t* dst = &sg.mtl.cur_ub_base_ptr[sg.mtl.cur_ub_offset];
    memcpy(dst, data.ptr, data.size);
    if (stage == SG_SHADERSTAGE_VERTEX) {
        @debug_assert(nil != sg.mtl.render_cmd_encoder);
        [sg.mtl.render_cmd_encoder setVertexBufferOffset:(NSUInteger)sg.mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_vertex_buffer_offset, 1);
    } else if (stage == SG_SHADERSTAGE_FRAGMENT) {
        @debug_assert(nil != sg.mtl.render_cmd_encoder);
        [sg.mtl.render_cmd_encoder setFragmentBufferOffset:(NSUInteger)sg.mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_fragment_buffer_offset, 1);
    } else if (stage == SG_SHADERSTAGE_COMPUTE) {
        @debug_assert(nil != sg.mtl.compute_cmd_encoder);
        [sg.mtl.compute_cmd_encoder setBufferOffset:(NSUInteger)sg.mtl.cur_ub_offset atIndex:mtl_slot];
        _sg_stats_add(metal.uniforms.num_set_compute_buffer_offset, 1);
    } else {
        SOKOL_UNREACHABLE;
    }
    sg.mtl.cur_ub_offset = _sg_roundup(sg.mtl.cur_ub_offset + (int)data.size, _SG_MTL_UB_ALIGN);
}

fn _sg_mtl_draw(int base_element, int num_elements, int num_instances) {
    sc := sg.mtl.state_cache&;
    @debug_assert(nil != sg.mtl.render_cmd_encoder);
    @debug_assert(sc.cur_pipeline && (sc.cur_pipeline.slot.id == sc.cur_pipeline_id.id));
    if (SG_INDEXTYPE_NONE != sc.cur_pipeline.cmn.index_type) {
        // indexed rendering
        @debug_assert(sc.cur_indexbuffer && (sc.cur_indexbuffer.slot.id == sc.cur_indexbuffer_id.id));
        const _sg_buffer_t* ib = sc.cur_indexbuffer;
        @debug_assert(ib.mtl.buf[ib.cmn.active_slot] != _SG_MTL_INVALID_SLOT_INDEX);
        const NSUInteger index_buffer_offset = (NSUInteger) (sc.cur_indexbuffer_offset + base_element * sc.cur_pipeline.mtl.index_size);
        [sg.mtl.render_cmd_encoder drawIndexedPrimitives:sc.cur_pipeline.mtl.prim_type
            indexCount:(NSUInteger)num_elements
            indexType:sc.cur_pipeline.mtl.index_type
            indexBuffer:_sg_mtl_id(ib.mtl.buf[ib.cmn.active_slot])
            indexBufferOffset:index_buffer_offset
            instanceCount:(NSUInteger)num_instances];
    } else {
        // non-indexed rendering
        [sg.mtl.render_cmd_encoder drawPrimitives:sc.cur_pipeline.mtl.prim_type
            vertexStart:(NSUInteger)base_element
            vertexCount:(NSUInteger)num_elements
            instanceCount:(NSUInteger)num_instances];
    }
}

fn _sg_mtl_dispatch(int num_groups_x, int num_groups_y, int num_groups_z) {
    @debug_assert(nil != sg.mtl.compute_cmd_encoder);
    @debug_assert(sg.mtl.state_cache.cur_pipeline && (sg.mtl.state_cache.cur_pipeline.slot.id == sg.mtl.state_cache.cur_pipeline_id.id));
    const _sg_pipeline_t* cur_pip = sg.mtl.state_cache.cur_pipeline;
    thread_groups: MTLSize = (
        width = (NSUInteger)num_groups_x,
        height = (NSUInteger)num_groups_y,
        depth = (NSUInteger)num_groups_z,
    );
    threads_per_threadgroup := cur_pip.mtl.threads_per_threadgroup;
    [sg.mtl.compute_cmd_encoder dispatchThreadgroups:thread_groups threadsPerThreadgroup:threads_per_threadgroup];
}

NSRange :: @struct(location: u64, length: u64);

fn inc_active_slot(it: ~T) void #where = {
    it.cmn.active_slot += 1;
    if (it.cmn.active_slot >= it.cmn.num_slots) {
        it.cmn.active_slot = 0;
    }
}

fn _sg_mtl_update_buffer(_sg_buffer_t* buf, const sg_range* data) {
    @debug_assert(buf && data && data.ptr && (data.size > 0));
    inc_active_slot(buf);
    __unsafe_unretained id<MTLBuffer> mtl_buf = _sg_mtl_id(buf.mtl.buf[buf.cmn.active_slot]);
    dst_ptr := @objc @as(rawptr) mtl_buf.contents();
    memcpy(dst_ptr, data.ptr, data.size);
    ASSERT_NOT_IOS();
    if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
        [mtl_buf didModifyRange:NSMakeRange(0, data.size)];
    }
}

fn _sg_mtl_append_buffer(_sg_buffer_t* buf, const sg_range* data, bool new_frame) {
    @debug_assert(buf && data && data.ptr && (data.size > 0));
    if new_frame {
        inc_active_slot(buf);
    }
    __unsafe_unretained id<MTLBuffer> mtl_buf = _sg_mtl_id(buf.mtl.buf[buf.cmn.active_slot]);
    uint8_t* dst_ptr = (uint8_t*) [mtl_buf contents];
    dst_ptr += buf.cmn.append_pos;
    memcpy(dst_ptr, data.ptr, data.size);
    ASSERT_NOT_IOS();
    if (_sg_mtl_resource_options_storage_mode_managed_or_shared() == MTLResourceStorageModeManaged) {
        r: NSRange = (location = (NSUInteger)buf.cmn.append_pos, length = (NSUInteger)data.size);
        @objc mtl_buf.didModifyRange(r);
    }
}

fn _sg_mtl_update_image(_sg_image_t* img, const sg_image_data* data) {
    @debug_assert(img && data);
    inc_active_slot(img);
    __unsafe_unretained id<MTLTexture> mtl_tex = _sg_mtl_id(img.mtl.tex[img.cmn.active_slot]);
    _sg_mtl_copy_image_data(img, mtl_tex, data);
}

fn _sg_mtl_push_debug_group((sg: *Sg.Self, name: CStr) void = {
    name := @objc NSString.stringWithUTF8String(name);
    @objc sg.encoder().pushDebugGroup(name);
}

fn _sg_mtl_pop_debug_group((sg: *Sg.Self) void =
    @objc sg.encoder().popDebugGroup(name);

fn encoder(sg: *Sg.Self) ObjCId = {
    enc := sg.mtl.render_cmd_encoder;
    @if(enc.is_nil(), sg.mtl.compute_cmd_encoder, enc)
}

fn mtl_device(sg: *Sg.Self) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, sg.mtl.device)
}

fn mtl_render_command_encoder(sg: *Sg.Self) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, sg.mtl.render_cmd_encoder)
}

fn sg_mtl_compute_command_encoder(sg: *Sg.Self) rawptr = {
    ASSERT_METAL();
    bit_cast_unchecked(ObjCId, rawptr, sg.mtl.compute_cmd_encoder)
}

fn sg_mtl_buffer_info sg_mtl_query_buffer_info(sg_buffer buf_id) {
    @debug_assert(sg.valid);
    res := zeroed sg_mtl_buffer_info;
    ASSERT_METAL();
    buf := sg.lookup(buf_id) || return(res);
    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        if (buf.mtl.buf[i] != 0) {
            res.buf[i] = (__bridge void*) _sg_mtl_id(buf.mtl.buf[i]);
        }
    }
    res.active_slot = buf.cmn.active_slot;
    res
}

fn sg_mtl_image_info sg_mtl_query_image_info(sg_image img_id) {
    @debug_assert(sg.valid);
    res := zeroed sg_mtl_image_info;
    ASSERT_METAL();
    img := sg.lookup(img_id) || return(res);
    for (int i = 0; i < SG_NUM_INFLIGHT_FRAMES; i++) {
        if (img.mtl.tex[i] != 0) {
            res.tex[i] = (__bridge void*) _sg_mtl_id(img.mtl.tex[i]);
        }
    }
    res.active_slot = img.cmn.active_slot;
    res
}

fn sg_mtl_query_sampler_info(id: Sg.Sampler) sg_mtl_sampler_info = {
    @debug_assert(sg.valid);
    res := zereod sg_mtl_sampler_info;
    ASSERT_METAL();
    smp := sg.lookup(id) || return(res);
    if (smp.mtl.sampler_state != 0) {
        res.smp = (__bridge void*) _sg_mtl_id(smp.mtl.sampler_state);
    }
    res
}

fn sg_mtl_shader_info sg_mtl_query_shader_info(sg_shader shd_id) {
    @debug_assert(sg.valid);
    sg_mtl_shader_info res;
    _sg_clear(&res, sizeof(res));
    ASSERT_METAL();
    shd := _sg_lookup_shader(&sg.pools, shd_id.id) || return(res);
    const int vertex_lib  = shd.mtl.vertex_func.mtl_lib;
    const int vertex_func = shd.mtl.vertex_func.mtl_func;
    const int fragment_lib  = shd.mtl.fragment_func.mtl_lib;
    const int fragment_func = shd.mtl.fragment_func.mtl_func;
    if (vertex_lib != 0) {
        res.vertex_lib = (__bridge void*) _sg_mtl_id(vertex_lib);
    }
    if (fragment_lib != 0) {
        res.fragment_lib = (__bridge void*) _sg_mtl_id(fragment_lib);
    }
    if (vertex_func != 0) {
        res.vertex_func = (__bridge void*) _sg_mtl_id(vertex_func);
    }
    if (fragment_func != 0) {
        res.fragment_func = (__bridge void*) _sg_mtl_id(fragment_func);
    }
    res
}

fn sg_mtl_pipeline_info sg_mtl_query_pipeline_info(sg_pipeline pip_id) {
    @debug_assert(sg.valid);
    res := zeroed sg_mtl_pipeline_info;
    ASSERT_METAL();
    pip := sg.lookup(id) || return(res);
    if (pip.mtl.rps != 0) {
        res.rps = (__bridge void*) _sg_mtl_id(pip.mtl.rps);
    }
    if (pip.mtl.dss != 0) {
        res.dss = (__bridge void*) _sg_mtl_id(pip.mtl.dss);
    }
    res
}
